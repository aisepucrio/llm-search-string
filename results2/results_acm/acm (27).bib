@inproceedings{10.1145/3387940.3391463,
author = {Omri, Safa and Sinz, Carsten},
title = {Deep Learning for Software Defect Prediction: A Survey},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391463},
doi = {10.1145/3387940.3391463},
abstract = {Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {209–214},
numpages = {6},
keywords = {deep learning, machine learning, software defect prediction, software quality assurance, software testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1016/j.neucom.2019.11.067,
author = {Qiao, Lei and Li, Xuesong and Umer, Qasim and Guo, Ping},
title = {Deep learning based software defect prediction},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {385},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.067},
doi = {10.1016/j.neucom.2019.11.067},
journal = {Neurocomput.},
month = apr,
pages = {100–110},
numpages = {11},
keywords = {Software defect prediction, Deep learning, Software quality, Software metrics, Robustness evaluation}
}

@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Software defect prediction, Metaheuristic feature selection, Whale optimization algorithm, Convolutional neural network, Kernel extreme learning machine}
}

@article{10.1007/s00500-021-06096-3,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {An empirical study toward dealing with noise and class imbalance issues in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06096-3},
doi = {10.1007/s00500-021-06096-3},
abstract = {The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models.},
journal = {Soft Comput.},
month = nov,
pages = {13465–13492},
numpages = {28},
keywords = {Software testing, Software fault prediction, Class imbalance, Noisy instance, Machine learning, Software metrics, Fault proneness}
}

@article{10.1007/s10515-020-00277-4,
author = {Esteves, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Understanding machine learning software defect predictions},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00277-4},
doi = {10.1007/s10515-020-00277-4},
abstract = {Software defects are well-known in software development and might cause several problems for users and developers aside. As a result, researches employed distinct techniques to mitigate the impacts of these defects in the source code. One of the most notable techniques focuses on defect prediction using machine learning methods, which could support developers in handling these defects before they are introduced in the production environment. These studies provide alternative approaches to predict the likelihood of defects. However, most of these works concentrate on predicting defects from a vast set of software features. Another key issue with the current literature is the lack of a satisfactory explanation of the reasons that drive the software to a defective state. Specifically, we use a tree boosting algorithm (XGBoost) that receives as input a training set comprising records of easy-to-compute characteristics of each module and outputs whether the corresponding module is defect-prone. To exploit the link between predictive power and model explainability, we propose a simple model sampling approach that finds accurate models with the minimum set of features. Our principal idea is that features not contributing to increasing the predictive power should not be included in the model. Interestingly, the reduced set of features helps to increase model explainability, which is important to provide information to developers on features related to each module of the code which is more defect-prone. We evaluate our models on diverse projects within Jureczko datasets, and we show that (i) features that contribute most for finding best models may vary depending on the project and (ii) it is possible to find effective models that use few features leading to better understandability. We believe our results are useful to developers as we provide the specific software features that influence the defectiveness of selected projects.},
journal = {Automated Software Engg.},
month = dec,
pages = {369–392},
numpages = {24},
keywords = {Software defects, Explainable models, Jureczko datasets, SHAP values}
}

@article{10.1016/j.asoc.2016.06.023,
author = {Mesquita, Diego P.P. and Rocha, Lincoln S. and Gomes, Joo Paulo P. and Rocha Neto, Ajalmar R.},
title = {Classification with reject option for software defect prediction},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.06.023},
doi = {10.1016/j.asoc.2016.06.023},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose the use of classification with reject option for software defect prediction (SDP) as a way to incorporate additional knowledge in the SDP process.We propose two variants of the extreme learning machine with reject option.It is proposed an ELM with reject option for imbalanced datasets.The proposed method is tested on five real world software datasets.An example is shown to illustrate how the rejected software modules can be further analyzed to improve the final SDP accuracy. ContextSoftware defect prediction (SDP) is an important task in software engineering. Along with estimating the number of defects remaining in software systems and discovering defect associations, classifying the defect-proneness of software modules plays an important role in software defect prediction. Several machine-learning methods have been applied to handle the defect-proneness of software modules as a classification problem. This type of yes or no decision is an important drawback in the decision-making process and if not precise may lead to misclassifications. To the best of our knowledge, existing approaches rely on fully automated module classification and do not provide a way to incorporate extra knowledge during the classification process. This knowledge can be helpful in avoiding misclassifications in cases where system modules cannot be classified in a reliable way. ObjectiveWe seek to develop a SDP method that (i) incorporates a reject option in the classifier to improve the reliability in the decision-making process; and (ii) makes it possible postpone the final decision related to rejected modules for an expert analysis or even for another classifier using extra domain knowledge. MethodWe develop a SDP method called rejoELM and its variant, IrejoELM. Both methods were built upon the weighted extreme learning machine (ELM) with reject option that makes it possible postpone the final decision of non-classified modules, the rejected ones, to another moment. While rejoELM aims to maximize the accuracy for a rejection rate, IrejoELM maximizes the F-measure. Hence, IrejoELM becomes an alternative for classification with reject option for imbalanced datasets. ResultsrejoEM and IrejoELM are tested on five datasets of source code metrics extracted from real world open-source software projects. Results indicate that rejoELM has an accuracy for several rejection rates that is comparable to some state-of-the-art classifiers with reject option. Although IrejoELM shows lower accuracies for several rejection rates, it clearly outperforms all other methods when the F-measure is used as a performance metric. ConclusionIt is concluded that rejoELM is a valid alternative for classification with reject option problems when classes are nearly equally represented. On the other hand, IrejoELM is shown to be the best alternative for classification with reject option on imbalanced datasets. Since SDP problems are usually characterized as imbalanced learning problems, the use of IrejoELM is recommended.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1085–1093},
numpages = {9},
keywords = {Classification with reject option, Extreme learning machines, Software defect prediction}
}

@article{10.1007/s11227-019-03051-w,
author = {NezhadShokouhi, Mohammad Mahdi and Majidi, Mohammad Ali and Rasoolzadegan, Abbas},
title = {Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03051-w},
doi = {10.1007/s11227-019-03051-w},
abstract = {As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods.},
journal = {J. Supercomput.},
month = jan,
pages = {602–635},
numpages = {34},
keywords = {Software defect prediction, Software metrics, Mahalanobis distance, Over-sampling, Feature extraction}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Artificial Neural Networks, Automated Parameter Tuning, Metrics, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on evaluation experiments using different machine learning techniques for defect prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1016/j.procs.2015.02.161,
author = {Arora, Ishani and Tetarwal, Vivek and Saha, Anju},
title = {Open Issues in Software Defect Prediction},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.161},
doi = {10.1016/j.procs.2015.02.161},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {906–912},
numpages = {7},
keywords = {data mining, defect prediction, machine learning, software quality, software testing}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics, Systematic mapping study}
}

@article{10.1007/s00500-021-06254-7,
author = {Sotto-Mayor, Bruno and Kalech, Meir},
title = {Cross-project smell-based defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06254-7},
doi = {10.1007/s00500-021-06254-7},
abstract = {Defect prediction is a technique introduced to optimize the testing phase of the software development pipeline by predicting which components in the software may contain defects. Its methodology trains a classifier with data regarding a set of features measured on each component from the target software project to predict whether the component may be defective or not. However, suppose the defective information is not available in the training set. In that case, we need to rely on an alternate approach that uses the training set of external projects to train the classifier. This approached is called cross-project defect prediction. Bad code smells are a category of features that have been previously explored in defect prediction and have been shown to be a good predictor of defects. Code smells are patterns of poor development in the code and indicate flaws in its design and implementation. Although they have been previously studied in the context of defect prediction, they have not been studied as features for cross-project defect prediction. In our experiment, we train defect prediction models for 100 projects to evaluate the predictive performance of the bad code smells. We implemented four cross-project approaches known in the literature and compared the performance of 37 smells with 56 code metrics, commonly used for defect prediction. The results show that the cross-project defect prediction models trained with code smells significantly improved 6.50% on the ROC AUC compared against the code metrics.},
journal = {Soft Comput.},
month = nov,
pages = {14171–14181},
numpages = {11},
keywords = {Cross-project defect prediction, Defect prediction, Code smell, Mining software repositories, Software quality, Software engineering}
}

@article{10.1155/2021/5069016,
author = {Balogun, Abdullateef O. and Basri, Shuib and Mahamad, Saipunidzam and Capretz, Luiz Fernando and Imam, Abdullahi Abubakar and Almomani, Malek A. and Adeyemo, Victor E. and Kumar, Ganesh and Dourado, Ant\'{o}nio},
title = {A Novel Rank Aggregation-Based Hybrid Multifilter Wrapper Feature Selection Method in Software Defect Prediction},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/5069016},
doi = {10.1155/2021/5069016},
abstract = {The high dimensionality of software metric features has long been noted as a data quality problem that affects the performance of software defect prediction (SDP) models. This drawback makes it necessary to apply feature selection (FS) algorithm(s) in SDP processes. FS approaches can be categorized into three types, namely, filter FS (FFS), wrapper FS (WFS), and hybrid FS (HFS). HFS has been established as superior because it combines the strength of both FFS and WFS methods. However, selecting the most appropriate FFS (filter rank selection problem) for HFS is a challenge because the performance of FFS methods depends on the choice of datasets and classifiers. In addition, the local optima stagnation and high computational costs of WFS due to large search spaces are inherited by the HFS method. Therefore, as a solution, this study proposes a novel rank aggregation-based hybrid multifilter wrapper feature selection (RAHMFWFS) method for the selection of relevant and irredundant features from software defect datasets. The proposed RAHMFWFS is divided into two stepwise stages. The first stage involves a rank aggregation-based multifilter feature selection (RMFFS) method that addresses the filter rank selection problem by aggregating individual rank lists from multiple filter methods, using a novel rank aggregation method to generate a single, robust, and non-disjoint rank list. In the second stage, the aggregated ranked features are further preprocessed by an enhanced wrapper feature selection (EWFS) method based on a dynamic reranking strategy that is used to guide the feature subset selection process of the HFS method. This, in turn, reduces the number of evaluation cycles while amplifying or maintaining its prediction performance. The feasibility of the proposed RAHMFWFS was demonstrated on benchmarked software defect datasets with Na\"{\i}ve Bayes and Decision Tree classifiers, based on accuracy, the area under the curve (AUC), and F-measure values. The experimental results showed the effectiveness of RAHMFWFS in addressing filter rank selection and local optima stagnation problems in HFS, as well as the ability to select optimal features from SDP datasets while maintaining or enhancing the performance of SDP models. To conclude, the proposed RAHMFWFS achieved good performance by improving the prediction performances of SDP models across the selected datasets, compared to existing state-of-the-arts HFS methods.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@article{10.1016/j.asoc.2017.01.050,
author = {Maua, Goran and Galinac Grbac, Tihana},
title = {Co-evolutionary multi-population genetic programming for classification in software defect prediction},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.01.050},
doi = {10.1016/j.asoc.2017.01.050},
abstract = {Evolving diverse ensembles using genetic programming has recently been proposed for classification problems with unbalanced data. Population diversity is crucial for evolving effective algorithms. Multilevel selection strategies that involve additional colonization and migration operations have shown better performance in some applications. Therefore, in this paper, we are interested in analysing the performance of evolving diverse ensembles using genetic programming for software defect prediction with unbalanced data by using different selection strategies. We use colonization and migration operators along with three ensemble selection strategies for the multi-objective evolutionary algorithm. We compare the performance of the operators for software defect prediction datasets with varying levels of data imbalance. Moreover, to generalize the results, gain a broader view and understand the underlying effects, we replicated the same experiments on UCI datasets, which are often used in the evolutionary computing community. The use of multilevel selection strategies provides reliable results with relatively fast convergence speeds and outperforms the other evolutionary algorithms that are often used in this research area and investigated in this paper. This paper also presented a promising ensemble strategy based on a simple convex hull approach and at the same time it raised the question whether ensemble strategy based on the whole population should also be investigated.},
journal = {Appl. Soft Comput.},
month = jun,
pages = {331–351},
numpages = {21},
keywords = {Classification, Coevolution, Genetic programming, Software defect prediction}
}

@article{10.1007/s10664-021-09984-2,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Weighted software metrics aggregation and its application to defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09984-2},
doi = {10.1007/s10664-021-09984-2},
abstract = {It is a well-known practice in software engineering to aggregate software metrics to assess software artifacts for various purposes, such as their maintainability or their proneness to contain bugs. For different purposes, different metrics might be relevant. However, weighting these software metrics according to their contribution to the respective purpose is a challenging task. Manual approaches based on experts do not scale with the number of metrics. Also, experts get confused if the metrics are not independent, which is rarely the case. Automated approaches based on supervised learning require reliable and generalizable training data, a ground truth, which is rarely available. We propose an automated approach to weighted metrics aggregation that is based on unsupervised learning. It sets metrics scores and their weights based on probability theory and aggregates them. To evaluate the effectiveness, we conducted two empirical studies on defect prediction, one on ca. 200 000 code changes, and another ca. 5 000 software classes. The results show that our approach can be used as an agnostic unsupervised predictor in the absence of a ground truth.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {34},
keywords = {Software assessment, Quantitative methods, Defect prediction, Software metrics, Aggregation, Weighting}
}

@article{10.1016/j.jss.2016.09.001,
author = {Andreou, Andreas S. and Chatzis, Sotirios P.},
title = {Software defect prediction using doubly stochastic Poisson processes driven by stochastic belief networks},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.001},
doi = {10.1016/j.jss.2016.09.001},
abstract = {This research aims at better addressing the challenges related with software defect prediction.We develop a novel Bayesian inference approach driven from appropriate metrics.Formulation of our method is based on a doubly stochastic homogeneous Poisson process.Our model better learns from data with multiple modes in their distributions.We evaluate generalization across software classes, subsequent releases, and projects. Accurate prediction of software defects is of crucial importance in software engineering. Software defect prediction comprises two major procedures: (i) Design of appropriate software metrics to represent characteristic software system properties; and (ii) development of effective regression models for count data, allowing for accurate prediction of the number of software defects. Although significant research effort has been devoted to software metrics design, research in count data regression has been rather limited. More specifically, most used methods have not been explicitly designed to tackle the problem of metrics-driven software defect counts prediction, thus postulating irrelevant assumptions, such as (log-)linearity of the modeled data. In addition, a lack of simple and efficient algorithms for posterior computation has made more elaborate hierarchical Bayesian approaches appear unattractive in the context of software defect prediction. To address these issues, in this paper we introduce a doubly stochastic Poisson process for count data regression, the failure log-rate of which is driven by a novel latent space stochastic feedforward neural network. Our approach yields simple and efficient updates for its complicated conditional distributions by means of sampling importance resampling and error backpropagation. We exhibit the efficacy of our approach using publicly available and benchmark datasets.},
journal = {J. Syst. Softw.},
month = dec,
pages = {72–82},
numpages = {11},
keywords = {Doubly stochastic Poisson process, Sampling importance resampling, Software defect prediction, Stochastic belief network}
}

@inproceedings{10.1145/2961111.2962620,
author = {Shippey, Thomas and Hall, Tracy and Counsell, Steve and Bowes, David},
title = {So You Need More Method Level Datasets for Your Software Defect Prediction? Voil\`{a}!},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962620},
doi = {10.1145/2961111.2962620},
abstract = {Context: Defect prediction research is based on a small number of defect datasets and most are at class not method level. Consequently our knowledge of defects is limited. Identifying defect datasets for prediction is not easy and extracting quality data from identified datasets is even more difficult. Goal: Identify open source Java systems suitable for defect prediction and extract high quality fault data from these datasets. Method: We used the Boa to identify candidate open source systems. We reduce 50,000 potential candidates down to 23 suitable for defect prediction using a selection criteria based on the system's software repository and its defect tracking system. We use an enhanced SZZ algorithm to extract fault information and calculate metrics using JHawk. Result: We have produced 138 fault and metrics datasets for the 23 identified systems. We make these datasets (the ELFF datasets) and our data extraction tools freely available to future researchers. Conclusions: The data we provide enables future studies to proceed with minimal effort. Our datasets significantly increase the pool of systems currently being used in defect analysis studies.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {12},
numpages = {6},
keywords = {Boa, Data Mining, Defect Prediction, Defect linking, Defects},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@article{10.1016/j.eswa.2019.113156,
author = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Poorsarvi-Tehrani, Pooria and Haghighi, Hassan},
title = {SLDeep: Statement-level software defect prediction using deep-learning model on static code features},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {147},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113156},
doi = {10.1016/j.eswa.2019.113156},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Defect, Software fault proneness, Machine learning, Fault prediction model, Software metric}
}

@inproceedings{10.1145/3412841.3442019,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Tang, Yutian and Fan, Ming and Catolino, Gemma},
title = {Just-in-time defect prediction for Android apps via imbalanced deep learning model},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442019},
doi = {10.1145/3412841.3442019},
abstract = {Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class imbalance issue by taking the prior probability of the two types of classes into account. We conduct experiments on a benchmark defect data consisting of 12 Android mobile apps. The results of rigorous experiments show that our proposed IDL model performs significantly better than 23 comparative imbalanced learning methods in terms of Matthews correlation coefficient performance indicator.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1447–1454},
numpages = {8},
keywords = {JIT defect prediction, imbalanced learning, mobile apps},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/2786805.2786814,
author = {Nam, Jaechang and Kim, Sunghun},
title = {Heterogeneous defect prediction},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786814},
doi = {10.1145/2786805.2786814},
abstract = {Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {508–519},
numpages = {12},
keywords = {Defect prediction, heterogeneous metrics, quality assurance},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1007/s10664-016-9468-y,
author = {Herbold, Steffen and Trautsch, Alexander and Grabowski, Jens},
title = {Global vs. local models for cross-project defect prediction},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9468-y},
doi = {10.1007/s10664-016-9468-y},
abstract = {Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1866–1902},
numpages = {37},
keywords = {Cross-project, Defect prediction, Local models}
}

@article{10.5555/3197793.3197817,
author = {Shukla, Swapnil and Radhakrishnan, T. and Muthukumaran, K. and Neti, Lalita Bhanu},
title = {Multi-objective cross-version defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {6},
issn = {1432-7643},
abstract = {Defect prediction models help software project teams to spot defect-prone source files of software systems. Software project teams can prioritize and put up rigorous quality assurance (QA) activities on these predicted defect-prone files to minimize post-release defects so that quality software can be delivered. Cross-version defect prediction is building a prediction model from the previous version of a software project to predict defects in the current version. This is more practical than the other two ways of building models, i.e., cross-project prediction model and cross- validation prediction models, as previous version of same software project will have similar parameter distribution among files. In this paper, we formulate cross-version defect prediction problem as a multi-objective optimization problem with two objective functions: (a) maximizing recall by minimizing misclassification cost and (b) maximizing recall by minimizing cost of QA activities on defect prone files. The two multi-objective defect prediction models are compared with four traditional machine learning algorithms, namely logistic regression, na\"{\i}ve Bayes, decision tree and random forest. We have used 11 projects from the PROMISE repository consisting of a total of 41 different versions of these projects. Our findings show that multi-objective logistic regression is more cost-effective than single-objective algorithms.},
journal = {Soft Comput.},
month = mar,
pages = {1959–1980},
numpages = {22},
keywords = {Cost-effectiveness, Cross-version defect prediction, Misclassification cost, Multi-objective optimization, Search-based software engineering}
}

@article{10.1007/s10515-019-00259-1,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Zhu, Xiaoke and Zhang, Hongyu and Xu, Baowen and Ying, Shi},
title = {Heterogeneous defect prediction with two-stage ensemble learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00259-1},
doi = {10.1007/s10515-019-00259-1},
abstract = {Heterogeneous defect prediction (HDP) refers to predicting defect-prone software modules in one project (target) using heterogeneous data collected from other projects (source). Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect data: (1) data could be linear inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Two-Stage Ensemble Learning (TSEL) approach to HDP, which contains two stages: ensemble multi-kernel domain adaptation (EMDA) stage and ensemble data sampling (EDS) stage. In the EMDA stage, we develop an Ensemble Multiple Kernel Correlation Alignment (EMKCA) predictor, which combines the advantage of multiple kernel learning and domain adaptation techniques. In the EDS stage, we employ RESample with replacement (RES) technique to learn multiple different EMKCA predictors and use average ensemble to combine them together. These two stages create an ensemble of defect predictors. Extensive experiments on 30 public projects show that the proposed TSEL approach outperforms a range of competing methods. The improvement is 20.14–33.92% in AUC, 36.05–54.78% in f-measure, and 5.48–19.93% in balance, respectively.},
journal = {Automated Software Engg.},
month = sep,
pages = {599–651},
numpages = {53},
keywords = {Heterogeneous defect prediction, Two-stage ensemble learning, Linear inseparability, Multiple kernel learning, Class imbalance, Data sampling, Domain adaptation}
}

@article{10.1007/s10515-017-0220-7,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Wu, Fei and Zhu, Xiaoke and Xu, Baowen and Ying, Shi},
title = {Cost-sensitive transfer kernel canonical correlation analysis for heterogeneous defect prediction},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0220-7},
doi = {10.1007/s10515-017-0220-7},
abstract = {Cross-project defect prediction (CPDP) refers to predicting defects in a target project using prediction models trained from historical data of other source projects. And CPDP in the scenario where source and target projects have different metric sets is called heterogeneous defect prediction (HDP). Recently, HDP has received much research interest. Existing HDP methods only consider the linear correlation relationship among the features (metrics) of the source and target projects, and such models are insufficient to evaluate nonlinear correlation relationship among the features. So these methods may suffer from the linearly inseparable problem in the linear feature space. Furthermore, existing HDP methods do not take the class imbalance problem into consideration. Unfortunately, the imbalanced nature of software defect datasets increases the learning difficulty for the predictors. In this paper, we propose a new cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) approach for HDP. CTKCCA can not only make the data distributions of source and target projects much more similar in the nonlinear feature space, where the learned features have favorable separability, but also utilize the different misclassification costs for defective and defect-free classes to alleviate the class imbalance problem. We perform the Friedman test with Nemenyi's post-hoc statistical test and the Cliff's delta effect size test for the evaluation. Extensive experiments on 28 public projects from five data sources indicate that: (1) CTKCCA significantly performs better than the related CPDP methods; (2) CTKCCA performs better than the related state-of-the-art HDP methods.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–245},
numpages = {45},
keywords = {Class imbalance, Cost-sensitive learning, Heterogeneous defect prediction, Kernel canonical correlation analysis, Transfer learning}
}

@article{10.1007/s10664-011-9173-9,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9173-9},
doi = {10.1007/s10664-011-9173-9},
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {531–577},
numpages = {47},
keywords = {Change metrics, Defect prediction, Source code metrics}
}

@inproceedings{10.1145/2786805.2804429,
author = {Kim, Mijung and Nam, Jaechang and Yeon, Jaehyuk and Choi, Soonhwang and Kim, Sunghun},
title = {REMI: defect prediction for efficient API testing},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2804429},
doi = {10.1145/2786805.2804429},
abstract = {Quality assurance for common APIs is important since the the reliability of APIs affects the quality of other systems using the APIs. Testing is a common practice to ensure the quality of APIs, but it is a challenging and laborious task especially for industrial projects. Due to a large number of APIs with tight time constraints and limited resources, it is hard to write enough test cases for all APIs. To address these challenges, we present a novel technique, REMI that predicts high risk APIs in terms of producing potential bugs. REMI allows developers to write more test cases for the high risk APIs. We evaluate REMI on a real-world industrial project, Tizen-wearable, and apply REMI to the API development process at Samsung Electronics. Our evaluation results show that REMI predicts the bug-prone APIs with reasonable accuracy (0.681 f-measure on average). The results also show that applying REMI to the Tizen-wearable development process increases the number of bugs detected, and reduces the resources required for executing test cases.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {990–993},
numpages = {4},
keywords = {API Testing, Defect Prediction, Quality Assurance},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2025113.2025156,
author = {Lee, Taek and Nam, Jaechang and Han, DongGyun and Kim, Sunghun and In, Hoh Peter},
title = {Micro interaction metrics for defect prediction},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025156},
doi = {10.1145/2025113.2025156},
abstract = {There is a common belief that developers' behavioral interaction patterns may affect software quality. However, widely used defect prediction metrics such as source code metrics, change churns, and the number of previous defects do not capture developers' direct interactions. We propose 56 novel micro interaction metrics (MIMs) that leverage developers' interaction information stored in the Mylyn data. Mylyn is an Eclipse plug-in, which captures developers' interactions such as file editing and selection events with time spent. To evaluate the performance of MIMs in defect prediction, we build defect prediction (classification and regression) models using MIMs, traditional metrics, and their combinations. Our experimental results show that MIMs significantly improve defect classification and regression accuracy.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {311–321},
numpages = {11},
keywords = {defect prediction, micro interaction metrics, mylyn},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1016/j.jss.2016.05.015,
author = {Chen, Tse-Hsun and Shang, Weiyi and Nagappan, Meiyappan and Hassan, Ahmed E. and Thomas, Stephen W.},
title = {Topic-based software defect explanation},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.015},
doi = {10.1016/j.jss.2016.05.015},
abstract = {Some topics are more defect-prone than others.Defect-prone topics are likely to remain so over time.Our topic-based metrics provide additional defect explanatory to baseline metrics.Our metrics outperform state-of-the-art topic-based cohesion and coupling metrics. Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the systems business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret.},
journal = {J. Syst. Softw.},
month = jul,
pages = {79–106},
numpages = {28},
keywords = {Code quality, Cohesion, Coupling, LDA, Metrics, Topic modeling}
}

@inproceedings{10.5555/3507788.3507810,
author = {Korlepara, Piyush and Grigoriou, Marios and Kontogiannis, Kostas and Brealey, Chris and Giammaria, Alberto},
title = {Combining domain expert knowledge and machine learning for the identification of error prone files},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Identifying as early as possible fault prone modules in order to facilitate continuous delivery in large software systems, has been an area where significant attention has been paid over the past few years. Recent efforts consider source code metrics and process metrics for training machine learning models to predict whether a software source code file is fault prone or not. In such prediction frameworks the accuracy of the trained model relies heavily on the features selected and the profiles of the metrics used for training the model which are unique to each system. Furthermore, these models act as black-boxes, where the end-user does not know how a specific prediction was reached. In this paper, we propose an approach which allows for domain expert knowledge to be combined with machine learning in order to yield fault-proneness prediction models that both exhibit high levels of recall and at the same time are able to provide explanations to the developers as to how and why these predictions were reached. For this paper we apply two rule-based inferencing techniques namely, Fuzzy reasoning, and Markov Logic Networks. The main contribution of this work is that it allows for expert developers to identify in the form of if-then rules domain logic that pertains to the fault-proneness of a source code file in the specific system being analysed. Results obtained from 19 open source systens indicate that MLNs perform better than Fuzzy Logic models and that project-customized rules achieve better results than generic rules. Furthermore, results indicate that its possible to compile a common set of rules that yields consistently acceptable results across different projects.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {153–162},
numpages = {10},
keywords = {continuous software engineering, fault-proneness prediction, process metrics, software repositories},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.1007/s10664-015-9376-6,
author = {Herzig, Kim and Just, Sascha and Zeller, Andreas},
title = {The impact of tangled code changes on defect prediction models},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9376-6},
doi = {10.1007/s10664-015-9376-6},
abstract = {When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets--in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {303–336},
numpages = {34},
keywords = {Data noise, Defect prediction, Untangling}
}

@article{10.1016/j.infsof.2013.09.001,
author = {Finlay, Jacqui and Pears, Russel and Connor, Andy M.},
title = {Data stream mining for predicting software build outcomes using source code metrics},
year = {2014},
issue_date = {February, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.09.001},
doi = {10.1016/j.infsof.2013.09.001},
abstract = {Context: Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective: This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method: This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results: The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion: Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {183–198},
numpages = {16},
keywords = {Concept drift detection, Data stream mining, Hoeffding tree, Jazz, Software metrics, Software repositories}
}

@inproceedings{10.1145/2811411.2811544,
author = {Siebra, Clauirton A. and Mello, Michael A. B.},
title = {The importance of replications in software engineering: a case study in defect prediction},
year = {2015},
isbn = {9781450337380},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811411.2811544},
doi = {10.1145/2811411.2811544},
abstract = {Prediction of defects in software is an important investigation area in software engineering, since such technique is able to return indications of parts of the code that are prone to contain problems. Thus, test teams can optimize the allocation of their resources by directing them to modules that are more defect-prone. The use of supervised learning is one of the approaches to support the design of prediction models. However, the erroneous use of training datasets can lead to poor models and, consequently, false results regarding accuracy. This work replicates important experiments of the area and shows how they could provide reliable results via the use of simple techniques of pre-processing. Based on the results, we discuss the importance of replications as method to find problems in current results and how this method is being motivated inside the software engineering area.},
booktitle = {Proceedings of the 2015 Conference on Research in Adaptive and Convergent Systems},
pages = {376–381},
numpages = {6},
keywords = {defect prediction, replication, supervised learning},
location = {Prague, Czech Republic},
series = {RACS '15}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Code change metrics, Adaptive maintenance, Commit types, Maintenance classification, Machine learning}
}

@inproceedings{10.1145/2786805.2786813,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen},
title = {Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786813},
doi = {10.1145/2786805.2786813},
abstract = {Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {496–507},
numpages = {12},
keywords = {Heterogeneous cross-company defect prediction (HCCDP), canonical correlation analysis (CCA), common metrics, company-specific metrics, unified metric representation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2499393.2499397,
author = {Tass\'{e}, Jos\'{e}e},
title = {Using code change types in an analogy-based classifier for short-term defect prediction},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499397},
doi = {10.1145/2499393.2499397},
abstract = {Current approaches for defect prediction usually analyze files (or modules) and their development as work is done on a given release, to predict post-release defects. What is missing is an approach for predicting bugs to be detected in a more short-term interval, even within the development of a particular version. In this paper, we propose a defect predictor that looks into change bursts in a given file, analyzing the number of changes and their types, and then predict whether the file is likely to have a bug found within the next 3 months after that change burst. An analogy-based classifier is used for this task: the prediction is made based on comparisons with similar change bursts that occurred in other files. New metrics are described to capture the change type of a file (e.g., small local change, massive change all in one place, multiple changes scattered throughout the file).},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {5},
numpages = {4},
keywords = {analogy-based classifier, change burst, change type metrics, defect prediction, short-term prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/2393596.2393669,
author = {Rahman, Foyzur and Posnett, Daryl and Devanbu, Premkumar},
title = {Recalling the "imprecision" of cross-project defect prediction},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393669},
doi = {10.1145/2393596.2393669},
abstract = {There has been a great deal of interest in defect prediction: using prediction models trained on historical data to help focus quality-control resources in ongoing development. Since most new projects don't have historical data, there is interest in cross-project prediction: using data from one project to predict defects in another. Sadly, results in this area have largely been disheartening. Most experiments in cross-project defect prediction report poor performance, using the standard measures of precision, recall and F-score. We argue that these IR-based measures, while broadly applicable, are not as well suited for the quality-control settings in which defect prediction models are used. Specifically, these measures are taken at specific threshold settings (typically thresholds of the predicted probability of defectiveness returned by a logistic regression model). However, in practice, software quality control processes choose from a range of time-and-cost vs quality tradeoffs: how many files shall we test? how many shall we inspect? Thus, we argue that measures based on a variety of tradeoffs, viz., 5%, 10% or 20% of files tested/inspected would be more suitable. We study cross-project defect prediction from this perspective. We find that cross-project prediction performance is no worse than within-project performance, and substantially better than random prediction!},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {61},
numpages = {11},
keywords = {empirical software engineering, fault prediction, inspection},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/3180155.3180197,
author = {Agrawal, Amritanshu and Menzies, Tim},
title = {Is "better data" better than "better data miners"? on the benefits of tuning SMOTE for defect prediction},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180197},
doi = {10.1145/3180155.3180197},
abstract = {We report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique.In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1050–1061},
numpages = {12},
keywords = {SMOTE, classification, data analytics for software engineering, defect prediction, preprocessing, search based SE, unbalanced data},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.knosys.2021.107541,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {DNNAttention: A deep neural network and attention based architecture for cross project defect number prediction},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107541},
doi = {10.1016/j.knosys.2021.107541},
journal = {Know.-Based Syst.},
month = dec,
numpages = {30},
keywords = {Cross project defect prediction, Deep neural network, Attention layer, Long short term memory (LSTM), Software defect number prediction}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Defect Prediction, Infrastructure-as-Code, Novelty Detection},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Aging Related Bugs, Empirical Software Engineering, Feature Selection Techniques, Imbalance Learning, Machine Learning, Predictive Modeling, Software Maintenance, Source Code Metrics},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3368089.3417062,
author = {Suh, Alexander},
title = {Adapting bug prediction models to predict reverted commits at Wayfair},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417062},
doi = {10.1145/3368089.3417062},
abstract = {Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1251–1262},
numpages = {12},
keywords = {reverted commits, software defect prediction, software deployment},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {cost-sensitive classification, defect prediction, software metrics},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@article{10.1016/j.infsof.2020.106269,
author = {Mahdieh, Mostafa and Mirian-Hosseinabadi, Seyed-Hassan and Etemadi, Khashayar and Nosrati, Ali and Jalali, Sajad},
title = {Incorporating fault-proneness estimations into coverage-based test case prioritization methods},
year = {2020},
issue_date = {May 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {121},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106269},
doi = {10.1016/j.infsof.2020.106269},
journal = {Inf. Softw. Technol.},
month = may,
numpages = {12},
keywords = {Regression testing, Test case prioritization, Defect prediction, Machine learning, Bug history}
}

@article{10.1016/j.jss.2016.06.006,
author = {Okutan, Ahmet and Taner Yildiz, Olcay},
title = {A novel kernel to predict software defectiveness},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.006},
doi = {10.1016/j.jss.2016.06.006},
abstract = {We propose new kernels for defect prediction that are based on the source code similarity.We model the relationship between source code similarity and defectiveness.The precomputed kernels are used with SVM and KNN classifiers.The proposed technique performs better than the SVM with linear kernel.It also achieves comparable performance when compared to the KNN classifier. Although the software defect prediction problem has been researched for a long time, the results achieved are not so bright. In this paper, we propose to use novel kernels for defect prediction that are based on the plagiarized source code, software clones and textual similarity. We generate precomputed kernel matrices and compare their performance on different data sets to model the relationship between source code similarity and defectiveness. Each value in a kernel matrix shows how much parallelism exists between the corresponding files of a software system chosen. Our experiments on 10 real world datasets indicate that support vector machines (SVM) with a precomputed kernel matrix performs better than the SVM with the usual linear kernel in terms of F-measure. Similarly, when used with a precomputed kernel, the k-nearest neighbor classifier (KNN) achieves comparable performance with respect to KNN classifier. The results from this preliminary study indicate that source code similarity can be used to predict defect proneness.},
journal = {J. Syst. Softw.},
month = sep,
pages = {109–121},
numpages = {13},
keywords = {Defect prediction, Kernel methods, SVM}
}

@article{10.1145/3384517,
author = {Kapur, Ritu and Sodhi, Balwinder},
title = {A Defect Estimator for Source Code: Linking Defect Reports with Programming Constructs Usage Metrics},
year = {2020},
issue_date = {April 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3384517},
doi = {10.1145/3384517},
abstract = {An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost.We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python.The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {12},
numpages = {35},
keywords = {AI in software engineering, Maintaining software, automated software engineering, software defect prediction, software faults and failures, software metrics, source code mining}
}

@article{10.1007/s10664-021-10024-2,
author = {Zamani, Shayan and Hemmati, Hadi},
title = {A pragmatic approach for hyper-parameter tuning in search-based test case generation},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10024-2},
doi = {10.1007/s10664-021-10024-2},
abstract = {Search-based test case generation, which is the application of meta-heuristic search for generating test cases, has been studied a lot in the literature, lately. Since, in theory, the performance of meta-heuristic search methods is highly dependent on their hyper-parameters, there is a need to study hyper-parameter tuning in this domain. In this paper, we propose a new metric (“Tuning Gain”), which estimates how cost-effective tuning a particular class is. We then predict “Tuning Gain” using static features of source code classes. Finally, we prioritize classes for tuning, based on the estimated “Tuning Gains” and spend the tuning budget only on the highly-ranked classes. To evaluate our approach, we exhaustively analyze 1,200 hyper-parameter configurations of a well-known search-based test generation tool (EvoSuite) for 250 classes of 19 projects from benchmarks such as SF110 and SBST2018 tool competition. We used a tuning approach called Meta-GA and compared the tuning results with and without the proposed class prioritization. The results show that for a low tuning budget, prioritizing classes outperforms the alternatives in terms of extra covered branches (10 times more than a traditional global tuning). In addition, we report the impact of different features of our approach such as search space size, tuning budgets, tuning algorithms, and the number of classes to tune, on the final results.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {35},
keywords = {Search-based testing, Hyper-parameter tuning, Test case generation, Source code metrics}
}

@article{10.1016/j.compeleceng.2018.02.043,
author = {Choudhary, Garvit Rajesh and Kumar, Sandeep and Kumar, Kuldeep and Mishra, Alok and Catal, Cagatay},
title = {Empirical analysis of change metrics for software fault prediction},
year = {2018},
issue_date = {Apr 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2018.02.043},
doi = {10.1016/j.compeleceng.2018.02.043},
journal = {Comput. Electr. Eng.},
month = apr,
pages = {15–24},
numpages = {10},
keywords = {Software fault prediction, Eclipse, Change log, Metrics, Software quality, Defect prediction}
}

@inproceedings{10.1007/978-3-030-87007-2_14,
author = {Peng\H{o}, Edit},
title = {Examining the Bug Prediction Capabilities of Primitive Obsession Metrics},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_14},
doi = {10.1007/978-3-030-87007-2_14},
abstract = {Bug prediction is an approach that helps make bug detection more automated during software development. Based on a bug dataset a prediction model is built to locate future bugs. Bug datasets contain information about previous defects in the code, process metrics, or source code metrics, etc. As code smells can indicate potential flaws in the source code, they can be used for bug prediction as well.In our previous work, we introduced several source code metrics to detect and describe the occurrence of Primitive Obsession in Java. This paper is a further study on three of the Primitive Obsession metrics. We integrated them into an existing, source code metrics-based bug dataset, and studied the effectiveness of the prediction built upon it. We performed a 10 fold cross-validation on the whole dataset and a cross-project validation as well. We compared the new models with the results of the original dataset. While the cross-validation showed no significant change, in the case of the cross-project validation, we have found that the amount of improvement exceeded the amount of deterioration by 5%. Furthermore, the variance added to the dataset was confirmed by correlation and PCA calculations.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {185–200},
numpages = {16},
keywords = {Bug prediction, Code smells, Primitive obsession, Static analysis, Refactoring},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-030-91265-9_11,
author = {Wei, Shaozhi and Mo, Ran and Xiong, Pu and Zhang, Siyuan and Zhao, Yang and Li, Zengyang},
title = {Predicting and Monitoring Bug-Proneness at the Feature Level},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_11},
doi = {10.1007/978-3-030-91265-9_11},
abstract = {Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {201–218},
numpages = {18},
keywords = {Code metrics, Machine learning, Feature bug prediction},
location = {Beijing, China}
}

@article{10.1007/s10664-019-09778-7,
author = {Titcheu Chekam, Thierry and Papadakis, Mike and Bissyand\'{e}, Tegawend\'{e} F. and Le Traon, Yves and Sen, Koushik},
title = {Selecting fault revealing mutants},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09778-7},
doi = {10.1007/s10664-019-09778-7},
abstract = {Mutant selection refers to the problem of choosing, among a large number of mutants, the (few) ones that should be used by the testers. In view of this, we investigate the problem of selecting the fault revealing mutants, i.e., the mutants that are killable and lead to test cases that uncover unknown program faults. We formulate two variants of this problem: the fault revealing mutant selection and the fault revealing mutant prioritization. We argue and show that these problems can be tackled through a set of ‘static’ program features and propose a machine learning approach, named FaRM, that learns to select and rank killable and fault revealing mutants. Experimental results involving 1,692 real faults show the practical benefits of our approach in both examined problems. Our results show that FaRM achieves a good trade-off between application cost and effectiveness (measured in terms of faults revealed). We also show that FaRM outperforms all the existing mutant selection methods, i.e., the random mutant sampling, the selective mutation and defect prediction (mutating the code areas pointed by defect prediction). In particular, our results show that with respect to mutant selection, our approach reveals 23% to 34% more faults than any of the baseline methods, while, with respect to mutant prioritization, it achieves higher average percentage of revealed faults with a median difference between 4% and 9% (from the random mutant orderings).},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {434–487},
numpages = {54},
keywords = {Mutation testing, Machine learning, Mutant selection, Mutant prioritization}
}

@article{10.1145/3127360.3127368,
author = {Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish},
title = {Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3127360.3127368},
doi = {10.1145/3127360.3127368},
abstract = {Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1109/ICSE43902.2021.00086,
author = {Ma, Wei and Chekam, Thierry Titcheu and Papadakis, Mike and Harman, Mark},
title = {MuDelta: Delta-Oriented Mutation Testing at Commit Time},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00086},
doi = {10.1109/ICSE43902.2021.00086},
abstract = {To effectively test program changes using mutation testing, one needs to use mutants that are relevant to the altered program behaviours. We introduce MuDelta, an approach that identifies commit-relevant mutants; mutants that affect and are affected by the changed program behaviours. Our approach uses machine learning applied on a combined scheme of graph and vector-based representations of static code features. Our results, from 50 commits in 21 Coreutils programs, demonstrate a strong prediction ability of our approach; yielding 0.80 (ROC) and 0.50 (PR-Curve) AUC values with 0.63 and 0.32 precision and recall values. These predictions are significantly higher than random guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall, and subsequently lead to strong relevant tests that kill 45% more relevant mutants than randomly sampled mutants (either sampled from those residing on the changed component(s) or from the changed lines). Our results also show that MuDelta selects mutants with 27% higher fault revealing ability in fault introducing commits. Taken together, our results corroborate the conclusion that commit-based mutation testing is suitable and promising for evolving software.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {897–909},
numpages = {13},
keywords = {commit-relevant mutants, continuous integration, machine learning, mutation testing, regression testing},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.5555/3507788.3507804,
author = {Ria and Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Process-metrics trends analysis for evaluating file-level error proneness},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Assessing the likelihood of a source code file being buggy or healthy in upcoming commits given its past behavior and its interaction with other files, has been an area where the software engineering community has paid significant attention over the years. Early efforts aimed on associating software metrics with maintainability indexes, while more recent efforts focused on the use of machine learning for classifying a software module as error prone or not. In most approaches to date, this analysis is primarily based on source code metrics or on information extracted from the system's source code, and to a lesser extend on information that relates to process metrics. In this paper, we propose a process-metrics based method for predicting the behavior of a file, based both on its GitHub commits and its interdependences with other co-committed files. More specifically, for each file and for each commit a file participates in, we compute a dependency score this file has with its other co-committed files. This score is appropriately amplified if the file is participating in a bug-fixing commit, or decayed over time if it does not. By examining, over several open source systems, the trend of that dependency score for every file as a product of time, for files whose outcome is known and that are used as gold standard, we report statistics which shed light on estimating the likelihood of whether these trends can predict the future behavior of a file or not.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3302541.3313101,
author = {Sch\"{o}rgenhumer, Andreas and Kahlhofer, Mario and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Can we Predict Performance Events with Time Series Data from Monitoring Multiple Systems?},
year = {2019},
isbn = {9781450362863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302541.3313101},
doi = {10.1145/3302541.3313101},
abstract = {Predicting performance-related events is an important part of proactive fault management. As a result, many approaches exist for the context of single systems. Surprisingly, despite its potential benefits, multi-system event prediction, i.e., using data from multiple, independent systems, has received less attention. We present ongoing work towards an approach for multi-system event prediction that works with limited data and can predict events for new systems. We present initial results showing the feasibility of our approach. Our preliminary evaluation is based on 20 days of continuous, preprocessed monitoring time series data of 90 independent systems. We created five multi-system machine learning models and compared them to the performance of single-system machine learning models. The results show promising prediction capabilities with accuracies and F1-scores over 90% and false-positive-rates below 10%.},
booktitle = {Companion of the 2019 ACM/SPEC International Conference on Performance Engineering},
pages = {9–12},
numpages = {4},
keywords = {event prediction, infrastructure monitoring data, multivariate timeseries, supervised machine learning},
location = {Mumbai, India},
series = {ICPE '19}
}

@article{10.1007/s10664-020-09843-6,
author = {Krishna, Rahul and Menzies, Tim},
title = {Learning actionable analytics from multiple software projects},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09843-6},
doi = {10.1007/s10664-020-09843-6},
abstract = {The current generation of software analytics tools are mostly prediction algorithms (e.g. support vector machines, naive bayes, logistic regression, etc). While prediction is useful, after prediction comes planning about what actions to take in order to improve quality. This research seeks methods that generate demonstrably useful guidance on “what to do” within the context of a specific software project. Specifically, we propose XTREE (for within-project planning) and BELLTREE (for cross-project planning) to generating plans that can improve software quality. Each such plan has the property that, if followed, it reduces the expected number of future defect reports. To find this expected number, planning was first applied to data from release x. Next, we looked for change in release x + 1 that conformed to our plans. This procedure was applied using a range of planners from the literature, as well as XTREE. In 10 open-source JAVA systems, several hundreds of defects were reduced in sections of the code that conformed to XTREE’s plans. Further, when compared to other planners, XTREE’s plans were found to be easier to implement (since they were shorter) and more effective at reducing the expected number of defects.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3468–3500},
numpages = {33},
keywords = {Data mining, Actionable analytics, Planning, Bellwethers, Defect prediction}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Empirical Study, Mutation Testing, Software Defect Prediction, Software Fault Prediction, Software Metrics},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.5555/3540261.3542395,
author = {Allamanis, Miltiadis and Jackson-Flux, Henry and Brockschmidt, Marc},
title = {Self-supervised bug detection and repair},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BUGLAB, an approach for self-supervised learning of bug detection and repair. BUGLAB co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BUGLAB improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {2134},
numpages = {12},
series = {NIPS '21}
}

@inproceedings{10.5555/2486788.2486839,
author = {Nam, Jaechang and Pan, Sinno Jialin and Kim, Sunghun},
title = {Transfer defect learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Many software defect prediction approaches have been proposed and most are effective in within-project prediction settings. However, for new projects or projects with limited training data, it is desirable to learn a prediction model by using sufficient training data from existing source projects and then apply the model to some target projects (cross-project defect prediction). Unfortunately, the performance of cross-project defect prediction is generally poor, largely because of feature distribution differences between the source and target projects. In this paper, we apply a state-of-the-art transfer learning approach, TCA, to make feature distributions in source and target projects similar. In addition, we propose a novel transfer defect learning approach, TCA+, by extending TCA. Our experimental results for eight open-source projects show that TCA+ significantly improves cross-project prediction performance.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {382–391},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.infsof.2017.11.005,
author = {Boucher, Alexandre and Badri, Mourad},
title = {Software metrics thresholds calculation techniques to predict fault-proneness: An empirical comparison},
year = {2018},
issue_date = {Apr 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {96},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.11.005},
doi = {10.1016/j.infsof.2017.11.005},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {38–67},
numpages = {30},
keywords = {Metrics thresholds, Class-level metrics, Object-oriented metrics, Faults, Fault-proneness prediction, Machine learning, Clustering, Cross-validation, Code quality, Object-oriented programming}
}

@article{10.1002/smr.2158,
author = {Grano, Giovanni and Titov, Timofey V. and Panichella, Sebastiano and Gall, Harald C.},
title = {Branch coverage prediction in automated testing},
year = {2019},
issue_date = {September 2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {31},
number = {9},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.2158},
doi = {10.1002/smr.2158},
abstract = {Software testing is crucial in continuous integration (CI). Ideally, at every commit, all the test cases should be executed, and moreover, new test cases should be generated for the new source code. This is especially true in a Continuous Test Generation (CTG) environment, where the automatic generation of test cases is integrated into the continuous integration pipeline. In this context, developers want to achieve a certain minimum level of coverage for every software build. However, executing all the test cases and, moreover, generating new ones for all the classes at every commit is not feasible. As a consequence, developers have to select which subset of classes has to be tested and/or targeted by test‐case generation. We argue that knowing a priori the branch coverage that can be achieved with test‐data generation tools can help developers into taking informed decision about those issues. In this paper, we investigate the possibility to use source‐code metrics to predict the coverage achieved by test‐data generation tools. We use four different categories of source‐code features and assess the prediction on a large data set involving more than 3'000 Java classes. We compare different machine learning algorithms and conduct a fine‐grained feature analysis aimed at investigating the factors that most impact the prediction accuracy. Moreover, we extend our investigation to four different search budgets. Our evaluation shows that the best model achieves an average 0.15 and 0.21 MAE on nested cross‐validation over the different budgets, respectively, on evosuite and randoop. Finally, the discussion of the results demonstrate the relevance of coupling‐related features for the prediction accuracy.In this paper, we predict the coverage achieved by test‐data generator tools using source‐code metrics. We build a Random Forest Regressor model with an average MAE of 0.2. This results substantially improve the performance of the state‐of‐art predictor.


image
image},
journal = {J. Softw. Evol. Process},
month = oct,
numpages = {18},
keywords = {automated software testing, coverage prediction, machine learning, software testing}
}

@article{10.1007/s10462-017-9563-5,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A study on software fault prediction techniques},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9563-5},
doi = {10.1007/s10462-017-9563-5},
abstract = {Software fault prediction aims to identify fault-prone software modules by using some underlying properties of the software project before the actual testing process begins. It helps in obtaining desired software quality with optimized cost and effort. Initially, this paper provides an overview of the software fault prediction process. Next, different dimensions of software fault prediction process are explored and discussed. This review aims to help with the understanding of various elements associated with fault prediction process and to explore various issues involved in the software fault prediction. We search through various digital libraries and identify all the relevant papers published since 1993. The review of these papers are grouped into three classes: software metrics, fault prediction techniques, and data quality issues. For each of the class, taxonomical classification of different techniques and our observations have also been presented. The review and summarization in the tabular form are also given. At the end of the paper, the statistical analysis, observations, challenges, and future directions of software fault prediction have been discussed.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {255–327},
numpages = {73},
keywords = {Fault prediction techniques, Software fault datasets, Software fault prediction, Software metrics, Taxonomic classification}
}

@article{10.1145/3345628,
author = {Kim, Yunho and Mun, Seokhyeon and Yoo, Shin and Kim, Moonzoo},
title = {Precise Learn-to-Rank Fault Localization Using Dynamic and Static Features of Target Programs},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3345628},
doi = {10.1145/3345628},
abstract = {Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {23},
numpages = {34},
keywords = {Fault localization, machine learning, mutation analysis, source file characteristics}
}

@article{10.1007/s11219-020-09515-0,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A public unified bug dataset for java and its assessment regarding metrics and bug prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09515-0},
doi = {10.1007/s11219-020-09515-0},
abstract = {Bug datasets have been created and used by many researchers to build and validate novel bug prediction models. In this work, our aim is to collect existing public source code metric-based bug datasets and unify their contents. Furthermore, we wish to assess the plethora of collected metrics and the capabilities of the unified bug dataset in bug prediction. We considered 5 public datasets and we downloaded the corresponding source code for each system in the datasets and performed source code analysis to obtain a common set of source code metrics. This way, we produced a unified bug dataset at class and file level as well. We investigated the diversion of metric definitions and values of the different bug datasets. Finally, we used a decision tree algorithm to show the capabilities of the dataset in bug prediction. We found that there are statistically significant differences in the values of the original and the newly calculated metrics; furthermore, notations and definitions can severely differ. We compared the bug prediction capabilities of the original and the extended metric suites (within-project learning). Afterwards, we merged all classes (and files) into one large dataset which consists of 47,618 elements (43,744 for files) and we evaluated the bug prediction model build on this large dataset as well. Finally, we also investigated cross-project capabilities of the bug prediction models and datasets. We made the unified dataset publicly available for everyone. By using a public unified dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
journal = {Software Quality Journal},
month = dec,
pages = {1447–1506},
numpages = {60},
keywords = {Bug dataset, Code metrics, Static code analysis, Bug prediction}
}

@inproceedings{10.1145/3324884.3418913,
author = {Xu, Xiangzhe},
title = {The classification and propagation of program comments},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418913},
doi = {10.1145/3324884.3418913},
abstract = {Natural language comments are like bridges between human logic and software semantics. Developers use comments to describe the function, implementation, and property of code snippets. This kind of connections contains rich information, like the potential types of a variable and the pre-condition of a method, among other things. In this paper, we categorize comments and use natural language processing techniques to extract information from them. Based on the semantics of programming languages, different rules are built for each comment category to systematically propagate comments among code entities. Then we use the propagated comments to check the code usage and comments consistency. Our demo system finds 37 bugs in real-world projects, 30 of which have been confirmed by the developers. Except for bugs in the code, we also find 304 pieces of defected comments. The 12 of them are misleading and 292 of them are not correct. Moreover, among the 41573 pieces of comments we propagate, 87 comments are for private native methods which had neither code nor comments. We also conduct a user study where we find that propagated comments are as good as human-written comments in three dimensions of consistency, naturalness, and meaningfulness.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1394–1396},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3092703.3092717,
author = {Sohn, Jeongju and Yoo, Shin},
title = {FLUCCS: using code and change metrics to improve fault localization},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092717},
doi = {10.1145/3092703.3092717},
abstract = {Fault localization aims to support the debugging activities of human developers by highlighting the program elements that are suspected to be responsible for the observed failure. Spectrum Based Fault Localization (SBFL), an existing localization technique that only relies on the coverage and pass/fail results of executed test cases, has been widely studied but also criticized for the lack of precision and limited effort reduction. To overcome restrictions of techniques based purely on coverage, we extend SBFL with code and change metrics that have been studied in the context of defect prediction, such as size, age and code churn. Using suspiciousness values from existing SBFL formulas and these source code metrics as features, we apply two learn-to-rank techniques, Genetic Programming (GP) and linear rank Support Vector Machines (SVMs). We evaluate our approach with a ten-fold cross validation of method level fault localization, using 210 real world faults from the Defects4J repository. GP with additional source code metrics ranks the faulty method at the top for 106 faults, and within the top five for 173 faults. This is a significant improvement over the state-of-the-art SBFL formulas, the best of which can rank 49 and 127 faults at the top and within the top five, respectively.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {273–283},
numpages = {11},
keywords = {Fault Localization, Genetic Programming, SBSE},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1007/978-3-030-87007-2_26,
author = {Szamosv\"{o}lgyi, Zsolt J\'{a}nos and V\'{a}radi, Endre Tam\'{a}s and T\'{o}th, Zolt\'{a}n and J\'{a}sz, Judit and Ferenc, Rudolf},
title = {Assessing Ensemble Learning Techniques in Bug Prediction},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_26},
doi = {10.1007/978-3-030-87007-2_26},
abstract = {The application of ensemble learning techniques is continuously increasing, since they have proven to be superior over traditional machine learning techniques in various domains. These algorithms could be employed for bug prediction purposes as well. Existing studies investigated the performance of ensemble learning techniques only for PROMISE and the NASA MDP public datasets; however, it is important to evaluate the ensemble learning techniques on additional public datasets in order to test the generalizability of the techniques. We investigated the performance of the two most widely-used ensemble learning techniques AdaBoost and Bagging on the Unified Bug Dataset, which encapsulates 3 class level public bug datasets in a uniformed format with a common set of software product metrics used as predictors. Additionally, we investigated the effect of using 3 different resampling techniques on the dataset. Finally, we studied the performance of using Decision Tree and Na\"{\i}ve Bayes as the weak learners in the ensemble learning. We also fine tuned the parameters of the weak learners to have the best possible end results.We experienced that AdaBoost with Decision Tree weak learner outperformed other configurations. We could achieve 54.61% F-measure value (81.96% Accuracy, 50.92% Precision, 58.90% Recall) with the configuration of 300 estimators and 0.05 learning rate. Based on the needs, one can apply RUS resampling to get a recall value up&nbsp;to 75.14% (of course losing precision at the same time).},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {368–381},
numpages = {14},
keywords = {AdaBoost, Bug prediction, Resampling, Unified bug dataset},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/2970276.2970339,
author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
title = {Too much automation? the bellwether effect and its implications for transfer learning},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970339},
doi = {10.1145/2970276.2970339},
abstract = {Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {Data Mining, Defect Prediction, Transfer learning},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/3338906.3338941,
author = {Jimenez, Matthieu and Rwemalika, Renaud and Papadakis, Mike and Sarro, Federica and Le Traon, Yves and Harman, Mark},
title = {The importance of accounting for real-world labelling when predicting software vulnerabilities},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338941},
doi = {10.1145/3338906.3338941},
abstract = {Previous work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {695–705},
numpages = {11},
keywords = {Machine Learning, Prediction Modelling, Software Vulnerabilities},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1007/s10664-021-09996-y,
author = {Laaber, Christoph and Basmaci, Mikael and Salza, Pasquale},
title = {Predicting unstable software benchmarks using static source code features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09996-y},
doi = {10.1007/s10664-021-09996-y},
abstract = {Software benchmarks are only as good as the performance measurements they yield. Unstable benchmarks show high variability among repeated measurements, which causes uncertainty about the actual performance and complicates reliable change assessment. However, if a benchmark is stable or unstable only becomes evident after it has been executed and its results are available. In this paper, we introduce a machine-learning-based approach to predict a benchmark’s stability without having to execute it. Our approach relies on 58 statically-computed source code features, extracted for benchmark code and code called by a benchmark, related to (1) meta information, e.g., lines of code (LOC), (2) programming language elements, e.g., conditionals or loops, and (3) potentially performance-impacting standard library calls, e.g., file and network input/output (I/O). To assess our approach’s effectiveness, we perform a large-scale experiment on 4,461 Go benchmarks coming from 230 open-source software (OSS) projects. First, we assess the prediction performance of our machine learning models using 11 binary classification algorithms. We find that Random Forest performs best with good prediction performance from 0.79 to 0.90, and 0.43 to 0.68, in terms of AUC and MCC, respectively. Second, we perform feature importance analyses for individual features and feature categories. We find that 7 features related to meta-information, slice usage, nested loops, and synchronization application programming interfaces (APIs) are individually important for good predictions; and that the combination of all features of the called source code is paramount for our model, while the combination of features of the benchmark itself is less important. Our results show that although benchmark stability is affected by more than just the source code, we can effectively utilize machine learning models to predict whether a benchmark will be stable or not ahead of execution. This enables spending precious testing time on reliable benchmarks, supporting developers to identify unstable benchmarks during development, allowing unstable benchmarks to be repeated more often, estimating stability in scenarios where repeated benchmark execution is infeasible or impossible, and warning developers if new benchmarks or existing benchmarks executed in new environments will be unstable.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {53},
keywords = {Performance testing, Software benchmarking, Performance variability, Source code features, Machine learning for software engineering, Go}
}

@article{10.1007/s11219-018-9430-x,
author = {Gergely, Tam\'{a}s and Balogh, Gergo? and Horv\'{a}th, Ferenc and Vancsics, B\'{e}la and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor},
title = {Differences between a static and a dynamic test-to-code traceability recovery method},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9430-x},
doi = {10.1007/s11219-018-9430-x},
abstract = {Recovering test-to-code traceability links may be required in virtually every phase of development. This task might seem simple for unit tests thanks to two fundamental unit testing guidelines: isolation (unit tests should exercise only a single unit) and separation (they should be placed next to this unit). However, practice shows that recovery may be challenging because the guidelines typically cannot be fully followed. Furthermore, previous works have already demonstrated that fully automatic test-to-code traceability recovery for unit tests is virtually impossible in a general case. In this work, we propose a semi-automatic method for this task, which is based on computing traceability links using static and dynamic approaches, comparing their results and presenting the discrepancies to the user, who will determine the final traceability links based on the differences and contextual information. We define a set of discrepancy patterns, which can help the user in this task. Additional outcomes of analyzing the discrepancies are structural unit testing issues and related refactoring suggestions. For the static test-to-code traceability, we rely on the physical code structure, while for the dynamic, we use code coverage information. In both cases, we compute combined test and code clusters which represent sets of mutually traceable elements. We also present an empirical study of the method involving 8 non-trivial open source Java systems.},
journal = {Software Quality Journal},
month = jun,
pages = {797–822},
numpages = {26},
keywords = {Code coverage, Refactoring, Structural test smells, Test-to-code traceability, Traceability link recovery, Unit testing}
}

@article{10.3233/KES-200029,
author = {Singh, Pradeep and Verma, Shrish},
title = {ACO based comprehensive model for software fault prediction},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {1},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-200029},
doi = {10.3233/KES-200029},
abstract = {The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization (ACO) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates ACO based mining approach with ROC based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on ACO based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of ACO based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the ACO based learner outperforms other benchmark techniques.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {63–71},
numpages = {9},
keywords = {Software metric, fault prediction, ACO}
}

@inproceedings{10.1145/1137983.1138012,
author = {Knab, Patrick and Pinzger, Martin and Bernstein, Abraham},
title = {Predicting defect densities in source code files with decision tree learners},
year = {2006},
isbn = {1595933972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137983.1138012},
doi = {10.1145/1137983.1138012},
abstract = {With the advent of open source software repositories the data available for defect prediction in source files increased tremendously. Although traditional statistics turned out to derive reasonable results the sheer amount of data and the problem context of defect prediction demand sophisticated analysis such as provided by current data mining and machine learning techniques.In this work we focus on defect density prediction and present an approach that applies a decision tree learner on evolution data extracted from the Mozilla open source web browser project. The evolution data includes different source code, modification, and defect measures computed from seven recent Mozilla releases. Among the modification measures we also take into account the change coupling, a measure for the number of change-dependencies between source files. The main reason for choosing decision tree learners, instead of for example neural nets, was the goal of finding underlying rules which can be easily interpreted by humans. To find these rules, we set up a number of experiments to test common hypotheses regarding defects in software entities. Our experiments showed, that a simple tree learner can produce good results with various sets of input data.},
booktitle = {Proceedings of the 2006 International Workshop on Mining Software Repositories},
pages = {119–125},
numpages = {7},
keywords = {data mining, decision tree learner, defect prediction},
location = {Shanghai, China},
series = {MSR '06}
}

@article{10.1016/j.eswa.2014.10.025,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {A comparison of some soft computing methods for software fault prediction},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.025},
doi = {10.1016/j.eswa.2014.10.025},
abstract = {Software fault prediction is implemented with ANN, SVM and ANFIS.First ANFIS implementation is applied to solve fault prediction problem.Parameters are discussed in neuro fuzzy approach.Experiments show that the application of ANFIS to the software fault prediction problem is highly reasonable. The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1872–1879},
numpages = {8},
keywords = {Adaptive neuro fuzzy systems, Artificial Neural Networks, McCabe metrics, Software fault prediction, Support Vector Machines}
}

@inproceedings{10.1007/978-3-030-58811-3_69,
author = {Hegedundefineds, P\'{e}ter},
title = {Inspecting JavaScript Vulnerability Mitigation Patches with Automated Fix Generation in Mind},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_69},
doi = {10.1007/978-3-030-58811-3_69},
abstract = {Software security has become a primary concern for both the industry and academia in recent years. As dependency on critical services provided by software systems grows globally, a potential security threat in such systems poses higher and higher risks (e.g. economical damage, a threat to human life, criminal activity).Finding potential security vulnerabilities at the code level automatically is a very popular approach to aid security testing. However, most of the methods based on machine learning and statistical models stop at listing potentially vulnerable code parts and leave their validation and mitigation to the developers. Automatic program repair could fill this gap by automatically generating vulnerability mitigation code patches. Nonetheless, it is still immature, especially in targeting security-relevant fixes.In this work, we try to establish a path towards automatic vulnerability fix generation techniques in the context of JavaScript programs. We inspect 361 actual vulnerability mitigation patches collected from vulnerability databases and GitHub. We found that vulnerability mitigation patches are not short on average and in many cases affect not just program code but test code as well. These results point towards that a general automatic repair approach targeting all the different types of vulnerabilities is not feasible. The analysis of the code properties and fix patterns for different vulnerability types might help in setting up a more realistic goal in the area of automatic JavaScript vulnerability repair.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {975–988},
numpages = {14},
keywords = {Security, Vulnerability, JavaScript, Prediction models, Automatic repair},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3293882.3330574,
author = {Li, Xia and Li, Wei and Zhang, Yuqun and Zhang, Lingming},
title = {DeepFL: integrating multiple fault diagnosis dimensions for deep fault localization},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330574},
doi = {10.1145/3293882.3330574},
abstract = {Learning-based fault localization has been intensively studied recently. Prior studies have shown that traditional Learning-to-Rank techniques can help precisely diagnose fault locations using various dimensions of fault-diagnosis features, such as suspiciousness values computed by various off-the-shelf fault localization techniques. However, with the increasing dimensions of features considered by advanced fault localization techniques, it can be quite challenging for the traditional Learning-to-Rank algorithms to automatically identify effective existing/latent features. In this work, we propose DeepFL, a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization. Although the approach is general, in this work, we collect various suspiciousness-value-based, fault-proneness-based and textual-similarity-based features from the fault localization, defect prediction and information retrieval areas, respectively. DeepFL has been studied on 395 real bugs from the widely used Defects4J benchmark. The experimental results show DeepFL can significantly outperform state-of-the-art TraPT/FLUCCS (e.g., localizing 50+ more faults within Top-1). We also investigate the impacts of deep model configurations (e.g., loss functions and epoch settings) and features. Furthermore, DeepFL is also surprisingly effective for cross-project prediction.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {169–180},
numpages = {12},
keywords = {Deep learning, Fault localization, Mutation testing},
location = {Beijing, China},
series = {ISSTA 2019}
}

@article{10.1007/s11219-019-09467-0,
author = {Du, Xiaoting and Zhou, Zenghui and Yin, Beibei and Xiao, Guanping},
title = {Cross-project bug type prediction based on transfer learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09467-0},
doi = {10.1007/s11219-019-09467-0},
abstract = {The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project.},
journal = {Software Quality Journal},
month = mar,
pages = {39–57},
numpages = {19},
keywords = {Bug prediction, Cross-project, Bug report, Transfer learning}
}

@inproceedings{10.1145/3127005.3127017,
author = {Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea},
title = {An Extensive Analysis of Efficient Bug Prediction Configurations},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127017},
doi = {10.1145/3127005.3127017},
abstract = {Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {Bug Prediction, Effort-Aware Evaluation},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1007/s11219-021-09564-z,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Corrective commit probability: a measure of the effort invested in bug fixing},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09564-z},
doi = {10.1007/s11219-021-09564-z},
abstract = {The effort invested in software development should ideally be devoted to the implementation of new features. But some of the effort is invariably also invested in corrective maintenance, that is in fixing bugs. Not much is known about what fraction of software development work is devoted to bug fixing, and what factors affect this fraction. We suggest the Corrective Commit Probability (CCP), which measures the probability that a commit reflects corrective maintenance, as an estimate of the relative effort invested in fixing bugs. We identify corrective commits by applying a linguistic model to the commit messages, achieving an accuracy of 93%, higher than any previously reported model. We compute the CCP of all large active GitHub projects (7,557 projects with 200+ commits in 2019). This leads to the creation of an investment scale, suggesting that the bottom 10% of projects spend less than 6% of their total effort on bug fixing, while the top 10% of projects spend at least 39% of their effort on bug fixing — more than 6 times more. Being a process metric, CCP is conditionally independent of source code metrics, enabling their evaluation and investigation. Analysis of project attributes shows that lower CCP (that is, lower relative investment in bug fixing) is associated with smaller files, lower coupling, use of languages like JavaScript and C# as opposed to PHP and C++, fewer code smells, lower project age, better perceived quality, fewer developers, lower developer churn, better onboarding, and better productivity.},
journal = {Software Quality Journal},
month = dec,
pages = {817–861},
numpages = {45},
keywords = {Corrective maintenance, Corrective commits, Effort estimate, Process metric}
}

@article{10.1016/j.ins.2019.08.077,
author = {Peng, Zhendong and Xiao, Xi and Hu, Guangwu and Kumar Sangaiah, Arun and Atiquzzaman, Mohammed and Xia, Shutao},
title = {ABFL: An autoencoder based practical approach for software fault localization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {510},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.08.077},
doi = {10.1016/j.ins.2019.08.077},
journal = {Inf. Sci.},
month = feb,
pages = {108–121},
numpages = {14},
keywords = {Fault localization, Debugging, SBFL, Autoencoder}
}

@article{10.4018/IJOSSP.2015010104,
author = {Lal, Sangeeta and Sardana, Neetu and Sureka, Ashish},
title = {Two Level Empirical Study of Logging Statements in Open Source Java Projects},
year = {2015},
issue_date = {January 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2015010104},
doi = {10.4018/IJOSSP.2015010104},
abstract = {Log statements present in source code provide important information to the software developers because they are useful in various software development activities. Most of the previous studies on logging analysis and prediction provide insights and results after analyzing only a few code constructs. In this paper, the authors perform an in-depth and large-scale analysis of logging code constructs at two levels. They answer nine research questions related to statistical and content analysis. Statistical analysis at file level reveals that fewer files consist of log statements but logged files have a greater complexity than that of non-logged files. Results show that a positive correlation exists between size and logging count of the logged files. Statistical analysis on catch-blocks show that try-blocks associated with logged catch-blocks have greater complexity than non-logged catch-blocks and the logging ratio of an exception type is project specific. Content-based analysis of catch-blocks reveals the presence of different topics in try-blocks associated with logged and non-logged catch-blocks.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {49–73},
numpages = {25},
keywords = {Debugging, Dirichlet Allocation LDA, Empirical Software Engineering and Measurement, Latent, Logging, Source Code Analysis, Source Code Metrics, Tracing}
}

@inproceedings{10.1145/3383219.3383264,
author = {Madeyski, Lech and Lewowski, Tomasz},
title = {MLCQ: Industry-Relevant Code Smell Data Set},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383264},
doi = {10.1145/3383219.3383264},
abstract = {Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code smells instead of imposing thresholds from the legacy literature. We gather samples from active open source projects manually verified for industry-relevance and provide repository links and revisions. Records in our MLCQ data set contain the type of smell, its severity and the exact location in source code, but do not contain any source code metrics which can be calculated using various tools. To open new research opportunities, we provide results of an extensive survey of developers involved in the study including a wide range of details concerning their professional experience in software development and many other characteristics. This allows us to track each code review to the developer's background. To the best of our knowledge, this is a unique trait of the presented data set.Conclusions The MLCQ data set with nearly 15000 code samples was created by software developers with professional experience who reviewed industry-relevant, contemporary Java open source projects. We expect that this data set should stay relevant for a longer time than data sets that base on code released years ago and, additionally, will enable researchers to investigate the relationship between developers' background and code smells' perception.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {342–347},
numpages = {6},
keywords = {bad code smells, code smells, data set, software development, software quality},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1016/j.eswa.2016.05.018,
author = {Arar, \"{O}mer Faruk and Ayan, K\"{u}r\c{s}at},
title = {Deriving thresholds of software metrics to predict faults on open source software},
year = {2016},
issue_date = {November 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.05.018},
doi = {10.1016/j.eswa.2016.05.018},
abstract = {We empirically examined if there are effective thresholds for software metrics.Open-source software systems were used as benchmarking datasets.The learner model was created using logistic regression and the Bender method.Experimental results revealed that some metrics have effective threshold values. Object-oriented metrics aim to exhibit the quality of source code and give insight to it quantitatively. Each metric assesses the code from a different aspect. There is a relationship between the quality level and the risk level of source code. The objective of this paper is to empirically examine whether or not there are effective threshold values for source code metrics. It is targeted to derive generalized thresholds that can be used in different software systems. The relationship between metric thresholds and fault-proneness was investigated empirically in this study by using ten open-source software systems. Three types of fault-proneness were defined for the software modules: non-fault-prone, more-than-one-fault-prone, and more-than-three-fault-prone. Two independent case studies were carried out to derive two different threshold values. A single set was created by merging ten datasets and was used as training data by the model. The learner model was created using logistic regression and the Bender method. Results revealed that some metrics have threshold effects. Seven metrics gave satisfactory results in the first case study. In the second case study, eleven metrics gave satisfactory results. This study makes contributions primarily for use by software developers and testers. Software developers can see classes or modules that require revising; this, consequently, contributes to an increment in quality for these modules and a decrement in their risk level. Testers can identify modules that need more testing effort and can prioritize modules according to their risk levels.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {106–121},
numpages = {16},
keywords = {Bender method, Logistic regression, Machine learning, Software fault prediction, Software quality metrics, Threshold}
}

@inproceedings{10.1109/IWoR.2019.00015,
author = {Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
title = {Toward proactive refactoring: an exploratory study on decaying modules},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IWoR.2019.00015},
doi = {10.1109/IWoR.2019.00015},
abstract = {Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells.To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.},
booktitle = {Proceedings of the 3rd International Workshop on Refactoring},
pages = {39–46},
numpages = {8},
keywords = {code quality, code smell, refactoring},
location = {Montreal, Quebec, Canada},
series = {IWOR '19}
}

@article{10.1007/s10489-020-01935-6,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of ensemble techniques for software fault prediction},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01935-6},
doi = {10.1007/s10489-020-01935-6},
abstract = {Previously, many researchers have performed analysis of various techniques for the software fault prediction (SFP). Oddly, the majority of such studies have shown the limited prediction capability and their performance for given software fault datasets was not persistent. In contrast to this, recently, ensemble techniques based SFP models have shown promising and improved results across different software fault datasets. However, many new as well as improved ensemble techniques have been introduced, which are not explored for SFP. Motivated by this, the paper performs an investigation on ensemble techniques for SFP. We empirically assess the performance of seven ensemble techniques namely, Dagging, Decorate, Grading, MultiBoostAB, RealAdaBoost, Rotation Forest, and Ensemble Selection. We believe that most of these ensemble techniques are not used before for SFP. We conduct a series of experiments on the benchmark fault datasets and use three distinct classification algorithms, namely, naive Bayes, logistic regression, and J48 (decision tree) as base learners to the ensemble techniques. Experimental analysis revealed that rotation forest with J48 as the base learner achieved the highest precision, recall, and G-mean 1 values of 0.995, 0.994, and 0.994, respectively and Decorate achieved the highest AUC value of 0.986. Further, results of statistical tests showed used ensemble techniques demonstrated a statistically significant difference in their performance among the used ones for SFP. Additionally, the cost-benefit analysis showed that SFP models based on used ensemble techniques might be helpful in saving software testing cost and effort for twenty out of twenty-eight used fault datasets.},
journal = {Applied Intelligence},
month = jun,
pages = {3615–3644},
numpages = {30},
keywords = {Software fault prediction, Ensemble techniques, PROMISE data repository, Empirical analysis}
}

@inproceedings{10.1109/ICSE43902.2021.00066,
author = {K\"{u}\c{c}\"{u}k, Yi\u{g}it and Henderson, Tim A. D. and Podgurski, Andy},
title = {Improving Fault Localization by Integrating Value and Predicate Based Causal Inference Techniques},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00066},
doi = {10.1109/ICSE43902.2021.00066},
abstract = {Statistical fault localization (SFL) techniques use execution profiles and success/failure information from software executions, in conjunction with statistical inference, to automatically score program elements based on how likely they are to be faulty. SFL techniques typically employ one type of profile data: either coverage data, predicate outcomes, or variable values. Most SFL techniques actually measure correlation, not causation, between profile values and success/failure, and so they are subject to confounding bias that distorts the scores they produce. This paper presents a new SFL technique, named UniVal, that uses causal inference techniques and machine learning to integrate information about both predicate outcomes and variable values to more accurately estimate the true failure-causing effect of program statements. UniVal was empirically compared to several coverage-based, predicate-based, and value-based SFL techniques on 800 program versions with real faults.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {649–660},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.4018/ijsse.2014070102,
author = {Hovsepyan, Aram and Scandariato, Riccardo and Steff, Maximilian and Joosen, Wouter},
title = {Design Churn as Predictor of Vulnerabilities?},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-3036},
url = {https://doi.org/10.4018/ijsse.2014070102},
doi = {10.4018/ijsse.2014070102},
abstract = {This paper evaluates a metric suite to predict vulnerable Java classes based on how much the design of an application has changed over time. It refers to this concept as design churn in analogy with code churn. Based on a validation on 10 Android applications, it shows that several design churn metrics are in fact significantly associated with vulnerabilities. When used to build a prediction model, the metrics yield an average precision of 0.71 and an average recall of 0.27.},
journal = {Int. J. Secur. Softw. Eng.},
month = jul,
pages = {16–31},
numpages = {16},
keywords = {Android Applications, Machine Learning, Security Vulnerability Prediction, Software Metrics}
}

@article{10.4018/IJOSSP.2016010102,
author = {Chahal, Kuljit Kaur and Saini, Munish},
title = {Open Source Software Evolution: A Systematic Literature Review Part 2},
year = {2016},
issue_date = {January 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016010102},
doi = {10.4018/IJOSSP.2016010102},
abstract = {This paper presents the results of a systematic literature review conducted to understand the Open Source Software OSS development process on the basis of evidence found in the empirical research studies. The study targets the OSS project evolution research papers to understand the methods and techniques employed for analysing the OSS evolution process. Our results suggest that there is lack of a uniform approach to analyse and interpret the results. The use of prediction techniques that just extrapolate the historic trends into the future should be a conscious task as it is observed that there are no long-term correlations in data of such systems. OSS evolution as a research area is still in nascent stage. Even after a number of empirical studies, the field has failed to establish a theory. There is need to formalize the field as a systematic and formal approach can produce better software.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {28–48},
numpages = {21},
keywords = {ARIMA Modelling, Automation Support, Co-Evolution, OSS Prediction, Programming Languages, Software Evolution Theory, Software Reuse}
}

@inproceedings{10.1007/978-3-030-87007-2_18,
author = {G\'{a}l, P\'{e}ter},
title = {Bug Prediction Capability of Primitive Enthusiasm Metrics},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_18},
doi = {10.1007/978-3-030-87007-2_18},
abstract = {Bugs in software development life cycle are unavoidable. Manually finding these bugs is not always the most effective way. To aid this, various bug prediction approaches which are using code metrics are developed and are also still in active development.In a previous work, the Primitive Enthusiasm code metrics were introduced to add detection capabilities for Primitive Obsession code smells. This paper explores the usability of the Primitive Enthusiasm metrics in a bug prediction scenario. To evaluate the new metrics, an existing source code bug data set was used. The correlation between existing metrics and the new PE metrics was investigated. Furthermore the effectiveness of bug prediction is investigated by building training models with and without the new metrics. Using a cross-project and a project version-based evaluation the results show that adding PE metrics can be beneficial for bug prediction.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {246–262},
numpages = {17},
keywords = {Bug prediction, Code metrics, Primitive obsession, Primitive enthusiasm, Static analysis},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Bug prediction, Feature selection, Software Quality},
location = {Bangalore, India},
series = {ISEC '15}
}

@article{10.1016/j.jss.2021.111041,
author = {Blasi, Arianna and Gorla, Alessandra and Ernst, Michael D. and Pezz\`{e}, Mauro and Carzaniga, Antonio},
title = {MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111041},
doi = {10.1016/j.jss.2021.111041},
journal = {J. Syst. Softw.},
month = nov,
numpages = {13},
keywords = {Software testing, Test oracle generation, Natural language processing}
}

@inproceedings{10.1145/3092703.3092731,
author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
title = {Boosting spectrum-based fault localization using PageRank},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092731},
doi = {10.1145/3092703.3092731},
abstract = {Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–272},
numpages = {12},
keywords = {PageRank, Software testing, Spectrum-based fault localization},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.5555/3340730.3340750,
author = {Catolino, Gemma and Di Nucci, Dario and Ferrucci, Filomena},
title = {Cross-project just-in-time bug prediction for mobile apps: an empirical assessment},
year = {2019},
publisher = {IEEE Press},
abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42,543 commits extracted from the Commit Guru platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {99–110},
numpages = {12},
keywords = {JIT bug prediction, empirical study, metrics},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@inproceedings{10.1145/3273934.3273936,
author = {Ferenc, Rudolf and T\'{o}th, Zolt\'{a}n and Lad\'{a}nyi, Gergely and Siket, Istv\'{a}n and Gyim\'{o}thy, Tibor},
title = {A Public Unified Bug Dataset for Java},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273936},
doi = {10.1145/3273934.3273936},
abstract = {Background: Bug datasets have been created and used by many researchers to build bug prediction models.Aims: In this work we collected existing public bug datasets and unified their contents.Method: We considered 5 public datasets which adhered to all of our criteria. We also downloaded the corresponding source code for each system in the datasets and performed their source code analysis to obtain a common set of source code metrics. This way we produced a unified bug dataset at class and file level that is suitable for further research (e.g. to be used in the building of new bug prediction models). Furthermore, we compared the metric definitions and values of the different bug datasets.Results: We found that (i) the same metric abbreviation can have different definitions or metrics calculated in the same way can have different names, (ii) in some cases different tools give different values even if the metric definitions coincide because (iii) one tool works on source code while the other calculates metrics on bytecode, or (iv) in several cases the downloaded source code contained more files which influenced the afferent metric values significantly.Conclusions: Apart from all these imprecisions, we think that having a common metric set can help in building better bug prediction models and deducing more general conclusions. We made the unified dataset publicly available for everyone. By using a public dataset as an input for different bug prediction related investigations, researchers can make their studies reproducible, thus able to be validated and verified.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {Bug dataset, code metrics, static code analysis},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3196398.3196435,
author = {de P\'{a}dua, Guilherme B. and Shang, Weiyi},
title = {Studying the relationship between exception handling practices and post-release defects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196435},
doi = {10.1145/3196398.3196435},
abstract = {Modern programming languages, such as Java and C#, typically provide features that handle exceptions. These features separate error-handling code from regular source code and aim to assist in the practice of software comprehension and maintenance. Nevertheless, their misuse can still cause reliability degradation or even catastrophic software failures. Prior studies on exception handling revealed the suboptimal practices of the exception handling flows and the prevalence of their anti-patterns. However, little is known about the relationship between exception handling practices and software quality. In this work, we investigate the relationship between software quality (measured by the probability of having post-release defects) and: (i) exception flow characteristics and (ii) 17 exception handling anti-patterns. We perform a case study on three Java and C# open-source projects. By building statistical models of the probability of post-release defects using traditional software metrics and metrics that are associated with exception handling practice, we study whether exception flow characteristics and exception handling anti-patterns have a statistically significant relationship with post-release defects. We find that exception flow characteristics in Java projects have a significant relationship with post-release defects. In addition, although the majority of the exception handing anti-patterns are not significant in the models, there exist anti-patterns that can provide significant explanatory power to the probability of post-release defects. Therefore, development teams should consider allocating more resources to improving their exception handling practices and avoid the anti-patterns that are found to have a relationship with post-release defects. Our findings also highlight the need for techniques that assist in handling exceptions in the software development practice.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {564–575},
numpages = {12},
keywords = {empirical software engineering, exception handling, software quality},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1145/3442694,
author = {Bluemke, Ilona and Malanowska, Agnieszka},
title = {Software Testing Effort Estimation and Related Problems: A Systematic Literature Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442694},
doi = {10.1145/3442694},
abstract = {Although testing effort estimation is a very important task in software project management, it is rarely described in the literature. There are many difficulties in finding any useful methods or tools for this purpose. Solutions to many other problems related to testing effort calculation are published much more often. There is also no research focusing on both testing effort estimation and all related areas of software engineering. To fill this gap, we performed a systematic literature review on both questions. Although our primary objective was to find some tools or implementable metods for test effort estimation, we have quickly discovered many other interesting topics related to the main one. The main contribution of this work is the presentation of the testing effort estimation task in a very wide context, indicating the relations with other research fields. This systematic literature review presents a detailed overview of testing effort estimation task, including challenges and approaches to automating it and the solutions proposed in the literature. It also exhaustively investigates related research topics, classifying publications that can be found in connection to the testing effort according to seven criteria formulated on the basis of our research questions. We present here both synthesis of our finding and the deep analysis of the stated research problems.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {53},
numpages = {38},
keywords = {Testing effort, systematic literature review, testing effort estimation, testing effort estimation-related problems}
}

@inproceedings{10.1145/2372251.2372285,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
title = {Method-level bug prediction},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372285},
doi = {10.1145/2372251.2372285},
abstract = {Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments---performed on 21 Java open-source (sub-)systems---show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {171–180},
numpages = {10},
keywords = {code metrics, fine-grained source code changes, method-level bug prediction},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1109/ESEM.2017.12,
author = {Ahmed, Iftekhar and Brindescu, Caius and Mannan, Umme Ayda and Jensen, Carlos and Sarma, Anita},
title = {An empirical examination of the relationship between code smells and merge conflicts},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.12},
doi = {10.1109/ESEM.2017.12},
abstract = {Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both "smelly" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {58–67},
numpages = {10},
keywords = {code smell, empirical analysis, machine learning, merge conflict},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@article{10.5555/2639037.2639042,
author = {Zazworka, Nico and Vetro', Antonio and Izurieta, Clemente and Wong, Sunny and Cai, Yuanfang and Seaman, Carolyn and Shull, Forrest},
title = {Comparing four approaches for technical debt identification},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
abstract = {Software systems accumulate technical debt (TD) when short-term goals in software development are traded for long-term goals (e.g., quick-and-dirty implementation to reach a release date versus a well-refactored implementation that supports the long-term health of the project). Some forms of TD accumulate over time in the form of source code that is difficult to work with and exhibits a variety of anomalies. A number of source code analysis techniques and tools have been proposed to potentially identify the code-level debt accumulated in a system. What has not yet been studied is if using multiple tools to detect TD can lead to benefits, that is, if different tools will flag the same or different source code components. Further, these techniques also lack investigation into the symptoms of TD "interest" that they lead to. To address this latter question, we also investigated whether TD, as identified by the source code analysis techniques, correlates with interest payments in the form of increased defect- and change-proneness. Comparing the results of different TD identification approaches to understand their commonalities and differences and to evaluate their relationship to indicators of future TD "interest." We selected four different TD identification techniques (code smells, automatic static analysis issues, grime buildup, and Modularity violations) and applied them to 13 versions of the Apache Hadoop open source software project. We collected and aggregated statistical measures to investigate whether the different techniques identified TD indicators in the same or different classes and whether those classes in turn exhibited high interest (in the form of a large number of defects and higher change-proneness). The outputs of the four approaches have very little overlap and are therefore pointing to different problems in the source code. Dispersed Coupling and Modularity violations were co-located in classes with higher defect-proneness. We also observed a strong relationship between Modularity violations and change-proneness. Our main contribution is an initial overview of the TD landscape, showing that different TD techniques are loosely coupled and therefore indicate problems in different locations of the source code. Moreover, our proxy interest indicators (change- and defect-proneness) correlate with only a small subset of TD indicators.},
journal = {Software Quality Journal},
month = sep,
pages = {403–426},
numpages = {24},
keywords = {ASA, Code smells, Grime, Modularity violations, Software maintenance, Software quality, Source code analysis, Technical debt}
}

@article{10.1016/j.infsof.2013.05.008,
author = {Novais, Renato Lima and Torres, Andr\'{e} and Mendes, Thiago Souto and Mendon\c{c}a, Manoel and Zazworka, Nico},
title = {Software evolution visualization: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.008},
doi = {10.1016/j.infsof.2013.05.008},
abstract = {Background: Software evolution is an important topic in software engineering. It generally deals with large amounts of data, as one must look at whole project histories as opposed to their current snapshot. Software visualization is the field of software engineering that aims to help people to understand software through the use of visual resources. It can be effectively used to analyze and understand the large amount of data produced during software evolution. Objective: This study investigates Software Evolution Visualization (SEV) approaches, collecting evidence about how SEV research is structured, synthesizing current evidence on the goals of the proposed approaches and identifying key challenges for its use in practice. Methods: A mapping study was conducted to analyze how the SEV area is structured. Selected primary studies were classified and analyzed with respect to nine research questions. Results: SEV has been used for many different purposes, especially for change comprehension, change prediction and contribution analysis. The analysis identified gaps in the studies with respect to their goals, strategies and approaches. It also pointed out to a widespread lack of empirical studies in the area. Conclusion: Researchers have proposed many SEV approaches during the past years, but some have failed to clearly state their goals, tie them back to concrete problems, or formally validate their usefulness. The identified gaps indicate that there still are many opportunities to be explored in the area.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1860–1883},
numpages = {24},
keywords = {Software evolution, Software visualization, Systematic mapping study}
}

@article{10.1016/j.infsof.2021.106686,
author = {Liu, Shiran and Guo, Zhaoqiang and Li, Yanhui and Lu, Hongmin and Chen, Lin and Xu, Lei and Zhou, Yuming and Xu, Baowen},
title = {Prioritizing code documentation effort: Can we do it simpler but better?},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106686},
doi = {10.1016/j.infsof.2021.106686},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {16},
keywords = {Code documentation, Program comprehension, PageRank, Metrics}
}

@article{10.1016/j.procs.2020.02.099,
author = {Li, Zhen and Jiang, Ying and Zhang, Xiao Jiang and Xu, Hai Yan},
title = {The Metric for Automatic Code Generation},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.02.099},
doi = {10.1016/j.procs.2020.02.099},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {279–286},
numpages = {8},
keywords = {Automatic code generation, Metric, Quality, Efficiency}
}

@inproceedings{10.5555/2664446.2664455,
author = {Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E.},
title = {Think locally, act globally: improving defect and effort prediction models},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {60–69},
numpages = {10},
keywords = {models, software metrics, techniques},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@inproceedings{10.1007/978-3-030-58811-3_67,
author = {ElGhondakly, Roaa and Moussa, Sherin and Badr, Nagwa},
title = {Handling Faults in Service Oriented Computing: A Comprehensive Study},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_67},
doi = {10.1007/978-3-030-58811-3_67},
abstract = {Recently, service-oriented computing paradigms have become a trending development direction, in which software systems are built using a set of loosely coupled services distributed over multiple locations through a service-oriented architecture. Such systems encounter different challenges, as integration, performance, reliability, availability, etc., which made all associated testing activities to be another major challenge to avoid their faults and system failures. Services are considered the substantial element in service-oriented computing. Thus, the quality of services and the service dependability in a web service composition have become essential to manage faults within these software systems. Many studies addressed web service faults from diverse perspectives. In this paper, a comprehensive study is conducted to investigate the different perspectives to manipulate web service faults, including fault tolerance, fault injection, fault prediction and fault localization. An extensive comparison is provided, highlighting the main research gaps, challenges and limitations of each perspective for web services. An analytical discussion is then followed to suggest future research directions that can be adopted to face such obstacles by improving fault handling capabilities for an efficient testing in service-oriented computing systems.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {947–959},
numpages = {13},
keywords = {Fault tolerance, Fault prediction, Fault injection, Quality of Service, Service testing, Service oriented computing},
location = {Cagliari, Italy}
}

@inproceedings{10.5555/2664446.2664480,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Can we predict types of code changes? an empirical analysis},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {217–226},
numpages = {10},
keywords = {machine learning, software maintenance, software quality},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@inproceedings{10.1145/2746194.2746198,
author = {Morrison, Patrick and Herzig, Kim and Murphy, Brendan and Williams, Laurie},
title = {Challenges with applying vulnerability prediction models},
year = {2015},
isbn = {9781450333764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2746194.2746198},
doi = {10.1145/2746194.2746198},
abstract = {Vulnerability prediction models (VPM) are believed to hold promise for providing software engineers guidance on where to prioritize precious verification resources to search for vulnerabilities. However, while Microsoft product teams have adopted defect prediction models, they have not adopted vulnerability prediction models (VPMs). The goal of this research is to measure whether vulnerability prediction models built using standard recommendations perform well enough to provide actionable results for engineering resource allocation. We define 'actionable' in terms of the inspection effort required to evaluate model results. We replicated a VPM for two releases of the Windows Operating System, varying model granularity and statistical learners. We reproduced binary-level prediction precision (~0.75) and recall (~0.2). However, binaries often exceed 1 million lines of code, too large to practically inspect, and engineers expressed preference for source file level predictions. Our source file level models yield precision below 0.5 and recall below 0.2. We suggest that VPMs must be refined to achieve actionable performance, possibly through security-specific metrics.},
booktitle = {Proceedings of the 2015 Symposium and Bootcamp on the Science of Security},
articleno = {4},
numpages = {9},
keywords = {churn, complexity, coverage, dependencies, metrics, prediction, vulnerabilities},
location = {Urbana, Illinois},
series = {HotSoS '15}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00028,
author = {Murali, Vijayaraghavan and Gross, Lee and Qian, Rebecca and Chandra, Satish},
title = {Industry-scale IR-based bug localization: a perspective from Facebook},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00028},
doi = {10.1109/ICSE-SEIP52600.2021.00028},
abstract = {We explore the application of Information Retrieval (IR) based bug localization methods at a large industrial setting, Facebook. Facebook's code base evolves rapidly, with thousands of code changes being committed to a monolithic repository every day. When a bug is detected, it is often time-sensitive and imperative to identify the commit causing the bug in order to either revert it or fix it. This is complicated by the fact that bugs often manifest with complex and unwieldy features, such as stack traces and other metadata. Code commits also have various features associated with them, ranging from developer comments to test results. This poses unique challenges to bug localization methods, making it a highly non-trivial operation.In this paper we lay out several practical concerns for industry-level IR-based bug localization, and propose Bug2Commit, a tool that is designed to address these concerns. We also assess the effectiveness of existing IR-based localization techniques from the software engineering community, and find that in the presence of complex queries or documents, which are common at Facebook, existing approaches do not perform as well as Bug2Commit. We evaluate Bug2Commit on three applications at Facebook: client-side crashes from the mobile app, server-side performance regressions, and mobile simulation tests for performance. We find that Bug2Commit outperforms the accuracy of existing approaches by up to 17%, leading to reduced time for triaging regressions and attributing bugs found in simulations.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {188–197},
numpages = {10},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/2832987.2833051,
author = {Abunadi, Ibrahim and Alenezi, Mamdouh},
title = {Towards Cross Project Vulnerability Prediction in Open Source Web Applications},
year = {2015},
isbn = {9781450334181},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2832987.2833051},
doi = {10.1145/2832987.2833051},
abstract = {Building secure software is challenging, time-consuming, and expensive. Software vulnerability prediction models that identify vulnerable software components are usually used to focus security efforts, with the aim of helping to reduce the time and effort needed to secure software. Existing vulnerability prediction models use process or product metrics and machine learning techniques to identify vulnerable software components. Cross project vulnerability prediction plays a significant role in appraising the most likely vulnerable software components, specifically for new or inactive projects. Little effort has been spent to deliver clear guidelines on how to choose the training data for project vulnerability prediction. In this work, we present an empirical study aiming at clarifying how useful cross project prediction techniques in predicting software vulnerabilities. Our study employs the classification provided by different machine learning techniques to improve the detection of vulnerable components. We have elaborately compared the prediction performance of five well-known classifiers. The study is conducted on a publicly available dataset of several PHP open source web applications and in the context of cross project vulnerability prediction, which represents one of the main challenges in the vulnerability prediction field.},
booktitle = {Proceedings of the The International Conference on Engineering &amp; MIS 2015},
articleno = {42},
numpages = {5},
keywords = {Cross-project vulnerability prediction, Data mining, Software quality, Software security},
location = {Istanbul, Turkey},
series = {ICEMIS '15}
}

@inproceedings{10.1145/3377811.3380344,
author = {Brindescu, Caius and Ahmed, Iftekhar and Leano, Rafael and Sarma, Anita},
title = {Planning for untangling: predicting the difficulty of merge conflicts},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380344},
doi = {10.1145/3377811.3380344},
abstract = {Merge conflicts are inevitable in collaborative software development and are disruptive. When they occur, developers have to stop their current work, understand the conflict and the surrounding code, and plan an appropriate resolution. However, not all conflicts are equally problematic---some can be easily fixed, while others might be complicated enough to need multiple people. Currently, there is not much support to help developers plan their conflict resolution. In this work, we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution. The ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts. In this work, we investigate the characteristics of difficult merge conflicts, and automatically classify them. We analyzed 6,380 conflicts across 128 java projects and found that merge conflict difficulty can be accurately predicted (AUC of 0.76) through machine learning algorithms, such as bagging.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {801–811},
numpages = {11},
keywords = {empirical analysis, merge conflict difficulty prediction, merge conflict resolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3340422.3343639,
author = {Kumar, Lov and Hota, Chinmay and Mahindru, Arvind and Neti, Lalita Bhanu Murthy},
title = {Android Malware Prediction Using Extreme Learning Machine with Different Kernel Functions},
year = {2019},
isbn = {9781450368490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340422.3343639},
doi = {10.1145/3340422.3343639},
abstract = {Android is currently the most popular smartphone platform which occupied 88% of global sale by the end of 2nd quarter 2018. With the popularity of these applications, it is also inviting cybercriminals to develop malware application for accessing important information from smartphones. The major objective of cybercriminals to develop Malware apps or Malicious apps to threaten the organization privacy data, user privacy data, and device integrity. Early identification of such malware apps can help the android user to save private data and device integrity. In this study, features extracted from intermediate code representations obtained using decompilation of APK file are used for providing requisite input data to develop the models for predicting android malware applications. These models are trained using extreme learning with multiple kernel functions ans also compared with the model trained using most frequently used classifiers like linear regression, decision tree, polynomial regression, and logistic regression. This paper also focuses on the effectiveness of data sampling techniques for balancing data and feature selection methods for selecting right sets of significant uncorrelated metrics. The high-value of accuracy and AUC confirm the predicting capability of data sampling, sets of metrics, and training algorithms to malware and normal applications.},
booktitle = {Proceedings of the 15th Asian Internet Engineering Conference},
pages = {33–40},
numpages = {8},
keywords = {Artificial neural network, Genetics algorithm, Maintainability, Object-Oriented Metrics, Parallel Computing},
location = {Phuket, Thailand},
series = {AINTEC '19}
}

@article{10.1016/j.infsof.2009.04.018,
author = {German, Daniel M. and Hassan, Ahmed E. and Robles, Gregorio},
title = {Change impact graphs: Determining the impact of prior codechanges},
year = {2009},
issue_date = {October, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.018},
doi = {10.1016/j.infsof.2009.04.018},
abstract = {The source code of a software system is in constant change. The impact of these changes spreads out across the software system and may lead to the sudden manifestation of failures in unchanged parts. To help developers fix such failures, we propose a method that, in a pre-processing stage, analyzes prior code changes to determine what functions have been modified. Next, given a particular period of time in the past, the functions changed during that period are propagated throughout the rest of the system using the dependence graph of the system. This information is visualized using Change Impact Graphs (CIGs). Through a case study based on the Apache Web Server, we demonstrate the benefit of using CIGs to investigate several real defects.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1394–1408},
numpages = {15},
keywords = {Change impact graph, Code changes, Defect detection, Software evolution}
}

@inproceedings{10.1145/1985441.1985456,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Comparing fine-grained source code changes and code churn for bug prediction},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985456},
doi = {10.1145/1985441.1985456},
abstract = {A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {83–92},
numpages = {10},
keywords = {code churn, nonlinear regression, prediction models, software bugs, source code changes},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@article{10.1007/s11219-021-09568-9,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Copula-based software metrics aggregation},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09568-9},
doi = {10.1007/s11219-021-09568-9},
abstract = {A quality model is a conceptual decomposition of an abstract notion of quality into relevant, possibly conflicting characteristics and further into measurable metrics. For quality assessment and decision making, metrics values are aggregated to characteristics and ultimately to quality scores. Aggregation has often been problematic as quality models do not provide the semantics of aggregation. This makes it hard to formally reason about metrics, characteristics, and quality. We argue that aggregation needs to be interpretable and mathematically well defined in order to assess, to compare, and to improve quality. To address this challenge, we propose a probabilistic approach to aggregation and define quality scores based on joint distributions of absolute metrics values. To evaluate the proposed approach and its implementation under realistic conditions, we conduct empirical studies on bug prediction of ca. 5000 software classes, maintainability of ca. 15000 open-source software systems, and on the information quality of ca. 100000 real-world technical documents. We found that our approach is feasible, accurate, and scalable in performance.},
journal = {Software Quality Journal},
month = dec,
pages = {863–899},
numpages = {37},
keywords = {Quality assessment, Quantitative methods, Software metrics, Aggregation, Multivariate statistical methods, Probabilistic models, Copula}
}

@article{10.1007/s10664-021-10026-0,
author = {Silva, Camila Costa and Galster, Matthias and Gilson, Fabian},
title = {Topic modeling in software engineering research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10026-0},
doi = {10.1007/s10664-021-10026-0},
abstract = {Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {62},
keywords = {Topic modeling, Text mining, Natural language processing, Literature analysis}
}

@inproceedings{10.1145/3387904.3389273,
author = {Terragni, Valerio and Salza, Pasquale and Pezz\`{e}, Mauro},
title = {Measuring Software Testability Modulo Test Quality},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389273},
doi = {10.1145/3387904.3389273},
abstract = {Comprehending the degree to which software components support testing is important to accurately schedule testing activities, train developers, and plan effective refactoring actions. Software testability estimates such property by relating code characteristics to the test effort. The main studies of testability reported in the literature investigate the relation between class metrics and test effort in terms of the size and complexity of the associated test suites. They report a moderate correlation of some class metrics to test-effort metrics, but suffer from two main limitations: (i) the results hardly generalize due to the small empirical evidence (datasets with no more than eight software projects); and (ii) mostly ignore the quality of the tests. However, considering the quality of the tests is important. Indeed, a class may have a low test effort because the associated tests are of poor quality, and not because the class is easier to test. In this paper, we propose an approach to measure testability that normalizes the test effort with respect to the test quality, which we quantify in terms of code coverage and mutation score. We present the results of a set of experiments on a dataset of 9,861 Java classes, belonging to 1,186 open source projects, with around 1.5 million of lines of code overall. The results confirm that normalizing the test effort with respect to the test quality largely improves the correlation between class metrics and the test effort. Better correlations result in better prediction power and thus better prediction of the test effort.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {241–251},
numpages = {11},
keywords = {Software Metrics, Software Testability, Test Effort, Test Quality},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1109/MSR.2019.00054,
author = {Montandon, Jo\~{a}o Eduardo and Silva, Luciana Lourdes and Valente, Marco Tulio},
title = {Identifying experts in software libraries and frameworks among GitHub users},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00054},
doi = {10.1109/MSR.2019.00054},
abstract = {Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {276–287},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1007/978-3-319-25945-1_9,
author = {Altinger, Harald and Herbold, Steffen and Grabowski, Jens and Wotawa, Franz},
title = {Novel Insights on Cross Project Fault Prediction Applied to Automotive Software},
year = {2015},
isbn = {9783319259444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-25945-1_9},
doi = {10.1007/978-3-319-25945-1_9},
abstract = {Defect prediction is a powerful tool that greatly helps focusing quality assurance efforts during development. In the case of the availability of fault data from a particular context, there are different ways of using such fault predictions in practice. Companies like Google, Bell Labs and Cisco make use of fault prediction, whereas its use within automotive industry has not yet gained a lot of attraction, although, modern cars require a huge amount of software to operate. In this paper, we want to contribute the adoption of fault prediction techniques for automotive software projects. Hereby we rely on a publicly available data set comprising fault data from three automotive software projects. When learning a fault prediction model from the data of one particular project, we achieve a remarkably high and nearly perfect prediction performance for the same project. However, when applying a cross-project prediction we obtain rather poor results. These results are rather surprising, because of the fact that the underlying projects are as similar as two distinct projects can possibly be within a certain application context. Therefore we investigate the reasons behind this observation through correlation and factor analyses techniques. We further report the obtained findings and discuss the consequences for future applications of Cross-Project Fault Prediction CPFP in the domain of automotive software.},
booktitle = {Proceedings of the 27th IFIP WG 6.1 International Conference on Testing Software and Systems - Volume 9447},
pages = {141–157},
numpages = {17},
keywords = {Automotive, Cross project fault prediction, Principal component analysis, Project fault prediction},
location = {Sharjah and Dubai, United Arab Emirates},
series = {ICTSS 2015}
}

@inproceedings{10.5555/3291291.3291293,
author = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Is it a bug or an enhancement? a text-based approach to classify change requests},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as "bug" for lack of a better classification support or of knowledge about the possible kinds.This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities.We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and JBoss indicate that issues can be classified with between 77% and 82% of correct decisions.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {2–16},
numpages = {15},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3387904.3389281,
author = {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan and Zhang, Shikun},
title = {Exploiting Code Knowledge Graph for Bug Localization via Bi-directional Attention},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389281},
doi = {10.1145/3387904.3389281},
abstract = {Bug localization automatic localize relevant source files given a natural language description of bug within a software project. For a large project containing hundreds and thousands of source files, developers need cost lots of time to understand bug reports generated by quality assurance and localize these buggy source files. Traditional methods are heavily depending on the information retrieval technologies which rank the similarity between source files and bug reports in lexical level. Recently, deep learning based models are used to extract semantic information of code with significant improvements for bug localization. However, programming language is a highly structural and logical language, which contains various relations within and cross source files. Thus, we propose KGBugLocator to utilize knowledge graph embeddings to extract these interrelations of code, and a keywords supervised bi-directional attention mechanism regularize model with interactive information between source files and bug reports. With extensive experiments on four different projects, we prove our model can reach the new the-state-of-art(SOTA) for bug localization.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {219–229},
numpages = {11},
keywords = {bug localization, code representation, deep learning, knowledge graph},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3293882.3338984,
author = {Grano, Giovanni},
title = {A new dimension of test quality: assessing and generating higher quality unit test cases},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338984},
doi = {10.1145/3293882.3338984},
abstract = {Unit tests form the first defensive line against the introduction of bugs in software systems. Therefore, their quality is of a paramount importance to produce robust and reliable software. To assess test quality, many organizations relies on metrics like code and mutation coverage. However, they are not always optimal to fulfill such a purpose. In my research, I want to make mutation testing scalable by devising a lightweight approach to estimate test effectiveness. Moreover, I plan to introduce a new metric measuring test focus—as a proxy for the effort needed by developers to understand and maintain a test— that both complements code coverage to assess test quality and can be used to drive automated test case generation of higher quality tests.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {419–423},
numpages = {5},
keywords = {Automated Testing, Software Testing, Test Quality},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3387904.3389253,
author = {Chen, Zhifei and Li, Yanhui and Chen, Bihuan and Ma, Wanwangying and Chen, Lin and Xu, Baowen},
title = {An Empirical Study on Dynamic Typing Related Practices in Python Systems},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389253},
doi = {10.1145/3387904.3389253},
abstract = {The dynamic typing discipline of Python allows developers to program at a high level of abstraction. However, type related bugs are commonly encountered in Python systems due to the lack of type declaration and static type checking. Especially, the misuse of dynamic typing discipline produces underlying bugs and increases maintenance efforts. In this paper, we introduce six types of dynamic typing related practices in Python programs, which are the common but potentially risky usage of dynamic typing discipline by developers. We also implement a tool named PYDYPE to detect them. Based on this tool, we conduct an empirical study on nine real-world Python systems (with the size of more than 460KLOC) to understand dynamic typing related practices. We investigate how widespread the dynamic typing related practices are, why they are introduced into the systems, whether their usage correlates with increased likelihood of bug occurring, and how developers fix dynamic typing related bugs. The results show that: (1) dynamic typing related practices exist inconsistently in different systems and Inconsistent Variable Types is most prevalent; (2) they are introduced into systems mainly during early development phase to promote development efficiency; (3) they have a significant positive correlation with bug occurring; (4) developers tend to add type checks or exception handling to fix dynamic typing related bugs. These results benefit future research in coding convention, language design, bug detection and fixing.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {83–93},
numpages = {11},
keywords = {Python, dynamic typing, empirical study},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.1007/s10664-017-9537-x,
author = {Trautsch, Fabian and Herbold, Steffen and Makedonski, Philip and Grabowski, Jens},
title = {Addressing problems with replicability and validity of repository mining studies through a smart data platform},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9537-x},
doi = {10.1007/s10664-017-9537-x},
abstract = {The usage of empirical methods has grown common in software engineering. This trend spawned hundreds of publications, whose results are helping to understand and improve the software development process. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the replicability and validity of approaches. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Furthermore, many studies use small data sets, which comprise of less than 10 projects. This poses a threat especially to the external validity of these studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created SmartSHARK, which implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of it and the mentioned problems. Additionally, we show how we have addressed the issues that we have identified during our work with SmartSHARK.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {1036–1083},
numpages = {48},
keywords = {Replicability, Smart data platform, Software analytics, Software mining, Validity}
}

@article{10.1016/j.ins.2017.09.006,
author = {Liu, Yong and Li, Zheng and Zhao, Ruilian and Gong, Pei},
title = {An optimal mutation execution strategy for cost reduction of mutation-based fault localization},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {422},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2017.09.006},
doi = {10.1016/j.ins.2017.09.006},
abstract = {Identifying faulty program entities has been recognized as one of the most expensive, tedious and time-consuming processes in software debugging activity. Fault localization techniques are designed to assist developers in locating faults by giving a ranking of the probability that program entities incur failures. Mutation-based fault localization (MBFL) is a recently proposed fault localization approach via mutation analysis, which uses the location of mutants to identify the faulty statements. With improved effectiveness, the MBFL also brings huge execution cost. To reduce the execution cost of MBFL technique, this paper proposes a dynamic mutation execution strategy (DMES) to prioritize the execution on both mutants and test cases. As fewer mutants and test cases are executed with DMES, the whole process will become faster and the cost will be decreased. At the same time, it is proved that the fault localization accuracy of MBFL with DMES is the same as that of the original MBFL. Furthermore, this paper discusses the lowest mutation execution cost of MBFL in theory and gives a quick solution to compute the theoretical minimal mutation execution cost of MBFL in the case of keeping fault localization accuracy non-decreasing. The empirical studies show that MBFL with the dynamic strategy proposed in this paper can reduce mutant-test execution cost by 32.487% and is close to the theoretical optimal cost. Furthermore, the additional run time required by utilizing our strategy is minimum and can be ignored.},
journal = {Inf. Sci.},
month = jan,
pages = {572–596},
numpages = {25},
keywords = {Dynamic mutation execution strategy, Mutation cost reduction, Mutation-based fault localization, Software debugging}
}

@inproceedings{10.5555/2819009.2819039,
author = {Theisen, Christopher and Herzig, Kim and Morrison, Patrick and Murphy, Brendan and Williams, Laurie},
title = {Approximating attack surfaces with stack traces},
year = {2015},
publisher = {IEEE Press},
abstract = {Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {199–208},
numpages = {10},
keywords = {attack surface, models, reliability, security, stack traces, testing, vulnerability},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1007/s10664-019-09694-w,
author = {Pascarella, Luca and Bruntink, Magiel and Bacchelli, Alberto},
title = {Classifying code comments in Java software systems},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09694-w},
doi = {10.1007/s10664-019-09694-w},
abstract = {Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how 14 diverse Java open and closed source software projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments; subsequently, we investigate how often each category occur by manually classifying more than 40,000 lines of code comments from the aforementioned projects. In addition, we investigate how to automatically classify code comments at line level into our taxonomy using machine learning; initial results are promising and suggest that an accurate classification is within reach, even when training the machine learner on projects different than the target one. Data and Materials [https://doi.org/10.5281/zenodo.2628361].},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1499–1537},
numpages = {39},
keywords = {Code comment classification, Code comments usage, Dataset}
}

@inproceedings{10.1145/3368089.3409693,
author = {P\^{a}rundefinedachi, Profir-Petru and Dash, Santanu Kumar and Allamanis, Miltiadis and Barr, Earl T.},
title = {Flexeme: untangling commits using lexical flows},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409693},
doi = {10.1145/3368089.3409693},
abstract = {Today, most developers bundle changes into commits that they submit to a shared code repository. Tangled commits intermix distinct concerns, such as a bug fix and a new feature. They cause issues for developers, reviewers, and researchers alike: they restrict the usability of tools such as git bisect, make patch comprehension more difficult, and force researchers who mine software repositories to contend with noise. We present a novel data structure, the 𝛿-NFG, a multiversion Program Dependency Graph augmented with name flows. A 𝛿-NFG directly and simultaneously encodes different program versions, thereby capturing commits, and annotates data flow edges with the names/lexemes that flow across them. Our technique, Flexeme, builds a 𝛿-NFG from commits, then applies Agglomerative Clustering using Graph Similarity to that 𝛿-NFG to untangle its commits. At the untangling task on a C# corpus, our implementation, Heddle, improves the state-of-the-art on accuracy by 0.14, achieving 0.81, in a fraction of the time: Heddle is 32 times faster than the previous state-of-the-art.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {63–74},
numpages = {12},
keywords = {clustering, commint untangling, graph kernels},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2970276.2970359,
author = {Wen, Ming and Wu, Rongxin and Cheung, Shing-Chi},
title = {Locus: locating bugs from software changes},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970359},
doi = {10.1145/2970276.2970359},
abstract = {Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {262–273},
numpages = {12},
keywords = {bug localization, information retrieval, software analytics, software changes},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/2901739.2901753,
author = {Trautsch, Fabian and Herbold, Steffen and Makedonski, Philip and Grabowski, Jens},
title = {Adressing problems with external validity of repository mining studies through a smart data platform},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901753},
doi = {10.1145/2901739.2901753},
abstract = {Research in software repository mining has grown considerably the last decade. Due to the data-driven nature of this venue of investigation, we identified several problems within the current state-of-the-art that pose a threat to the external validity of results. The heavy re-use of data sets in many studies may invalidate the results in case problems with the data itself are identified. Moreover, for many studies data and/or the implementations are not available, which hinders a replication of the results and, thereby, decreases the comparability between studies. Even if all information about the studies is available, the diversity of the used tooling can make their replication even then very hard. Within this paper, we discuss a potential solution to these problems through a cloud-based platform that integrates data collection and analytics. We created the prototype SmartSHARK that implements our approach. Using SmartSHARK, we collected data from several projects and created different analytic examples. Within this article, we present SmartSHARK and discuss our experiences regarding the use of SmartSHARK and the mentioned problems.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {97–108},
numpages = {12},
keywords = {smart data, software analytics, software mining},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3338906.3338935,
author = {Koyuncu, Anil and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and Le Traon, Yves},
title = {iFixR: bug report driven program repair},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338935},
doi = {10.1145/3338906.3338935},
abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {314–325},
numpages = {12},
keywords = {Information retrieval, automatic patch generation, fault localization},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3194104.3194112,
author = {Di Nucci, Dario and Palomba, Fabio and De Lucia, Andrea},
title = {Evaluating the adaptive selection of classifiers for cross-project bug prediction},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194112},
doi = {10.1145/3194104.3194112},
abstract = {Bug prediction models are used to locate source code elements more likely to be defective. One of the key factors influencing their performances is related to the selection of a machine learning method (a.k.a., classifier) to use when discriminating buggy and non-buggy classes. Given the high complementarity of stand-alone classifiers, a recent trend is the definition of ensemble techniques, which try to effectively combine the predictions of different stand-alone machine learners. In a recent work we proposed ASCI, a technique that dynamically selects the right classifier to use based on the characteristics of the class on which the prediction has to be done. We tested it in a within-project scenario, showing its higher accuracy with respect to the Validation and Voting strategy. In this paper, we continue on the line of research, by (i) evaluating ASCI in a global and local cross-project setting and (ii) comparing its performances with those achieved by a stand-alone and an ensemble baselines, namely Naive Bayes and Validation and Voting, respectively. A key finding of our study shows that ASCI is able to perform better than the other techniques in the context of cross-project bug prediction. Moreover, despite local learning is not able to improve the performances of the corresponding models in most cases, it is able to improve the robustness of the models relying on ASCI.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {48–54},
numpages = {7},
keywords = {bug prediction, cross-project, ensemble classifiers},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@inproceedings{10.1007/11767077_6,
author = {Raza, Aoun and Vogel, Gunther and Pl\"{o}dereder, Erhard},
title = {Bauhaus: a tool suite for program analysis and reverse engineering},
year = {2006},
isbn = {3540346635},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767077_6},
doi = {10.1007/11767077_6},
abstract = {The maintenance and evolution of critical software with high requirements for reliability is an extremely demanding, time consuming and expensive task. Errors introduced by ad-hoc changes might have disastrous effects on the system and must be prevented under all circumstances, which requires the understanding of the details of source code and system design. This paper describes Bauhaus, a comprehensive tool suite that supports program understanding and reverse engineering on all layers of abstraction, from source code to architecture.},
booktitle = {Proceedings of the 11th Ada-Europe International Conference on Reliable Software Technologies},
pages = {71–82},
numpages = {12},
location = {Porto, Portugal},
series = {Ada-Europe'06}
}

@article{10.1016/j.infsof.2019.106203,
author = {Raki\'{c}, Gordana and T\'{o}th, Melinda and Budimac, Zoran},
title = {Toward recursion aware complexity metrics},
year = {2020},
issue_date = {Feb 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {118},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106203},
doi = {10.1016/j.infsof.2019.106203},
journal = {Inf. Softw. Technol.},
month = feb,
numpages = {20},
keywords = {Software maintainability, Source code readability, Source code comprehension, Debugging, Source code complexity, Complexity metrics}
}

@inproceedings{10.5555/2486788.2486929,
author = {Femmer, Henning and Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David},
title = {Detecting inconsistencies in wrappers: a case study},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Exchangeability between software components such as operating systems, middleware, databases, and hardware components is a common requirement in many software systems. One way to enable exchangeability is to promote indirect use through a common interface and an implementation for each component that wraps the original component. As developers use the interface instead of the underlying component, they assume that the software system will behave in a specific way independently of the actual component in use. However, differences in the implementations of the wrappers may lead to different behavior when one component is changed for another, which might lead to failures in the field. This work reports on a simple, yet effective approach to detect these differences. The approach is based on tool-supported reviews leveraging lightweight static analysis and machine learning. The approach is evaluated in a case study that analyzes NASAs Operating System Abstraction Layer (OSAL), which is used in various space missions. We detected 84 corner-case issues of which 57 turned out to be bugs that could have resulted in runtime failures.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1022–1031},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1007/s10664-012-9219-7,
author = {Thomas, Stephen W. and Hemmati, Hadi and Hassan, Ahmed E. and Blostein, Dorothea},
title = {Static test case prioritization using topic models},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9219-7},
doi = {10.1007/s10664-012-9219-7},
abstract = {Software development teams use test suites to test changes to their source code. In many situations, the test suites are so large that executing every test for every source code change is infeasible, due to time and resource constraints. Development teams need to prioritize their test suite so that as many distinct faults as possible are detected early in the execution of the test suite. We consider the problem of static black-box test case prioritization (TCP), where test suites are prioritized without the availability of the source code of the system under test (SUT). We propose a new static black-box TCP technique that represents test cases using a previously unused data source in the test suite: the linguistic data of the test cases, i.e., their identifier names, comments, and string literals. Our technique applies a text analysis algorithm called topic modeling to the linguistic data to approximate the functionality of each test case, allowing our technique to give high priority to test cases that test different functionalities of the SUT. We compare our proposed technique with existing static black-box TCP techniques in a case study of multiple real-world open source systems: several versions of Apache Ant and Apache Derby. We find that our static black-box TCP technique outperforms existing static black-box TCP techniques, and has comparable or better performance than two existing execution-based TCP techniques. Static black-box TCP methods are widely applicable because the only input they require is the source code of the test cases themselves. This contrasts  with other TCP techniques which require access to the SUT runtime behavior, to the SUT specification models, or to the SUT source code.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {182–212},
numpages = {31},
keywords = {Test case prioritization, Testing and debugging, Topic models}
}

@article{10.1007/s10115-018-1241-7,
author = {Ndenga, Malanga Kennedy and Ganchev, Ivaylo and Mehat, Jean and Wabwoba, Franklin and Akdag, Herman},
title = {Performance and cost-effectiveness of change burst metrics in predicting software faults},
year = {2019},
issue_date = {July      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {60},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1241-7},
doi = {10.1007/s10115-018-1241-7},
abstract = {The purpose of this study is to determine a type of software metric at file level exhibiting the best prediction performance. Studies have shown that software process metrics are better predictors of software faults than software product metrics. However, there is need for a specific software process metric which can guarantee the best fault prediction performances consistently across different experimental contexts. We collected software metrics data from Open Source Software projects. We used logistic regression and linear regression algorithms to predict bug status and number of bugs corresponding to a file, respectively. The prediction performance of these models was evaluated against numerical and graphical prediction model performance measures. We found that change burst metrics exhibit the best numerical performance measures and have the highest fault detection probability and least cost of misclassification of software components.},
journal = {Knowl. Inf. Syst.},
month = jul,
pages = {275–302},
numpages = {28},
keywords = {Change burst, Cost of misclassification, Performance measures, Software faults, Software process metrics}
}

@inproceedings{10.1145/3236024.3236065,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
title = {Improving IR-based bug localization with context-aware query reformulation},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236065},
doi = {10.1145/3236024.3236065},
abstract = {Recent findings suggest that Information Retrieval (IR)-based bug localization techniques do not perform well if the bug report lacks rich structured information (e.g., relevant program entity names). Conversely, excessive structured information (e.g., stack traces) in the bug report might not always help the automated localization either. In this paper, we propose a novel technique--BLIZZARD-- that automatically localizes buggy entities from project source using appropriate query reformulation and effective information retrieval. In particular, our technique determines whether there are excessive program entities or not in a bug report (query), and then applies appropriate reformulations to the query for bug localization. Experiments using 5,139 bug reports show that our technique can localize the buggy source documents with 7%--56% higher Hit@10, 6%--62% higher MAP@10 and 6%--62% higher MRR@10 than the baseline technique. Comparison with the state-of-the-art techniques and their variants report that our technique can improve 19% in MAP@10 and 20% in MRR@10 over the state-of-the-art, and can improve 59% of the noisy queries and 39% of the poor queries.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {621–632},
numpages = {12},
keywords = {Debugging automation, bug localization, bug report quality, graph-based term weighting, information retrieval, query reformulation},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1007/s10664-018-9672-z,
author = {Chaparro, Oscar and Florez, Juan Manuel and Marcus, Andrian},
title = {Using bug descriptions to reformulate queries during text-retrieval-based bug localization},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9672-z},
doi = {10.1007/s10664-018-9672-z},
abstract = {Text Retrieval (TR)-based approaches for bug localization rely on formulating an initial query based on the full text of a bug report. When the query fails to retrieve the buggy code artifacts, developers can reformulate the query and retrieve more candidate code documents. Existing research on query reformulation focuses mostly on leveraging relevance feedback from the user or on expanding the original query with additional information. We hypothesize that the title of the bug reports, the observed behavior, expected behavior, steps to reproduce, and code snippets provided by the users in bug descriptions, contain the most relevant information for retrieving the buggy code artifacts, and that other parts of the descriptions contain more irrelevant terms, which hinder retrieval. This paper proposes and evaluates a set of query reformulation strategies based on the selection of existing information in bug descriptions, and the removal of irrelevant parts from the original query. The results show that selecting the bug report title and the observed behavior is the strategy that performs best across various TR-based bug localization approaches and code granularities, as it leads to retrieving the buggy code artifacts within the top-N results for 25.6% more queries (on average) than without query reformulation. This strategy is highly applicable and consistent across different thresholds N. Selecting the steps to reproduce or the expected behavior (when provided in the bug reports) along with the bug title and the observed behavior leads to higher performance (i.e., between 31.4% and 41.7% more queries) and comparable consistency, yet it is applicable in fewer cases. These reformulation strategies are easy to use and are independent of the underlying retrieval technique.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2947–3007},
numpages = {61},
keywords = {Bug descriptions, Query reformulation, Bug localization, Text retrieval}
}

@article{10.4018/ijossp.2014040101,
author = {Syeed, M.M. Mahbubul and Hammouda, Imed and Syst\"{a}, Tarja},
title = {Prediction Models and Techniques for Open Source Software Projects: A Systematic Literature Review},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/ijossp.2014040101},
doi = {10.4018/ijossp.2014040101},
abstract = {Open Source Software OSS is currently a widely adopted approach to developing and distributing software. For effective adoption of OSS, fundamental knowledge of project development is needed. This often calls for reliable prediction models to simulate project evolution and to envision project future. These models provide help in supporting preventive maintenance and building quality software. This paper reports on a systematic literature survey aimed at the identification and structuring of research that offer prediction models and techniques in analyzing OSS projects. In this review, we systematically selected and reviewed 52 peer reviewed articles that were published between January, 2000 and March, 2013. The study outcome provides insight in what constitutes the main contributions of the field, identifies gaps and opportunities, and distills several important future research directions.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {1–39},
numpages = {39},
keywords = {Fault Prediction, OSS Community, Open Source Software, Prediction, Systematic Literature Review}
}

@inproceedings{10.1145/3341105.3373914,
author = {Muslija, Adnan and Enoiu, Eduard},
title = {On the measurement of software complexity for plc industrial control systems using TIQVA},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373914},
doi = {10.1145/3341105.3373914},
abstract = {In the safety-critical domain (e.g. transportation, nuclear, aerospace and automotive), large-scale embedded systems implemented using Programmable Logic Controllers (PLCs) are widely used to provide supervisory control. Software complexity metrics, such as code size and cyclomatic complexity, have been used in the software engineering community for predicting quality metrics such as maintainability, bug proneness and robustness. However, since there is no available approach and tool support for measuring software complexity of PLC programs, we developed a tool called TIQVA in an effort to measure complexity for this type of software. We show how to measure different software complexity metrics such as lines of code, cyclomatic complexity, and information flow for a popular PLC programming language named Function Block Diagram (FBD). We evaluate the tool using data provided by Bombardier Transportation from a Train Control Management System (TCMS). In addition, we report some empirical and industrial evidence showing how TIQVA can be used to provide some experimental evidence to support the use of these metrics to estimate testing effort for an industrial control software. The results from this evaluation indicate that other specific dimensions of PLC programs (e.g., function block relationships, block coupling and timing) could be used to improve the measurement of complexity for industrial embedded software.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1556–1565},
numpages = {10},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1007/s00500-016-2284-x,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of some software fault prediction techniques for the number of faults prediction},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {24},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2284-x},
doi = {10.1007/s00500-016-2284-x},
abstract = {During the software development process, prediction of the number of faults in software modules can be more helpful instead of predicting the modules being faulty or non-faulty. Such an approach may help in more focused software testing process and may enhance the reliability of the software system. Most of the earlier works on software fault prediction have used classification techniques for classifying software modules into faulty or non-faulty categories. The techniques such as Poisson regression, negative binomial regression, genetic programming, decision tree regression, and multilayer perceptron can be used for the prediction of the number of faults. In this paper, we present an experimental study to evaluate and compare the capability of six fault prediction techniques such as genetic programming, multilayer perceptron, linear regression, decision tree regression, zero-inflated Poisson regression, and negative binomial regression for the prediction of number of faults. The experimental investigation is carried out for eighteen software project datasets collected from the PROMISE data repository. The results of the investigation are evaluated using average absolute error, average relative error, measure of completeness, and prediction at level l measures. We also perform Kruskal---Wallis test and Dunn's multiple comparison test to compare the relative performance of the considered fault prediction techniques.},
journal = {Soft Comput.},
month = dec,
pages = {7417–7434},
numpages = {18},
keywords = {Dunn's multiple comparison test, Genetic programming, Kruskal---Wallis test, Multilayer perceptron, Software fault prediction, Zero-inflated Poisson regression}
}

@article{10.1007/s10796-013-9438-5,
author = {Carrera, \'{A}lvaro and Iglesias, Carlos A. and Garijo, Mercedes},
title = {Beast methodology: An agile testing methodology for multi-agent systems based on behaviour driven development},
year = {2014},
issue_date = {April     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {2},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-013-9438-5},
doi = {10.1007/s10796-013-9438-5},
abstract = {This paper presents a testing methodology to apply Behaviour Driven Development (BDD) techniques while developing Multi-Agent Systems (MASs), termed BEhavioural Agent Simple Testing (BEAST) Methodology. This methodology is supported by the open source framework (BEAST Tool) which automatically generates test cases skeletons from BDD scenarios specifications. The developed framework allows the testing of MASs based on JADE or JADEX platforms. In addition, this framework offers a set of configurable Mock Agents with the aim of being able to execute tests while the MAS is under development. The BEAST Methodology presents transparent traceability from user requirements to test cases. Thus, the stakeholders can be aware of the project status. The methodology and the associated tool have been validated in the development of a MAS for fault diagnosis in FTTH (Fiber To The Home) networks. The results have been measured in quantifiable way obtaining a reduction of the tests implementation time.},
journal = {Information Systems Frontiers},
month = apr,
pages = {169–182},
numpages = {14},
keywords = {Agile, Behaviour-driven development, Methodology, Mock-agents, Multi-agent systems, Test}
}

@inproceedings{10.1145/3194095.3194100,
author = {Flynn, Lori and Snavely, William and Svoboda, David and VanHoudnos, Nathan and Qin, Richard and Burns, Jennifer and Zubrow, David and Stoddard, Robert and Marce-Santurio, Guillermo},
title = {Prioritizing alerts from multiple static analysis tools, using classification models},
year = {2018},
isbn = {9781450357371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194095.3194100},
doi = {10.1145/3194095.3194100},
abstract = {Static analysis (SA) tools examine code for flaws without executing the code, and produce warnings ("alerts") about possible flaws. A human auditor then evaluates the validity of the purported code flaws. The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project's budget and schedule. An alert triaging tool enables strategically prioritizing alerts for examination, and could use classifier confidence. We developed and tested classification models that predict if static analysis alerts are true or false positives, using a novel combination of multiple static analysis tools, features from the alerts, alert fusion, code base metrics, and archived audit determinations. We developed classifiers using a partition of the data, then evaluated the performance of the classifier using standard measurements, including specificity, sensitivity, and accuracy. Test results and overall data analysis show accurate classifiers were developed, and specifically using multiple SA tools increased classifier accuracy, but labeled data for many types of flaws were inadequately represented (if at all) in the archive data, resulting in poor predictive accuracy for many of those flaws.},
booktitle = {Proceedings of the 1st International Workshop on Software Qualities and Their Dependencies},
pages = {13–20},
numpages = {8},
keywords = {accurate, alert, classification, rapid, static analysis},
location = {Gothenburg, Sweden},
series = {SQUADE '18}
}

@inproceedings{10.1145/3379597.3387474,
author = {Akbar, Shayan A. and Kak, Avinash C.},
title = {A Large-Scale Comparative Evaluation of IR-Based Tools for Bug Localization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387474},
doi = {10.1145/3379597.3387474},
abstract = {This paper reports on a large-scale comparative evaluation of IR-based tools for automatic bug localization. We have divided the tools in our evaluation into the following three generations: (1) The first-generation tools, now over a decade old, that are based purely on the Bag-of-Words (BoW) modeling of software libraries. (2) The somewhat more recent second-generation tools that augment BoW-based modeling with two additional pieces of information: historical data, such as change history, and structured information such as class names, method names, etc. And, finally, (3) The third-generation tools that are currently the focus of much research and that also exploit proximity, order, and semantic relationships between the terms. It is important to realize that the original authors of all these three generations of tools have mostly tested them on relatively small-sized datasets that typically consisted no more than a few thousand bug reports. Additionally, those evaluations only involved Java code libraries. The goal of the present paper is to present a comprehensive large-scale evaluation of all three generations of bug-localization tools with code libraries in multiple languages. Our study involves over 20,000 bug reports drawn from a diverse collection of Java, C/C++, and Python projects. Our results show that the third-generation tools are significantly superior to the older tools. We also show that the word embeddings generated using code files written in one language are effective for retrieval from code libraries in other languages.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {21–31},
numpages = {11},
keywords = {bug localization, information retrieval, source code search, word embeddings},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/1463788.1463819,
author = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Is it a bug or an enhancement? a text-based approach to classify change requests},
year = {2008},
isbn = {9781450378826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1463788.1463819},
doi = {10.1145/1463788.1463819},
abstract = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as "bug" for lack of a better classification support or of knowledge about the possible kinds.This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities.We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and JBoss indicate that issues can be classified with between 77% and 82% of correct decisions.},
booktitle = {Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds},
articleno = {23},
numpages = {15},
location = {Ontario, Canada},
series = {CASCON '08}
}

@inproceedings{10.1145/3180155.3180212,
author = {Hora, Andre and Silva, Danilo and Valente, Marco Tulio and Robbes, Romain},
title = {Assessing the threat of untracked changes in software evolution},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180212},
doi = {10.1145/3180155.3180212},
abstract = {While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1102–1113},
numpages = {12},
keywords = {mining software repositories, refactoring, software evolution},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2791060.2791093,
author = {Souto, Sabrina and Gopinath, Divya and d'Amorim, Marcelo and Marinov, Darko and Khurshid, Sarfraz and Batory, Don},
title = {Faster bug detection for software product lines with incomplete feature models},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791093},
doi = {10.1145/2791060.2791093},
abstract = {A software product line (SPL) is a family of programs that are differentiated by features --- increments in functionality. Systematically testing an SPL is challenging because it requires running each test of a test suite against a combinatorial number of programs. Feature models capture dependencies among features and can (1) reduce the space of programs to test and (2) enable accurate categorization of failing tests as failures of programs or the tests themselves, not as failures due to illegal combinations of features. In practice, sadly, feature models are not always available.We introduce SPLif, the first approach for testing SPLs that does not require the a priori availability of feature models. Our insight is to use a profile of passing and failing test runs to quickly identify failures that are indicative of real problems in test or code rather than specious failures due to illegal feature combinations.Experimental results on five SPLs and one large configurable system (GCC) demonstrate the effectiveness of our approach. SPLif enabled the discovery of five news bugs in GCC, three of which have already been fixed.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {151–160},
numpages = {10},
keywords = {GCC, feature models, software testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2972206.2972222,
author = {Hirzel, Matthias and Brachth\"{a}user, Jonathan Immanuel and Klaeren, Herbert},
title = {Prioritizing Regression Tests for Desktop and Web-Applications based on the Execution Frequency of Modified Code},
year = {2016},
isbn = {9781450341356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972206.2972222},
doi = {10.1145/2972206.2972222},
abstract = {Regression testing can be very time expensive when running all available test cases. Test prioritization seeks to find faults early by reordering tests. Existing techniques decide in which order tests should be run based on coverage data, knowledge of code changes, historical data of prior test execution or a combination of them. Others postpone tests if similar ones are already selected for early execution. However, these approaches do not take into account that tests which appear similar still might explore different parts of the application's state space and thus can result in different test outcome. Approaches based on structural coverage or on historical data might ignore small tests focusing on behavior that rarely changes. In this paper, we present a novel prioritization technique that is based on the frequencies with which modified code parts are executed by the tests. Our technique assumes that multiple executions of a modified code part (under different contexts) have a higher chance to reveal faults than a single execution of this code. For this purpose, we use both the output of regression test selection as well as test traces obtained during test development. We propose multiple variants of our technique, including a feedback mechanism to optimize the prioritization order dynamically, and compare them in an evaluation of Java-based applications to existing approaches using the standard APFD metric. The results show that our technique is highly competitive.},
booktitle = {Proceedings of the 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
articleno = {11},
numpages = {12},
keywords = {Empirical study, Execution frequency, Fault detection effectiveness, Regression testing, Test case prioritization},
location = {Lugano, Switzerland},
series = {PPPJ '16}
}

@article{10.1007/s10664-020-09841-8,
author = {Rahman, Akond and Farhana, Effat and Williams, Laurie},
title = {The ‘as code’ activities: development anti-patterns for infrastructure as code},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09841-8},
doi = {10.1007/s10664-020-09841-8},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3430–3467},
numpages = {38},
keywords = {Anti-pattern, Bugs, Configuration script, Continuous deployment, Defect, Devops, Infrastructure as code, Practice, Puppet, Quality}
}

@inproceedings{10.1145/3213846.3213856,
author = {Lee, Jaekwon and Kim, Dongsun and Bissyand\'{e}, Tegawend\'{e} F. and Jung, Woosung and Le Traon, Yves},
title = {Bench4BL: reproducibility study on the performance of IR-based bug localization},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213856},
doi = {10.1145/3213846.3213856},
abstract = {In recent years, the use of Information Retrieval (IR) techniques to automate the localization of buggy files, given a bug report, has shown promising results. The abundance of approaches in the literature, however, contrasts with the reality of IR-based bug localization (IRBL) adoption by developers (or even by the research community to complement other research approaches). Presumably, this situation is due to the lack of comprehensive evaluations for state-of-the-art approaches which offer insights into the actual performance of the techniques.  We report on a comprehensive reproduction study of six state-of-the-art IRBL techniques. This study applies not only subjects used in existing studies (old subjects) but also 46 new subjects (61,431 Java files and 9,459 bug reports) to the IRBL techniques. In addition, the study compares two different version matching (between bug reports and source code files) strategies to highlight some observations related to performance deterioration. We also vary test file inclusion to investigate the effectiveness of IRBL techniques on test files, or its noise impact on performance. Finally, we assess potential performance gain if duplicate bug reports are leveraged.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {61–72},
numpages = {12},
keywords = {Reproducibility studies, bug localization, information retrieval},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@article{10.1007/s11390-020-0047-8,
author = {Lee, Jung-Been and Lee, Taek and In, Hoh Peter},
title = {Topic Modeling Based Warning Prioritization from Change Sets of Software Repository},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0047-8},
doi = {10.1007/s11390-020-0047-8},
abstract = {Many existing warning prioritization techniques seek to reorder the static analysis warnings such that true positives are provided first. However, excessive amount of time is required therein to investigate and fix prioritized warnings because some are not actually true positives or are irrelevant to the code context and topic. In this paper, we propose a warning prioritization technique that reflects various latent topics from bug-related code blocks. Our main aim is to build a prioritization model that comprises separate warning priorities depending on the topic of the change sets to identify the number of true positive warnings. For the performance evaluation of the proposed model, we employ a performance metric called warning detection rate, widely used in many warning prioritization studies, and compare the proposed model with other competitive techniques. Additionally, the effectiveness of our model is verified via the application of our technique to eight industrial projects of a real global company.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1461–1479},
numpages = {19},
keywords = {automated static analysis, topic modeling, warning prioritization}
}

@inproceedings{10.1145/2723742.2723759,
author = {Shobe, Joseph F. and Karim, Md Yasser and Kagdi, Huzefa},
title = {How Often does a Source Code Unit Change within a Release Window?},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723759},
doi = {10.1145/2723742.2723759},
abstract = {To form a training set for a source-code change prediction model, e.g., using the association rule mining or machine learning techniques, commits from the source code history are needed. The traceability between releases and commits would facilitate a systematic choice of history in units of the project evolution scale (i.e., commits that constitute a software release). For example, the major release 25.0 in Chrome is mapped to the earliest revision 157687 and latest revision 165096 in the trunk. Using this traceability, an empirical study is reported on the frequency distribution of file changes for different release windows. In Chrome, the majority (50%) of the committed files change only once between a pair of consecutive releases. This trend is reversed after expanding the window size to at least 10. That is, the majority (50%) of the files change multiple times when commits constituting 10 or greater releases are considered. These results suggest that a training set of at least 10 releases is needed to provide a prediction coverage for majority of the files.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {166–175},
numpages = {10},
keywords = {Commit History, Empirical Studies, Mining Software Repositories, Software Releases},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/2647508.2647527,
author = {Hirzel, Matthias},
title = {Selective regression testing for web applications created with google web toolkit},
year = {2014},
isbn = {9781450329262},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647508.2647527},
doi = {10.1145/2647508.2647527},
abstract = {Today's web applications are highly dynamic and powerful software components that may change often. Mostly, they are based on JavaScript or AJAX. A common way to ensure correct behaviour is to use selective regression tests. Nevertheless, especially on the client side, testing is hard. One way to ease the development and the testing process of dynamic web applications is to use the Google Web Toolkit (GWT). This framework enables the development in Java and transfers the code via a compiler into JavaScript. However, it does not support regression testing with test selection. As far as we know, this paper presents the first selective regression testing technique for GWT-based web applications. In order to determine test cases that have to be rerun, it compares the Java code of two versions of the application, localizes and classifies changes in the code, and traces the mapping of Java source code to JavaScript code. We have implemented our technique as a prototype Eclipse plug-in and have conducted an evaluation of the tool.},
booktitle = {Proceedings of the 2014 International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
pages = {110–121},
numpages = {12},
keywords = {control flow graph, debugging, fault localization, google web toolkit, selective regression testing, web applications},
location = {Cracow, Poland},
series = {PPPJ '14}
}

@inproceedings{10.5555/3291291.3291310,
author = {Barrak, Amine and Laverdi\`{e}re, Marc-Andr\'{e} and Khomh, Foutse and An, Le and Merlo, Ettore},
title = {Just-in-time detection of protection-impacting changes on WordPress and MediaWiki},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Access control mechanisms based on roles and privileges restrict the access of users to security sensitive resources in a multi-user software system. Unintentional privilege protection changes may occur during the evolution of a system, which may introduce security vulnerabilities; threatening user's confidential data, and causing other severe problems. In this paper, we use the Pattern Traversal Flow Analysis technique to identify definite protection differences in WordPress and MediaWiki systems. We analyse the evolution of privilege protections across 211 and 193 releases from respectively WordPress and Mediawiki, and observe that around 60% of commits affect privileges protections in both projects. We refer to these commits as protection-impacting change (PIC) commits. To help developers identify PIC commits just-in-time, we extract a series of metrics from commit logs and source code, and build statistical models. The evaluation of these models revealed that they can achieve a precision up to 73.8% and a recall up to 98.8% in WordPress and for MediaWiki, a precision up to 77.2% and recall up to 97.8%. Among the metrics examined, commit churn, bug fixing, author experiences and code complexity between two releases are the most important predictors in the models. We performed a qualitative analysis of false positives and false negatives and observe that PIC commits detectors should ignore documentation-only commits and process code changes without the comments.Software organizations can use our proposed approach and models, to identify unintentional privilege protection changes as soon as they are introduced, in order to prevent the introduction of vulnerabilities in their systems.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {178–188},
numpages = {11},
keywords = {privilege protection changes, protection impacting changes, reliability, security vulnerabilities},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.5555/978-3-030-87007-2_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {i–xl},
location = {Cagliari, Italy}
}

@article{10.1007/s10664-021-09978-0,
author = {Di Sorbo, Andrea and Panichella, Sebastiano},
title = {Exposed! A case study on the vulnerability-proneness of Google Play Apps},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09978-0},
doi = {10.1007/s10664-021-09978-0},
abstract = {Mobile applications are used for accomplishing everyday life activities, such as shopping, banking, and social communications. To leverage the features of mobile apps, users often need to share sensitive information. However, recent research demonstrated that most of such apps present critical security and privacy defects. In this context, we define as vulnerability-proneness the risk level(s) that users meet in downloading specific apps, to better understand whether (1) users select apps with lower risk levels and if (2) vulnerability-proneness of an app might affect its success. We use as proxy to measure such risk level the “number of different types of potential security issues exhibited by the app”. We conjecture that the vulnerability-proneness levels may vary based on (i) the types of data handled by the app, and (ii) the operations for which the app is supposed to be used. Hence, we investigate how the vulnerability-proneness of apps varies when observing (i) different app categories, and (ii) apps with different success levels. Finally, to increase the awareness of both users and developers on the vulnerability-proneness of apps, we evaluate the extent to which contextual information provided by the app market can be exploited to estimate the vulnerability-proneness levels of mobile apps. Results of our study show that apps in the Medical category exhibit the lowest levels of vulnerability-proneness. Besides, while no strong relations between vulnerability-proneness and average rating are observed, apps with a higher number of downloads tend to have higher vulnerability-proneness levels, but lower vulnerability-proneness density. Finally, we found that apps’ contextual information can be used to predict, in the early stages, the vulnerability-proneness levels of mobile apps.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {31},
keywords = {Mobile applications, Vulnerability-proneness, Empirical study}
}

@article{10.1007/s10664-015-9402-8,
author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
title = {A survey on the use of topic models when mining software repositories},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9402-8},
doi = {10.1007/s10664-015-9402-8},
abstract = {Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {1843–1919},
numpages = {77},
keywords = {LDA, LSI, Survey, Topic modeling}
}

@inproceedings{10.1145/2483760.2483781,
author = {Devaki, Pranavadatta and Thummalapenta, Suresh and Singhania, Nimit and Sinha, Saurabh},
title = {Efficient and flexible GUI test execution via test merging},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483781},
doi = {10.1145/2483760.2483781},
abstract = {As a test suite evolves, it can accumulate redundant tests. To address this problem, many test-suite reduction techniques, based on different measures of redundancy, have been developed. A more subtle problem, that can also cause test-suite bloat and that has not been addressed by existing research, is the accumulation of similar tests. Similar tests are not redundant by any measure; but, they contain many common actions that are executed repeatedly, which over a large test suite, can degrade execution time substantially.  We present a test merging technique for GUI tests. Given a test suite, the technique identifies the tests that can be merged and creates a merged test, which covers all the application states that are exercised individually by the tests, but with the redundant common steps executed only once. The key novelty in the merging technique is that it compares the dynamic states induced by the tests to identify a semantically meaningful interleaving of steps from different tests. The technique not only improves the efficiency of test execution, but also ensures that there is no loss in the fault-revealing power of the original tests. In the empirical studies, conducted using four open-source web applications and one proprietary enterprise web application, in which over $3300$ test cases and 19600 test steps were analyzed, the technique reduced the number of test steps by 29% and the test-execution time by 39%.},
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {34–44},
numpages = {11},
keywords = {Test-suite reduction, dynamic analysis, test merging},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.1145/2884781.2884803,
author = {M\"{u}ller, Sebastian C. and Fritz, Thomas},
title = {Using (bio)metrics to predict code quality online},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884803},
doi = {10.1145/2884781.2884803},
abstract = {Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the difficulty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository.In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classifiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {452–463},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2810146.2810152,
author = {An, Le and Khomh, Foutse},
title = {An Empirical Study of Crash-inducing Commits in Mozilla Firefox},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810152},
doi = {10.1145/2810146.2810152},
abstract = {Software crashes are feared by software organisations and end users. Many software organisations have embedded automatic crash reporting tools in their software systems to help development teams track and fix crash-related bugs. Previous techniques, which focus on the triaging of crash-types and crash-related bugs, can help software practitioners increase their debugging efficiency on crashes. But, these techniques can only be applied after the crashes occurred and already affected a large population of users. To help software organisations detect and address crash-prone code early, we conduct a case study of commits that would lead to crashes, called "crash-inducing commits", in Mozilla Firefox. We found that crash-inducing commits are often submitted by developers with less experience. Developers perform more addition and deletion of lines of code in crash-inducing commits. We built predictive models to help software practitioners detect and fix crash-prone bugs early on. Our predictive models achieve a precision of 61.4% and a recall of 95.0%. Software organisations can use our proposed predictive models to track and fix crash-prone commits early on before they negatively impact users; increasing bug fixing efficiency and user-perceived quality.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {5},
numpages = {10},
keywords = {Crash analysis, bug triaging, mining software repositories, prediction model},
location = {Beijing, China},
series = {PROMISE '15}
}

@article{10.1007/s10664-013-9292-6,
author = {Bettenburg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E.},
title = {Towards improving statistical modeling of software engineering data: think locally, act globally!},
year = {2015},
issue_date = {April     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9292-6},
doi = {10.1007/s10664-013-9292-6},
abstract = {Much research in software engineering (SE) is focused on modeling data collected from software repositories. Insights gained over the last decade suggests that such datasets contain a high amount of variability in the data. Such variability has a detrimental effect on model quality, as suggested by recent research. In this paper, we propose to split the data into smaller homogeneous subsets and learn sets of individual statistical models, one for each subset, as a way around the high variability in such data. Our case study on a variety of SE datasets demonstrates that such local models can significantly outperform traditional models with respect to model fit and predictive performance. However, we find that analysts need to be aware of potential pitfalls when building local models: firstly, the choice of clustering algorithm and its parameters can have a substantial impact on model quality. Secondly, the data being modeled needs to have enough variability to take full advantage of local modeling. For example, our case study on social data shows no advantage of local over global modeling, as clustering fails to derive appropriate subsets. Lastly, the interpretation of local models can become very complex when there is a large number of variables or data subsets. Overall, we find that a hybrid approach between local and traditional global modeling, such as Multivariate Adaptive Regression Splines (MARS) combines the best of both worlds. MARS models are non-parametric and thus do not require prior calibration of parameters, are easily interpretable by analysts and outperform local, as well as traditional models out of the box in four out of five datasets in our case study.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {294–335},
numpages = {42},
keywords = {Clustering, Software metrics, Statistical modeling}
}

@inproceedings{10.1145/2901739.2901740,
author = {Guo, Jin and Rahimi, Mona and Cleland-Huang, Jane and Rasin, Alexander and Hayes, Jane Huffman and Vierhauser, Michael},
title = {Cold-start software analytics},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901740},
doi = {10.1145/2901739.2901740},
abstract = {Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as "who is the expert?," "which classes are fault prone?," or even "who are the domain experts for these fault-prone classes?" Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {142–153},
numpages = {12},
keywords = {cold-start, configuration, software analytics},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/2639490.2639505,
author = {Wiese, Igor Scaliante and C\^{o}go, Filipe Roseiro and R\'{e}, Reginaldo and Steinmacher, Igor and Gerosa, Marco Aur\'{e}lio},
title = {Social metrics included in prediction models on software engineering: a mapping study},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639505},
doi = {10.1145/2639490.2639505},
abstract = {Context: Previous work that used prediction models on Software Engineering included few social metrics as predictors, even though many researchers argue that Software Engineering is a social activity. Even when social metrics were considered, they were classified as part of other dimensions, such as process, history, or change. Moreover, few papers report the individual effects of social metrics. Thus, it is not clear yet which social metrics are used in prediction models and what are the results of their use in different contexts. Objective: To identify, characterize, and classify social metrics included in prediction models reported in the literature. Method: We conducted a mapping study (MS) using a snowballing citation analysis. We built an initial seed list adapting strings of two previous systematic reviews on software prediction models. After that, we conducted backward and forward citation analysis using the initial seed list. Finally, we visited the profile of each distinct author identified in the previous steps and contacted each author that published more than 2 papers to ask for additional candidate studies. Results: We identified 48 primary studies and 51 social metrics. We organized the metrics into nine categories, which were divided into three groups - communication, project, and commit-related. We also mapped the applications of each group of metrics, indicating their positive or negative effects. Conclusions: This mapping may support researchers and practitioners to build their prediction models considering more social metrics.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {72–81},
numpages = {10},
keywords = {mapping study, prediction models, social metrics, social network analysis},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1007/978-3-319-30806-7_12,
author = {Dashevskyi, Stanislav and Brucker, Achim D. and Massacci, Fabio},
title = {On the Security Cost of Using a Free and Open Source Component in a Proprietary Product},
year = {2016},
isbn = {9783319308050},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-30806-7_12},
doi = {10.1007/978-3-319-30806-7_12},
abstract = {The work presented in this paper is motivated by the need to estimate the security effort of consuming Free and Open Source Software FOSS components within a proprietary software supply chain of a large European software vendor. To this extent we have identified three different cost models: centralized the company checks each component and propagates changes to the different product groups, distributed each product group is in charge of evaluating and fixing its consumed FOSS components, and hybrid only the least used components are checked individually by each development team. We investigated publicly available factors e.\"{\i} \'{z}g., development activity such as commits, code size, or fraction of code size in different programming languages to identify which one has the major impact on the security effort of using a FOSS component in a larger software product.},
booktitle = {Proceedings of the 8th International Symposium on Engineering Secure Software and Systems - Volume 9639},
pages = {190–206},
numpages = {17},
keywords = {Free and open source software usage, Free and open source software vulnerabilities, Security maintenance costs},
location = {London, UK},
series = {ESSoS 2016}
}

@article{10.1007/s10664-019-09704-x,
author = {German, Daniel M. and Adams, Bram and Stewart, Kate},
title = {cregit: Token-level blame information in git version control repositories},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09704-x},
doi = {10.1007/s10664-019-09704-x},
abstract = {The blame feature of version control systems is widely used--both by practitioners and researchers--to determine who has last modified a given line of code, and the commit where this contribution was made. The main disadvantage of blame is that, when a line is modified several times, it only shows the last commit that modified it--occluding previous changes to other areas of the same line. In this paper, we developed a method to increase the granularity of blame in git: instead of tracking lines of code, this method is capable of tracking tokens in source code. We evaluate its effectiveness with an empirical study in which we compare the accuracy of blame in git (per line) with our proposed blame-per-token method. We demonstrate that, in 5 large open source systems, blame-per-token is capable of properly identifying the commit that introduced a token with an accuracy between 94.5% and 99.2%, while blame-per-line can only achieve an accuracy between 75% and 91% (with a margin of error of +/-5% and a confidence interval of 95%). We also classify the reasons why either blame method fails, highlighting each method's weaknesses. The blame-per-token method has been implemented in an open source tool called cregit, which is currently in use by the Linux Foundation to identify the persons who have contributed to the source code of the Linux kernel.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2725–2763},
numpages = {39}
}

@inproceedings{10.1145/2652524.2652535,
author = {Herzig, Kim and Nagappan, Nachiappan},
title = {The impact of test ownership and team structure on the reliability and effectiveness of quality test runs},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652535},
doi = {10.1145/2652524.2652535},
abstract = {Context: Software testing is a crucial step in most software development processes. Testing software is a key component to manage and assess the risk of shipping quality products to customers. But testing is also an expensive process and changes to the system need to be tested thoroughly which may take time. Thus, the quality of a software product depends on the quality of its underlying testing process and on the effectiveness and reliability of individual test cases.Goal: In this paper, we investigate the impact of the organizational structure of test owners on the reliability and effectiveness of the corresponding test cases. Prior empirical research on organizational structure has focused only on developer activity. We expand the scope of empirical knowledge by assessing the impact of organizational structure on testing activities.Method: We performed an empirical study on the Windows build verification test suites (BVT) and relate effectiveness and reliability measures of each test run to the complexity and size of the organizational sub-structure that enclose all owners of test cases executed.Results: Our results show, that organizational structure impacts both test effectiveness and test execution reliability. We are also able to predict effectiveness and reliability with fairly high precision and recall values.Conclusion: We suggest to review test suites with respect to their organizational composition. As indicated by the results of this study, this would increase the effectiveness and reliability, development speed and developer satisfaction.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {2},
numpages = {10},
keywords = {effectiveness, empirical software engineering, organizational structure, reliability, software testing},
location = {Torino, Italy},
series = {ESEM '14}
}

@article{10.1007/s10664-019-09735-4,
author = {Brindescu, Caius and Ahmed, Iftekhar and Jensen, Carlos and Sarma, Anita},
title = {An empirical investigation into merge conflicts and their effect on software quality},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09735-4},
doi = {10.1007/s10664-019-09735-4},
abstract = {Merge conflicts are known to cause extra effort for developers, but little is known about their effect on software. While some research has been done, many questions remain. To better understand merge conflicts and their impact we performed an empirical study about the types, frequency, and impact of merge conflicts, where impact is measured in terms of bug fixing commits associated with conflicts. We analyzed 143 open source projects and found that almost 1 in 5 merges cause conflicts. In 75.23% of these cases, a developer needed to reflect on the program logic to resolve it. We also found that the code associated with a merge conflict is twice as likely to have a bug. When the code associated with merge conflicts require manual intervention, the code is 26\texttimes{} more likely to have a bug.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {562–590},
numpages = {29},
keywords = {Version control, Software merging, Merge conflicts, Software quality, Empirical study, Mining software repositories}
}

@inproceedings{10.1145/2499393.2499404,
author = {Lochmann, Klaus and Ramadani, Jasmin and Wagner, Stefan},
title = {Are comprehensive quality models necessary for evaluating software quality?},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499404},
doi = {10.1145/2499393.2499404},
abstract = {The concept of software quality is very complex and has many facets. Reflecting all these facets and at the same time measuring everything related to these facets results in comprehensive but large quality models and extensive measurements. In contrast, there are also many smaller, focused quality models claiming to evaluate quality with few measures.We investigate if and to what extent it is possible to build a focused quality model with similar evaluation results as a comprehensive quality model but with far less measures needed to be collected and, hence, reduced effort. We make quality evaluations with the comprehensive Quamoco base quality model and build focused quality models based on the same set of measures and data from over 2,000 open source systems. We analyse the ability of the focused model to predict the results of the Quamoco model by comparing them with a random predictor as a baseline. We calculate the standardised accuracy measure SA and effect sizes.We found that for the Quamoco model and its 378 automatically collected measures, we can build a focused model with only 10 measures but an accuracy of 61% and a medium to high effect size. We conclude that we can build focused quality models to get an impression of a system's quality similar to comprehensive models. However, when including manually collected measures, the accuracy of the models stayed below 50%. Hence, manual measures seem to have a high impact and should therefore not be ignored in a focused model.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {3},
numpages = {9},
keywords = {quality evaluation, quality model, software quality},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@inproceedings{10.1145/2972958.2972966,
author = {Borges, Hudson and Hora, Andre and Valente, Marco Tulio},
title = {Predicting the Popularity of GitHub Repositories},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972966},
doi = {10.1145/2972958.2972966},
abstract = {GitHub is the largest source code repository in the world. It provides a git-based source code management platform and also many features inspired by social networks. For example, GitHub users can show appreciation to projects by adding stars to them. Therefore, the number of stars of a repository is a direct measure of its popularity. In this paper, we use multiple linear regressions to predict the number of stars of GitHub repositories. These predictions are useful both to repository owners and clients, who usually want to know how their projects are performing in a competitive open source development market. In a large-scale analysis, we show that the proposed models start to provide accurate predictions after being trained with the number of stars received in the last six months. Furthermore, specific models---generated using data from repositories that share the same growth trends---are recommended for repositories with slow growth and/or for repositories with less stars. Finally, we evaluate the ability to predict not the number of stars of a repository but its rank among the GitHub repositories. We found a very strong correlation between predicted and real rankings (Spearman's rho greater than 0.95).},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {9},
numpages = {10},
keywords = {GitHub, Open Source Development, Popularity, Prediction Models, Social Coding},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@article{10.1007/s10664-013-9285-5,
author = {Lucia, Andrea and Penta, Massimiliano and Oliveto, Rocco and Panichella, Annibale and Panichella, Sebastiano},
title = {Labeling source code with information retrieval methods: an empirical study},
year = {2014},
issue_date = {October   2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9285-5},
doi = {10.1007/s10664-013-9285-5},
abstract = {To support program comprehension, software artifacts can be labeled--for example within software visualization tools--with a set of representative words, hereby referred to as labels. Such labels can be obtained using various approaches, including Information Retrieval (IR) methods or other simple heuristics. They provide a bird-eye's view of the source code, allowing developers to look over software components fast and make more informed decisions on which parts of the source code they need to analyze in detail. However, few empirical studies have been conducted to verify whether the extracted labels make sense to software developers. This paper investigates (i) to what extent various IR techniques and other simple heuristics overlap with (and differ from) labeling performed by humans; (ii) what kinds of source code terms do humans use when labeling software artifacts; and (iii) what factors--in particular what characteristics of the artifacts to be labeled--influence the performance of automatic labeling techniques. We conducted two experiments in which we asked a group of students (38 in total) to label 20 classes from two Java software systems, JHotDraw and eXVantage. Then, we analyzed to what extent the words identified with an automated technique--including Vector Space Models, Latent Semantic Indexing (LSI), latent Dirichlet allocation (LDA), as well as customized heuristics extracting words from specific source code elements--overlap with those identified by humans. Results indicate that, in most cases, simpler automatic labeling techniques--based on the use of words extracted from class and method names as well as from class comments--better reflect human-based labeling. Indeed, clustering-based approaches (LSI and LDA) are more worthwhile to be used for source code artifacts having a high verbosity, as well as for artifacts requiring more effort to be manually labeled. The obtained results help to define guidelines on how to build effective automatic labeling techniques, and provide some insights on the actual usefulness of automatic labeling techniques during program comprehension tasks.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {1383–1420},
numpages = {38},
keywords = {Empirical studies, Information retrieval, Program comprehension, Software artifact labeling}
}

@article{10.1023/A:1020511004267,
author = {Khoshgoftaar, Taghi M. and Yuan, Xiaojing and Allen, Edward B. and Jones, Wendell D. and Hudepohl, John P.},
title = {Uncertain Classification of Fault-Prone Software Modules},
year = {2002},
issue_date = {December 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {7},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1020511004267},
doi = {10.1023/A:1020511004267},
abstract = {Many development organizations try to minimize faults in software as a means for improving customer satisfaction. Assuring high software quality often entails time-consuming and costly development processes. A software quality model based on software metrics can be used to guide enhancement efforts by predicting which modules are fault-prone. This paper presents statistical techniques to determine which predictions by a classification tree should be considered uncertain. We conducted a case study of a large legacy telecommunications system. One release was the basis for the training dataset, and the subsequent release was the basis for the evaluation dataset. We built a classification tree using the TREEDISC algorithm, which is based on χ2 tests of contingency tables. The model predicted whether a module was likely to have faults discovered by customers, or not, based on software product, process, and execution metrics. We simulated practical use of the model by classifying the modules in the evaluation dataset. The model achieved useful accuracy, in spite of the very small proportion of fault-prone modules in the system. We assessed whether the classes assigned to the leaves were appropriate by statistical tests, and found sizable subsets of modules with uncertain classification. Discovering which modules have uncertain classifications allows sophisticated enhancement strategies to resolve uncertainties. Moreover, TREEDISC is especially well suited to identifying uncertain classifications.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {297–318},
numpages = {22},
keywords = {CHAID, Software metrics, TREEDISC, classification trees, fault-prone modules, software quality, telecommunications}
}

@article{10.1016/j.websem.2010.04.009,
author = {Tappolet, Jonas and Kiefer, Christoph and Bernstein, Abraham},
title = {Semantic web enabled software analysis},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {2–3},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2010.04.009},
doi = {10.1016/j.websem.2010.04.009},
abstract = {One of the most important decisions researchers face when analyzing software systems is the choice of a proper data analysis/exchange format. In this paper, we present EvoOnt, a set of software ontologies and data exchange formats based on OWL. EvoOnt models software design, release history information, and bug-tracking meta-data. Since OWL describes the semantics of the data, EvoOnt (1) is easily extendible, (2) can be processed with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The contribution of this paper is that it introduces a novel software evolution ontology that vastly simplifies typical software evolution analysis tasks. In detail, we show the usefulness of EvoOnt by repeating selected software evolution and analysis experiments from the 2004-2007 Mining Software Repositories Workshops (MSR). We demonstrate that if the data used for analysis were available in EvoOnt then the analyses in 75% of the papers at MSR could be reduced to one or at most two simple queries within off-the-shelf SPARQL tools. In addition, we present how the inherent capabilities of the Semantic Web have the potential of enabling new tasks that have not yet been addressed by software evolution researchers, e.g., due to the complexities of the data integration.},
journal = {Web Semant.},
month = jul,
pages = {225–240},
numpages = {16},
keywords = {Bug prediction, Software comprehension framework, Software evolution, Software release similarity}
}

@article{10.1145/308769.308790,
author = {Wahl, Nancy J.},
title = {An overview of regression testing},
year = {1999},
issue_date = {Jan. 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/308769.308790},
doi = {10.1145/308769.308790},
abstract = {Regression testing is an important part of the software development life cycle. Many articles have been published lately detailing the different approaches. This article is an overview of regression testing in the following areas: types of regression testing; unit, integration and system level testing, regression testing of global variables, regression testing of object-oriented software, comparisons of selective regression techniques, and cost comparisons of the types of regression testing.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {69–73},
numpages = {5}
}

@article{10.1007/s10664-016-9427-7,
author = {\'{O} Cinn\'{e}ide, Mel and Hemati Moghadam, Iman and Harman, Mark and Counsell, Steve and Tratt, Laurence},
title = {An experimental search-based approach to cohesion metric evaluation},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9427-7},
doi = {10.1007/s10664-016-9427-7},
abstract = {In spite of several decades of software metrics research and practice, there is little understanding of how software metrics relate to one another, nor is there any established methodology for comparing them. We propose a novel experimental technique, based on search-based refactoring, to `animate' metrics and observe their behaviour in a practical setting. Our aim is to promote metrics to the level of active, opinionated objects that can be compared experimentally to uncover where they conflict, and to understand better the underlying cause of the conflict. Our experimental approaches include semi-random refactoring, refactoring for increased metric agreement/disagreement, refactoring to increase/decrease the gap between a pair of metrics, and targeted hypothesis testing. We apply our approach to five popular cohesion metrics using ten real-world Java systems, involving 330,000 lines of code and the application of over 78,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in a remarkable 55 % of cases, that Low-level Similarity-based Class Cohesion (LSCC) is the best representative of the set of metrics we investigate while Sensitive Class Cohesion (SCOM) is the least representative, and we discover several hitherto unknown differences between the examined metrics. We also use our approach to investigate the impact of including inheritance in a cohesion metric definition and find that doing so dramatically changes the metric.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {292–329},
numpages = {38},
keywords = {Empirical studies, Refactoring, Software metrics}
}

@inproceedings{10.5555/227726.227759,
author = {Ogasawara, Hideto and Yamada, Atsushi and Kojo, Michiko},
title = {Experiences of software quality management using metrics through the life-cycle},
year = {1996},
isbn = {0818672463},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Many software quality metrics to objectively grasp software products and process have been proposed in the past decades. In actual projects, quality metrics has been widely applied to manage software quality. However, there are still several problems with providing effective feedback to intermediate software products and the software development process. We have proposed a software quality management using quality metrics which are easily and automatically measured. The purpose of this proposal is to establish a method for building in software quality by regularly measuring and reviewing. The paper outlines a model for building in software quality using quality metrics, and describes examples of its application to actual projects and its results. As the results, it was found that quality metrics can be used to detect and remove problems with process and products in each phase. Regular technical reviews using quality metrics and information on the change of the regularly measured results was also found to have a positive influence on the structure and module size of programs. Further, in the test phase, it was found that with the proposed model, the progress of corrective action could be quickly and accurately grasped.},
booktitle = {Proceedings of the 18th International Conference on Software Engineering},
pages = {179–188},
numpages = {10},
keywords = {automatic measurement, corrective action, life-cycle, problem detection, problem removal, program testing, programs, regular technical reviews, software development management, software development process, software metrics, software products, software quality, software quality management, software quality metrics, test phase},
location = {Berlin, Germany},
series = {ICSE '96}
}

@inproceedings{10.1145/1370788.1370794,
author = {Watanabe, Shinya and Kaiya, Haruhiko and Kaijiri, Kenji},
title = {Adapting a fault prediction model to allow inter languagereuse},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370794},
doi = {10.1145/1370788.1370794},
abstract = {An important step in predicting error prone modules in a project is to construct the prediction model by using training data of that project, but the resulting prediction model depends on the training data. Therefore it is difficult to apply the model to other projects. The training data consists of metrics data and bug data, and these data should be prepared for each project. Metrics data can be computed by using metric tools, but it is not so easy to collect bug data. In this paper, we try to reuse the generated prediction model. By using the metrics and bug data which are computed from C++ and Java projects, we have evaluated the possibility of applying the prediction model, which is generated based on one project, to other projects, and have proposed compensation techniques for applying to other projects. We showed the evaluation result based on open source projects.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {19–24},
numpages = {6},
keywords = {error prone, inter language prediction, metrics, open source},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@article{10.1007/s10664-013-9298-0,
author = {Robbes, Romain and R\"{o}thlisberger, David and Tanter, \'{E}ric},
title = {Object-oriented software extensions in practice},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9298-0},
doi = {10.1007/s10664-013-9298-0},
abstract = {As software evolves, data types have to be extended, possibly with new data variants or new operations. Object-oriented design is well-known to support data extensions well. In fact, most popular books showcase data extensions to illustrate how objects adequately support software evolution. Conversely, operation extensions are typically better supported by a functional design. A large body of programming language research has been devoted to the challenge of properly supporting both kinds of extensions. While this challenge is well-known from a language design standpoint, it has not been studied empirically. We perform such a study on a large sample of Smalltalk projects (over half a billion lines of code) and their evolution over more than 130,000 committed changes. Our study of extensions during software evolution finds that extensions are indeed prevalent evolution tasks, and that both kinds of extensions are equally common in object-oriented software. We also discuss findings about: the evolution of the kinds of extensions over time; the viability of the Visitor pattern as an object-oriented solution to operation extensions; the change-proneness of extensions; and the prevalence of extensions by third parties. This study suggests that object-oriented design alone is not sufficient, and that practical support for both kinds of program decomposition approaches are in fact needed, either by the programming language or by the development environment.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {745–782},
numpages = {38},
keywords = {Data extensions, Empirical studies, Mining software repositories, Object-oriented programming, Operation extensions, Software evolution}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

