@inproceedings{10.1007/978-3-319-97310-4_11,
author = {Zhenlei, Wang and Suyun, Zhao and Yangming, Liu and Hong, Chen and Cuiping, Li and Xiran, Sun},
title = {Fuzzy Rough Based Feature Selection by Using Random Sampling},
year = {2018},
isbn = {978-3-319-97309-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-97310-4_11},
doi = {10.1007/978-3-319-97310-4_11},
abstract = {Feature selection, i.e., Attribute reduction, is one of the most important applications of fuzzy rough set theory. The application of attribute reduction based on fuzzy rough set is inefficient or even unfeasible on large scale data. Considering the random sampling technique is an effective method to statistically reduce the calculation on large scale data, we introduce it into the fuzzy rough based feature selection algorithm. This paper thus proposes a random reduction algorithm based on random sampling. The main contribution of this paper is the introduction of the idea of random sampling in the selection of attributes based on minimum redundancy and maximum correlation. First, in each iteration the significance of attribute is not computed on all the objects in the whole datasets, but on part of randomly selected objects. By this way, the maximum relevant attribute is chosen on the condition of less calculation. Secondly, in the process of choosing attribute in each iteration, the sample is different so as to select the minimum redundancy attribute. Finally, the experimental results show that the reduction algorithm can obviously reduce the running time of the reduction algorithm on the condition of limited classification accuracy loss.},
booktitle = {PRICAI 2018: Trends in Artificial Intelligence: 15th Pacific Rim International Conference on Artificial Intelligence, Nanjing, China, August 28–31, 2018, Proceedings, Part II},
pages = {91–99},
numpages = {9},
keywords = {Randomly sampling, Fuzzy rough set, Attribute reduction, Maximum relevance, Minimum redundancy},
location = {Nanjing, China}
}

@article{10.1016/j.infsof.2020.106389,
author = {Chac\'{o}n-Luna, Ana Eva and Guti\'{e}rrez, Antonio Manuel and Galindo, Jos\'{e} A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Systematic literature review, Experiment, Case study, Empirical strategies, Software product lines}
}

@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {tooling roadmap, tool support, state of practice, spl in practice},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3433996.3434024,
author = {Zhao, Jiaxuan and Ji, Suqin},
title = {Clustering ensemble of massive high dimensional data based on BLB and stratified sampling framework},
year = {2020},
isbn = {9781450388641},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433996.3434024},
doi = {10.1145/3433996.3434024},
abstract = {Clustering is an important research content for data analysis. It can obtain the underlying structure of data from the original data set. Clustering on massive high-dimensional data sets is still a huge challenge. In this paper, we propose a clustering ensemble algorithm based on BLB and stratified sampling framework for massive high-dimensional data. From two aspects of sample and feature, we use the BLB (Bag of Little Bootstrap) algorithm to divide the original data set into several small-scale data subsets, then use feature stratified sampling to obtain a low-dimensional subset. we generate base clustering results on multiple small-scale low-dimensional subsets, and finally obtain a cluster integration results by link-based consensus functions. The experimental results on synthetic data sets and UCI real data sets show that the algorithm proposed in this paper is effective for clustering massive high-dimensional data.},
booktitle = {Proceedings of the 2020 Conference on Artificial Intelligence and Healthcare},
pages = {154–160},
numpages = {7},
keywords = {feature stratified sampling, consensus function, clustering integration, BLB sampling},
location = {Taiyuan, China},
series = {CAIH2020}
}

@article{10.1016/j.eswa.2021.115191,
author = {Kamalov, Firuz},
title = {Orthogonal variance decomposition based feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115191},
doi = {10.1016/j.eswa.2021.115191},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {11},
keywords = {Data mining, Wrapper methods, Total sensitivity index, Sensitivity index, Sobol decomposition, Variance decomposition, Feature selection}
}

@article{10.34768/amcs-2021-0047,
author = {Kusy, Maciej and Zajdel, Roman},
title = {A Weighted Wrapper Approach to Feature Selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Walter de Gruyter &amp; Co.},
address = {USA},
volume = {31},
number = {4},
issn = {1641-876X},
url = {https://doi.org/10.34768/amcs-2021-0047},
doi = {10.34768/amcs-2021-0047},
abstract = {This paper considers feature selection as a problem of an aggregation of three state-of-the-art filtration methods: Pearson’s linear correlation coefficient, the ReliefF algorithm and decision trees. A new wrapper method is proposed which, on the basis of a fusion of the above approaches and the performance of a classifier, is capable of creating a distinct, ordered subset of attributes that is optimal based on the criterion of the highest classification accuracy obtainable by a convolutional neural network. The introduced feature selection uses a weighted ranking criterion. In order to evaluate the effectiveness of the solution, the idea is compared with sequential feature selection methods that are widely known and used wrapper approaches. Additionally, to emphasize the need for dimensionality reduction, the results obtained on all attributes are shown. The verification of the outcomes is presented in the classification tasks of repository data sets that are characterized by a high dimensionality. The presented conclusions confirm that it is worth seeking new solutions that are able to provide a better classification result while reducing the number of input features.},
journal = {Int. J. Appl. Math. Comput. Sci.},
month = dec,
pages = {685–696},
numpages = {12},
keywords = {classification accuracy, convolutional neural network, weighted combined ranking, feature significance, wrapper approach, feature selection}
}

@article{10.1016/j.eswa.2021.115558,
author = {Nystrup, Peter and Kolm, Petter N. and Lindstr\"{o}m, Erik},
title = {Feature selection in jump models},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {184},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115558},
doi = {10.1016/j.eswa.2021.115558},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {14},
keywords = {Regime switching, Unsupervised learning, Clustering, Time series, Sequential data, High-dimensional}
}

@inproceedings{10.1145/3382026.3425774,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {A multi-company empirical evaluation of a framework that evaluates the convenience of adopting product line engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425774},
doi = {10.1145/3382026.3425774},
abstract = {Companies considering adopting a product line engineering approach should ideally analyze the pros and cons to determine the sound reasons for this decision. In order to support this analysis, in previous work we proposed the APPLIES evaluation framework. This framework provides information to evaluate the convenience of adopting a product line engineering approach.This paper presents an empirical evaluation of APPLIES. This experience includes 18 potential practitioners that used the framework to evaluate the convenience of adopting product line engineering in 19 different companies. The collected evidence was used to evaluate the perceived usefulness, intention to use and ease of use of APPLIES. The results presented increase confidence that APPLIESis a useful tool, but also identify some possibilities for improvement. In addition, four categories for classifying potential adopters of product line engineering emerged during the analysis of the results: unprepared adopter, potential adopter, ready adopter, and unmotivated adopter. These categories could be useful to classify companies that are considering adopting product line engineering.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {13–20},
numpages = {8},
keywords = {product line engineering adoption, perceived usefulness, intention to use, ease of use, Empirical evaluation},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@article{10.1016/j.asoc.2021.107625,
author = {Jiang, Zhi and Zhang, Yong and Wang, Jun},
title = {A multi-surrogate-assisted dual-layer ensemble feature selection algorithm},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {110},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107625},
doi = {10.1016/j.asoc.2021.107625},
journal = {Appl. Soft Comput.},
month = oct,
numpages = {11},
keywords = {Surrogate model, Ensemble, Feature selection, Particle swarm optimization}
}

@inproceedings{10.1007/978-3-030-89847-2_7,
author = {Zhang, Winston and Turkestani, Najla Al and Bianchi, Jonas and Le, Celia and Deleat-Besson, Romain and Ruellas, Antonio and Cevidanes, Lucia and Yatabe, Marilia and Gon\c{c}alves, Joao and Benavides, Erika and Soki, Fabiana and Prieto, Juan and Paniagua, Beatriz and Gryak, Jonathan and Najarian, Kayvan and Soroushmehr, Reza},
title = {Feature Selection for Privileged Modalities in Disease Classification},
year = {2021},
isbn = {978-3-030-89846-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89847-2_7},
doi = {10.1007/978-3-030-89847-2_7},
abstract = {Multimodal data allows supervised learning while considering multiple complementary views of a problem, improving final diagnostic performance of trained models. Data modalities that are missing or difficult to obtain in clinical situations can still be incorporated into model training using the learning using privileged information (LUPI) framework. However, noisy or redundant features in the privileged modality space can limit the amount of knowledge transferred to the diagnostic model during the LUPI learning process. We consider the problem of selecting desirable features from both standard features which are available during both model training and testing, and privileged features which are only available during model training. A novel filter feature selection method named NMIFS+ is introduced that considers redundancy between standard and privileged feature spaces. The algorithm is evaluated on two disease classification datasets with privileged modalities. Results demonstrate an improvement in diagnostic performance over comparable filter selection algorithms.},
booktitle = {Multimodal Learning for Clinical Decision Support: 11th International Workshop, ML-CDS 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings},
pages = {69–80},
numpages = {12},
keywords = {Clinical decision support, Multimodal data, Feature selection, Knowledge transfer, Mutual information, Privileged learning},
location = {Strasbourg, France}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {software product lines, machine learning, configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-47426-3_62,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {Group Based Unsupervised Feature Selection},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_62},
doi = {10.1007/978-3-030-47426-3_62},
abstract = {Unsupervised feature selection is an important task in machine learning applications, yet challenging due to the unavailability of class labels. Although a few unsupervised methods take advantage of external sources of correlations within feature groups in feature selection, they are limited to genomic data, and suffer poor accuracy because they ignore input data or encourage features from the same group. We propose a framework which facilitates unsupervised filter feature selection methods to exploit input data and feature group information simultaneously, encouraging features from different groups. We use this framework to incorporate feature group information into Laplace Score algorithm. Our method achieves high accuracy compared to other popular unsupervised feature selection methods (30% maximum improvement of Normalized Mutual Information (NMI)) with low computational costs (50 times lower than embedded methods on average). It has many real world applications, particularly the ones that use image, text and genomic data, whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {805–817},
numpages = {13},
keywords = {[inline-graphic not available: see fulltext] norm minimisation., Feature groups, Unsupervised feature selection},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3382025.3414951,
author = {Heradio, Ruben and Fernandez-Amoros, David and Galindo, Jos\'{e} A. and Benavides, David},
title = {Uniform and scalable SAT-sampling for configurable systems},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414951},
doi = {10.1145/3382025.3414951},
abstract = {Several relevant analyses on configurable software systems remain intractable because they require examining vast and highly-constrained configuration spaces. Those analyses could be addressed through statistical inference, i.e., working with a much more tractable sample that later supports generalizing the results obtained to the entire configuration space. To make this possible, the laws of statistical inference impose an indispensable requirement: each member of the population must be equally likely to be included in the sample, i.e., the sampling process needs to be "uniform". Various SAT-samplers have been developed for generating uniform random samples at a reasonable computational cost. Unfortunately, there is a lack of experimental validation over large configuration models to show whether the samplers indeed produce genuine uniform samples or not. This paper (i) presents a new statistical test to verify to what extent samplers accomplish uniformity and (ii) reports the evaluation of four state-of-the-art samplers: Spur, QuickSampler, Unigen2, and Smarch. According to our experimental results, only Spur satisfies both scalability and uniformity.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {17},
numpages = {11},
keywords = {variability modeling, uniform sampling, software product lines, configurable systems, SAT},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.5555/3546258.3546374,
author = {Hamer, Victor and Dupont, Pierre},
title = {An importance weighted feature selection stability measure},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Current feature selection methods, especially applied to high dimensional data, tend to suffer from instability since marginal modifications in the data may result in largely distinct selected feature sets. Such instability strongly limits a sound interpretation of the selected variables by domain experts. Defining an adequate stability measure is also a research question. In this work, we propose to incorporate into the stability measure the importances of the selected features in predictive models. Such feature importances are directly proportional to feature weights in a linear model. We also consider the generalization to a non-linear setting.We illustrate, theoretically and experimentally, that current stability measures are subject to undesirable behaviors, for example, when they are jointly optimized with predictive accuracy. Results on micro-array and mass-spectrometric data show that our novel stability measure corrects for overly optimistic stability estimates in such a bi-objective context, which leads to improved decision-making. It is also shown to be less prone to the underor over-estimation of the stability value in feature spaces with groups of highly correlated variables.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {116},
numpages = {57},
keywords = {feature importance, bioinformatics, bi-objective optimization, selection stability, feature selection}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3338906.3341467,
author = {Greiner, Sandra},
title = {On extending single-variant model transformations for reuse in software product line engineering},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341467},
doi = {10.1145/3338906.3341467},
abstract = {Software product line engineering (SPLE) aims at increasing productivity by following the principles of variability and organized reuse. Combining the discipline with model-driven software engineering (MDSE) seeks to intensify this effect by raising the level of abstraction. Typically, a product line developed in a model-driven way is composed of various kinds of models, like class diagrams and database schemata. To automatically generate further necessary representations from a initial (source) model, model transformations may create a respective target model. In annotative approaches to SPLE, variability annotations, which are boolean expressions over the features of the product line, state in which products a (model) element is visible. State-of-the-art single-variant model transformations (SVMT), however, do not consider variability annotations additionally associated with model elements. Thus, multi-variant model transformations (MVMT) should bridge the gap between existing SPLE and MDSE approaches by reusing already existing technology to propagate annotations additionally to the the target. The present contribution gives an overview on the research we conduct to reuse SVMTs in model-driven SPLE and provides a plan on which steps are still to be taken.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1160–1163},
numpages = {4},
keywords = {software evolution, feature propagation, annotative approach, Model-driven Software Product Line Engineering, (multi-variant) model transformations},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1007/s10515-020-00273-8,
author = {Velez, Miguel and Jamshidi, Pooyan and Sattler, Florian and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {ConfigCrusher: towards white-box performance analysis for configurable systems},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00273-8},
doi = {10.1007/s10515-020-00273-8},
abstract = {Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems.},
journal = {Automated Software Engg.},
month = dec,
pages = {265–300},
numpages = {36},
keywords = {Dynamic analysis, Static analysis, Performance analysis, Configurable systems}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3358960.3379137,
author = {Alves Pereira, Juliana and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc},
title = {Sampling Effect on Performance Prediction of Configurable Systems: A Case Study},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379137},
doi = {10.1145/3358960.3379137},
abstract = {Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single "dominant" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {277–288},
numpages = {12},
keywords = {software product lines, performance prediction, machine learning, configurable systems},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1016/j.procs.2021.08.065,
author = {Bajcsi, Ad\'{e}l and Andreica, Anca and Chira, Camelia},
title = {Towards feature selection for digital mammogram classification},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.08.065},
doi = {10.1016/j.procs.2021.08.065},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {632–641},
numpages = {10},
keywords = {MIAS, Random Forest classification, Decision Tree classification, classification, mammogram, PCA, GA feature selection, GLRLM feature extraction}
}

@inproceedings{10.1007/978-3-030-87199-4_42,
author = {Sadri, Amir Reza and Azarianpour Esfahani, Sepideh and Chirra, Prathyush and Antunes, Jacob and Pattiam Giriprakash, Pavithran and Leo, Patrick and Madabhushi, Anant and Viswanath, Satish E.},
title = {SPARTA: An Integrated Stability, Discriminability, and Sparsity Based Radiomic Feature Selection Approach},
year = {2021},
isbn = {978-3-030-87198-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87199-4_42},
doi = {10.1007/978-3-030-87199-4_42},
abstract = {In order to ensure that a radiomics-based machine learning model will robustly generalize to new, unseen data (which may harbor significant variations compared to the discovery cohort), radiomic features are often screened for stability via test/retest or cross-site evaluation. However, as stability screening is often conducted independent of the feature selection process, the resulting feature set may not be simultaneously optimized for discriminability, stability, as well as sparsity. In this work, we present a novel radiomic feature selection approach termed SPARse sTable lAsso (SPARTA), uniquely developed to identify a highly discriminative and sparse set of features which are also stable to acquisition or institution variations. The primary contribution of this work is the integration of feature stability as a generalizable regularization term into a least absolute shrinkage and selection operator (LASSO)-based optimization function. Secondly, we utilize a unique non-convex sparse relaxation approach inspired by proximal algorithms to provide a computationally efficient convergence guarantee for our novel algorithm. SPARTA was evaluated on three different multi-institutional imaging cohorts to identify the most relevant radiomic features for distinguishing: (a) healthy from diseased lesions in 147 prostate cancer patients via T2-weighted MRI, (b) healthy subjects from Crohn’s disease patients via 170 CT enterography scans, and (c) responders and non-responders to chemoradiation in 82 rectal cancer patients via T2w MRI. When compared to 3 state-of-the-art feature selection schemes, features selected via SPARTA yielded significantly higher classifier performance on unseen data in multi-institutional validation (hold-out AUCs of 0.91, 0.91, and 0.93 in the 3 cohorts).},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part III},
pages = {445–455},
numpages = {11},
keywords = {Convex optimization, Feature stability, Feature selection},
location = {Strasbourg, France}
}

@article{10.1016/j.knosys.2019.105272,
author = {Xue, Hui and Song, Yu and Xu, Hai-Ming},
title = {Multiple indefinite kernel learning for feature selection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {191},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105272},
doi = {10.1016/j.knosys.2019.105272},
journal = {Know.-Based Syst.},
month = mar,
numpages = {12},
keywords = {DC programming, Indefinite kernel, Multiple indefinite kernel learning, Feature selection}
}

@article{10.1145/3136625,
author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
title = {Feature Selection: A Data Perspective},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3136625},
doi = {10.1145/3136625},
abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {94},
numpages = {45},
keywords = {Feature selection}
}

@article{10.3233/JIFS-189876,
author = {Tripathi, Diwakar and Ramachandra Reddy, B. and Padmanabha Reddy, Y.C.A. and Shukla, Alok Kumar and Kumar, Ravi Kant and Sharma, Neeraj Kumar and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Trajkovic, Ljiljana},
title = {BAT algorithm based feature selection: Application in credit scoring},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189876},
doi = {10.3233/JIFS-189876},
abstract = {Credit scoring plays a vital role for financial institutions to estimate the risk associated with a credit applicant applied for credit product. It is estimated based on applicants’ credentials and directly affects to viability of issuing institutions. However, there may be a large number of irrelevant features in the credit scoring dataset. Due to irrelevant features, the credit scoring models may lead to poorer classification performances and higher complexity. So, by removing redundant and irrelevant features may overcome the problem with large number of features. In this work, we emphasized on the role of feature selection to enhance the predictive performance of credit scoring model. Towards to feature selection, Binary BAT optimization technique is utilized with a novel fitness function. Further, proposed approach aggregated with “Radial Basis Function Neural Network (RBFN)”, “Support Vector Machine (SVM)” and “Random Forest (RF)” for classification. Proposed approach is validated on four bench-marked credit scoring datasets obtained from UCI repository. Further, the comprehensive investigational results analysis are directed to show the comparative performance of the classification tasks with features selected by various approaches and other state-of-the-art approaches for credit scoring.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {5561–5570},
numpages = {10},
keywords = {BAT algorithm, credit score, feature selection}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Challenges, Multiple-case study, Software product lines, Variability management}
}

@article{10.1007/s10618-020-00731-7,
author = {Borboudakis, Giorgos and Tsamardinos, Ioannis},
title = {Extending greedy feature selection algorithms to multiple solutions},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-020-00731-7},
doi = {10.1007/s10618-020-00731-7},
abstract = {Most feature selection methods identify only a single solution. This is acceptable for predictive purposes, but is not sufficient for knowledge discovery if multiple solutions exist. We propose a strategy to extend a class of greedy methods to efficiently identify multiple solutions, and show under which conditions it identifies all solutions. We also introduce a taxonomy of features that takes the existence of multiple solutions into account. Furthermore, we explore different definitions of statistical equivalence of solutions, as well as methods for testing equivalence. A novel algorithm for compactly representing and visualizing multiple solutions is also introduced. In experiments we show that (a) the proposed algorithm is significantly more computationally efficient than the TIE* algorithm, the only alternative approach with similar theoretical guarantees, while identifying similar solutions to it, and (b) that the identified solutions have similar predictive performance.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1393–1434},
numpages = {42},
keywords = {Stepwise selection, Multiple feature selection, Multiple solutions, Feature selection}
}

@inproceedings{10.1145/3453800.3453811,
author = {He, Songyao and Li, Han and Guo, Qing and Yang, Fan and Lai, Yongxuan and Lin, Kaibiao},
title = {Feature weighted dual random sampling cluster Ensemble},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453811},
doi = {10.1145/3453800.3453811},
abstract = {Cluster ensemble combines multiple partitions of a set of objects into a stable and robust one. To obtain good ensemble performance, base clusterings are required to take into account quality and diversity. Recently, in spite of some researches focus on consensus function to prove ensemble quality, how to produce high-diversity and high-quality base clusterings at general step without global screening remains an open problem. For high-dimensional data, the clustering algorithm suitable for common data sets is extremely inefficient and there is basically no cluster in high-dimensional space. To get around this conundrum, subspace cluster ensemble was proposed. At present, although the random subspace cluster methods show good diversity, the quality of base clustering remains to be improved. This paper proposes an improved algorithm of dual random subspace cluster ensemble methods to ensure the high diversity of the base clustering while improving the quality at general step without global feature screening. Our method reduces the irrelevant, uninformative features of the class structure at the generation step of random subspace, making the class structure more obvious. The experiment demonstrates the effectiveness of our method.},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {54–59},
numpages = {6},
keywords = {the Mltimodality test, Subspace cluster ensemble, Group Pearson correlation},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@article{10.1016/j.procs.2020.03.228,
author = {Don, S.},
title = {Random Subset Feature Selection and Classification of Lung Sound},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.228},
doi = {10.1016/j.procs.2020.03.228},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {313–322},
numpages = {10},
keywords = {Feature Selection, Fractal Dimension, RSFS, SFS, Random Sampling, Classification}
}

@article{10.1016/j.ins.2019.02.009,
author = {Moran, Michal and Gordon, Goren},
title = {Curious Feature Selection},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {485},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.009},
doi = {10.1016/j.ins.2019.02.009},
journal = {Inf. Sci.},
month = jun,
pages = {42–54},
numpages = {13},
keywords = {Feature selection, Data science, Big data, Reinforcement learning, Curiosity loop, Intrinsic motivation learning}
}

@inproceedings{10.1145/3374135.3385309,
author = {Acharya, Deepak Bhaskar and Zhang, Huaming},
title = {Feature Selection and Extraction for Graph Neural Networks},
year = {2020},
isbn = {9781450371056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374135.3385309},
doi = {10.1145/3374135.3385309},
abstract = {Graph Neural Networks (GNNs) have been a latest hot research topic in data science, due to the fact that they use the ubiquitous data structure graphs as the underlying elements for constructing and training neural networks. In a GNN, each node has numerous features associated with it. The entire task (for example, classification, or clustering) utilizes the features of the nodes to make decisions, at node level or graph level. In this paper, (1) we extend the feature selection algorithm presented in via Gumbel Softmax to GNNs. We conduct a series of experiments on our feature selection algorithms, using various benchmark datasets: Cora, Citeseer and Pubmed. (2) We implement a mechanism to rank the extracted features. We demonstrate the effectiveness of our algorithms, for both feature selection and ranking. For the Cora dataset, (1) we use the algorithm to select 225 features out of 1433 features. Our experimental results demonstrate their effectiveness for the same classification problem. (2) We extract features such that they are linear combinations of the original features, where the coefficients for extracted features are non-negative and sum up to one. We propose an algorithm to rank the extracted features in the sense that when using them for the same classification problem, the accuracy goes down gradually for the extracted features within the rank 1 - 50, 51 - 100, 100 - 150, and 151 - 200.},
booktitle = {Proceedings of the 2020 ACM Southeast Conference},
pages = {252–255},
numpages = {4},
keywords = {Feature Extraction, Feature Selection, Graph Neural Networks, Gumbel-Softmax},
location = {Tampa, FL, USA},
series = {ACMSE '20}
}

@article{10.1016/j.asoc.2021.107729,
author = {Manikandan, G. and Abirami, S.},
title = {An efficient feature selection framework based on information theory for high dimensional data},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107729},
doi = {10.1016/j.asoc.2021.107729},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {25},
keywords = {Mutual information, High dimensional data, Bioinformatics, Microarray, Feature redundancy, Feature relevancy, Feature fusion, Feature selection}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@article{10.1016/j.inffus.2018.11.008,
author = {Bol\'{o}n-Canedo, Ver\'{o}nica and Alonso-Betanzos, Amparo},
title = {Ensembles for feature selection: A review and future trends},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.11.008},
doi = {10.1016/j.inffus.2018.11.008},
journal = {Inf. Fusion},
month = dec,
pages = {1–12},
numpages = {12},
keywords = {Feature selection, Ensemble learning}
}

@article{10.1016/j.patrec.2019.08.017,
author = {Cordeiro de Amorim, Renato},
title = {Unsupervised feature selection for large data sets},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.08.017},
doi = {10.1016/j.patrec.2019.08.017},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {183–189},
numpages = {7},
keywords = {Big data, Clustering, Unsupervised feature selection}
}

@inproceedings{10.1007/978-3-030-86523-8_25,
author = {Don\`{a}, J\'{e}r\'{e}mie and Gallinari, Patrick},
title = {Differentiable Feature Selection, A Reparameterization Approach},
year = {2021},
isbn = {978-3-030-86522-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86523-8_25},
doi = {10.1007/978-3-030-86523-8_25},
abstract = {We consider the task of feature selection for reconstruction which consists in choosing a small subset of features from which whole data instances can be reconstructed. This is of particular importance in several contexts involving for example costly physical measurements, sensor placement or information compression. To break the intrinsic combinatorial nature of this problem, we formulate the task as optimizing a binary mask distribution enabling an accurate reconstruction. We then face two main challenges. One concerns differentiability issues due to the binary distribution. The second one corresponds to the elimination of redundant information by selecting variables in a correlated fashion which requires modeling the covariance of the binary distribution. We address both issues by introducing a relaxation of the problem via a novel reparameterization of the logitNormal distribution. We demonstrate that the proposed method provides an effective exploration scheme and leads to efficient feature selection for reconstruction through evaluation on several high dimensional image benchmarks. We show that the method leverages the intrinsic geometry of the data, facilitating reconstruction. (We refer to  for a complete version of the article including proofs, thorough experimental details and results.)},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III},
pages = {414–429},
numpages = {16},
keywords = {Representation learning, Sparse methods},
location = {Bilbao, Spain}
}

@inproceedings{10.1145/3448016.3457295,
author = {Neutatz, Felix and Biessmann, Felix and Abedjan, Ziawasch},
title = {Enforcing Constraints for Machine Learning Systems via Declarative Feature Selection: An Experimental Study},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457295},
doi = {10.1145/3448016.3457295},
abstract = {Responsible usage of Machine Learning (ML) systems in practice does not only require enforcing high prediction quality, but also accounting for other constraints, such as fairness, privacy, or execution time. One way to address multiple user-specified constraints on ML systems is feature selection. Yet, optimizing feature selection strategies for multiple metrics is difficult to implement and has been underrepresented in previous experimental studies. Here, we propose Declarative Feature Selection (DFS) to simplify the design and validation of ML systems satisfying diverse user-specified constraints. We benchmark and evaluate a representative series of feature selection algorithms. From our extensive experimental results, we derive concrete suggestions on when to use which strategy and show that a meta-learning-driven optimizer can accurately predict the right strategy for an ML task at hand. These results demonstrate that feature selection can help to build ML systems that meet combinations of user-specified constraints, independent of the ML methods used.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1345–1358},
numpages = {14},
keywords = {robustness, privacy, meta learning, machine learning, feature selection, fairness, declarative ml, declarative machine learning, declarative feature selection, bias, DFS},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@article{10.1007/s10115-020-01519-3,
author = {Urkullu, Ari and P\'{e}rez, Aritz and Calvo, Borja},
title = {Statistical model for reproducibility in ranking-based feature selection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01519-3},
doi = {10.1007/s10115-020-01519-3},
abstract = {The stability of feature subset selection algorithms has become crucial in real-world problems due to the need for consistent experimental results across different replicates. Specifically, in this paper, we analyze the reproducibility of ranking-based feature subset selection algorithms. When applied to data, this family of algorithms builds an ordering of variables in terms of a measure of relevance. In order to quantify the reproducibility of ranking-based feature subset selection algorithms, we propose a model that takes into account all the different sized subsets of top-ranked features. The model is fitted to data through the minimization of an error function related to the expected values of Kuncheva’s consistency index for those subsets. Once it is fitted, the model provides practical information about the feature subset selection algorithm analyzed, such as a measure of its expected reproducibility or its estimated area under the receiver operating characteristic curve regarding the identification of relevant features. We test our model empirically using both synthetic and a wide range of real data. The results show that our proposal can be used to analyze feature subset selection algorithms based on rankings in terms of their reproducibility and their performance.},
journal = {Knowl. Inf. Syst.},
month = feb,
pages = {379–410},
numpages = {32},
keywords = {High dimensionality, Reproducibility, Stability, Feature selection}
}

@article{10.1016/j.compbiomed.2021.104664,
author = {Prabha, Anju and Yadav, Jyoti and Rani, Asha and Singh, Vijander},
title = {Design of intelligent diabetes mellitus detection system using hybrid feature selection based XGBoost classifier},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104664},
doi = {10.1016/j.compbiomed.2021.104664},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {9},
keywords = {XGBoost, Feature selection, MFCC, PPG, Diabetes detection}
}

@inproceedings{10.1007/978-3-030-87361-5_12,
author = {Wang, Bo and Yao, Hongxun},
title = {3D Reconstruction from Single-View Image Using Feature Selection},
year = {2021},
isbn = {978-3-030-87360-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87361-5_12},
doi = {10.1007/978-3-030-87361-5_12},
abstract = {Recovering the 3D shape of an object from single-view image with deep neural network has been attracting increasing attention in the past few years. Recent approaches based on convolutional neural networks have shown excellent results on single-view image. Most of them, however, have many model’s parameters or fewer parameters with performance degradation. Therefore, in this work we propose a feature selection module to balance this problem. This module first calculates the uncertain degree map to obtain the feature coordinates which means some coarse parts needs to be corrected. Then using these coordinates, features in several feature maps are selected. Finally, use MLP Layer to obtain fine features by taking features selected as input. Training and Inference are slightly different in this module. Using this module, we achieve better performance with about 18% parameters addition and comparable performance with about 30% model’s parameters decrease based on the Pix2Vox [1] framework.},
booktitle = {Image and Graphics: 11th International Conference, ICIG 2021, Haikou, China, August 6–8, 2021, Proceedings, Part III},
pages = {143–152},
numpages = {10},
keywords = {Feature selection, Single-view image, 3D reconstruction},
location = {Haikou, China}
}

@article{10.1007/s10115-021-01616-x,
author = {Alalga, Abdelouahid and Benabdeslem, Khalid and Mansouri, Dou El Kefel},
title = {3-3FS: ensemble method for semi-supervised multi-label feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {11},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-021-01616-x},
doi = {10.1007/s10115-021-01616-x},
abstract = {Feature selection has received considerable attention over the past decade. However, it is continuously challenged by new emerging issues. Semi-supervised multi-label learning is one of these promising novel approaches. In this work, we refer to it as an approach that combines data consisting of a huge amount of unlabeled instances with a small number of multi-labeled instances. Semi-supervised multi-label feature selection, like conventional feature selection algorithms, has a rather poor record as regards stability (i.e. robustness with respect to changes in data). To address this weakness and improve the robustness of the feature selection process in high-dimensional data, this document develops an ensemble methodology based on a 3-way resampling of data: (1) Bagging, (2) a random subspace method (RSM) and (3) an additional random sub-labeling strategy (RSL). The proposed framework contributes to enhancing the stability of feature selection algorithms and to improving their performance. Our research findings illustrate that bagging and RSM help improve the stability of the feature selection process and increase learning accuracy, while RSL addresses label correlation, which is a major concern with multi-label data. The paper presents the key findings of a series of experiments, which we conducted on selected benchmark data sets in the classification task. Results are promising, highlighting that the proposed method either outperforms state-of-the-art algorithms or produces at least comparable results.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {2969–2999},
numpages = {31},
keywords = {Ensemble learning, Semi-supervised multi-label learning, Feature selection}
}

@article{10.1016/j.knosys.2021.107167,
author = {Wan, Jihong and Chen, Hongmei and Yuan, Zhong and Li, Tianrui and Yang, Xiaoling and Sang, BinBin},
title = {A novel hybrid feature selection method considering feature interaction in neighborhood rough set▪},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107167},
doi = {10.1016/j.knosys.2021.107167},
journal = {Know.-Based Syst.},
month = sep,
numpages = {18},
keywords = {Hybrid data, Uncertainty measures, Multi-neighborhood calculation, Feature correlations, Interaction feature selection, Neighborhood rough set}
}

@inproceedings{10.1145/3394486.3403200,
author = {Haug, Johannes and Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji},
title = {Leveraging Model Inherent Variable Importance for Stable Online Feature Selection},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403200},
doi = {10.1145/3394486.3403200},
abstract = {Feature selection can be a crucial factor in obtaining robust and accurate predictions. Online feature selection models, however, operate under considerable restrictions; they need to efficiently extract salient input features based on a bounded set of observations, while enabling robust and accurate predictions. In this work, we introduce FIRES, a novel framework for online feature selection. The proposed feature weighting mechanism leverages the importance information inherent in the parameters of a predictive model. By treating model parameters as random variables, we can penalize features with high uncertainty and thus generate more stable feature sets. Our framework is generic in that it leaves the choice of the underlying model to the user. Strikingly, experiments suggest that the model complexity has only a minor effect on the discriminative power and stability of the selected feature sets. In fact, using a simple linear model, FIRES obtains feature sets that compete with state-of-the-art methods, while dramatically reducing computation time. In addition, experiments show that the proposed framework is clearly superior in terms of feature selection stability.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1478–1502},
numpages = {25},
keywords = {uncertainty, stability, feature selection, data streams},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1007/978-3-030-72240-1_34,
author = {Purpura, Alberto and Buchner, Karolina and Silvello, Gianmaria and Susto, Gian Antonio},
title = {Neural Feature Selection for Learning to Rank},
year = {2021},
isbn = {978-3-030-72239-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72240-1_34},
doi = {10.1007/978-3-030-72240-1_34},
abstract = {LEarning TO Rank (LETOR) is a research area in the field of Information Retrieval (IR) where machine learning models are employed to rank a set of items. In the past few years, neural LETOR approaches have become a competitive alternative to traditional ones like LambdaMART. However, neural architectures performance grew proportionally to their complexity and size. This can be an obstacle for their adoption in large-scale search systems where a model size impacts latency and update time. For this reason, we propose an architecture-agnostic approach based on a neural LETOR model to reduce the size of its input by up&nbsp;to 60% without affecting the system performance. This approach also allows to reduce a LETOR model complexity and, therefore, its training and inference time up&nbsp;to 50%.},
booktitle = {Advances in  Information Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 – April 1, 2021, Proceedings, Part II},
pages = {342–349},
numpages = {8},
keywords = {Deep learning, Feature selection, Learning to rank}
}

@inproceedings{10.5555/3540261.3541780,
author = {Wu, Xinxing and Cheng, Qiang},
title = {Algorithmic stability and generalization of an unsupervised feature selection algorithm},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically confirmed.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1519},
numpages = {16},
series = {NIPS '21}
}

@article{10.1016/j.ins.2021.06.005,
author = {Xiong, Chuanzhen and Qian, Wenbin and Wang, Yinglong and Huang, Jintao},
title = {Feature selection based on label distribution and fuzzy mutual information},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.005},
doi = {10.1016/j.ins.2021.06.005},
journal = {Inf. Sci.},
month = oct,
pages = {297–319},
numpages = {23},
keywords = {Fuzzy rough set, Label distribution, Multi-label data, Granular computing, Label enhancement, Feature selection}
}

@inproceedings{10.1007/978-3-030-61401-0_27,
author = {Zajdel, Roman and Kusy, Maciej and Kluska, Jacek and Zabinski, Tomasz},
title = {Weighted Feature Selection Method for Improving Decisions in Milling Process Diagnosis},
year = {2020},
isbn = {978-3-030-61400-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61401-0_27},
doi = {10.1007/978-3-030-61401-0_27},
abstract = {In this article, a new feature selection method is introduced. It is based on the weighted combined ranking score which fuses feature significance provided by three approaches: the Pearson’s linear correlation coefficient, ReliefF and single decision tree. During the successive steps, we eliminate the least significant features using binary weights corresponding to individual features. The utilized data set is represented by 1709 records and 44 attributes determined based on the signals acquired in the milling process. The efficiency of the proposed method is tested on reduced and original data set by a multilayer perceptron classifier. The obtained results confirm the usefulness of the solution.},
booktitle = {Artificial Intelligence and Soft Computing: 19th International Conference, ICAISC 2020, Zakopane, Poland, October 12-14, 2020, Proceedings, Part I},
pages = {280–291},
numpages = {12},
keywords = {Milling process, Multilayer perceptron, Combined ranking, Weights, Feature selection},
location = {Zakopane, Poland}
}

@article{10.1016/j.cviu.2021.103273,
author = {Chen, Xiuhong and Lu, Yun and Zhang, Jun and Zhu, Xingyu},
title = {Margin-based discriminant embedding guided sparse matrix regression for image supervised feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103273},
doi = {10.1016/j.cviu.2021.103273},
journal = {Comput. Vis. Image Underst.},
month = nov,
numpages = {12},
keywords = {Classification, Discriminant embedding, Margin, Sparse matrix regression, Supervised feature selection, Two dimensional image}
}

@inproceedings{10.1007/978-3-030-89691-1_23,
author = {Horio, Tomoya and Kudo, Mineichi},
title = {Feature Selection with Class Hierarchy for Imbalance Problems},
year = {2021},
isbn = {978-3-030-89690-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89691-1_23},
doi = {10.1007/978-3-030-89691-1_23},
abstract = {In this paper, we aim to improve the classification performance in imbalance data by mitigating the impact of the curse of dimensionality especially in minority classes of a few samples. We exploit a class hierarchy realized as a binary tree whose node has a subset of classes. We construct such a binary tree in a top-down way by taking into consideration the separability of classes and the size of the feature subset. It is expected that the generalization performance is improved, especially in minority classes having a small number of samples, and that the interpretability of the decision rule is enhanced by the smallness of the number of features. Experimental results showed a remarkable improvement is by the proposed method in large-scale problems with many classes, e.g. from 48% to 62% in the balanced accuracy. In addition, only one feature was chosen in every node of the class hierarchy in all the four datasets, bringing a high interpretability of the classification rules.},
booktitle = {Progress in Artificial Intelligence and Pattern Recognition: 7th International Workshop on Artificial Intelligence and Pattern Recognition, IWAIPR 2021, Havana, Cuba, October 5–7, 2021, Proceedings},
pages = {229–238},
numpages = {10},
keywords = {Class hierarchy, Class-dependent feature selection, Imbalanced problems},
location = {Havana, Cuba}
}

@article{10.1007/s00521-018-3500-7,
author = {Shi, Yong and Miao, Jianyu and Niu, Lingfeng},
title = {Feature selection with MCP$$^2$$ regularization},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3500-7},
doi = {10.1007/s00521-018-3500-7},
abstract = {Feature selection, as a fundamental component of building robust models, plays an important role in many machine learning and data mining tasks. Recently, with the development of sparsity research, both theoretical and empirical studies have suggested that the sparsity is one of the intrinsic properties of real world data and sparsity regularization has been applied into feature selection models successfully. In view of the remarkable performance of non-convex regularization, in this paper, we propose a novel non-convex yet Lipschitz continuous sparsity regularization term, named MCP$$^2$$, and apply it into feature selection. To solve the resulting non-convex model, a new algorithm in the framework of the ConCave–Convex Procedure is given at the same time. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6699–6709},
numpages = {11},
keywords = {CCCP, MCP, sparsity regularization, Feature selection}
}

@article{10.1007/s10994-019-05795-1,
author = {Sechidis, Konstantinos and Azzimonti, Laura and Pocock, Adam and Corani, Giorgio and Weatherall, James and Brown, Gavin},
title = {Efficient feature selection using shrinkage estimators},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {8–9},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-019-05795-1},
doi = {10.1007/s10994-019-05795-1},
abstract = {Information theoretic feature selection methods quantify the importance of each feature by estimating mutual information terms to capture: the relevancy, the redundancy and the complementarity. These terms are commonly estimated by maximum likelihood, while an under-explored area of research is how to use shrinkage methods instead. Our work suggests a novel shrinkage method for data-efficient estimation of information theoretic terms. The small sample behaviour makes it particularly suitable for estimation of discrete distributions with large number of categories (bins). Using our novel estimators we derive a framework for generating feature selection criteria that capture any high-order feature interaction for redundancy and complementarity. We perform a thorough empirical study across datasets from diverse sources and using various evaluation measures. Our first finding is that our shrinkage based methods achieve better results, while they keep the same computational cost as the simple maximum likelihood based methods. Furthermore, under our framework we derive efficient novel high-order criteria that outperform state-of-the-art methods in various tasks.},
journal = {Mach. Learn.},
month = sep,
pages = {1261–1286},
numpages = {26},
keywords = {Shrinkage estimators, Mutual information, High order feature selection, Feature selection}
}

@article{10.1007/s10664-021-09964-6,
author = {Duchien, Laurence and Gr\"{u}nbacher, Paul and Th\"{u}m, Thomas},
title = {Foreword to the Special Issue on Configurable Systems},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09964-6},
doi = {10.1007/s10664-021-09964-6},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {3}
}

@inproceedings{10.5555/3524938.3525925,
author = {Yamada, Yutaro and Lindenbaum, Ofir and Negahban, Sahand and Kluger, Yuval},
title = {Feature selection using stochastic gates},
year = {2020},
publisher = {JMLR.org},
abstract = {Feature selection problems have been extensively studied in the setting of linear estimation (e.g. LASSO), but less emphasis has been placed on feature selection for non-linear functions. In this study, we propose a method for feature selection in neural network estimation problems. The new procedure is based on probabilistic relaxation of the l0 norm of features, or the count of the number of selected features. Our l0-based regularization relies on a continuous relaxation of the Bernoulli distribution; such relaxation allows our model to learn the parameters of the approximate Bernoulli distributions via gradient descent. The proposed framework simultaneously learns either a nonlinear regression or classification function while selecting a small subset of features. We provide an information-theoretic justification for incorporating Bernoulli distribution into feature selection. Furthermore, we evaluate our method using synthetic and real-life data to demonstrate that our approach outperforms other commonly used methods in both predictive performance and feature selection.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {987},
numpages = {12},
series = {ICML'20}
}

@inproceedings{10.1145/3307630.3342421,
author = {Chac\'{o}n-Luna, Ana E. and Ruiz, Elvira G. and Galindo, Jos\'{e} A. and Benavides, David},
title = {Variability Management in a Software Product Line Unaware Company: Towards a Real Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342421},
doi = {10.1145/3307630.3342421},
abstract = {Software Product Lines (SPL) enable systematic reuse within an organization thus, enabling the reduction of costs, efforts, development time and the average number of defects per product. However, there is little empirical evidence of SPL adoption in the literature, which makes it difficult to strengthen or elaborate adjustments or improvements to SPL frameworks. In this article, we present the first steps towards an empirical evaluation by showing how companies that do not know about of SPL manage variability in their products, pointing out the strengths and weaknesses of their approaches. To this end, we present the design of a case study that we plan to carry out in the future in two companies to evaluate how companies perform variability management when they are not aware of software product lines. Our assumption is that most of the companies manage variability but no many of them are aware of software product lines. In addition, the first preliminary results of the case study applied in a company are presented.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {82–89},
numpages = {8},
keywords = {variability management, software product lines, a case study},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10489-021-02288-4,
author = {Wu, Xinping and Chen, Hongmei and Li, Tianrui and Wan, Jihong},
title = {Semi-supervised feature selection with minimal redundancy based on local adaptive},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02288-4},
doi = {10.1007/s10489-021-02288-4},
abstract = {With the speedy development of network technology, diverse data increase by hundreds of millions per hour, causing increasing pressure on the acquisition of data labels. Semi-supervised feature selection has been among the forefront of dimensionality reduction research due to the outstanding achievements of “small labels” and “high efficiency”. Especially, the graph-based methods use data with missing labels completely and effectively, prompting it to become a research hotspots in semi-supervised feature selection. However, the existing graph-based methods do not take into account the effects of outliers, noise, and redundancy of selected features simultaneously. To solve those problems, a novel semi-supervised feature selection method based on local adaptive and minimal redundancy is proposed. The local structure is flexibly assigned weights according to the data conditions, thereby reducing the impact of outliers and noise; moreover, a high similarity penalty mechanism is introduced in the feature mapping matrix to promote discrimination and low redundancy of the selected feature subset. In addition, an iterative method is designed and its convergence is proved theoretically and experimentally. Finally, the proposed algorithm is verified to be stable and effective through experiments from five aspects on sixteen public datasets.},
journal = {Applied Intelligence},
month = nov,
pages = {8542–8563},
numpages = {22},
keywords = {Redundancy minimization, Local adaptive least squares regression, Feature selection, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-47426-3_61,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {A Framework for Feature Selection to Exploit Feature Group Structures},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_61},
doi = {10.1007/978-3-030-47426-3_61},
abstract = {Filter feature selection methods play an important role in machine learning tasks when low computational costs, classifier independence or simplicity is important. Existing filter methods predominantly focus only on the input data and do not take advantage of the external sources of correlations within feature groups to improve the classification accuracy. We propose a framework which facilitates supervised filter feature selection methods to exploit feature group information from external sources of knowledge and use this framework to incorporate feature group information into minimum Redundancy Maximum Relevance (mRMR) algorithm, resulting in GroupMRMR algorithm. We show that GroupMRMR achieves high accuracy gains over mRMR (up&nbsp;to 35%) and other popular filter methods (up&nbsp;to 50%). GroupMRMR has same computational complexity as that of mRMR, therefore, does not incur additional computational costs. Proposed method has many real world applications, particularly the ones that use genomic, text and image data whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {792–804},
numpages = {13},
keywords = {Squared [inline-graphic not available: see fulltext] norm minimisation, Feature groups, Filter feature selection},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3377930.3389815,
author = {Binder, Martin and Moosbauer, Julia and Thomas, Janek and Bischl, Bernd},
title = {Multi-objective hyperparameter tuning and feature selection using filter ensembles},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389815},
doi = {10.1145/3377930.3389815},
abstract = {Both feature selection and hyperparameter tuning are key tasks in machine learning. Hyperparameter tuning is often useful to increase model performance, while feature selection is undertaken to attain sparse models. Sparsity may yield better model interpretability and lower cost of data acquisition, data handling and model inference. While sparsity may have a beneficial or detrimental effect on predictive performance, a small drop in performance may be acceptable in return for a substantial gain in sparseness. We therefore treat feature selection as a multi-objective optimization task. We perform hyperparameter tuning and feature selection simultaneously because the choice of features of a model may influence what hyperparameters perform well.We present, benchmark, and compare two different approaches for multi-objective joint hyperparameter optimization and feature selection: The first uses multi-objective model-based optimization. The second is an evolutionary NSGA-II-based wrapper approach to feature selection which incorporates specialized sampling, mutation and recombination operators. Both methods make use of parameterized filter ensembles.While model-based optimization needs fewer objective evaluations to achieve good performance, it incurs computational overhead compared to the NSGA-II, so the preferred choice depends on the cost of evaluating a model on given data.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {471–479},
numpages = {9},
keywords = {multiobjective optimization, model-based optimization, hyperparameter optimization, feature selection, evolutionary algorithms},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3377024.3377045,
author = {Ferreira, Fischer and Vale, Gustavo and Diniz, Jo\~{a}o P. and Figueiredo, Eduardo},
title = {On the proposal and evaluation of a test-enriched dataset for configurable systems},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377045},
doi = {10.1145/3377024.3377045},
abstract = {Configurable systems offer advantages compared to single systems since developers should maintain a unique platform to address a diversity of deployment contexts and usages. To ensure that all configurations correctly execute, developers spend considerable effort testing different system configurations. This testing process is essential because configurations that fail may potentially hurt user experience and degrade the reputation of a project. Previous studies have reported and created repositories of open-source configurable systems, although they neglected their test suites. Considering the importance of testing configurable systems, we reviewed the literature to find test suites of open-source configurable systems. As we found only 10 configurable systems with test suite available and considering that a test suite for configurable systems may be useful for different research topics, we created test suites for 20 additional configurable systems and evaluated the test suites coverage of all 30 configurable systems. Surprisingly, our test suites were able to find several failures in existing systems, mainly because of feature interactions, which enforces the need of test suites available for open source configurable systems. Aiming at finding common characteristics for fault-prone components (e.g., classes) on configurable systems, we group them based on software quality metrics (e.g., coupling between objects and lines of code). As result, we found that 44% of the configurable systems of our dataset have failures and these failures are concentrated in few classes.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {16},
numpages = {10},
keywords = {testing configurable systems, software failures, feature interactions, dataset of open-source configurable systems},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3292500.3330856,
author = {Li, Jundong and Guo, Ruocheng and Liu, Chenghao and Liu, Huan},
title = {Adaptive Unsupervised Feature Selection on Attributed Networks},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330856},
doi = {10.1145/3292500.3330856},
abstract = {Attributed networks are pervasive in numerous of high-impact domains. As opposed to conventional plain networks where only pairwise node dependencies are observed, both the network topology and node attribute information are readily available on attributed networks. More often than not, the nodal attributes are depicted in a high-dimensional feature space and are therefore notoriously difficult to tackle due to the curse of dimensionality. Additionally, features that are irrelevant to the network structure could hinder the discovery of actionable patterns from attributed networks. Hence, it is important to leverage feature selection to find a high-quality feature subset that is tightly correlated to the network structure. Few of the existing efforts either model the network structure at a macro-level by community analysis or directly make use of the binary relations. Consequently, they fail to exploit the finer-grained tie strength information for feature selection and may lead to suboptimal results. Motivated by the sociology findings, in this work, we investigate how to harness the tie strength information embedded on the network structure to facilitate the selection of relevant nodal attributes. Methodologically, we propose a principled unsupervised feature selection framework ADAPT to find informative features that can be used to regenerate the observed links and further characterize the adaptive neighborhood structure of the network. Meanwhile, an effective optimization algorithm for the proposed ADAPT framework is also presented. Extensive experimental studies on various real-world attributed networks validate the superiority of the proposed ADAPT framework.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {92–100},
numpages = {9},
keywords = {unsupervised feature selection, tie strength, attributed networks, adaptive neighborhood structure},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1016/j.knosys.2019.07.027,
author = {Huang, Dong and Cai, Xiaosha and Wang, Chang-Dong},
title = {Unsupervised feature selection with multi-subspace randomization and collaboration},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {182},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.07.027},
doi = {10.1016/j.knosys.2019.07.027},
journal = {Know.-Based Syst.},
month = oct,
numpages = {15},
keywords = {Ensemble learning, Multi-subspace randomization and collaboration, High-dimensional data, Unsupervised feature selection}
}

@inproceedings{10.1145/3023956.3023957,
author = {Wille, David and Runge, Tobias and Seidl, Christoph and Schulze, Sandro},
title = {Extractive software product line engineering using model-based delta module generation},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023957},
doi = {10.1145/3023956.3023957},
abstract = {To satisfy demand for customized products, companies commonly apply so-called clone-and-own strategies by copying functionality from existing products and modifying it to create product variants that have to be developed, maintained, and evolved in isolation. In previous work, we introduced a variability mining technique to identify variability information (commonalities and differences) in block-based model variants (e.g., MATLAB/Simulink models), which can be used to guide manual transition from clone-and-own to managed reuse of a software product line (SPL). In this paper, we present a procedure that uses the extracted variability information to generate a transformational delta-oriented SPL fully automatically. We generate a delta language specifically tailored to transforming models in the analyzed modeling language and utilize it to generate delta modules expressing variation of the SPL's implementation artifacts. The procedure seamlessly integrates with our variability mining technique and allows to fully adopt a managed reuse strategy (i.e., generation of products from a single code base) without manual overhead. We show the feasibility of the procedure by applying it to state chart and MATLAB/Simulink model variants from two industrial case studies.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {36–43},
numpages = {8},
keywords = {variability mining, model-based, extractive product line engineering, delta modeling, clone-and-own},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1007/s11390-020-9864-z,
author = {Chen, Yi-Fan and Zhao, Xiang and Liu, Jin-Yuan and Ge, Bin and Zhang, Wei-Ming},
title = {Item Cold-Start Recommendation with Personalized Feature Selection},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9864-z},
doi = {10.1007/s11390-020-9864-z},
abstract = {The problem of recommending new items to users (often referred to as item cold-start recommendation) remains a challenge due to the absence of users’ past preferences for these items. Item features from side information are typically leveraged to tackle the problem. Existing methods formulate regression methods, taking item features as input and user ratings as output. These methods are confronted with the issue of overfitting when item features are high-dimensional, which greatly impedes the recommendation experience. Availing of high-dimensional item features, in this work, we opt for feature selection to solve the problem of recommending top-N new items. Existing feature selection methods find a common set of features for all users, which fails to differentiate users’ preferences over item features. To personalize feature selection, we propose to select item features discriminately for different users. We study the personalization of feature selection at the level of the user or user group. We fulfill the task by proposing two embedded feature selection models. The process of personalized feature selection filters out the dimensions that are irrelevant to recommendations or unappealing to users. Experimental results on real-life datasets with high-dimensional side information reveal that the proposed method is effective in singling out features that are crucial to top-N recommendation and hence improving performance.},
journal = {J. Comput. Sci. Technol.},
month = oct,
pages = {1217–1230},
numpages = {14},
keywords = {personalized feature selection, item cold-start top-N recommendation, high-dimensionality}
}

@article{10.1007/s10489-020-01822-0,
author = {Sha, Zhi-Chao and Liu, Zhang-Meng and Ma, Chen and Chen, Jun},
title = {Feature selection for multi-label classification by maximizing full-dimensional conditional mutual information},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01822-0},
doi = {10.1007/s10489-020-01822-0},
abstract = {Conditional mutual information (CMI) maximization is a promising criterion for feature selection in a computationally efficient stepwise way, but it is hard to be applied comprehensively because of imprecise probability calculation and heavy computational load. Many dimension-reduced CMI-based and mutual information (MI)-based methods have been reported to achieve state-of-art performances in terms of classification. However, model deviations are introduced into the CMI and MI formulations in these methods during dimension reduction. In this paper, we start with the full-dimensional CMI to deal with the feature selection problem, so as to retain full inter-feature and feature-label mutual information when selecting new features. The cost function is approximated and simplified from a mathematical perspective to overcome the difficulties for maximizing the original full-dimensional CMI. A relationship is established between the proposed feature selection criterion and the one based on Hilbert-Schmidt independence, which explains qualitatively how the new criterion succeeds to achieve relevance maximization and redundance minimization simultaneously. Experiments on real-world datasets demonstrate the predominance of the proposed method over the existing ones.},
journal = {Applied Intelligence},
month = jan,
pages = {326–340},
numpages = {15},
keywords = {Hilbert-Schmidt independence criterion (HSIC), Redundance minimization, Relevance maximization, Conditional mutual information, Classification, Feature selection}
}

@article{10.1007/s10845-021-01875-z,
author = {Wang, Yu and Cui, Wei and Vuong, Nhu Khue and Chen, Zhenghua and Zhou, Yu and Wu, Min},
title = {Feature selection and domain adaptation for cross-machine product quality prediction},
year = {2021},
issue_date = {Apr 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {4},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-021-01875-z},
doi = {10.1007/s10845-021-01875-z},
abstract = {Today’s manufacturing systems are becoming increasingly complex, dynamic and connected hence continual prediction of manufactured product quality is a key to look for patterns that can eventually lead to improved accuracy and productivity. Recent developments in artificial intelligence, especially machine learning have shown great potential to transform the manufacturing domain through analytics for processing vast amounts of manufacturing data generated (Esmaeilian et al. in J Manuf Syst 39:79–100, 2016). Although prediction models have been built to predict product quality with good accuracy, they assume that same distribution applies on training data and testing data hence fail to produce satisfying results when machines work under different conditions with varying data distribution. Na\"{\i}ve re-collection and re-annotation of data for each new working condition can be very expensive thus is not a feasible solution. To cope with this problem, we adopt transfer learning approach called domain adaptation to transfer the knowledge learned from one labelled operating condition (source domain) to another operating condition (target domain) without labels. Particularly, we propose an end-to-end framework for cross-machine product quality prediction, which is able to alleviate domain shift problem. To facilitate the cross-machine prediction performance, a systematic feature selection approach is designed and integrated to generate most suitable feature set to characterize the collected data. Comprehensive experiments have been conducted using actual manufacturing data and the results demonstrate significant improvement on cross-machine product quality prediction as compared to conventional techniques.},
journal = {J. Intell. Manuf.},
month = nov,
pages = {1573–1584},
numpages = {12},
keywords = {Product quality prediction, Cross-machine, Transfer learning, Domain adaptation, Feature selection}
}

@article{10.1145/3383685,
author = {Goldstein, Orpaz and Kachuee, Mohammad and Karkkainen, Kimmo and Sarrafzadeh, Majid},
title = {Target-Focused Feature Selection Using Uncertainty Measurements in Healthcare Data},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3383685},
doi = {10.1145/3383685},
abstract = {Healthcare big data remains under-utilized due to various incompatibility issues between the domains of data analytics and healthcare. The lack of generalizable iterative feature acquisition methods under budget and machine learning models that allow reasoning with a model’s uncertainty are two examples. Meanwhile, a boost to the available data is currently under way with the rapid growth in the Internet of Things applications and personalized healthcare. For the healthcare domain to be able to adopt models that take advantage of this big data, machine learning models should be coupled with more informative, germane feature acquisition methods, consequently adding robustness to the model’s results. We introduce an approach to feature selection that is based on Bayesian learning, allowing us to report the level of uncertainty in the model, combined with false-positive and false-negative rates. In addition, measuring target-specific uncertainty lifts the restriction on feature selection being target agnostic, allowing for feature acquisition based on a target of focus. We show that acquiring features for a specific target is at least as good as deep learning feature selection methods and common linear feature selection approaches for small non-sparse datasets, and surpasses these when faced with real-world data that is larger in scale and sparseness.},
journal = {ACM Trans. Comput. Healthcare},
month = may,
articleno = {15},
numpages = {17},
keywords = {machine learning for health, machine learning, healthcare big data, health informatics, Healthcare feature selection, Bayesian learning}
}

@article{10.1016/j.eswa.2021.115290,
author = {Hijazi, Neveen Mohammed and Faris, Hossam and Aljarah, Ibrahim},
title = {A parallel metaheuristic approach for ensemble feature selection based on multi-core architectures},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115290},
doi = {10.1016/j.eswa.2021.115290},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {30},
keywords = {Ensemble learning, Feature selection, Parallel processing, Evolutionary computation, Meta-heuristics}
}

@article{10.1109/TCBB.2019.2948330,
author = {Hind, Jade and Lisboa, Paulo and Hussain, Abir J. and Al-Jumeily, Dhiya},
title = {A Novel Approach to Detecting Epistasis using Random Sampling Regularisation},
year = {2020},
issue_date = {Sept.-Oct. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2948330},
doi = {10.1109/TCBB.2019.2948330},
abstract = {Epistasis is a progressive approach that complements the `common disease, common variant' hypothesis that highlights the potential for connected networks of genetic variants collaborating to produce a phenotypic expression. Epistasis is commonly performed as a pairwise or limitless-arity capacity that considers variant networks as either variant vs variant or as high order interactions. This type of analysis extends the number of tests that were previously performed in a standard approach such as Genome-Wide Association Study (GWAS), in which False Discovery Rate (FDR) is already an issue, therefore by multiplying the number of tests up to a factorial rate also increases the issue of FDR. Further to this, epistasis introduces its own limitations of computational complexity and intensity that are generated based on the analysis performed; to consider the most intense approach, a multivariate analysis introduces a time complexity of O(n!). Proposed in this paper is a novel methodology for the detection of epistasis using interpretable methods and best practice to outline interactions through filtering processes. Using a process of Random Sampling Regularisation which randomly splits and produces sample sets to conduct a voting system to regularise the significance and reliability of biological markers, SNPs. Preliminary results are promising, outlining a concise detection of interactions. Results for the detection of epistasis, in the classification of breast cancer patients, indicated eight outlined risk candidate interactions from five variants and a singular candidate variant with high protective association.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {1535–1545},
numpages = {11}
}

@inproceedings{10.1145/2814189.2815365,
author = {Medeiros, Fl\'{a}vio},
title = {Safely evolving configurable systems},
year = {2015},
isbn = {9781450337229},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814189.2815365},
doi = {10.1145/2814189.2815365},
abstract = {Developers use configuration options to tailor systems to different platforms. This configurability leads to exponential configuration spaces and traditional tools (e.g., gcc) check only one configuration at a time. As a result, developers introduce configuration-related issues (i.e., bad smells and faults) that appear only when we select certain configuration options. By interviewing 40 developers and performing a survey with 202 developers, we found that configuration- related issues are harder to detect and more critical than is- sues that appear in all configurations. We propose a strategy to detect configuration-related issues and a catalogue of refactorings to remove bad smells in preprocessor directives. We found 131 faults and 500 bad smells in 40 real-world configurable systems, including Apache and Libssh.},
booktitle = {Companion Proceedings of the 2015 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
pages = {85–86},
numpages = {2},
keywords = {Configurable Systems},
location = {Pittsburgh, PA, USA},
series = {SPLASH Companion 2015}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11042-018-6083-5,
author = {Deng, Xuelian and Li, Yuqing and Weng, Jian and Zhang, Jilian},
title = {Feature selection for text classification: A review},
year = {2019},
issue_date = {Feb 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6083-5},
doi = {10.1007/s11042-018-6083-5},
abstract = {Big multimedia data is heterogeneous in essence, that is, the data may be a mixture of video, audio, text, and images. This is due to the prevalence of novel applications in recent years, such as social media, video sharing, and location based services (LBS), etc. In many multimedia applications, for example, video/image tagging and multimedia recommendation, text classification techniques have been used extensively to facilitate multimedia data processing. In this paper, we give a comprehensive review on feature selection techniques for text classification. We begin by introducing some popular representation schemes for documents, and similarity measures used in text classification. Then, we review the most popular text classifiers, including Nearest Neighbor (NN) method, Na\"{\i}ve Bayes (NB), Support Vector Machine (SVM), Decision Tree (DT), and Neural Networks. Next, we survey four feature selection models, namely the filter, wrapper, embedded and hybrid, discussing pros and cons of the state-of-the-art feature selection approaches. Finally, we conclude the paper and give a brief introduction to some interesting feature selection work that does not belong to the four models.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {3797–3816},
numpages = {20},
keywords = {Text classifiers, Text classification, Multimedia, Feature Selection}
}

@article{10.1016/j.ins.2019.01.041,
author = {Chen, Hongmei and Li, Tianrui and Fan, Xin and Luo, Chuan},
title = {Feature selection for imbalanced data based on neighborhood rough sets},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {483},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.041},
doi = {10.1016/j.ins.2019.01.041},
journal = {Inf. Sci.},
month = may,
pages = {1–20},
numpages = {20},
keywords = {Discernibility matrix, Imbalanced data, Feature selection, Rough set theory}
}

@article{10.1007/s11063-020-10250-7,
author = {Wen, Guoqiu and Zhu, Yonghua and Zhan, Mengmeng and Tan, Malong},
title = {Sparse Low-Rank and Graph Structure Learning for Supervised Feature Selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10250-7},
doi = {10.1007/s11063-020-10250-7},
abstract = {Spectral feature selection (SFS) is superior to conventional feature selection methods in many aspects, by extra importing a graph matrix to preserve the subspace structure of data. However, the graph matrix of classical SFS that is generally constructed by original data easily outputs a suboptimal performance of feature selection because of the redundancy. To address this, this paper proposes a novel feature selection method via coupling the graph matrix learning and feature data learning into a unified framework, where both steps can be iteratively update until achieving the stable solution. We also apply a low-rank constraint to obtain the intrinsic structure of data to improve the robustness of learning model. Besides, an optimization algorithm is proposed to solve the proposed problem and to have fast convergence. Compared to classical and state-of-the-art feature selection methods, the proposed method achieved the competitive results on twelve real data sets.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1793–1809},
numpages = {17},
keywords = {Spectral feature selection, Orthogonal constraint, Low-rank constraint, Graph learning}
}

@article{10.1504/ijbidm.2021.118925,
author = {Amazal, Houda and Ramdani, Mohammed and Kissi, Mohamed},
title = {Ensemble feature selection approach for imbalanced textual data using MapReduce},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {19},
number = {4},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2021.118925},
doi = {10.1504/ijbidm.2021.118925},
abstract = {Feature selection is a fundamental pre-processing phase in text classification. It speeds up machine learning algorithms and improves classification accuracy. In big data context, feature selection techniques have to deal with two major issues which are the huge dimensionality and the imbalancing aspect of data. However, the libraries of big data frameworks, such as Hadoop, only implement a few single feature selection methods whose robustness does not meet the requirements imposed by the large amount of data. To deal with this, we propose in this paper a distributed ensemble feature selection (DEFS) approach for imbalanced large dataset using MapReduce. A set of experiments are being conducted on four datasets to confirm the improvement brought about by the proposed approach. The reported results show that in most cases our method results in better classification performance than other widely used feature selection techniques.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {395–417},
numpages = {22},
keywords = {ensemble feature selection? EFS? imbalance data? MapReduce? text classification}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.ins.2018.10.052,
author = {Palma-Mendoza, Raul-Jose and de-Marcos, Luis and Rodriguez, Daniel and Alonso-Betanzos, Amparo},
title = {Distributed correlation-based feature selection in spark},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {496},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.10.052},
doi = {10.1016/j.ins.2018.10.052},
journal = {Inf. Sci.},
month = sep,
pages = {287–299},
numpages = {13},
keywords = {Correlation, CFS, Apache spark, Big data, Scalability, Feature selection}
}

@article{10.1007/s10878-020-00666-1,
author = {He, Zaobo and Sai, Akshita Maradapu Vera Venkata and Huang, Yan and seo, Daehee and Zhang, Hanzhou and Han, Qilong},
title = {Differentially private approximate aggregation based on feature selection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {41},
number = {2},
issn = {1382-6905},
url = {https://doi.org/10.1007/s10878-020-00666-1},
doi = {10.1007/s10878-020-00666-1},
abstract = {Privacy-preserving data aggregation is an important problem that has attracted extensive study. The state-of-the-art techniques for solving this problem is differential privacy, which offers a strong privacy guarantee without making strong assumptions about the attacker. However, existing solutions cannot effectively query data aggregation from high-dimensional datasets under differential privacy guarantee. Particularly, when the input dataset contains large number of dimensions, existing solutions must inject large scale of noise into returned aggregates. To address the above issue, this paper proposes an algorithm for querying differentially private approximate aggregates from high-dimensional datasets. Given a dataset D, our algorithm first develops a ε′-differentially private feature selection method that is based on a data sampling process over a kd-tree, which allows us to obtain a differentially private low-dimensional dataset with representative instances. After that, our algorithm samples independent samples from the kd-tree aiming at obtaining (α′,δ′)-approximate aggregates. Finally, a model is proposed to determine the relevance between privacy and utility budgets such that the final aggregate still satisfies the accuracy requirements specified by data consumers. Intuitively, the proposed algorithm circumvents the dilemma of both dimensionality and the height threshold of kd-tree, as it samples a low-dimensional dataset S and queries aggregates from S, instead of the kd-tree. Satisfying user-specified privacy and utility budgets after multiple-stages approximation is significantly challenging, and we presents a novel model to determine the parameters’ relevance.},
journal = {J. Comb. Optim.},
month = feb,
pages = {318–327},
numpages = {10},
keywords = {kd-tree, Sampling, Differential privacy, Data aggregation}
}

@article{10.1016/j.infsof.2021.106652,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Zhang, Tao and Yang, Dan and Li, Wei},
title = {A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106652},
doi = {10.1016/j.infsof.2021.106652},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {16},
keywords = {Empirical study, Feature selection, Stack trace, Crash localization}
}

@article{10.1016/j.eswa.2021.114887,
author = {Miao, Fahui and Yao, Li and Zhao, Xiaojie},
title = {Symbiotic organisms search algorithm using random walk and adaptive Cauchy mutation on the feature selection of sleep staging},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114887},
doi = {10.1016/j.eswa.2021.114887},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {17},
keywords = {Sleep staging, Feature selection, Symbiotic organisms search algorithm, Random walk, Adaptive Cauchy mutation}
}

@article{10.1007/s00500-020-04911-x,
author = {Wang, Hongzhi and He, Chengquan and Li, Zhuping},
title = {A new ensemble feature selection approach based on genetic algorithm},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {20},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-04911-x},
doi = {10.1007/s00500-020-04911-x},
abstract = {In the ensemble feature selection method, if the weight adjustment is performed on each feature subset used, the ensemble effect can be significantly different; therefore, how to find the optimized weight vector is a key and challenging problem. Aiming at this optimization problem, this paper proposes an ensemble feature selection approach based on genetic algorithm (EFS-BGA). After each base feature selector generates a feature subset, the EFS-BGA method obtains the optimized weight of each feature subset through genetic algorithm, which is different from traditional genetic algorithm directly processing single features. We divide the EFS-BGA algorithm into two types. The first is a complete ensemble feature selection method; based on the first, we further propose the selective EFS-BGA model. After that, through mathematical analysis, we theoretically explain why weight adjustment is an optimization problem and how to optimize. Finally, through the comparative experiments on multiple data sets, the advantages of the EFS-BGA algorithm in this paper over the previous ensemble feature selection algorithms are explained in practice.},
journal = {Soft Comput.},
month = oct,
pages = {15811–15820},
numpages = {10},
keywords = {Genetic algorithm, Optimization problem, Ensemble feature selection}
}

@article{10.1007/s10489-020-02184-3,
author = {Wang, Youwei and Feng, Lizhou},
title = {An adaptive boosting algorithm based on weighted feature selection and category classification confidence},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02184-3},
doi = {10.1007/s10489-020-02184-3},
abstract = {Adaptive boosting (Adaboost) is a typical ensemble learning algorithm, which has been studied and widely used in classification tasks. Traditional Adaboost algorithms ignore the sample weights while selecting the most useful features, and most of them ignore the fact that the performances of weak classifiers on each category are always different. On this basis, a weighted feature selection and category classification confidence based Adaboost algorithm is proposed in this paper. The first contribution, is that we propose a weighted feature selection to select the most useful features, which can both distinguish the majority of all samples and the previous misclassified samples. The second contribution, is that we improve the traditional error rate calculation method and propose a category based error rate calculation method to combine the classification abilities of Adaboost on different categories. A detailed performances comparison of various Adaboost algorithms are carried out on eight typical datasets. The experimental results show that the proposed algorithm obtains significant improvement on classification accuracy compared to typical Adaboost algorithms when different datasets especially the unbalanced datasets are used.},
journal = {Applied Intelligence},
month = oct,
pages = {6837–6858},
numpages = {22},
keywords = {Classification accuracy, Classification ability, Error rate, Feature selection, Ensemble learning}
}

@article{10.1016/j.asoc.2021.107112,
author = {Almaghrabi, Fatima and Xu, Dong-Ling and Yang, Jian-Bo},
title = {An evidential reasoning rule based feature selection for improving trauma outcome prediction},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107112},
doi = {10.1016/j.asoc.2021.107112},
journal = {Appl. Soft Comput.},
month = may,
numpages = {15},
keywords = {Imbalance classes, ReliefF, Random forest, Evidential reasoning rule, Trauma, Feature selection}
}

@inproceedings{10.1145/3365109.3368792,
author = {Cherrington, Marianne and Airehrour, David and Lu, Joan and Xu, Qiang and Wade, Stephen and Madanian, Samaneh},
title = {Feature Selection Methods for Linked Data: Limitations, Capabilities and Potentials},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368792},
doi = {10.1145/3365109.3368792},
abstract = {Feature selection is an important pre-processing, data mining, and knowledge discovery tool for data analysis. By eliminating redundant and irrelevant features from high-dimensional data, feature selection diminishes the 'curse of dimensionality' to improve performance. Data are becoming increasingly complex; heterogeneous data may often be viewed as natural collections of linked objects. Linked data are structured data that are connected with other data sources through the use of semantic queries. It is increasingly prevalent in social media websites and biological networks. Many feature selection methods assume independent and identically distributed data (IID), a condition violated with linked data. In this paper, a review of current feature selection techniques for linked data is presented. Several approaches are examined in various contexts so that performance issues and ongoing challenges can be assessed. The major contribution of this paper is to underscore contemporary uses and limitations of linked data feature selection techniques with the purpose of informing existing capabilities and current potentials for key areas of future research and application.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {103–112},
numpages = {10},
keywords = {linked data (ld), high-dimensional data (hdd), heterogeneous data, feature selection (fs), dimensionality reduction},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@article{10.1016/j.cl.2018.01.002,
author = {Braz, Larissa and Gheyi, Rohit and Mongiovi, Melina and Ribeiro, M\'{a}rcio and Medeiros, Fl\'{a}vio and Teixeira, Leopoldo and Souto, Sabrina},
title = {A change-aware per-file analysis to compile configurable systems with #ifdefs      },
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.002},
doi = {10.1016/j.cl.2018.01.002},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {427–450},
numpages = {24},
keywords = {Impact analysis, Configurable systems, #ifdef, Compilation}
}

@inproceedings{10.1007/978-3-030-77967-2_8,
author = {Grzyb, Joanna and Topolski, Mariusz and Wo\'{z}niak, Micha\l{}},
title = {Application of Multi-objective Optimization to Feature Selection for a Difficult Data Classification Task},
year = {2021},
isbn = {978-3-030-77966-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77967-2_8},
doi = {10.1007/978-3-030-77967-2_8},
abstract = {Many different decision problems require taking a compromise between the various goals we want to achieve into account. A specific group of features often decides the state of a given object. An example of such a task is the feature selection that allows increasing the decision’s quality while minimizing the cost of features or the total budget. The work’s main purpose is to compare feature selection methods such as the classical approach, the one-objective optimization, and the multi-objective optimization. The article proposes a feature selection algorithm using the Genetic Algorithm with various criteria, i.e., the cost and accuracy. In this way, the optimal Pareto points for the nonlinear problem of multi-criteria optimization were obtained. These points constitute a compromise between two conflicting objectives. By carrying out various experiments on various base classifiers, it has been shown that the proposed approach can be used in the task of optimizing difficult data.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part III},
pages = {81–94},
numpages = {14},
keywords = {Classification, Cost-sensitive, Feature selection, Multi-objective optimization},
location = {Krakow, Poland}
}

@article{10.1016/j.imavis.2017.09.004,
author = {Lee, Pui Yi and Loh, Wei Ping and Chin, Jeng Feng},
title = {Feature selection in multimedia},
year = {2017},
issue_date = {November 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2017.09.004},
doi = {10.1016/j.imavis.2017.09.004},
abstract = {Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining. Reviews on 70 relevant literatures from 2001 to 2017 from multiple databasesStudy data types, methods, search strategies, performance measures, and challengesThe trend of FS shows the increased prominence of heuristic search strategies.},
journal = {Image Vision Comput.},
month = nov,
pages = {29–42},
numpages = {14},
keywords = {Search strategies, Multimedia, Feature selection, Data mining}
}

@inproceedings{10.1145/3205651.3208227,
author = {Saito, Shota and Shirakawa, Shinichi and Akimoto, Youhei},
title = {Embedded feature selection using probabilistic model-based optimization},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208227},
doi = {10.1145/3205651.3208227},
abstract = {In machine learning, feature selection is a commonly used technique for improving the predictive performance and interpretability of a trained model. Feature selection techniques are classified into three approaches: the filter, wrapper, and embedded approaches. The embedded approach performs the feature selection process during the model training and achieves a good balance between performance and computational cost in general. In the paper, we propose a novel embedded feature selection method using probabilistic model-based evolutionary optimization. We introduce the multivariate Bernoulli distribution, which determines the selection of features, and we optimize its parameters during the training. The distribution parameter update rule is the same as that of the population-based incremental learning (PBIL), but we simultaneously update the parameters of the machine learning model using an ordinary gradient descent method. This method can be easily implemented into non-linear models, such as neural networks. Moreover, we incorporate the penalty term into the objective function to control the number of selected feature. We apply the proposed method with the neural network model to the feature selection of three classification problems. The proposed method achieves competitive performance and reasonable computational cost compared with conventional feature selection methods.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1922–1925},
numpages = {4},
keywords = {neural network, natural gradient, information geometric optimization, feature selection, embedded approach},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.patrec.2018.04.007,
author = {Cilia, Nicole Dalia and De Stefano, Claudio and Fontanella, Francesco and Scotto di Freca, Alessandra},
title = {A ranking-based feature selection approach for handwritten character recognition},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.04.007},
doi = {10.1016/j.patrec.2018.04.007},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {77–86},
numpages = {10},
keywords = {65D17, 65D05, 41A10, 41A05, Handwritten character recognition, Feature selection}
}

@article{10.3233/JIFS-202647,
author = {Qiu, Chenye and Liu, Ning},
title = {A novel three layer particle swarm optimization for feature selection},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-202647},
doi = {10.3233/JIFS-202647},
abstract = {Feature selection (FS) is a vital data preprocessing task which aims at selecting a small subset of features while maintaining a high level of classification accuracy. FS is a challenging optimization problem due to the large search space and the existence of local optimal solutions. Particle swarm optimization (PSO) is a promising technique in selecting optimal feature subset due to its rapid convergence speed and global search ability. But PSO suffers from stagnation or premature convergence in complex FS problems. In this paper, a novel three layer PSO (TLPSO) is proposed for solving FS problem. In the TLPSO, the particles in the swarm are divided into three layers according to their evolution status and particles in different layers are treated differently to fully investigate their potential. Instead of learning from those historical best positions, the TLPSO uses a random learning exemplar selection strategy to enrich the searching behavior of the swarm and enhance the population diversity. Further, a local search operator based on the Gaussian distribution is performed on the elite particles to improve the exploitation ability. Therefore, TLPSO is able to keep a balance between population diversity and convergence speed. Extensive comparisons with seven state-of-the-art meta-heuristic based FS methods are conducted on 18 datasets. The experimental results demonstrate the competitive and reliable performance of TLPSO in terms of improving the classification accuracy and reducing the number of features.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {2469–2483},
numpages = {15},
keywords = {local search operator, random exemplar selection, three layer structure, particle swarm optimization, Feature selection}
}

@article{10.3233/KES-190134,
author = {Venkatesh, B. and Anuradha, J.},
title = {A fuzzy gaussian rank aggregation ensemble feature selection method for microarray data},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {4},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-190134},
doi = {10.3233/KES-190134},
abstract = {In Microarray Data, it is complicated to achieve more classification accuracy due to the presence of high dimensions, irrelevant and noisy data. And also It had more gene expression data and fewer samples. To increase the classification accuracy and the processing speed of the model, an optimal number of features need to extract, this can be achieved by applying the feature selection method. In this paper, we propose a hybrid ensemble feature selection method. The proposed method has two phases, filter and wrapper phase in filter phase ensemble technique is used for aggregating the feature ranks of the Relief, minimum redundancy Maximum Relevance (mRMR), and Feature Correlation (FC) filter feature selection methods. This paper uses the Fuzzy Gaussian membership function ordering for aggregating the ranks. In wrapper phase, Improved Binary Particle Swarm Optimization (IBPSO) is used for selecting the optimal features, and the RBF Kernel-based Support Vector Machine (SVM) classifier is used as an evaluator. The performance of the proposed model are compared with state of art feature selection methods using five benchmark datasets. For evaluation various performance metrics such as Accuracy, Recall, Precision, and F1-Score are used. Furthermore, the experimental results show that the performance of the proposed method outperforms the other feature selection methods.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {289–301},
numpages = {13},
keywords = {FC and kernel SVM, mRMR, relief, IBPSO, ensemble method, Hybrid feature selection}
}

@article{10.1007/s10772-021-09866-4,
author = {Ji, Xunsheng and Jiang, Kun and Xie, Jie},
title = {LBP-based bird sound classification using improved feature selection algorithm},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-021-09866-4},
doi = {10.1007/s10772-021-09866-4},
abstract = {Local binary pattern (LBP)-based features for bird sound classification were investigated in this study, including both one-dimensional (LBP-1D) and two-dimensional (LBP-2D) local binary patterns. Specifically, the discrete wavelet transform was first used as a pooling method to generate multi-level features in both time (LBP-1D-T) and frequency domain (LBP-1D-F) signals. To obtain richer time–frequency information of bird sounds, uniform patterns (LBP-2D) were extracted from the log-scaled Mel spectrogram. To fully exploit the complementarity of different LBP features, a hybrid fusion method was implemented. Next, neighborhood component analysis (NCA) was employed as a feature selection method to remove redundant information in the fused feature set. In order to reduce the running time of NCA and improve the classification accuracy, an improved feature selection method (DSNCA) was proposed. Finally, two machine learning algorithms: K-nearest neighbor and support vector machine were used for classification. Experimental results on 43 bird species of North American wood-warblers indicated that LBP-2D achieved a higher balanced-accuracy than LBP-1D-T and LBP-1D-F (86.33%, 81.05% and 70.02%, respectively). In addition, the highest classification accuracy was up to 88.70%, using hybrid fusion.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {1033–1045},
numpages = {13},
keywords = {Hybrid fusion, Wavelet decomposition, Local binary pattern, Bird sound classification}
}

@article{10.1007/s11227-020-03533-2,
author = {Shin, Mincheol and Park, Geunchul and Park, Chan Yeol and Lee, Jongmin and Kim, Mucheol},
title = {Application-specific feature selection and clustering approach with HPC system profiling data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {7},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03533-2},
doi = {10.1007/s11227-020-03533-2},
abstract = {Exascale computing,
 the next-generation computing environment, is expected to be applied to scientific and engineering applications. Accordingly, high-performance computing (HPC) technology is also being developed to improve the performance and high-speed parallelism of many-core processors. Previous researches on improving HPC performance have developed in the form of improving the overall system performance by analyzing the state of the system occurring in the range of the knowledge of expert. However, performance events occurring in a processor in a many-core environment have a large number of indicators, and it is difficult to analyze the correlation between them. In this paper, we propose an application-specific feature selection and clustering approach with HPC system profiling data. The proposed approach performs PCA-based feature selections for efficient performance analysis methods. In addition, the application-specific characteristics from profiling data can be analyzed by unsupervised learning. In our experiments, we evaluated highly parallel supercomputers with NAS parallel benchmark and were able to cluster applications efficiently.},
journal = {J. Supercomput.},
month = jul,
pages = {6817–6831},
numpages = {15},
keywords = {Knights Landing processor, Many-core systems, System profiling, Feature selection, Performance enhancement, High performance computing}
}

@inproceedings{10.1145/3233027.3236404,
author = {Gazzillo, Paul and Koc, Ugur and Nguyen, ThanhVu and Wei, Shiyi},
title = {Localizing configurations in highly-configurable systems},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236404},
doi = {10.1145/3233027.3236404},
abstract = {The complexity of configurable systems has grown immensely, and it is only getting more complex. Such systems are a challenge for software testing and maintenance, because bugs and other defects can and do appear in any configuration. One common requirement for many development tasks is to identify the configurations that lead to a given defect or some other program behavior. We distill this requirement down to a challenge question: given a program location in a source file, what are valid configurations that include the location? The key obstacle is scalability. When there are thousands of configuration options, enumerating all combinations is exponential and infeasible. We provide a set of target programs of increasing difficulty and variations on the challenge question so that submitters of all experience levels can try out solutions. Our hope is to engage the community and stimulate new and interesting approaches to the problem of analyzing configurations.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {269–273},
numpages = {5},
keywords = {variability, testing, program analysis, configurations},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1504/ijica.2021.116653,
author = {Durga, S. and Daniel, Esther and Kanmani, S. Deepa and Philip, Jinsa Mary},
title = {Cardiac arrhythmia classification using sequential feature selection and decision tree classifier method},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {12},
number = {4},
issn = {1751-648X},
url = {https://doi.org/10.1504/ijica.2021.116653},
doi = {10.1504/ijica.2021.116653},
abstract = {Cardiac arrhythmia is referred to as a condition in which the heart's normal functionality is restricted resulting in cardiovascular diseases. Effective and well-timed monitoring is very much essential to save human life. During the past few years, keeping track of when and how arrhythmias occur has gained a lot of significance as it leads way towards many life-threatening issues like stroke, sudden cardiac arrest and also heart failure. This paper provides an evaluation of various classification algorithms based on feature selection techniques that improve the performance of the cardio monitoring system. The pre-eminent features are sorted out using feature selection methods. The feature selection methods enable to decide the features that can contribute to improve the performance. The paper also gives efficient combinations of distinct classification algorithms along with feature selections which improves the accuracy. Few popular machine learning algorithms namely na\"{\i}ve Bayes, support vector machine and decision tree from contemporary literatures were applied to evaluate the performance with feature selection methods. The experimental result shows that the decision tree classifier with sequential feature selection provides improved accuracy of 84.13%.},
journal = {Int. J. Innov. Comput. Appl.},
month = jan,
pages = {175–182},
numpages = {7},
keywords = {sequential feature selection, decision tree classifier, accuracy, classification algorithms, cardiac arrhythmia}
}

@article{10.2478/acss-2019-0015,
author = {Ali, Tariq and Nawaz, Asif and Sadia, Hafiza Ayesha},
title = {Genetic Algorithm Based Feature Selection Technique for Electroencephalography Data},
year = {2019},
issue_date = {Dec 2019},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {24},
number = {2},
issn = {2255-8691},
url = {https://doi.org/10.2478/acss-2019-0015},
doi = {10.2478/acss-2019-0015},
abstract = {High dimensionality is a well-known problem that has a huge number of highlights in the data, yet none is helpful for a particular data mining task undertaking, for example, classification and grouping. Therefore, selection of features is used frequently to reduce the data set dimensionality. Feature selection is a multi-target errand, which diminishes dataset dimensionality, decreases the running time, and furthermore enhances the expected precision. In the study, our goal is to diminish the quantity of features of electroencephalography data for eye state classification and achieve the same or even better classification accuracy with the least number of features. We propose a genetic algorithm-based feature selection technique with the KNN classifier. The accuracy is improved with the selected feature subset using the proposed technique as compared to the full feature set. Results prove that the classification precision of the proposed strategy is enhanced by 3 % on average when contrasted with the accuracy without feature selection.},
journal = {Appl. Comput. Syst.},
month = dec,
pages = {119–127},
numpages = {9},
keywords = {genetic algorithms, feature extraction, evolutionary computation, Classification algorithms}
}

@article{10.1504/ijcvr.2021.116558,
author = {Mandal, Sunandan and Singh, Bikesh Kumar and Thakur, Kavita},
title = {Majority voting-based hybrid feature selection in machine learning paradigm for epilepsy detection using EEG},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1752-9131},
url = {https://doi.org/10.1504/ijcvr.2021.116558},
doi = {10.1504/ijcvr.2021.116558},
abstract = {This article presents a combination of statistical and discrete wavelet transform (DWT)-based features for the identification of epileptic seizures in electroencephalogram (EEG) signals. A total of 150 quantitative features are extracted from EEG signals. A multi-criteria hybrid feature selection is proposed by combining six feature ranking methods using the majority voting technique to identify the most relevant EEG markers. Kernel-based support vector machine is used to evaluate the proposed approach along with a hybrid classifier namely support vector neural network (SVNN) which is a combination of support vector machine (SVM) and artificial neural network (ANN). For performance evaluation of the proposed method, a benchmarked database is used. A comparative study of various types of SVM and SVNN with ten-fold and hold-out cross-validation techniques is conducted. The highest classification accuracy (CA) of 98.18% and 100% sensitivity is achieved with a fine Gaussian SVM classifier with hold-out data division protocol.},
journal = {Int. J. Comput. Vision Robot.},
month = jan,
pages = {385–400},
numpages = {15},
keywords = {classification, multi-criteria feature selection, wavelet transform, epilepsy, EEG quantitative features}
}

@inproceedings{10.1007/978-3-031-12700-7_36,
author = {Marjit, Shyam and Talukdar, Upasana and Hazarika, Shyamanta M.},
title = {Enhancing EEG-Based Emotion Recognition Using MultiDomain Features and&nbsp;Genetic Algorithm Based Feature Selection},
year = {2021},
isbn = {978-3-031-12699-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12700-7_36},
doi = {10.1007/978-3-031-12700-7_36},
abstract = {Electroencephalography (EEG) based emotion recognition has become a subtle research area because of its promising applications. An effective emotion recognition relies on significant and stable features. In this paper, we propose an EEG based emotion recognition methodology based on a hybrid feature extraction combined with Genetic Algorithm (GA) based feature selection. The features are extracted from three domains: time, frequency and discrete wavelet The proposal is evaluated on DEAP dataset where the emotional states are classified using a GA optimized Multi-Layer Perceptron. The proposed model identifies a. two classes of emotions viz. Low/High Valence with an average accuracy of 95.96% and Low/High Arousal with an average accuracy of 95.39%, b. four classes of emotions viz. High Valence-Low Arousal, High Valence-High Arousal, Low Valence-Low Arousal and Low Valence-High Arousal with 91.88% accuracy, which are better compared to the existing results reported in the literature.},
booktitle = {Pattern Recognition and Machine Intelligence: 9th International Conference, PReMI 2021, Kolkata, India, December 15–18, 2021, Proceedings},
pages = {345–353},
numpages = {9},
keywords = {EEG, Emotions, Time Domain, Frequency Domain, Genetic Algorithm, Feature Selection, Multi-Layer Perceptron},
location = {Kolkata, India}
}

@article{10.1007/s00521-020-04971-y,
author = {Ben Brahim, Afef},
title = {Stable feature selection based on instance learning, redundancy elimination and efficient subsets fusion},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04971-y},
doi = {10.1007/s00521-020-04971-y},
abstract = {Feature selection is frequently used as a preprocessing step to data mining and is attracting growing attention due to the increasing amounts of data emerging from different domains. The large data dimensionality increases the noise and thus the error of learning algorithms. Filter methods for feature selection are specially very fast and useful for high-dimensional datasets. Existing methods focus on producing feature subsets that improve predictive performance, but they often suffer from instability. Instance-based filters, for example, are considered as one of the most effective methods that rank features based on instances neighborhood. However, as the feature weight fluctuates with the instances, small changes in training data result in a different selected subset of features. By another hand, some other filters generate stable results but lead to a modest predictive performance. The absence of a trade-off between stability and classification accuracy decreases the reliability of the feature selection results. In order to deal with this issue, we propose filter methods that improve stability of feature selection while preserving an optimal predictive accuracy and without increasing the complexity of the feature selection algorithms. The proposed approaches first use the strength of instance learning to identify initial sets of relevant features, and the advantage of aggregation techniques to increase the stability of the final set in a second stage. Two classification algorithms are used to evaluate the predictive performance of our proposed instance-based filters compared to state-of-the-art algorithms. The obtained results show the efficiency of our methods in improving both classification accuracy and feature selection stability for high-dimensional datasets.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1221–1232},
numpages = {12},
keywords = {Stability, Instance-based learning, High dimensionality, Feature selection}
}

@article{10.1007/s10489-019-01543-z,
author = {Shahee, Shaukat Ali and Ananthakumar, Usha},
title = {An effective distance based feature selection approach for imbalanced data},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01543-z},
doi = {10.1007/s10489-019-01543-z},
abstract = {Class imbalance is one of the critical areas in classification. The challenges become more severe when the data set has a large number of features. Traditional classifiers generally favour the majority class because of skewed class distributions. In recent years, feature selection is being used to select the appropriate features for better classification of minority class. However, these studies are limited to imbalance that arise between the classes. In addition to between class imbalance, within class imbalance, along with large number of features, adds additional complexity and results in poor performance of the classifier. In the current study, we propose an effective distance based feature selection method (ED-Relief) that uses a sophisticated distance measure, in order to tackle simultaneous occurrence of between and within class imbalance. This method has been tested on a variety of simulated experiments and real life data sets and the results are compared with the traditional Relief method and some of the well known recent distance based feature selection methods. The results clearly show the superiority of the proposed effective distance based feature selection method.},
journal = {Applied Intelligence},
month = mar,
pages = {717–745},
numpages = {29},
keywords = {Jeffreys divergence, Classification, Effective distance, Feature selection, Imbalanced data}
}

@phdthesis{10.5555/AAI28865638,
author = {Rendleman, Michael C. and A., Braun, Terry and M., Buatti, John and Guadalupe, Canahuate, and J, Smith, Brian},
advisor = {L, Casavant, Thomas},
title = {Representative Random Sampling for Feature Engineering of -Omics Data: Using Machine Learning to Identify Biomarkers for Head and Neck Squamous Cell Carcinoma},
year = {2021},
isbn = {9798790625770},
publisher = {The University of Iowa},
abstract = {High-dimensional cancer data can be burdensome to analyze, with complex relationships between molecular measurements, clinical diagnostics, and treatment outcomes. Data-driven computational approaches may be key to identifying research targets with potential clinical or research use, also known as biomarkers. To this end, we designed a framework for engineering and identifying biomarkers for survival model building, applying a variety of established and novel feature engineering methods on publicly available Head and Neck Squamous Cell Carcinoma (HNSCC) data. This dataset includes over 500 cases and spans numerous data types including clinical data, RNA sequencing, and tumor-normal DNA variation. Given the limited size of the dataset, a specialized sampling technique was devised to increase reliability of performance estimation with less computation. Traditionally, resampling methods such as cross validation or repeated holdout have been used to estimate model performance, as they produce more robust estimates. Because exploratory evaluations in the feature selection framework required an intractable manual inspection and assessment process, we propose employing a novel holdout sampling procedure, Representative Random Sampling (RRS). RRS first quantizes the continuous outcome into equipopulous bins of minimum size and then selects the holdout set via stratified sampling. Utilizing thorough simulations on synthetic molecular data, we have determined that this approach yields at least modest reductions in error and bias when compared to standard holdout, though direct cross validation may still be significantly more effective at reducing error and bias. Additionally, model selection has a large effect on error and bias estimation: RRS produced the most consistent decreases in error and bias with random forest-based models. Using RRS, a two-stage analysis framework enables evaluation and selection of prospective biomarker features which are then applied to survival modeling. Thousands of raw and processed molecular features were assessed on their ability to predict clinical diagnostics and patient survival, ultimately supporting a predictive survival model that outperformed corresponding clinical models. Model analysis demonstrated associations between patient outcomes and biological pathways and processes, several of which are the subject of recent and ongoing oncology research in HNSCC and other cancers. Additionally, unsupervised transformations of RNA expression data facilitated by denoising autoencoders (DAE) were found to strengthen prognostic models against overfitting and in predictive performance.},
note = {AAI28865638}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Stacking, Hybrid feature selection, Ensemble machine learning, Machine learning, Code smell}
}

@inproceedings{10.1007/978-3-030-89657-7_21,
author = {Gawinecki, Maciej and Szmyd, Wojciech and \.{Z}uchowicz, Urszula and Walas, Marcin},
title = {What Makes a Good Movie Recommendation? Feature Selection for Content-Based Filtering},
year = {2021},
isbn = {978-3-030-89656-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89657-7_21},
doi = {10.1007/978-3-030-89657-7_21},
abstract = {Nowadays, recommendation systems are becoming ubiquitous, especially in the entertainment industry, such as movie streaming services. In More-Like-This recommendation approach, movies are suggested based on attributes of a&nbsp;currently inspected movie. However, it is not obvious which features are the best predictors for similarity, as perceived by users. To address this problem, we developed and evaluated a&nbsp;recommendation system consisting of nine features and a&nbsp;variety of their representations. We crowdsourced relevance judgments for more than 5 thousand movie recommendations to evaluate the configurations of&nbsp;several dozen of&nbsp;movie features. From five embedding techniques for textual attributes, we selected Universal Sentence Encoder model as the best representation method for producing recommendations. Evaluation of movie features relevance showed that summary and categories extracted from Wikipedia led to the highest similarity on user perceptions in comparison to other analyzed features. We applied the feature weighting methods, commonly used in classification tasks, to determine optimal weights for a given feature set. Our results showed that we can reduce features to only genres, summary, plot, categories, and release year without losing the quality of&nbsp;recommendations.},
booktitle = {Similarity Search and Applications: 14th International Conference, SISAP 2021, Dortmund, Germany, September 29 – October 1, 2021, Proceedings},
pages = {280–294},
numpages = {15},
keywords = {Feature weighting, Feature selection, Content-based filtering, Recommender system},
location = {Dortmund, Germany}
}

@inproceedings{10.1007/978-3-030-64881-7_20,
author = {Jin, Hao and Kitamura, Takashi and Choi, Eun-Hye and Tsuchiya, Tatsuhiro},
title = {A Comparative Study on Combinatorial and Random Testing for Highly Configurable Systems},
year = {2020},
isbn = {978-3-030-64880-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64881-7_20},
doi = {10.1007/978-3-030-64881-7_20},
abstract = {Highly configurable systems (HCSs), such as software product lines, have complex configuration spaces. Combinatorial Testing and Random Testing are the main approaches to testing of HCSs. In this paper, we empirically compare their strengths with respect to scalability and diversity of sampled configurations (i.e., tests). We choose Icpl&nbsp;and QuickSampler&nbsp;to respectively represent Combinatorial Testing and Random Testing. Experiments are conducted to evaluate the t-way coverage criterion of generated test suites for HCS benchmarks.},
booktitle = {Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9–11, 2020, Proceedings},
pages = {302–309},
numpages = {8},
keywords = {Software product line, Random testing, Combinatorial testing},
location = {Naples, Italy}
}

@article{10.1007/s10115-020-01526-4,
author = {Morillo-Salas, Jos\'{e} Luis and Bol\'{o}n-Canedo, Ver\'{o}nica and Alonso-Betanzos, Amparo},
title = {Dealing with heterogeneity in the context of distributed feature selection for classification},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01526-4},
doi = {10.1007/s10115-020-01526-4},
abstract = {Advances in the information technologies have greatly contributed to the advent of larger datasets. These datasets often come from distributed sites, but even so, their large size usually means they cannot be handled in a centralized manner. A possible solution to this problem is to distribute the data over several processors and combine the different results. We propose a methodology to distribute feature selection processes based on selecting relevant and discarding irrelevant features. This preprocessing step is essential for current high-dimensional sets, since it allows the input dimension to be reduced. We pay particular attention to the problem of data imbalance, which occurs because the original dataset is unbalanced or because the dataset becomes unbalanced after data partitioning. Most works approach unbalanced scenarios by oversampling, while our proposal tests both over- and undersampling strategies. Experimental results demonstrate that our distributed approach to classification obtains comparable accuracy results to a centralized approach, while reducing computational time and efficiently dealing with data imbalance.},
journal = {Knowl. Inf. Syst.},
month = jan,
pages = {233–276},
numpages = {44},
keywords = {Oversampling, Unbalanced data, Distributed learning, Feature selection}
}

@inproceedings{10.1007/978-3-030-95388-1_37,
author = {Chen, Guo and Zheng, Junyao and Yang, Shijun and Zhou, Jieying and Wu, Weigang},
title = {FSAFA-stacking2: An Effective Ensemble Learning Model for Intrusion Detection with Firefly Algorithm Based Feature Selection},
year = {2021},
isbn = {978-3-030-95387-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95388-1_37},
doi = {10.1007/978-3-030-95388-1_37},
abstract = {This paper presents a two-layer ensemble learning model stacking2 based on the Stacking framework to deal with the problems of lack of generalization ability and low detection rate of single model intrusion detection system. The stacking2 uses SAMME, GBDT, and RF to generate the primary learner in the first layer and constructs the meta learner using the logistic regression algorithm in the second layer. The meta learner learns from the class probability outputs produced by the primary learner. In order to solve “the curse of dimensionality” of intrusion detection dataset, this paper proposes the feature selection approach based on firefly algorithm (FSAFA), which is used to select the optimal feature subsets. Based on the selected optimal feature subsets, the training set and test set are reconstructed and then applied to stacking2. As a result, a FSAFA based stacking2 intrusion detection model is proposed. The UNSW-NB15 and NSL-KDD datasets are chosen to verify the effectiveness of the proposed model. The experiment results show that the stacking2 intrusion detection model has better generalization ability than the individual learner based intrusion detection models. Compared with other typical algorithms, the FSAFA based stacking2 intrusion detection model has good performance in detection rate.},
booktitle = {Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part II},
pages = {555–570},
numpages = {16},
keywords = {Firefly algorithm, Feature selection, Stacking, Ensemble learning, Intrusion detection}
}

@inproceedings{10.1145/3132847.3133055,
author = {Wei, Xiaokai and Cao, Bokai and Yu, Philip S.},
title = {Unsupervised Feature Selection with Heterogeneous Side Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133055},
doi = {10.1145/3132847.3133055},
abstract = {Compared to supervised feature selection, unsupervised feature selection tends to be more challenging due to the lack of guidance from class labels. Along with the increasing variety of data sources, many datasets are also equipped with certain side information of heterogeneous structure. Such side information can be critical for feature selection when class labels are unavailable. In this paper, we propose a new feature selection method, SideFS, to exploit such rich side information. We model the complex side information as a heterogeneous network and derive instance correlations to guide subsequent feature selection. Representations are learned from the side information network and the feature selection is performed in a unified framework. Experimental results show that the proposed method can effectively enhance the quality of selected features by incorporating heterogeneous side information.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2359–2362},
numpages = {4},
keywords = {unsupervised learning, side information, heterogeneous information network, feature selection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.5555/3288251.3288291,
author = {Zheng, Wei and Zhu, Xiaofeng and Zhu, Yonghua and Hu, Rongyao and Lei, Cong},
title = {Dynamic graph learning for spectral feature selection},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Previous spectral feature selection methods generate the similarity graph via ignoring the negative effect of noise and redundancy of the original feature space, and ignoring the association between graph matrix learning and feature selection, so that easily producing suboptimal results. To address these issues, this paper joints graph learning and feature selection in a framework to obtain optimal selected performance. More specifically, we use the least square loss function and an ℓ2,1-norm regularization to remove the effect of noisy and redundancy features, and use the resulting local correlations among the features to dynamically learn a graph matrix from a low-dimensional space of original data. Experimental results on real data sets show that our method outperforms the state-of-the-art feature selection methods for classification tasks.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29739–29755},
numpages = {17},
keywords = {Spectral feature selection, Optimization, Graph learning}
}

@article{10.1007/s00521-020-05409-1,
author = {Ahmed, Shameem and Ghosh, Kushal Kanti and Garcia-Hernandez, Laura and Abraham, Ajith and Sarkar, Ram},
title = {Improved coral reefs optimization with adaptive β-hill climbing for feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05409-1},
doi = {10.1007/s00521-020-05409-1},
abstract = {For any classification problem, the dimension of the feature vector used for classification has great importance. This is because, in a high-dimensional feature vector, it is found that some are non-informative or even redundant as they do not contribute to the learning process of the classifier. Rather, they may be the reason for low classification accuracy and high training time of the learning model. To address this issue, researchers apply various feature selection (FS) methods as found in the literature. In recent years, meta-heuristic algorithms have been proven to be effective in solving FS problems. The Coral Reefs Optimizer (CRO) which is a cellular type evolutionary algorithms has good tuning between its exploration and exploitation ability. This has motivated us to present an improved version of CRO with the inclusion of adaptive β-hill climbing to increase the exploitation ability of CRO. The proposed method is assessed on 18 standard UCI-datasets by means of three distinct classifiers, KNN, Random Forest and Naive Bayes classifiers. It is also analyzed with 10 state-of-the-art meta-heuristics FS procedure, and the outputs show an excellent performance of the proposed FS method reaching better results than the previous methods considered here for comparison. The source code of this work is publicly available at .},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6467–6486},
numpages = {20},
keywords = {Hybrid optimization, Adaptive β-hill climbing, Coral reefs optimization, UCI, Feature selection, Meta-heuristic}
}

@inproceedings{10.1145/3377930.3390198,
author = {Leon, Miguel and Parkkila, Christoffer and Tidare, Jonatan and Xiong, Ning and Astrand, Elaine},
title = {Impact of NSGA-II objectives on EEG feature selection related to motor imagery},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390198},
doi = {10.1145/3377930.3390198},
abstract = {The selection of ElectroEncephaloGram (EEG) features with functional relevance to Motor Imagery (MI) is a crucial task for successful outcome in Brain-Computer Interface (BCI)-based motor rehabilitation. Individual EEG patterns during MI requires subject-dependent feature selection, which is an arduous task due to the complexity and large number of features. One solution is to use metaheuristics, e.g. Genetic Algorithm (GA), to avoid an exhaustive search which is impractical. In this work, one of the most widely used GA, NSGA-II, is used with an hierarchical individual representation to facilitate the exclusion of EEG channels irrelevant for MI. In essence, the performance of different objectives in NSGA-II was evaluated on a previously recorded MI EEG data set. Empirical results show that k-Nearest Neighbors (k-NN) combined with Pearson's Correlation (PCFS) as objective functions yielded higher classification accuracy as compared to the other objective-combinations (73% vs. 69%). Linear Discriminant Analysis (LDA) combined with Feature Reduction (FR) as objective functions maximized the reduction of features (99.6%) but reduced classification performance (65.6%). All classifier objectives combined with PCFS selected similar features in accordance with expected activity patterns during MI. In conclusion, PCFS and a classifier as objective functions constitutes a good trade-off solution for MI data.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1134–1142},
numpages = {9},
keywords = {mental imagery, feature selection, fast elitist non-dominated sorting genetic algorithm (NSGA-II), ElectroEncephaloGram (EEG)},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1016/j.infsof.2012.02.005,
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
title = {Issue-based variability management},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.005},
doi = {10.1016/j.infsof.2012.02.005},
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {933–950},
numpages = {18},
keywords = {Requirements engineering, Rationale management, Product line engineering, Empirical software engineering}
}

@article{10.1109/TCBB.2021.3053181,
author = {Liu, Liangliang and Tang, Shaojie and Wu, Fang-Xiang and Wang, Yu-Ping and Wang, Jianxin},
title = {An Ensemble Hybrid Feature Selection Method for Neuropsychiatric Disorder Classification},
year = {2021},
issue_date = {May-June 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3053181},
doi = {10.1109/TCBB.2021.3053181},
abstract = {Magnetic resonance imagings (MRIs) are providing increased access to neuropsychiatric disorders that can be made available for advanced data analysis. However, the single type of data limits the ability of psychiatrists to distinguish the subclasses of this disease. In this paper, we propose an ensemble hybrid features selection method for the neuropsychiatric disorder classification. The method consists of a 3D DenseNet and a XGBoost, which are used to select the image features from structural MRI images and the phenotypic feature from phenotypic records, respectively. The hybrid feature is composed of image features and phenotypic features. The proposed method is validated in the Consortium for Neuropsychiatric Phenomics (CNP) dataset, where samples are classified into one of the four classes (healthy controls (HC), attention deficit hyperactivity disorder (ADHD), bipolar disorder (BD), and schizophrenia (SD)). Experimental results show that the hybrid feature can improve the performance of classification methods. The best accuracy of binary and multi-class classification can reach 91.22 and 78.62 percent, respectively. We analyze the importance of phenotypic features and image features in different classification tasks. The importance of the structure MRI images is highlighted by incorporating phenotypic features with image features to generate hybrid features. We also visualize the features of three neuropsychiatric disorders and analyze their locations in the brain region.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {1459–1471},
numpages = {13}
}

@inproceedings{10.1007/978-3-030-86653-2_12,
author = {Trevizan, Bernardo and Recamonde-Mendoza, Mariana},
title = {Ensemble Feature Selection Compares to&nbsp;Meta-analysis for Breast Cancer Biomarker Identification from Microarray Data},
year = {2021},
isbn = {978-3-030-86652-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86653-2_12},
doi = {10.1007/978-3-030-86653-2_12},
abstract = {Identifying stable and precise biomarkers is a key challenge in precision medicine. A promising approach in this direction is exploring omics data, such as transcriptome generated by microarrays, to discover candidate biomarkers. This, however, involves the fundamental issue of finding the most discriminative features in high-dimensional datasets. We proposed a homogeneous ensemble feature selection (EFS) method to extract candidate biomarkers of breast cancer from microarray datasets. Ensemble diversity is introduced by bootstraps and by the integration of seven microarray studies. As a baseline method, we used the random effect model meta-analysis, a state-of-the-art approach in the integrative analysis of microarrays for biomarkers discovery. We compared five feature selection (FS) methods as base selectors and four algorithms as base classifiers. Our results showed that the variance FS method is the most stable among the tested methods regardless of the classifier and that stability is higher within datasets than across datasets, indicating high sample heterogeneity among studies. The predictive performance of the top 20 genes selected with both approaches was evaluated with six independent microarray studies, and in four of these, we observed a superior performance of our EFS approach as compared to meta-analysis. EFS recall was as high as 85%, and the median F1-scores surpassed 80% for most of our experiments. We conclude that homogeneous EFS is a promising methodology for candidate biomarkers identification, demonstrating stability and predictive performance as satisfactory as the statistical reference method.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part I},
pages = {162–178},
numpages = {17},
location = {Cagliari, Italy}
}

@article{10.1016/j.aei.2021.101433,
author = {Yu, Song and Tan, Weimin and Zhang, Chengming and Fang, Yun and Tang, Chao and Hu, Dong},
title = {Research on hybrid feature selection method of power transformer based on fuzzy information entropy},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2021.101433},
doi = {10.1016/j.aei.2021.101433},
journal = {Adv. Eng. Inform.},
month = oct,
numpages = {11},
keywords = {Fuzzy information entropy, Multi-objective programming, Feature selection, DGA, Power transformer}
}

@inproceedings{10.1145/3168365.3168382,
author = {Muniz, Raphael and Braz, Larissa and Gheyi, Rohit and Andrade, Wilkerson and Fonseca, Baldoino and Ribeiro, M\'{a}rcio},
title = {A Qualitative Analysis of Variability Weaknesses in Configurable Systems with #ifdefs},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168382},
doi = {10.1145/3168365.3168382},
abstract = {A number of critical configurable systems are implemented using #ifdefs, such as Linux. Some tools and strategies are proposed to avoid these directives. However, these systems still have weaknesses, leading to vulnerable code, and may impact millions of users. There is a lack of studies regarding the perception of developers of configurable systems with #ifdefs related to weaknesses, and the strategies and tools they use to identify and remove them. Moreover, few works study the characteristics of weaknesses. To better understand the problem, we conduct two studies. In the first one, we qualitatively analyze 27 variability weaknesses of Apache HTTPD, Linux and OpenSSL reported on their bug trackers. In the second study, we conduct a survey with 110 developers of the previous configurable systems. Overall, our results show evidences that, although developers care about weaknesses, they may not detect some weaknesses reported in the bug trackers, and do not use proper tools to deal with them. They take on median 15 days and 4 discussion messages to solve them. Some weaknesses occur due to two feature interactions, and most of them can be detected by the all macros enabled sampling approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {51–58},
numpages = {8},
keywords = {Variability Weaknesses, Survey, Security, Preprocessor, Configurable Systems, #ifdefs},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3319619.3326771,
author = {Ayodele, Mayowa},
title = {Application of estimation of distribution algorithm for feature selection},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326771},
doi = {10.1145/3319619.3326771},
abstract = {Feature selection is a machine learning concept that entails selecting relevant features while eliminating irrelevant and redundant features. This process helps to speed up learning. In this paper, an Estimation of Distribution Algorithm (EDA) is applied to a feature selection problem originating from a legal business. The EDA was able to generate a realistic solution to the real-world problem.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {43–44},
numpages = {2},
keywords = {support vector machine, feature selection, estimation of distribution algorithm},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1016/j.compbiomed.2020.103991,
author = {Sreejith, S. and Khanna Nehemiah, H. and Kannan, A.},
title = {Clinical data classification using an enhanced SMOTE and chaotic evolutionary feature selection},
year = {2020},
issue_date = {Nov 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.103991},
doi = {10.1016/j.compbiomed.2020.103991},
journal = {Comput. Biol. Med.},
month = nov,
numpages = {14},
keywords = {Multi Verse Optimisation, SMOTE, Classification, Chaotic maps, Feature selection, Class imbalance, Clinical decision support system}
}

@inproceedings{10.1145/3437802.3437835,
author = {Pilnenskiy, Nikita and Smetannikov, Ivan},
title = {BagMeLiF: stable boosting-based hybrid-ensemble feature selection algorithm for high-dimensional data},
year = {2021},
isbn = {9781450388054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437802.3437835},
doi = {10.1145/3437802.3437835},
abstract = {The problem of selecting features for a data set with a small number of objects is one of the most complex ones. Significant features selected for such data sets can vary quite a lot depending on how sub-sampling was performed during validation. This effect is called low feature set stability and signals on low reliability of the selected features. We propose a feature selection algorithm that is based on bagging procedure of feature selection filters quality measures ensemble and allows to obtain more stable feature sets, than would be obtained by running conventional algorithms, called BagMeLiF. This algorithm is based on MeLiF algorithm and will outperform original algorithm both in F1 score and stability with hyperparameter k around 0.7–0.9 if the dataset is well-balanced, but if it is not, then k around 0.1–0.2 will the best which is a quite straightforwardly applicable result.},
booktitle = {Proceedings of the 2020 1st International Conference on Control, Robotics and Intelligent System},
pages = {196–201},
numpages = {6},
keywords = {Machine Learning, Feature Selection, Ensembles, Dimensionality reduction},
location = {Xiamen, China},
series = {CCRIS '20}
}

@article{10.1007/s10994-018-5765-6,
author = {Shang, Ronghua and Meng, Yang and Liu, Chiyang and Jiao, Licheng and Esfahani, Amir M. Ghalamzan and Stolkin, Rustam},
title = {Unsupervised feature selection based on kernel fisher discriminant analysis and regression learning},
year = {2019},
issue_date = {Apr 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {4},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5765-6},
doi = {10.1007/s10994-018-5765-6},
abstract = {In this paper, we propose a new feature selection method called kernel fisher discriminant analysis and regression learning based algorithm for unsupervised feature selection. The existing feature selection methods are based on either manifold learning or discriminative techniques, each of which has some shortcomings. Although some studies show the advantages of two-steps method benefiting from both manifold learning and discriminative techniques, a joint formulation has been shown to be more efficient. To do so, we construct a global discriminant objective term of a clustering framework based on the kernel method. We add another term of regression learning into the objective function, which can impose the optimization to select a low-dimensional representation of the original dataset. We use L
2,1-norm of the features to impose a sparse structure upon features, which can result in more discriminative features. We propose an algorithm to solve the optimization problem introduced in this paper. We further discuss convergence, parameter sensitivity, computational complexity, as well as the clustering and classification accuracy of the proposed algorithm. In order to demonstrate the effectiveness of the proposed algorithm, we perform a set of experiments with different available datasets. The results obtained by the proposed algorithm are compared against the state-of-the-art algorithms. These results show that our method outperforms the existing state-of-the-art methods in many cases on different datasets, but the improved performance comes with the cost of increased time complexity.},
journal = {Mach. Learn.},
month = apr,
pages = {659–686},
numpages = {28},
keywords = {Feature selection, Sparse constraint, Regression learning, Manifold learning, Kernel fisher discriminant analysis}
}

@inproceedings{10.1145/3129676.3130240,
author = {Chaudhry, Muhammad Umar and Kim, Sang-Wook and Lee, Jee-Hyong},
title = {An Effective Feature Selection method using Monte Carlo Search},
year = {2017},
isbn = {9781450350273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129676.3130240},
doi = {10.1145/3129676.3130240},
abstract = {Feature selection is the challenging problem in the field of machine learning. The task is to identify the optimal feature subset by eliminating the redundant and irrelevant features from the dataset. The problem becomes more complicated when dealing with high-dimensional datasets. In this paper, we propose the novel technique based on Monte Carlo Tree Search (MCTS) to find the best feature subset to classify the dataset in hand. The effectiveness and validity of the proposed method is demonstrated by experimenting on many real world datasets.},
booktitle = {Proceedings of the International Conference on Research in Adaptive and Convergent Systems},
pages = {44–45},
numpages = {2},
keywords = {Monte Carlo Search, Heuristic Feature Selection, Feature Selection},
location = {Krakow, Poland},
series = {RACS '17}
}

@article{10.1016/j.asoc.2021.107897,
author = {Mehedi, Ibrahim Mustafa and Ahmadipour, Masoud and Salam, Zainal and Ridha, Hussein Mohammed and Bassi, Hussein and Rawa, Muhyaddin Jamal Hosin and Ajour, Mohammad and Abusorrah, Abdullah and Abdullah, Md. Pauzi},
title = {Optimal feature selection using modified cuckoo search for classification of power quality disturbances},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107897},
doi = {10.1016/j.asoc.2021.107897},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {15},
keywords = {Power quality disturbances, Wavelet packet transform, Modified cuckoo search, Multiclass support vector machine, Optimal feature selection}
}

@article{10.1007/s10489-019-01431-6,
author = {Wang, Chenxi and Lin, Yaojin and Liu, Jinghua},
title = {Feature selection for multi-label learning with missing labels},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01431-6},
doi = {10.1007/s10489-019-01431-6},
abstract = {In multi-label learning, feature selection is a non-ignorable preprocessing step which can alleviate the negative effect of high-dimensionality. To address this problem, a number of effective information theory based feature selection algorithms for multi-label learning are proposed. However, these existing algorithms assume that the label space of multi-label training data is complete. In practice, the standpoint does not always hold true, due to the ambiguity among class labels or the cost effort to fully annotate instances. In this paper, we first define the new concepts of multi-label information entropy and multi-label mutual information. Then, feature redundancy, feature independence, and feature interaction are defined, respectively. In which, feature interaction is used to select more valuable features which may be ignored due to the incomplete label space. Moreover, a multi-label feature selection method with missing labels is proposed. Finally, extensive experiments conducted on eight publicly available data sets verify the effectiveness of the proposed algorithm via comparing it with state-of-the-art methods.},
journal = {Applied Intelligence},
month = aug,
pages = {3027–3042},
numpages = {16},
keywords = {Feature interaction, Feature selection, Missing labels, Multi-label learning, Neighborhood mutual information}
}

@inproceedings{10.1007/978-3-030-41418-4_17,
author = {Chen, Yuntianyi and Gu, Yongfeng and He, Lulu and Xuan, Jifeng},
title = {Regression Models for Performance Ranking of Configurable Systems: A Comparative Study},
year = {2019},
isbn = {978-3-030-41417-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-41418-4_17},
doi = {10.1007/978-3-030-41418-4_17},
abstract = {Finding the best configurations for a highly configurable system is challenging. Existing studies learned regression models to predict the performance of potential configurations. Such learning suffers from the low accuracy and the high effort of examining the actual performance for data labeling. A recent approach uses an iterative strategy to sample a small number of configurations from the training pool to reduce the number of sampled ones. In this paper, we conducted a comparative study on the rank-based approach of configurable systems with four regression methods. These methods are compared on 21 evaluation scenarios of 16 real-world configurable systems. We designed three research questions to check the impacts of different methods on the rank-based approach. We find out that the decision tree method of Classification And Regression Tree (CART) and the ensemble learning method of Gradient Boosted Regression Trees (GBRT) can achieve better ranks among four regression methods under evaluation; the sampling strategy in the rank-based approach is useful to save the cost of sampling configurations; the measurement, i.e., rank difference correlates with the relative error in several evaluation scenarios.},
booktitle = {Structured Object-Oriented Formal Language and Method: 9th International Workshop, SOFL+MSVL 2019, Shenzhen, China, November 5, 2019, Revised Selected Papers},
pages = {243–258},
numpages = {16},
keywords = {Regression methods, Performance prediction, Sampling, Software configurations},
location = {Shenzhen, China}
}

@article{10.1007/s10489-019-01518-0,
author = {Zhang, Rui and Zhang, Zuoquan},
title = {Feature selection with Symmetrical Complementary Coefficient for quantifying feature interactions},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01518-0},
doi = {10.1007/s10489-019-01518-0},
abstract = {In the field of machine learning and data mining, feature interaction is a ubiquitous issue that cannot be ignored and has attracted more attention in recent years. In this paper, we proposed the Symmetrical Complementary Coefficient which can quantify feature interactions very well. Based on it, we improved the Sequential Forward Selection (SFS) algorithm and proposed a new feature subset searching algorithm called SCom-SFS which only needs to consider the feature interactions between adjacent features on a given sequence instead of all of them. Moreover, discovered feature interactions can speed up the process of searching for the optimal feature subset. In addition, we have improved the ReliefF algorithm by screening out representative samples from the original data set, and need not to sample the samples. The improved ReliefF algorithm has been proved to be more efficient and reliable. An effective and complete feature selection algorithm RRSS is obtained through the combination of the two modified algorithms. According to the experimental results, the proposed algorithm RRSS outperformed five classic and two latest feature selection algorithms in terms of size of resulting feature subset, Accuracy, Kappa coefficient, and adjusted Mean-Square Error (MSE).},
journal = {Applied Intelligence},
month = jan,
pages = {101–118},
numpages = {18},
keywords = {Symmetrical Complementary Coefficient, Random Forest, Feature interaction, Sequential Forward Selection, ReliefF, Feature selection}
}

@article{10.1016/j.jpdc.2019.12.015,
author = {Venkataramana, Lokeswari and Jacob, Shomona Gracia and Ramadoss, Rajavel},
title = {A Parallel Multilevel Feature Selection algorithm for improved cancer classification},
year = {2020},
issue_date = {Apr 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.12.015},
doi = {10.1016/j.jpdc.2019.12.015},
journal = {J. Parallel Distrib. Comput.},
month = apr,
pages = {78–98},
numpages = {21},
keywords = {Classification accuracy, Oncogenes and proteins, Horizontal &amp; Vertical partition, Parallel Random Forest, Parallel Multilevel Feature Selection}
}

@article{10.1007/s10664-020-09848-1,
author = {Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Treude, Christoph},
title = {The impact of automated feature selection techniques on the interpretation of defect models},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09848-1},
doi = {10.1007/s10664-020-09848-1},
abstract = {The interpretation of defect models heavily relies on software metrics that are used to construct them. Prior work often uses feature selection techniques to remove metrics that are correlated and irrelevant in order to improve model performance. Yet, conclusions that are derived from defect models may be inconsistent if the selected metrics are inconsistent and correlated. In this paper, we systematically investigate 12 automated feature selection techniques with respect to the consistency, correlation, performance, computational cost, and the impact on the interpretation dimensions. Through an empirical investigation of 14 publicly-available defect datasets, we find that (1) 94–100% of the selected metrics are inconsistent among the studied techniques; (2) 37–90% of the selected metrics are inconsistent among training samples; (3) 0–68% of the selected metrics are inconsistent when the feature selection techniques are applied repeatedly; (4) 5–100% of the produced subsets of metrics contain highly correlated metrics; and (5) while the most important metrics are inconsistent among correlation threshold values, such inconsistent most important metrics are highly-correlated with the Spearman correlation of 0.85–1. Since we find that the subsets of metrics produced by the commonly-used feature selection techniques (except for AutoSpearman) are often inconsistent and correlated, these techniques should be avoided when interpreting defect models. In addition to introducing AutoSpearman which mitigates correlated metrics better than commonly-used feature selection techniques, this paper opens up new research avenues in the automated selection of features for defect models to optimise for interpretability as well as performance.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3590–3638},
numpages = {49},
keywords = {Software analytics, Defect prediction, Model interpretation, Feature selection}
}

@article{10.1016/j.asoc.2019.106041,
author = {Wang, Xiao-han and Zhang, Yong and Sun, Xiao-yan and Wang, Yong-li and Du, Chang-he},
title = {Multi-objective feature selection based on artificial bee colony: An acceleration approach with variable sample size},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {88},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.106041},
doi = {10.1016/j.asoc.2019.106041},
journal = {Appl. Soft Comput.},
month = mar,
numpages = {8},
keywords = {Variable sample size, Multi-objective optimization, Feature selection, Artificial bee colony}
}

@inproceedings{10.1145/3443467.3443722,
author = {NingMin, Shen and YaQiong, Fan and Peng, Long},
title = {Feature Selection Algorithm based on Sparse Effective Distance combined with sPCA for Microarray Data},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443722},
doi = {10.1145/3443467.3443722},
abstract = {Feature selection is an efficient pre-processing method for high-dimensional data. It can promote the accuracy of classifiers by removing redundancy and outliers. However, the traditional feature selection method pay more attention on the correlation between samples and features while ignoring the correlation between samples and samples. When dealing with high-dimensional data with small sample sizes, those methods have poor performance. In this paper, we propose a new feature selection method based on sparse effective distance, which consider the local and global similarities among all the samples. Meanwhile, the proposed feature selection method combine with sparse principal component analysis (sPCA) to improve the sparsity and structure of loadings. The experiments were performed on eight gene expression data sets, and the results show that the proposed method can simultaneously improve the accuracy of classification and the stability.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {16–20},
numpages = {5},
keywords = {feature selection, sparse effective distance, sparse principal component analysis, structure loadings},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1007/978-3-030-88004-0_14,
author = {Huang, Zixuan and Zheng, Huicheng and Chen, Manwei},
title = {Foreground Feature Selection and Alignment for Adaptive Object Detection},
year = {2021},
isbn = {978-3-030-88003-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88004-0_14},
doi = {10.1007/978-3-030-88004-0_14},
abstract = {Recently, remarkable progress has been witnessed in adaptive object detection, which aims to mitigate the distributional shifts between source domain and target domain. Domain-adversarial learning methods align the features of different levels to minimize the domain discrepancy, which have been proven effective for adapting object detectors. Most domain adaptation methods align whole-image features. Therefore, foreground alignment may be interfered by the backgrounds. In this work, we propose Foreground Feature Alignment Framework (FFAF) that strengthens the foreground alignment. One of our key contributions is the Foreground Selection Module (FSM), which captures the foreground features that are crucial for object detection and helpful for subsequent feature alignment. Additionally, we align the foreground features by integrating multi-level domain classifiers. Multi-level Domain adaptation (MDA) can simultaneously bridge the domain gap at various representation levels. We evaluate our method with multiple experiments, whose results demonstrate that our method achieves significant improvements in different cross-domain object detection tasks.},
booktitle = {Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29 – November 1, 2021, Proceedings, Part I},
pages = {166–178},
numpages = {13},
keywords = {Object detection, Domain adaptation, Foreground feature alignment},
location = {Beijing, China}
}

@article{10.1007/s10115-017-1145-y,
author = {Palma-Mendoza, Raul-Jose and Rodriguez, Daniel and De-Marcos, Luis},
title = {Distributed ReliefF-based feature selection in Spark},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1145-y},
doi = {10.1007/s10115-017-1145-y},
abstract = {Feature selection (FS) is a key research area in the machine learning and data mining fields; removing irrelevant and redundant features usually helps to reduce the effort required to process a dataset while maintaining or even improving the processing algorithm's accuracy. However, traditional algorithms designed for executing on a single machine lack scalability to deal with the increasing amount of data that have become available in the current Big Data era. ReliefF is one of the most important algorithms successfully implemented in many FS applications. In this paper, we present a completely redesigned distributed version of the popular ReliefF algorithm based on the novel Spark cluster computing model that we have called DiReliefF. The effectiveness of our proposal is tested on four publicly available datasets, all of them with a large number of instances and two of them with also a large number of features. Subsets of these datasets were also used to compare the results to a non-distributed implementation of the algorithm. The results show that the non-distributed implementation is unable to handle such large volumes of data without specialized hardware, while our design can process them in a scalable way with much better processing times and memory usage.},
journal = {Knowl. Inf. Syst.},
month = oct,
pages = {1–20},
numpages = {20},
keywords = {Apache Spark, Big Data, Distributed algorithm, Feature selection, ReliefF}
}

@article{10.1016/j.patcog.2012.09.005,
author = {Ye, Yunming and Wu, Qingyao and Zhexue Huang, Joshua and Ng, Michael K. and Li, Xutao},
title = {Stratified sampling for feature subspace selection in random forests for high dimensional data},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {46},
number = {3},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2012.09.005},
doi = {10.1016/j.patcog.2012.09.005},
abstract = {For high dimensional data a large portion of features are often not informative of the class of the objects. Random forest algorithms tend to use a simple random sampling of features in building their decision trees and consequently select many subspaces that contain few, if any, informative features. In this paper we propose a stratified sampling method to select the feature subspaces for random forests with high dimensional data. The key idea is to stratify features into two groups. One group will contain strong informative features and the other weak informative features. Then, for feature subspace selection, we randomly select features from each group proportionally. The advantage of stratified sampling is that we can ensure that each subspace contains enough informative features for classification in high dimensional data. Testing on both synthetic data and various real data sets in gene classification, image categorization and face recognition data sets consistently demonstrates the effectiveness of this new method. The performance is shown to better that of state-of-the-art algorithms including SVM, the four variants of random forests (RF, ERT, enrich-RF, and oblique-RF), and nearest neighbor (NN) algorithms.},
journal = {Pattern Recogn.},
month = mar,
pages = {769–787},
numpages = {19},
keywords = {Stratified sampling, Random forests, High-dimensional data, Ensemble classifier, Decision trees, Classification}
}

@article{10.1007/s11042-017-5582-0,
author = {Hu, Haojie and Wang, Rong and Nie, Feiping and Yang, Xiaojun and Yu, Weizhong},
title = {Fast unsupervised feature selection with anchor graph and ℓ2,1-norm regularization},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {17},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-017-5582-0},
doi = {10.1007/s11042-017-5582-0},
abstract = {Graph-based unsupervised feature selection has been proven to be effective in dealing with unlabeled and high-dimensional data. However, most existing methods face a number of challenges primarily due to their high computational complexity. In light of the ever-increasing size of data, these approaches tend to be inefficient in dealing with large-scale data sets. We propose a novel approach, called Fast Unsupervised Feature Selection (FUFS), to efficiently tackle this problem. Firstly, an anchor graph is constructed by means of a parameter-free adaptive neighbor assignment strategy. Meanwhile, an approximate nearest neighbor search technique is introduced to speed up the anchor graph construction. The ℓ2,1-norm regularization is then performed to select more valuable features. Experiments on several large-scale data sets demonstrate the effectiveness and efficiency of the proposed method.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {22099–22113},
numpages = {15},
keywords = {Unsupervised feature selection, Anchor graph, 1-norm, ℓ2}
}

@article{10.1007/s10916-019-1351-0,
author = {Karthiga, R. and Mangai, S.},
title = {Feature Selection Using Multi-Objective Modified Genetic Algorithm in Multimodal Biometric System},
year = {2019},
issue_date = {Jul 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {7},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1351-0},
doi = {10.1007/s10916-019-1351-0},
abstract = {Today the multimodal biometric system has become a major area of study that is identified with applications of a large size in a recognition system. The feature selection is probably found to be the best factor to be optimized and is an on-going challenge in the midst of the optimization problems in the human recognition system. The feature selection aspires to bring down the number of the features, remove all types of redundant data and noise which result in a very high rate of recognition. The step further effects on the human recognition system and its performance. The work further presents a newer biometric system of verification that was multimodal and based on three different features which are the face, the hand vein, and the ear. This has today emerged as an extensively researched topic which spans various disciplines like signal processing, pattern recognition, and also computer vision. The features have been extracted by making use of the Incremental Principal Component Analysis (IPCA). Further, the work presented another novel algorithm of feature selection which was based on the Multi-Objective Modified Genetic Algorithm (MOM-GA). The Genetic Algorithm (GA) had been modified by means of introducing a levy search as opposed to a process of mutation. The algorithm has also proved to be an effective method of computation in which the search space is found to be highly dimensional. A classifier that makes use of the K-Nearest Neighbour (KNN) for classifying all accurate features is used. There were some investigations that were carried out and these results proved that this MOM-GA feature selection algorithm had been found as that which can generate certain excellent results using a minimal set of chosen features.},
journal = {J. Med. Syst.},
month = jul,
pages = {1–11},
numpages = {11},
keywords = {Feature selection, Genetic algorithm (GA), Incremental principal component analysis (IPCA), Levy search and K-nearest neighbor (KNN), Multi-objective modified using genetic algorithm (MOM-GA), Multimodal biometric system}
}

@inproceedings{10.1007/978-3-030-72699-7_2,
author = {Renau, Quentin and Dreo, Johann and Doerr, Carola and Doerr, Benjamin},
title = {Towards Explainable Exploratory Landscape Analysis: Extreme Feature Selection for Classifying BBOB Functions},
year = {2021},
isbn = {978-3-030-72698-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72699-7_2},
doi = {10.1007/978-3-030-72699-7_2},
abstract = {Facilitated by the recent advances of Machine Learning&nbsp;(ML), the automated design of optimization heuristics is currently shaking up evolutionary computation&nbsp;(EC). Where the design of hand-picked guidelines for choosing a most suitable heuristic has long dominated research activities in the field, automatically trained heuristics are now seen to outperform human-derived choices even for well-researched optimization tasks. ML-based EC is therefore not any more a futuristic vision, but has become an integral part of our community.A key criticism that ML-based heuristics are often faced with is their potential lack of explainability, which may hinder future developments. This applies in particular to supervised learning techniques which extrapolate algorithms’ performance based on exploratory landscape analysis&nbsp;(ELA). In such applications, it is not uncommon to use dozens of problem features to build the models underlying the specific algorithm selection or configuration task. Our goal in this work is to analyze whether this many features are indeed needed. Using the classification of the BBOB test functions as testbed, we show that a surprisingly small number of features – often less than four – can suffice to achieve a 98% accuracy. Interestingly, the number of features required to meet this threshold is found to decrease with the problem dimension. We show that the classification accuracy transfers to settings in which several instances are involved in training and testing. In the leave-one-instance-out setting, however, classification accuracy drops significantly, and the transformation-invariance of the features becomes a decisive success factor.},
booktitle = {Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {17–33},
numpages = {17},
keywords = {Exploratory landscape analysis, Feature selection, Black-box optimization}
}

@article{10.1016/j.compbiomed.2021.104771,
author = {Narin, Ali},
title = {Accurate detection of COVID-19 using deep features based on X-Ray images and feature selection methods},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104771},
doi = {10.1016/j.compbiomed.2021.104771},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {11},
keywords = {COVID-19, X-ray, Deep features, Feature selection, PSO, ACO}
}

@article{10.1007/s00521-021-05830-0,
author = {Kumar, Saravanapriya and John, Bagyamani},
title = {A novel gaussian based particle swarm optimization gravitational search algorithm for feature selection and classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05830-0},
doi = {10.1007/s00521-021-05830-0},
abstract = {A Gaussian based Particle Swarm Optimization Gravitational Search Algorithm (GPSOGSA) is being proposed for extensive feature selection that serves highly in making effective predictions. GPSOGSA helps to overcome the problem of being stuck into the local optima and influences the local searching ability, thus it aims to bridge the gap of exploration and exploitation. The algorithm also limits the usage of too many parameters like acceleration factors, maximum velocity, inertia weight that plays a vital role in PSO, GSA and PSOGSA. The efficacy of the algorithm has been tested upon unimodal and multimodal benchmark functions. We have also evaluated the performance of the algorithm by applying it on various benchmark datasets. The algorithm uses a wrapper-based approach that includes Support Vector Machine as a learner algorithm, and improves both the execution time and the performance accuracy. The findings show that the proposed algorithm could escape from local optimum and converges faster than the PSO, GSA and PSOGSA algorithms.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12301–12315},
numpages = {15},
keywords = {Classification, Feature selection, Gravitational Search Algorithm (GSA), Gaussian Particle Swarm Optimization Gravitational Search Algorithm (GPSOGSA), Hybrid wrapper-based feature selection, Nature inspired algorithm, Particle Swarm Optimization (PSO), Support Vector Machine (SVM)}
}

@article{10.1007/s00521-020-05400-w,
author = {Liu, Liu and Wang, Rujing and Xie, Chengjun and Li, Rui and Wang, Fangyuan and Zhou, Man and Teng, Yue},
title = {Learning region-guided scale-aware feature selection for object detection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05400-w},
doi = {10.1007/s00521-020-05400-w},
abstract = {Scale variation is one of the major challenges in object detection task. Modern region-based object detection architectures often adopt Feature Pyramid Network (FPN) as feature extraction neck to achieve multi-scale feature representation in solving scale variation problem. However, due to the rough feature selection strategy in Region of Interest (RoI) feature extraction step, these methods might not perform well on object detection under strong scale variation. In this work, we are motivated by the limitations of current FPN-based two-stage object detectors and then present a novel module, namely scale-aware feature selective (SAFS) module, that flexibly and adaptively selects feature levels in two-stage object detectors. Specifically, we firstly build the RoI Pyramid in standard FPN structure to extract RoI features from various scale levels. Next, in order to achieve scale-aware mechanism for solving scale variation issue, we develop a novel weighting gate function containing one set of trainable parameters to automatically learn the fusion weight for each RoI feature level, which relieves the limitation of hard feature selection strategy guided by online instance size. Outputs from the RoI features with the learned weights are fused for classification and bounding box regression. Furthermore, we design a multi-level SAFS architecture to obtain different types of RoI feature combinations that ensures our method is more robust to various instance scales. Experimental results show that our SAFS module is very compatible with most of two-stage object detectors and could achieve state-of-the-art results with Average Precision of 48.3 on COCO test-dev and other popular object detection benchmarks. Our code will be made publicly available.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6389–6403},
numpages = {15},
keywords = {Scale variation, Object detection, RoI Pyramid, Scale-aware feature selective}
}

@article{10.1016/j.knosys.2018.12.031,
author = {Wang, Mingwei and Wu, Chunming and Wang, Lizhe and Xiang, Daxiang and Huang, Xiaohui},
title = {A feature selection approach for hyperspectral image based on modified ant lion optimizer},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {168},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.12.031},
doi = {10.1016/j.knosys.2018.12.031},
journal = {Know.-Based Syst.},
month = mar,
pages = {39–48},
numpages = {10},
keywords = {Hyperspectral image, Feature selection, Wavelet support vector machine, Ant lion optimizer, L\'{e}vy flight}
}

@article{10.1007/s10994-017-5648-2,
author = {Sechidis, Konstantinos and Brown, Gavin},
title = {Simple strategies for semi-supervised feature selection},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {2},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-017-5648-2},
doi = {10.1007/s10994-017-5648-2},
abstract = {What is the simplest thing you can do to solve a problem? In the context of semi-supervised feature selection, we tackle exactly this--how much we can gain from two simple classifier-independent strategies. If we have some binary labelled data and some unlabelled, we could assume the unlabelled data are all positives, or assume them all negatives. These minimalist, seemingly naive, approaches have not previously been studied in depth. However, with theoretical and empirical studies, we show they provide powerful results for feature selection, via hypothesis testing and feature ranking. Combining them with some "soft" prior knowledge of the domain, we derive two novel algorithms (Semi-JMI, Semi-IAMB) that outperform significantly more complex competing methods, showing particularly good performance when the labels are missing-not-at-random. We conclude that simple approaches to this problem can work surprisingly well, and in many situations we can provably recover the exact feature selection dynamics, as if we had labelled the entire dataset.},
journal = {Mach. Learn.},
month = feb,
pages = {357–395},
numpages = {39},
keywords = {Feature selection, Positive unlabelled, Semi-supervised}
}

@inproceedings{10.1007/978-3-030-65745-1_16,
author = {Engly, Andreas Heidelbach and Larsen, Anton Ruby and Meng, Weizhi},
title = {Evaluation of Anomaly-Based Intrusion Detection with Combined Imbalance Correction and Feature Selection},
year = {2020},
isbn = {978-3-030-65744-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65745-1_16},
doi = {10.1007/978-3-030-65745-1_16},
abstract = {Intrusion detection systems (IDSs) are an important security mechanism to protect computing resources under various environments. To detect malicious unknown events, machine learning is often used to support anomaly-based detection. However, such kind of detection often requires high quality data to ensure accuracy, which may face several issues like imbalanced data and ineffective features. In this work, we aim to evaluate a combined approach of both imbalance correction and feature selection, and explore how much it can mitigate the issues. As a study, we generate several feature-selected and imbalance-corrected datasets based on NSL-KDD data and conduct experiments on Random Forests, Neural Networks and Gradient-Boosting Machines. The results indicate that the combined approach can significantly improve the detection performance on the refined data as compared to being trained on the original data, by 10% in overall accuracy and 24% in overall F1-score.},
booktitle = {Network and System Security: 14th International Conference, NSS 2020, Melbourne, VIC, Australia, November 25–27, 2020, Proceedings},
pages = {277–291},
numpages = {15},
keywords = {Anomaly detection, Machine learning, Imbalanced data, Feature selection, Intrusion detection},
location = {Melbourne, VIC, Australia}
}

@article{10.1007/s11042-019-07811-x,
author = {Ghosh, Manosij and Kundu, Tuhin and Ghosh, Dipayan and Sarkar, Ram},
title = {Feature selection for facial emotion recognition using late hill-climbing based memetic algorithm},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {18},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-07811-x},
doi = {10.1007/s11042-019-07811-x},
abstract = {Facial Emotion Recognition (FER) is an important research domain which allows us to provide a better interactive environment between humans and computers. Some standard and popular features extracted from facial expression images include Uniform Local Binary Pattern (uLBP), Horizontal-Vertical Neighborhood Local Binary Pattern (hvnLBP), Gabor filters, Histogram of Oriented Gradients (HOG) and Pyramidal HOG (PHOG). However, these feature vectors may contain some features that are irrelevant or redundant in nature, thereby increasing the overall computational time as well as recognition error of a classification system. To counter this problem, we have proposed a new feature selection (FS) algorithm based on Late Hill Climbing and Memetic Algorithm (MA). A novel local search technique called Late Acceptance Hill Climbing through Redundancy and Relevancy (LAHCRR) has been used in this regard. It combines the concepts of Local Hill-Climbing and minimal-Redundancy Maximal-Relevance (mRMR) to form a more effective local search mechanism in MA. The algorithm is then evaluated on the said feature vectors extracted from the facial images of two popular FER datasets, namely RaFD and JAFFE. LAHCRR is used as local search in MA to form Late Hill Climbing based Memetic Algorithm (LHCMA). LHCMA is compared with state-of-the-art methods. The experimental outcomes show that the proposed FS algorithm reduces the feature dimension to a significant amount as well as increases the recognition accuracy as compared to other methods.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {25753–25779},
numpages = {27},
keywords = {Feature Selection, Late Acceptance Hill Climbing, Memetic Algorithm, Facial Emotion Recognition, RAFD, JAFFE}
}

@article{10.1162/neco_a_01163,
author = {Hu, Haojie and Wang, Rong and Yang, Xiaojun and Nie, Feiping},
title = {Scalable and flexible unsupervised feature selection},
year = {2019},
issue_date = {March 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {31},
number = {3},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco_a_01163},
doi = {10.1162/neco_a_01163},
abstract = {Recently, graph-based unsupervised feature selection algorithms GUFS have been shown to efficiently handle prevalent high-dimensional unlabeled data. One common drawback associated with existing graph-based approaches is that they tend to be time-consuming and in need of large storage, especially when faced with the increasing size of data. Research has started using anchors to accelerate graph-based learning model for feature selection, while the hard linear constraint between the data matrix and the lower-dimensional representation is usually overstrict in many applications. In this letter, we propose a flexible linearization model with anchor graph and \'{y}21-norm regularization, which can deal with large-scale data sets and improve the performance of the existing anchor-based method. In addition, the anchor-based graph Laplacian is constructed to characterize the manifold embedding structure by means of a parameter-free adaptive neighbor assignment strategy. An efficient iterative algorithm is developed to address the optimization problem, and we also prove the convergence of the algorithm. Experiments on several public data sets demonstrate the effectiveness and efficiency of the method we propose. p&gt;},
journal = {Neural Comput.},
month = mar,
pages = {517–537},
numpages = {21}
}

@inproceedings{10.1145/3205651.3208305,
author = {Mostert, Werner and Malan, Katherine and Engelbrecht, Andries},
title = {Filter versus wrapper feature selection based on problem landscape features},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208305},
doi = {10.1145/3205651.3208305},
abstract = {Feature selection is a complex problem used across many fields, such as computer vision and data mining. Feature selection algorithms extract a subset of features from a greater feature set which can improve algorithm accuracy by discarding features that are less significant in achieving the goal function. Current approaches are often computationally expensive, provide insignificant increases in predictor performance, and can lead to overfitting. This paper investigates the binary feature selection problem and the applicability of using filter and wrapper techniques guided by fitness landscape characteristics. It is shown that using filter methods are more appropriate for problems where the fitness does not provide sufficient information to guide search as needed by wrapper techniques.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1489–1496},
numpages = {8},
keywords = {feature selection problem, fitness landscapes, hamming distance in a level, neutrality},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@inproceedings{10.1145/2993236.2993250,
author = {Braz, Larissa and Gheyi, Rohit and Mongiovi, Melina and Ribeiro, M\'{a}rcio and Medeiros, Fl\'{a}vio and Teixeira, Leopoldo},
title = {A change-centric approach to compile configurable systems with #ifdefs},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993250},
doi = {10.1145/2993236.2993250},
abstract = {Configurable systems typically use #ifdefs to denote variability. Generating and compiling all configurations may be time-consuming. An alternative consists of using variability-aware parsers, such as TypeChef. However, they may not scale. In practice, compiling the complete systems may be costly. Therefore, developers can use sampling strategies to compile only a subset of the configurations. We propose a change-centric approach to compile configurable systems with #ifdefs by analyzing only configurations impacted by a code change (transformation). We implement it in a tool called CHECKCONFIGMX, which reports the new compilation errors introduced by the transformation. We perform an empirical study to evaluate 3,913 transformations applied to the 14 largest files of BusyBox, Apache HTTPD, and Expat configurable systems. CHECKCONFIGMX finds 595 compilation errors of 20 types introduced by 41 developers in 214 commits (5.46% of the analyzed transformations). In our study, it reduces by at least 50% (an average of 99%) the effort of evaluating the analyzed transformations by comparing with the exhaustive approach without considering a feature model. CHECKCONFIGMX may help developers to reduce compilation effort to evaluate fine-grained transformations applied to configurable systems with #ifdefs.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {109–119},
numpages = {11},
keywords = {#ifdefs, Configurable Systems, compilation errors},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.5555/3288251.3288293,
author = {Lei, Cong and Zhu, Xiaofeng},
title = {Unsupervised feature selection via local structure learning and sparse learning},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Feature self-representation has become the backbone of unsupervised feature selection, since it is almost insensitive to noise data. However, feature selection methods based on feature self-representation have the following drawbacks: 1) The self-representation coefficient matrix is fixed and can not be fine-tuned according to the structure of data. 2) they do not consider the manifold structure of data, thus unable to further increase the performance of feature selection. To solve the above problems, this paper proposes an unsupervised feature selection algorithm that combines feature self-representation and manifold learning. Specifically, we first utilize feature self-representation to construct the model. After that, the self-representation coefficient matrix is dynamically adjusted to the optimal state based on the similarity matrix. Then, we use low-rank representation to explore the global manifold structure of the data. Finally, we combine sparse learning with feature selection. The experimental results on twelve datasets show that the proposed method outperforms all the competing methods.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29605–29622},
numpages = {18},
keywords = {Feature selection, Hypergraph representation, Sparse feature selection, Subspace learning}
}

@inproceedings{10.1145/3238147.3238201,
author = {Mukelabai, Mukelabai and Ne\v{s}i\'{c}, Damir and Maro, Salome and Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp},
title = {Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238201},
doi = {10.1145/3238147.3238201},
abstract = {Highly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {155–166},
numpages = {12},
keywords = {Analysis, Highly Configurable Systems, Product Lines},
location = {Montpellier, France},
series = {ASE '18}
}

@inproceedings{10.5555/3106050.3106052,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Benduhn, Fabian and Leich, Thomas and Saake, Gunter},
title = {Efficient mutation testing in configurable systems},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Mutation testing is a technique to evaluate the quality of test cases by assessing their ability to detect faults. Mutants are modified versions of the original program that are generated automatically and should contain faults similar to those caused by developers' mistakes. For configurable systems, existing approaches propose mutation operators to produce faults that may only exist in some configurations. However, due to the number of possible configurations, generating and testing all mutants for each program is not feasible. To tackle this problem, we discuss to use static analysis and adopt the idea of T-wise testing to limit the number of mutants. In particular, we i) discuss dependencies that exist in configurable systems, ii) how we can use them to identify code to mutate, and iii) assess the expected outcome. Our preliminary results show that variability analysis can help to reduce the number of mutants and, thus, costs for testing.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {2–8},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@inproceedings{10.1145/3424978.3424980,
author = {Wang, Xiran and Zhou, Jun},
title = {Human Behavior Recognition Based on the Genetic Algorithm Feature Selection and Sensor Data},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3424980},
doi = {10.1145/3424978.3424980},
abstract = {In multi-sensor behavior recognition, considering that human behavior is the result of multi-node collaboration, sensors in various parts are related under the same behavior. In this study, three acceleration sensors were deployed to collect data. Besides acceleration data, RSS data between different sensors is also collected to avoid the impact of the single sensor data on the recognition results and improve the accuracy of behavior recognition. In order to avoid the subjective effect of manual feature selection on multi-sensor behavior recognition, genetic algorithm is used to select the optimal feature subset from all features as input of the classifier to establish a classification model. The experimental results show the feasibility and effectiveness of using RSS data and genetic algorithm to select features, and the recognition accuracy rate reaches 95.63%.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {2},
numpages = {5},
keywords = {The optimal subset of features, RSS data, Multi-sensors, Genetic algorithm},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.1007/978-3-030-77967-2_51,
author = {Balabaeva, Ksenia and Kovalchuk, Sergey},
title = {Comparison of Efficiency, Stability and Interpretability of Feature Selection Methods for Multiclassification Task on Medical Tabular Data},
year = {2021},
isbn = {978-3-030-77966-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77967-2_51},
doi = {10.1007/978-3-030-77967-2_51},
abstract = {Feature selection is an important step of machine learning pipeline. Certain models may select features intrinsically without human interactions or additional algorithms applied. Such algorithms usually belong to neural networks class. Others require help of a researcher or feature selection algorithms. However, it is hard to know beforehand which variables contain the most relevant information and which may cause difficulties for a model to learn the correct relations. In that respect, researchers have been developing feature selection algorithms. To understand what methods perform better on tabular medical data, we have conducted a set of experiments to measure accuracy, stability and compare interpretation capacities of different feature selection approaches. Moreover, we propose an application of Bayesian Inference to the task of feature selection that may provide more interpretable and robust solution. We believe that high stability and interpretability are as important as classification accuracy especially in predictive tasks in medicine.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part III},
pages = {623–633},
numpages = {11},
keywords = {Kbest, Recursive feature elimination, eXAI, XAI, Explainable artificial intelligence, Bayesian inference, Feature selection},
location = {Krakow, Poland}
}

@inproceedings{10.1145/2623330.2623635,
author = {Xu, Zhixiang and Huang, Gao and Weinberger, Kilian Q. and Zheng, Alice X.},
title = {Gradient boosted feature selection},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623635},
doi = {10.1145/2623330.2623635},
abstract = {A feature selection algorithm should ideally satisfy four conditions: reliably extract relevant features; be able to identify non-linear feature interactions; scale linearly with the number of features and dimensions; allow the incorporation of known sparsity structure. In this work we propose a novel feature selection algorithm, Gradient Boosted Feature Selection (GBFS), which satisfies all four of these requirements. The algorithm is flexible, scalable, and surprisingly straight-forward to implement as it is based on a modification of Gradient Boosted Trees. We evaluate GBFS on several real world data sets and show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-specific side information.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {522–531},
numpages = {10},
keywords = {feature selection, gradient boosting, large-scale},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1609/aaai.v33i01.33013534,
author = {Feng, Chao and Qian, Chao and Tang, Ke},
title = {Unsupervised feature selection by pareto optimization},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33013534},
doi = {10.1609/aaai.v33i01.33013534},
abstract = {Dimensionality reduction is often employed to deal with the data with a huge number of features, which can be generally divided into two categories: feature transformation and feature selection. Due to the interpretability, the efficiency during inference and the abundance of unlabeled data, unsupervised feature selection has attracted much attention. In this paper, we consider its natural formulation, column subset selection (CSS), which is to minimize the reconstruction error of a data matrix by selecting a subset of features. We propose an anytime randomized iterative approach POCSS, which minimizes the reconstruction error and the number of selected features simultaneously. Its approximation guarantee is well bounded. Empirical results exhibit the superior performance of POCSS over the state-of-the-art algorithms.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {434},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Software defect prediction, Metaheuristic feature selection, Whale optimization algorithm, Convolutional neural network, Kernel extreme learning machine}
}

@inproceedings{10.1007/978-3-030-45715-0_6,
author = {Barbiero, Pietro and Lutton, Evelyne and Squillero, Giovanni and Tonda, Alberto},
title = {A Novel Outlook on Feature Selection as a Multi-objective Problem},
year = {2019},
isbn = {978-3-030-45714-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45715-0_6},
doi = {10.1007/978-3-030-45715-0_6},
abstract = {Feature selection is the process of choosing, or removing, features to obtain the most informative feature subset of minimal size. Such subsets are used to improve performance of machine learning algorithms and enable human understanding of the results. Approaches to feature selection in literature exploit several optimization algorithms. Multi-objective methods also have been proposed, minimizing at the same time the number of features and the error. While most approaches assess error resorting to the average of a stochastic K-fold cross-validation, comparing averages might be misleading. In this paper, we show how feature subsets with different average error might in fact be non-separable when compared using a statistical test. Following this idea, clusters of non-separable optimal feature subsets are identified. The performance in feature selection can thus be evaluated by verifying how many of these optimal feature subsets an algorithm is able to identify. We thus propose a multi-objective optimization approach to feature selection, EvoFS, with the objectives to i. minimize feature subset size, ii. minimize test error on a 10-fold cross-validation using a specific classifier, iii. maximize the analysis of variance value of the lowest-performing feature in the set. Experiments on classification datasets whose feature subsets can be exhaustively evaluated show that our approach is able to always find the best feature subsets. Further experiments on a high-dimensional classification dataset, that cannot be exhaustively analyzed, show that our approach is able to find more optimal feature subsets than state-of-the-art feature selection algorithms.},
booktitle = {Artificial Evolution: 14th International Conference, \'{E}volution Artificielle, EA 2019, Mulhouse, France, October 29–30, 2019, Revised Selected Papers},
pages = {68–81},
numpages = {14},
keywords = {Multi-objective evolutionary algorithms, Evolutionary algorithms, Multi-objective optimization, Machine learning, Feature selection},
location = {Mulhouse, France}
}

@article{10.1007/s11042-021-11416-8,
author = {Thiyagarajan, Akila and Gunasekar, Kumaragurubaran},
title = {An improved feature selection based classifier for prediction of different regions in sar images},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {25},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-11416-8},
doi = {10.1007/s11042-021-11416-8},
abstract = {Satellite images play an essential role in various applications like geographical information systems, remote sensing, ecology, and oceanography. Synthetic Aperture Radar (SAR) imaging is used to achieve high-resolution images on earth. However, these images are positively affected by unnecessary noises by compression and transmission errors. The noise removal process is a challenging task that it had artefacts and blurring of images. Existing researches and studies proposed various de-noising techniques to improve the accuracy of these images, and that attains specific application. These techniques had not yet attained the high performances due to the inaccurate prediction of objects. The primary aim of this research is to enhance the classification accuracy of different regions like water region, residential area, land region, and forest region from SAR images. From input SAR images, the features are extracted by using proposed hybrid saliency mapping and pyramid histogram of oriented gradients. The most important features are selected by using the Improved Principal Component Analysis (IPCA) technique. Further, the classification of regions is achieved by using a novel forest classifier. The performance of the proposed framework ha analyzed with the measures of accuracy, specificity, sensitivity, precision, recall, and f-score. In the result analysis, the proposed method had achieved 98% of accuracy compared than the state-of-the-art algorithms. From the estimation results, it is concluded that the proposed approach offers better results with increased accuracy for the prediction of different objects in SAR images.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {33641–33662},
numpages = {22},
keywords = {Random forest classifier, Principal component analysis, Pyramid histogram of oriented gradients, Satellite image processing, Synthetic aperture radar (SAR)}
}

@article{10.1007/s11277-018-5309-1,
author = {Xu, Huali and Yu, Shuhao and Chen, Jiajun and Zuo, Xukun},
title = {An Improved Firefly Algorithm for Feature Selection in Classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-5309-1},
doi = {10.1007/s11277-018-5309-1},
abstract = {Feature selection functions as an important method of receiving data so as to make the amount of features decrease. While solving the issue of classifying there exists numerous features having no relevance and being unnecessary which have the potential of making classification performance decrease. Firefly algorithm (FA) functions as an efficient method to make computation which is efficient and progressive. Nevertheless, the conventional FA is easily fallen into the local optima which imposes unsatisfactory practice on feature selection. In this research, one proposal was put forward, the firefly algorithm that combines the binary firefly algorithm with opposition-based learning to select features in classification. Experiment outcomes indicate the fact that the means put forward surpasses PSO and the conventional firefly algorithm.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2823–2834},
numpages = {12},
keywords = {Classification, Evolutionary computation, Feature selection, Firefly algorithm, Opposition-based learning}
}

@article{10.1007/s10115-017-1059-8,
author = {Li, Yun and Li, Tao and Liu, Huan},
title = {Recent advances in feature selection and its applications},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {53},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1059-8},
doi = {10.1007/s10115-017-1059-8},
abstract = {Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval.},
journal = {Knowl. Inf. Syst.},
month = dec,
pages = {551–577},
numpages = {27},
keywords = {Data mining, Feature selection, Survey}
}

@article{10.1016/j.asoc.2015.01.035,
author = {Bol\'{o}n-Canedo, V. and S\'{a}nchez-Maro\~{n}o, N. and Alonso-Betanzos, A.},
title = {Distributed feature selection},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.01.035},
doi = {10.1016/j.asoc.2015.01.035},
abstract = {Graphical abstractDisplay Omitted HighlightsFeature selection is indispensable when dealing with microarray data.A new method for distributing the filtering process is proposed.The data is distributed by features and then merged in a final subset.The method is tested on 8 microarray datasets.The classification accuracy is maintained and the time considerably shortened. Feature selection is often required as a preliminary step for many pattern recognition problems. However, most of the existing algorithms only work in a centralized fashion, i.e. using the whole dataset at once. In this research a new method for distributing the feature selection process is proposed. It distributes the data by features, i.e. according to a vertical distribution, and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy. The effectiveness of our proposal is tested on microarray data, which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size. The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non-partitioned datasets.},
journal = {Appl. Soft Comput.},
month = may,
pages = {136–150},
numpages = {15},
keywords = {Distributed learning, Feature selection, Microarray data}
}

@article{10.1007/s00521-019-04082-3,
author = {Pes, Barbara},
title = {Ensemble feature selection for high-dimensional data: a stability analysis across multiple domains},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04082-3},
doi = {10.1007/s00521-019-04082-3},
abstract = {Selecting a subset of relevant features is crucial to the analysis of high-dimensional datasets coming from a number of application domains, such as biomedical data, document and image analysis. Since no single selection algorithm seems to be capable of ensuring optimal results in terms of both predictive performance and stability (i.e. robustness to changes in the input data), researchers have increasingly explored the effectiveness of “ensemble” approaches involving the combination of different selectors. While interesting proposals have been reported in the literature, most of them have been so far evaluated in a limited number of settings (e.g. with data from a single domain and in conjunction with specific selection approaches), leaving unanswered important questions about the large-scale applicability and utility of ensemble feature selection. To give a contribution to the field, this work presents an empirical study which encompasses different kinds of selection algorithms (filters and embedded methods, univariate and multivariate techniques) and different application domains. Specifically, we consider 18 classification tasks with heterogeneous characteristics (in terms of number of classes and instances-to-features ratio) and experimentally evaluate, for feature subsets of different cardinalities, the extent to which an ensemble approach turns out to be more robust than a single selector, thus providing useful insight for both researchers and practitioners.},
journal = {Neural Comput. Appl.},
month = may,
pages = {5951–5973},
numpages = {23},
keywords = {High-dimensional data analysis, Ensemble approaches, Stability of feature selection algorithms, Feature selection}
}

@inproceedings{10.1007/978-3-319-49586-6_26,
author = {Chaudhary, Mandar S. and Gonzalez, Doel L. and Bello, Gonzalo A. and Angus, Michael P. and Desai, Dhara and Harenberg, Steve and Doraiswamy, P. Murali and Semazzi, Fredrick H. M. and Kumar, Vipin and Samatova, Nagiza F.},
title = {Causality-Guided Feature Selection},
year = {2016},
isbn = {978-3-319-49585-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49586-6_26},
doi = {10.1007/978-3-319-49586-6_26},
abstract = {Identifying meaningful features that drive a phenomenon (response) of interest in complex systems of interconnected factors is a challenging problem. Causal discovery methods have been previously applied to estimate bounds on causal strengths of factors on a response or to identify meaningful interactions between factors in complex systems, but these approaches have been used only for inferential purposes. In contrast, we posit that interactions between factors with a potential causal association on a given response could be viable candidates not only for hypothesis generation but also for predictive modeling. In this work, we propose a causality-guided feature selection methodology that identifies factors having a potential cause-effect relationship in complex systems, and selects features by clustering them based on their causal strength with respect to the response. To this end, we estimate statistically significant causal effects on the response of factors taking part in potential causal relationships, while addressing associated technical challenges, such as multicollinearity in the data. We validate the proposed methodology for predicting response in five real-world datasets from the domain of climate science and biology. The selected features show predictive skill and consistent performance across different domains.},
booktitle = {Advanced Data Mining and Applications: 12th International Conference, ADMA 2016, Gold Coast, QLD, Australia, December 12-15, 2016, Proceedings},
pages = {391–405},
numpages = {15},
keywords = {Causal Effect, Feature Selection Method, Climate Index, Principal Component Regression, Climate Science},
location = {Gold Coast, Australia}
}

@inproceedings{10.1609/aaai.v33i01.33012438,
author = {Correia, Alvaro H. C. and Lecue, Freddy},
title = {Human-in-the-loop feature selection},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33012438},
doi = {10.1609/aaai.v33i01.33012438},
abstract = {Feature selection is a crucial step in the conception of Machine Learning models, which is often performed via data-driven approaches that overlook the possibility of tapping into the human decision-making of the model's designers and users. We present a human-in-the-loop framework that interacts with domain experts by collecting their feedback regarding the variables (of few samples) they evaluate as the most relevant for the task at hand. Such information can be modeled via Reinforcement Learning to derive a per-example feature selection method that tries to minimize the model's loss function by focusing on the most pertinent variables from a human perspective. We report results on a proof-of-concept image classification dataset and on a real-world risk classification task in which the model successfully incorporated feedback from experts to improve its accuracy.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {301},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@inproceedings{10.1145/3278312.3278316,
author = {Win, Thee Zin and Kham, Nang Saing Moon},
title = {Mutual Information-based Feature Selection Approach to Reduce High Dimension of Big Data},
year = {2018},
isbn = {9781450365567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278312.3278316},
doi = {10.1145/3278312.3278316},
abstract = {As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.},
booktitle = {Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence},
pages = {3–7},
numpages = {5},
keywords = {Feature Selection, High Dimensional Data, Mutual Information, Redundant Features},
location = {Ha Noi, Viet Nam},
series = {MLMI '18}
}

@inproceedings{10.1145/2648511.2648527,
author = {Villela, Karina and Silva, Adeline and Vale, Tassio and de Almeida, Eduardo Santana},
title = {A survey on software variability management approaches},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648527},
doi = {10.1145/2648511.2648527},
abstract = {Variability Management (VM) is a key practice in the development of variant-rich systems. Over the years, attention has been paid to VM approaches adopted by traditional software product lines. The increasing demand for dynamic and highly configurable systems, however, calls for a closer look at the approaches used to develop these systems. We therefore conducted a survey with practitioners from organizations developing variant-rich systems in order to characterize the state of the practice. We also wanted to identify factors that might influence the adoption of specific VM approaches as well as the perception of problems/difficulties posed by those. We analyzed the answers of 31 respondents from thirteen countries and found that there is a correlation between the business domain and the adopted VM approaches. With regard to the problems/difficulties, the difficulty of assuring the quality of maintenance due to the explosion of dependencies was a major issue. This paper reports on relevant findings that could help companies to better understand their problems and researchers to design new/improved solutions.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {147–156},
numpages = {10},
keywords = {product line, state-of-the-practice, survey, variability, variability management},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.asoc.2017.04.055,
author = {Lee, Chia-Yen and Chen, Bo-Syun},
title = {Mutually-exclusive-and-collectively-exhaustive feature selection scheme},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.04.055},
doi = {10.1016/j.asoc.2017.04.055},
journal = {Appl. Soft Comput.},
month = jul,
pages = {961–971},
numpages = {11},
keywords = {Feature selection, Mutually-exclusive-and-collectively-exhaustive, Data mining, Semiconductor manufacturing, Bioinformatics}
}

@article{10.1007/s11265-021-01637-3,
author = {Chen, Jiwei and Tang, Guojian},
title = {A Feature Selection Model to Filter Periodic Variable Stars with Data-sensitive Light-variable Characteristics},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {7},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01637-3},
doi = {10.1007/s11265-021-01637-3},
abstract = {At present, autonomous management and operation of spacecraft are the main direction and objective of the development of space technology to lighten the burden of ground measurement and control, reduce the cost of operation and management, and expand the application scope of spacecraft. To select the suitable periodic variable stars with a certain quantity at the given conditions of spacecraft, we study the autonomous navigation method of optical variable spacecraft and propose a feature selection model to filter periodic variable stars with light-variable characteristics. It mainly focuses on the learning processes of the pulsating optical variable light variation star clock model, the high precision pulsating optical variable autonomous navigation algorithm and the optical variable light variation characteristic mechanism with the measurement method. From experiments, the sample of the periodic variable star is selected, forms a database of 132 initial candidate samples and 16 navigation sample stars. So, time measurement can be conducted to take advantage of the nature of periodic variable stars that take days as its cycle, ground-based observation and ground-based application can be conducted with the wide spectrum of periodic variable star observation. It can meet the requirements of spacecraft.},
journal = {J. Signal Process. Syst.},
month = jul,
pages = {733–744},
numpages = {12},
keywords = {Machine learning filter, Feature selection model, Periodic variable stars}
}

@article{10.1016/j.comnet.2021.108591,
author = {Wazirali, Raniyah and Ahmad, Rami and Abu-Ein, Ashraf Abdel-Karim},
title = {Sustaining accurate detection of phishing URLs using SDN and feature selection approaches},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108591},
doi = {10.1016/j.comnet.2021.108591},
journal = {Comput. Netw.},
month = dec,
numpages = {13},
keywords = {Recursive feature elimination, Phishing URL, SDN, CNN, Deep learning, Phishing detection}
}

@article{10.1145/3149119,
author = {Abal, Iago and Melo, Jean and St\u{a}nciulescu, \c{S}tefan and Brabrand, Claus and Ribeiro, M\'{a}rcio and W\k{a}sowski, Andrzej},
title = {Variability Bugs in Highly Configurable Systems: A Qualitative Analysis},
year = {2018},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3149119},
doi = {10.1145/3149119},
abstract = {Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {10},
numpages = {34},
keywords = {Bugs, Linux, feature interactions, software variability}
}

@inproceedings{10.1145/3479645.3479668,
author = {Krisnabayu, Rifky Yunus and Ridok, Achmad and Setia Budi, Agung},
title = {Hepatitis Detection using Random Forest based on SVM-RFE (Recursive Feature Elimination) Feature Selection and SMOTE},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479668},
doi = {10.1145/3479645.3479668},
abstract = {Hepatitis is a dangerous disease because it is a contagious disease and it is not easy to diagnose the disease early. Due to the difficulty of making an early diagnosis, the disease has the potential to become even more severe and increase the mortality rate. Therefore, it is necessary to develop predictive methods that can be used for the early detection of this disease. In this study, a hepatitis prediction method was developed using a random forest (RF) algorithm combined with feature selection using SVM-RFE (recursive feature elimination). Then, because the dataset used does not have a balanced distribution between classes, which is only 20% for the minority class, SMOTE (synthetic minority oversampling technique) is used to deal with this problem. To determine the best parameters in the model, Grid-Search is used as the tuning hyper-parameters. The classifier built with this approach produces 0.879 accuracy, 0.902 precision, and 0.966 ROC performance. This classifier proved to be better than the other classifiers.},
booktitle = {Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology},
pages = {151–156},
numpages = {6},
keywords = {Hepatitis prediction, Random forest, Recursive feature elimination, Synthetic minority oversampling technique},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/3391812.3396274,
author = {Bhandari, Shilpa and Kukreja, Avinash K. and Lazar, Alina and Sim, Alex and Wu, Kesheng},
title = {Feature Selection Improves Tree-based Classification for Wireless Intrusion Detection},
year = {2020},
isbn = {9781450379809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391812.3396274},
doi = {10.1145/3391812.3396274},
abstract = {With the growth of 5G wireless technologies and IoT, it become urgent to develop robust network security systems, such as intrusions detection systems (IDS) to keep the networks secure. These IDS systems need to detect unauthorized access and attacks in real-time. However, most of the modern IDS are built based on complex machine learning models that are time-consuming to train. In this work, we propose a methodology using the SHapley Additive exPlanations (SHAP) in combination with tree-based classifiers. SHAP can be used to select consistent and small feature subsets to reduce the execution time and improve classification accuracy. We demonstrate the proposed approach with the Aegean Wi-Fi Intrusion Dataset (AWID) dataset in a series of multi-class classification experiments. Among the four classes ("normal", "injection", "flooding" and "impersonation"), it is well-known that the class impersonation is hard to be classified accurately. Tests show that we can use about 10% of the initial feature set without reducing the overall prediction accuracy. With this reduced set of features, the training time could be reduced as much as a factor of four, while slightly improving the discriminating ability to identify impersonation instances. This study suggests that by reducing the number of features, the classification algorithms are able to focus on key trends that differentiates the "attacks" classes from the "normal" class. Using a reduces subset of features improves IDS's accuracy and performance. Also, SHAP dependence plots capture the relationship between individual features and the classification decision.},
booktitle = {Proceedings of the 3rd International Workshop on Systems and Network Telemetry and Analytics},
pages = {19–26},
numpages = {8},
keywords = {wi-fi network, intrusion detection, feature importance, classification},
location = {Stockholm, Sweden},
series = {SNTA '20}
}

@article{10.5555/3288251.3288287,
author = {Lu, Guangquan and Li, Bo and Yang, Weiwei and Yin, Jian},
title = {Unsupervised feature selection with graph learning via low-rank constraint},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Feature selection is one of the most important machine learning procedure, and it has been successfully applied to make a preprocessing before using classification and clustering methods. High-dimensional features often appear in big data, and it's characters block data processing. So spectral feature selection algorithms have been increasing attention by researchers. However, most feature selection methods, they consider these tasks as two steps, learn similarity matrix from original feature space (may be include redundancy for all features), and then conduct data clustering. Due to these limitations, they do not get good performance on classification and clustering tasks in big data processing applications. To address this problem, we propose an Unsupervised Feature Selection method with graph learning framework, which can reduce the redundancy features influence and utilize a low-rank constraint on the weight matrix simultaneously. More importantly, we design a new objective function to handle this problem. We evaluate our approach by six benchmark datasets. And all empirical classification results show that our new approach outperforms state-of-the-art feature selection approaches.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29531–29549},
numpages = {19},
keywords = {Feature selection, Graph learning, Spectral clustering}
}

@article{10.1016/j.patcog.2016.09.007,
author = {Zhou, Xuan and Gao, Xin and Wang, Jiajun and Yu, Hui and Wang, Zhiyong and Chi, Zheru},
title = {Eye tracking data guided feature selection for image classification},
year = {2017},
issue_date = {Mar 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.09.007},
doi = {10.1016/j.patcog.2016.09.007},
journal = {Pattern Recogn.},
month = mar,
pages = {56–70},
numpages = {15},
keywords = {SVM-RFE, mRMR, Quantum genetic algorithm (QGA), Feature selection, Eye tracking}
}

@article{10.3233/JIFS-191568,
author = {Subbiah, Siva Sankari and Chinnappan, Jayakumar},
title = {An improved short term load forecasting with ranker based feature selection technique},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-191568},
doi = {10.3233/JIFS-191568},
abstract = {The load forecasting is the significant task carried out by the electricity providing utility companies for estimating the future electricity load. The proper planning, scheduling, functioning, and maintenance of the power system rely on the accurate forecasting of the electricity load. In this paper, the clustering-based filter feature selection is proposed for assisting the forecasting models in improving the short term load forecasting performance. The Recurrent Neural Network based Long Short Term Memory (LSTM) is developed for forecasting the short term load and compared against Multilayer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Regression (SVR) and Random Forest (RF). The performance of the forecasting model is improved by reducing the curse of dimensionality using filter feature selection such as Fast Correlation Based Filter (FCBF), Mutual Information (MI), and RReliefF. The clustering is utilized to group the similar load patterns and eliminate the outliers. The feature selection identifies the relevant features related to the load by taking samples from each cluster. To show the generality, the proposed model is experimented by using two different datasets from European countries. The result shows that the forecasting models with selected features produce better performance especially the LSTM with RReliefF outperformed other models.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6783–6800},
numpages = {18},
keywords = {Load forecasting, feature selection, clustering, deep learning, long short term memory}
}

@article{10.1007/s11063-017-9619-1,
author = {Seijo-Pardo, Borja and Bol\'{o}n-Canedo, Ver\'{o}nica and Alonso-Betanzos, Amparo},
title = {Testing Different Ensemble Configurations for Feature Selection},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {46},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-017-9619-1},
doi = {10.1007/s11063-017-9619-1},
abstract = {In recent years, ensemble learning has become a prolific area of study in pattern recognition, based on the assumption that using and combining different learning models in the same problem could lead to better performance results than using a single model. This idea of ensemble learning has traditionally been used for classification tasks, but has more recently been adapted to other machine learning tasks such as clustering and feature selection. We propose several feature selection ensemble configurations based on combining rankings of features from individual rankers according to the combination method and threshold value used. The performance of each proposed ensemble configuration was tested for synthetic datasets (to assess the adequacy of the selection), real classical datasets (with more samples than features), and DNA microarray datasets (with more features than samples). Five different classifiers were studied in order to test the suitability of the proposed ensemble configurations and assess the results.},
journal = {Neural Process. Lett.},
month = dec,
pages = {857–880},
numpages = {24},
keywords = {Ranking aggregation, Fisher's ratio, Feature selection, Ensemble learning, DNA microarray, Classification}
}

@article{10.1016/j.jbi.2020.103580,
author = {Steinmeyer, Christian and Wiese, Lena},
title = {Sampling methods and feature selection for mortality prediction with neural networks},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {111},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2020.103580},
doi = {10.1016/j.jbi.2020.103580},
journal = {J. of Biomedical Informatics},
month = nov,
numpages = {12},
keywords = {Sampling, Neural nets, Mortality prediction, Machine learning, Medical information systems}
}

@inproceedings{10.1145/3093241.3093246,
author = {Adachi, Yusuke and Onimura, Naoya and Yamashita, Takanori and Hirokawa, Sachio},
title = {Classification of Imbalanced Documents by Feature Selection},
year = {2017},
isbn = {9781450352413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093241.3093246},
doi = {10.1145/3093241.3093246},
abstract = {We previously worked on category classification problem of reuter 's newspaper article using SVM and feature selection. In the study, feature selection by SVM-score [Sakai, Hirokawa, 2012] showed high accuracy. It was also expected to be superior to other standard indicators in case data is imbalanced. This study aimed to show the effectiveness of feature selection by SVM-score in machine learning with imbalanced data. For the reuter's data, F-measure was calculated in the classification experiment of all 13 categories. As a result, feature selection by SVM-score shows high f-measure and precision. In addition, we found feature words of negative example improve the classification performance.},
booktitle = {Proceedings of the International Conference on Compute and Data Analysis},
pages = {228–232},
numpages = {5},
keywords = {text mining, reuters, feature selection, SVM},
location = {Lakeland, FL, USA},
series = {ICCDA '17}
}

@article{10.1016/j.eswa.2020.113176,
author = {Gokalp, Osman and Tasci, Erdal and Ugur, Aybars},
title = {A novel wrapper feature selection algorithm based on iterated greedy metaheuristic for sentiment classification},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {146},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113176},
doi = {10.1016/j.eswa.2020.113176},
journal = {Expert Syst. Appl.},
month = may,
numpages = {10},
keywords = {Machine learning, Metaheuristic, Iterated greedy, Feature selection, Sentiment classification}
}

@article{10.1016/j.procs.2019.09.049,
author = {Tun\c{c}, Ali},
title = {Feature Selection in Credibility Study For Finance Sector},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {158},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.09.049},
doi = {10.1016/j.procs.2019.09.049},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {254–259},
numpages = {6},
keywords = {Credit Score, Feature Selection, Gain Ratio, Information Gain}
}

@article{10.1016/j.asoc.2019.105859,
author = {Zhang, Pin},
title = {A novel feature selection method based on global sensitivity analysis with application in machine learning-based prediction model},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105859},
doi = {10.1016/j.asoc.2019.105859},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {12},
keywords = {Principal components analysis, Feature selection, Sensitivity analysis, Machine learning, Settlement, Shield tunnel}
}

@inproceedings{10.1145/3397391.3397403,
author = {Wu, Jiamin and Chen, Shengjia and Zhou, Wenbin and Wang, Ningya and Fan, Ziling},
title = {Evaluation of Feature Selection Methods using Bagging and Boosting Ensemble Techniques on High Throughput Biological Data},
year = {2020},
isbn = {9781450377249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397391.3397403},
doi = {10.1145/3397391.3397403},
abstract = {Feature selection technique has become a basic but desired technique when analyzing high-throughput biological data due to its nature of large p and small n. In recent years, ensemble learning based feature selection methods have been widely proposed and studied. Ensemble methods employ multiple learning algorithms to obtain better predictive performance than any of the constituent learning algorithms separately. Also, the feature selected by ensemble classifiers can yield more accurate classification performance and more robust results. In our work, the bagging algorithm Random Forest (RF), and the boosting algorithms Gradient Boosting Decision Tree (GBDT), Extreme Gradient Boosting (XGBoost) are the main research objects. We compared the accuracy and robustness of three algorithms on six different datasets from TCGA database. Also, the three feature selection algorithms are further ensembled using a bagging procedure for the purpose of comparison with the original single classifier. The results of our work indicated that for single base feature selectors, boosting algorithms all outperform than bagging one in both performance and robustness. By applying the bagging-based feature selection procedure, the robustness of three single base feature selectors is improved significantly, but the accuracy of them is slightly reduced. GBDT with bagging-based feature selection procedure achieved the best performance using our proposed comprehensive metric which balances equally accuracy and robustness.},
booktitle = {Proceedings of the 2020 10th International Conference on Biomedical Engineering and Technology},
pages = {170–175},
numpages = {6},
keywords = {High throughput biological data, Feature selection, Ensemble learning},
location = {Tokyo, Japan},
series = {ICBET '20}
}

@article{10.1016/j.eswa.2019.02.005,
author = {Chen, Lan-lan and Zhang, Ao and Lou, Xiao-guang},
title = {Cross-subject driver status detection from physiological signals based on hybrid feature selection and transfer learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.02.005},
doi = {10.1016/j.eswa.2019.02.005},
journal = {Expert Syst. Appl.},
month = dec,
pages = {266–280},
numpages = {15},
keywords = {Driver status, Physiological signals, Cross-subject feature evaluation, Hybrid feature selection, Transfer learning}
}

@article{10.1007/s00500-019-03757-2,
author = {Tsai, Cheng-Jung},
title = {New feature selection and voting scheme to improve classification accuracy},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03757-2},
doi = {10.1007/s00500-019-03757-2},
abstract = {Classification is a classic technique employed in data mining. Many ensemble learning methods have been introduced to improve the predictive accuracy of classification. A typical ensemble learning method consists of three steps: selection, building, and integration. Of the three steps, the first and third significantly affect the predictive accuracy of the classification. In this paper, we propose a new selection and integration scheme. Our method can improve the accuracy of subtrees and maintain their diversity. Through a new voting scheme, the predictive accuracy of ensemble learning is improved. We also theoretically analyzed the selection and integration steps of our method. The results of experimental analyses show that our method can achieve better accuracy than two state-of-the-art tree-based ensemble learning approaches.},
journal = {Soft Comput.},
month = nov,
pages = {12017–12030},
numpages = {14},
keywords = {Data mining, Classification, Decision tree, Ensemble learning, Feature selection, Voting}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.5555/3122009.3242031,
author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
title = {On the stability of feature selection algorithms},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The "stability" of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is 'unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging--we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {6345–6398},
numpages = {54},
keywords = {feature selection, stability}
}

@inproceedings{10.1007/978-3-319-92639-1_20,
author = {Pes, Barbara},
title = {Evaluating Feature Selection Robustness on High-Dimensional Data},
year = {2018},
isbn = {978-3-319-92638-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-92639-1_20},
doi = {10.1007/978-3-319-92639-1_20},
abstract = {With the explosive growth of high-dimensional data, feature selection has become a crucial step of machine learning tasks. Though most of the available works focus on devising selection strategies that are effective in identifying small subsets of predictive features, recent research has also highlighted the importance of investigating the robustness of the selection process with respect to sample variation. In presence of a high number of features, indeed, the selection outcome can be very sensitive to any perturbations in the set of training records, which limits the interpretability of the results and their subsequent exploitation in real-world applications. This study aims to provide more insight about this critical issue by analysing the robustness of some state-of-the-art selection methods, for different levels of data perturbation and different cardinalities of the selected feature subsets. Furthermore, we explore the extent to which the adoption of an ensemble selection strategy can make these algorithms more robust, without compromising their predictive performance. The results on five high-dimensional datasets, which are representatives of different domains, are presented and discussed.},
booktitle = {Hybrid Artificial Intelligent Systems: 13th International Conference, HAIS 2018, Oviedo, Spain, June 20-22, 2018, Proceedings},
pages = {235–247},
numpages = {13},
keywords = {Feature selection robustness, Ensemble techniques, High-dimensional data},
location = {Oviedo, Spain}
}

@article{10.3233/JIFS-169022,
author = {Jiang, Shengyi and Wang, Lianxi and Xiao, Zheng and Li, Kenli},
title = {A clustering-based feature selection via feature separability},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {31},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169022},
doi = {10.3233/JIFS-169022},
abstract = {With the extensive increase of the amount of data, such as text categorization, genomic microarray data, bio-informatics and digital images, there are more and more challenges in feature selection. Recently, feature selection has been widely studied in supervised learning, but there is significantly less work in unsupervised learning because of the absence of class information and explicit search criteria. In this work, we introduce a new measure to assess the importance of features in terms of feature separability. A clustering-based feature selection algorithm is then introduced to conduct the feature selection. The proposed algorithm with nearly linear time complexity selects final feature subset through a ranking procedure based on the separabilities of features and it is applicable to datasets of mixed nature. Experimental results on UCI datasets show that our method, by retaining relevant features, can obtain similar or even better results of classification and clustering for most datasets, and it outperforms other traditional supervised and unsupervised feature selection methods in terms of dimensionality reduction and classification accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {927–937},
numpages = {11},
keywords = {Feature selection, feature separability, clustering, unsupervised learning}
}

@article{10.1007/s10586-017-1182-z,
author = {Manikandan, R. P. S. and Kalpana, A. M.},
title = {RETRACTED ARTICLE: Feature selection using fish swarm optimization in big data},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {Suppl 5},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-1182-z},
doi = {10.1007/s10586-017-1182-z},
abstract = {The rapid advances in the field of information and communication technology has made the ubiquitous type of computing along with the internet of things extremely popular. Such applications have created the volumes of the data that are available for the analysis as well as the classification which is an aid to the process of decision making. Among the several methods that are used for the purpose of dealing with the big data, feature selection is found to be very effective. One of the common approaches that involve the searching using a subset of features that have been relevant to that of the topic or will represent an accurate description of this dataset. But unfortunately, the searching using this type of a subset is a problem that is combinatorial and may also be quite time consuming. The meta-heuristic algorithms have been commonly used for the purpose of facilitating the choice of features. Artificial fish swarm optimization (AFSO) algorithms will employ the fish swarming behavior to be the means of overcoming the combinatorial problems. The AFSA has now proved to be highly successful in the applications of a diverse nature. The results of the experiment show that this method proposed will achieve better performance than that of the other methods.},
journal = {Cluster Computing},
month = sep,
pages = {10825–10837},
numpages = {13},
keywords = {Fish swarm optimization, Meta-heuristics, Feature selection, Big data}
}

@inproceedings{10.1145/3236024.3236074,
author = {Jamshidi, Pooyan and Velez, Miguel and K\"{a}stner, Christian and Siegmund, Norbert},
title = {Learning to sample: exploiting similarities across environments to learn performance models for configurable systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236074},
doi = {10.1145/3236024.3236074},
abstract = {Most software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transfer-learning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {71–82},
numpages = {12},
keywords = {Software performance, configurable systems, transfer learning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1016/j.jvcir.2018.09.020,
author = {Su, Yuting and Bai, Xu and Li, Wu and Jing, Peiguang and Zhang, Jing and Liu, Jing},
title = {Graph regularized low-rank tensor representation for feature selection},
year = {2018},
issue_date = {Oct 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {56},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2018.09.020},
doi = {10.1016/j.jvcir.2018.09.020},
journal = {J. Vis. Comun. Image Represent.},
month = oct,
pages = {234–244},
numpages = {11},
keywords = {Unsupervised feature selection, Low-rank tensor representation, Graph embedding, Subspace clustering}
}

@article{10.1016/j.jbi.2021.103763,
author = {Speiser, Jaime Lynn},
title = {A random forest method with feature selection for developing medical prediction models with clustered and longitudinal data},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103763},
doi = {10.1016/j.jbi.2021.103763},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {11},
keywords = {Random forest, Feature selection, Variable selection, Longitudinal outcomes, Clustered outcomes, Binary mixed model forest, Health ABC, GLMM, BiMM, AUC, LASSO, DGP, MCC, ROC}
}

@article{10.1016/j.cie.2020.106536,
author = {Aremu, Oluseun Omotola and Cody, Roya Allison and Hyland-Wood, David and McAree, Peter Ross},
title = {A relative entropy based feature selection framework for asset data in predictive maintenance},
year = {2020},
issue_date = {Jul 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2020.106536},
doi = {10.1016/j.cie.2020.106536},
journal = {Comput. Ind. Eng.},
month = jul,
numpages = {13},
keywords = {Predictive maintenance, Asset management, Machine learning, Feature selection, Feature engineering, Information theory, Relative entropy}
}

@article{10.1007/s11390-020-9323-x,
author = {Peynirci, G\"{o}k\c{c}er and Emina\u{g}ao\u{g}lu, Mete and Karabulut, Korhan},
title = {Feature Selection for Malware Detection on the Android Platform Based on Differences of IDF Values},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {4},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9323-x},
doi = {10.1007/s11390-020-9323-x},
abstract = {Android is the mobile operating system most frequently targeted by malware in the smartphone ecosystem, with a market share significantly higher than its competitors and a much larger total number of applications. Detection of malware before being published on official or unofficial application markets is critically important due to the typical end users’ widespread security inadequacy. In this paper, a novel feature selection method is proposed along with an Android malware detection approach. The feature selection method proposed in this study makes use of permissions, API calls, and strings as features, which are statically extractable from the Android executables (APK files) and it can be used in a machine learning process with different algorithms to detect malware on the Android platform. A novel document frequencybased approach, namely Delta IDF, was designed and implemented for feature selection. Delta IDF was tested upon three universal benchmark datasets that contain Android malware samples and highly promising results were obtained by using several binary classification algorithms.},
journal = {J. Comput. Sci. Technol.},
month = jul,
pages = {946–962},
numpages = {17},
keywords = {malware detection, Android, feature selection, inverse document frequency, static analysis}
}

@inproceedings{10.1145/3071178.3079193,
author = {Alhakbani, Haya and al-Rifaie, Mohammad Majid},
title = {Feature selection using stochastic diffusion search},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3079193},
doi = {10.1145/3071178.3079193},
abstract = {The method introduced in this paper uses stochastic diffusion search (SDS) to select the most relevant feature subset for the classification task. In this algorithm, SDS is adapted to find a suitable feature subset. Moreover, support vector machine (SVM) is used as a classifier to evaluate the predictive accuracy of the agent. The proposed method exhibits a statistically significant outperformance when compared with the performance of the classifier without the SDS-powered features selections. Additionally, the results have been also compared with other methods from the literature over nine datasets. It is shown that the proposed SDS based feature selection (SDS-FS) offers a competitive performance with other methods on datasets with feature size greater than 10. The behaviour of the proposed algorithm has been investigated in the context of global exploration and local exploitation.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {385–392},
numpages = {8},
keywords = {swarm intelligence, feature selections, dimensionality reduction},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.asoc.2020.106651,
author = {Purushothaman, R. and Rajagopalan, S.P. and Dhandapani, Gopinath},
title = {Hybridizing Gray Wolf Optimization (GWO) with Grasshopper Optimization Algorithm (GOA) for text feature selection and clustering},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106651},
doi = {10.1016/j.asoc.2020.106651},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {14},
keywords = {And text clustering, FCM, GOA, GWO, Preprocessing steps, Feature selection}
}

@article{10.1016/j.asoc.2021.107745,
author = {Moldovan, Dorin and Slowik, Adam},
title = {Energy consumption prediction of appliances using machine learning and multi-objective binary grey wolf optimization for feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107745},
doi = {10.1016/j.asoc.2021.107745},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {23},
keywords = {Grey wolf optimizer, Multi-objective, Feature selection, Energy consumption prediction of appliances, MOORA}
}

@article{10.1007/s00521-020-05342-3,
author = {Choudhary, Meenakshi and Tiwari, Vivek and Uduthalapally, Venkanna},
title = {Iris presentation attack detection based on best-k feature selection from YOLO inspired RoI},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05342-3},
doi = {10.1007/s00521-020-05342-3},
abstract = {Obfuscating an iris recognition system through forged iris samples has been a major security threat in iris-based authentication. Therefore, a detection mechanism is essential that may explicitly discriminate between the live iris and forged (attack) patterns. The majority of existing methods analyze the eye image as a whole to find discriminatory features for fake and real iris. However, many attacks do not alter the entire eye image, instead merely the iris region is affected. It infers that the iris embodies the region of interest (RoI) for an exhaustive search towards identifying forged iris patterns. This paper introduces a novel framework that locates RoI using the YOLO approach and performs selective image enhancement to enrich the core textural details. The YOLO approach tightly bounds the iris region without any pattern loss, where the textural analysis through local and global descriptors is expected to be efficacious. Afterward, various handcrafted and CNN based methods are employed to extract the discriminative textural features from the RoI. Later, the best-k features are identified through the Friedman test as the optimal feature set and combined using score-level fusion. Further, the proposed approach is assessed on six different iris databases using predefined intra-dataset, cross-dataset, and combined-dataset validation protocols. The experimental outcomes exhibit that the proposed method results in significant error reduction with the state of the arts.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {5609–5629},
numpages = {21},
keywords = {Score-level fusion, RoI localization, Iris presentation attack detection, Image enhancement, Feature selection, DarkNet-19}
}

@article{10.1007/s11045-018-0595-z,
author = {Tao, Gao and Liu, Zhanwen and Cao, Jinpei and Liang, Shan},
title = {Local difference ternary sequences descriptor based on unsupervised min redundancy mutual information feature selection},
year = {2020},
issue_date = {Jul 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {3},
issn = {0923-6082},
url = {https://doi.org/10.1007/s11045-018-0595-z},
doi = {10.1007/s11045-018-0595-z},
abstract = {Texture feature description research have received significant attention in recent years. It is widely known that the local texture feature descriptor can achieve good performance under various image conditions, such as geometric size variation, different poses, complex illumination and partial occlusion. Although Local Difference Binary is an acknowledged excellent feature descriptor, it only computes the intensity and gradient difference on pairwise grid cells and ignores the image grid texture intensity and gradient. This paper proposes a novel local texture descriptor, named as Local Difference Ternary (LDT), which can not only represent difference and texture information of the grid cells intensity and gradient simultaneously, but also capture richer detailed texture information. In addition, the Unsupervised Min Redundancy Mutual Information (UMRMI) for feature selection is proposed to select the optimal subset of LDT features for achieving more powerfully discriminative ability. For the purpose of further improving the efficiency and effectiveness of UMRMI, we extend UMRMI to k-means space, namely k-UMRMI. Furthermore, a multi-degree scheme is adopted to achieve richer texture description. Finally, Radial Function Neural Network is employed for classification, which is an excellent classifier, especially for larger samples. Several experimental results on certain benchmark face databases demonstrate that our proposed method is remarkably superior to some other state-of-the-art approaches under various image conditions.},
journal = {Multidimensional Syst. Signal Process.},
month = jul,
pages = {771–791},
numpages = {21},
keywords = {Multi-degrees, Feature selection, LDB, Texture feature extraction}
}

@inproceedings{10.1145/3038912.3052622,
author = {Lampos, Vasileios and Zou, Bin and Cox, Ingemar Johansson},
title = {Enhancing Feature Selection Using Word Embeddings: The Case of Flu Surveillance},
year = {2017},
isbn = {9781450349130},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3038912.3052622},
doi = {10.1145/3038912.3052622},
abstract = {Health surveillance systems based on online user-generated content often rely on the identification of textual markers that are related to a target disease. Given the high volume of available data, these systems benefit from an automatic feature selection process. This is accomplished either by applying statistical learning techniques, which do not consider the semantic relationship between the selected features and the inference task, or by developing labour-intensive text classifiers. In this paper, we use neural word embeddings, trained on social media content from Twitter, to determine, in an unsupervised manner, how strongly textual features are semantically linked to an underlying health concept. We then refine conventional feature selection methods by a priori operating on textual variables that are sufficiently close to a target concept. Our experiments focus on the supervised learning problem of estimating influenza-like illness rates from Google search queries. A "flu infection" concept is formulated and used to reduce spurious and potentially confounding features that were selected by previously applied approaches. In this way, we also address forms of scepticism regarding the appropriateness of the feature space, alleviating potential cases of overfitting. Ultimately, the proposed hybrid feature selection method creates a more reliable model that, according to our empirical analysis, improves the inference performance (Mean Absolute Error) of linear and nonlinear regressors by 12% and 28.7%, respectively.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
pages = {695–704},
numpages = {10},
keywords = {word embeddings, user-generated content, syndromic surveillance, search query logs, regularised regression, influenza-like illness, gaussian processes, feature selection, computational health},
location = {Perth, Australia},
series = {WWW '17}
}

@article{10.1007/s10489-019-01420-9,
author = {Zhang, Yong and Li, Hai-Gang and Wang, Qing and Peng, Chao},
title = {A filter-based bare-bone particle swarm optimization algorithm for unsupervised feature selection},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01420-9},
doi = {10.1007/s10489-019-01420-9},
abstract = {Due to good exploration capability, particle swarm optimization (PSO) has shown advantages on solving supervised feature selection problems. Compared with supervised and semi-supervised cases, unsupervised feature selection becomes very difficult as a result of no label information. This paper studies a novel PSO-based unsupervised feature selection method, called filter-based bare-bone particle swarm optimization algorithm (FBPSO). Two filter-based strategies are proposed to speed up the convergence of the algorithm. One is a space reduction strategy based on average mutual information, which is used to remove irrelevant and weakly relevant features fast; another is a local filter search strategy based on feature redundancy, which is used to improve the exploitation capability of the swarm. And, a feature similarity-based evaluation function and a parameter-free update strategy of particle are introduced to enhance the performance of FBPSO. Experimental results on some typical datasets confirm superiority and effectiveness of the proposed FBPSO.},
journal = {Applied Intelligence},
month = aug,
pages = {2889–2898},
numpages = {10},
keywords = {Unsupervised, Particle swarm optimization, Feature selection}
}

@inproceedings{10.1145/3200947.3201035,
author = {Theodorou, Theodoros and Mporas, Iosif and Potamitis, Ilyas and Fakotakis, Nikos},
title = {Data-Driven Audio Feature Selection for Audio Quality Recognition in Broadcast News},
year = {2018},
isbn = {9781450364331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3200947.3201035},
doi = {10.1145/3200947.3201035},
abstract = {In1 this paper, we describe automatic audio quality recognition architecture for radio broadcast news based on audio feature selection, using the discrimination ability of the audio descriptors as a criterion of selection. Specifically, we labeled streams of broadcast news transmissions according to their audio quality based on the human auditory perception. Parameterization algorithms extract a large set of audio descriptors and an algorithm of data-driven criteria rank the descriptors' relevance. After that, the feature subsets fed machine learning algorithms for classification. This methodology showed that the k-nearest neighbor classifier provides significantly good results, considering the achieved accuracy. Moreover, the experimental framework verifies the assumption that discarding irrelevant audio descriptors before the classification stage works in favor to the overall identification performance.},
booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
articleno = {5},
numpages = {6},
keywords = {broadcast news, audio feature selection, Automatic audio quality recognition},
location = {Patras, Greece},
series = {SETN '18}
}

@article{10.1016/j.knosys.2021.106855,
author = {Kim, Hansu and Lee, Tae Hee and Kwon, Taejoon},
title = {Normalized neighborhood component feature selection and feasible-improved weight allocation for input variable selection},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {218},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.106855},
doi = {10.1016/j.knosys.2021.106855},
journal = {Know.-Based Syst.},
month = apr,
numpages = {14},
keywords = {Normalized neighborhood component feature selection, Feasible-improved weight allocation, Input variable selection, Multi-response system, Design optimization, Body-in-white}
}

@inproceedings{10.1145/3106237.3106273,
author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
title = {Finding near-optimal configurations in product lines by random sampling},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106273},
doi = {10.1145/3106237.3106273},
abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {61–71},
numpages = {11},
keywords = {finding optimal configurations, searching configuration spaces, software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1007/s10489-018-1151-0,
author = {Nizami, Imran Fareed and Majid, Muhammad and Khurshid, Khawar},
title = {New feature selection algorithms for no-reference image quality assessment},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1151-0},
doi = {10.1007/s10489-018-1151-0},
abstract = {No reference image quality assessment (NR-IQA) is a challenging task since reference images are usually unavailable in real world scenarios. The performance of NR-IQA techniques is vastly dependent on the features utilized to predict the image quality. Many NR-IQA techniques have been proposed that extract features in different domains like spatial, discrete cosine transform and wavelet transform. These NR-IQA techniques have the possibility to contain redundant features, which result in degradation of quality score prediction. Recently impact of general purpose feature selection algorithms on NR-IQA techniques has shown promising results. But these feature selection algorithms have the tendency to select irrelevant features and discard relevant features. This paper presents fifteen new feature selection algorithms specifically designed for NR-IQA, which are based on Spearman rank ordered correlation constant (SROCC), linear correlation constant (LCC), Kendall correlation constant (KCC) and root mean squared error (RMSE). The proposed feature selection algorithms are applied on the extracted features of existing NR-IQA techniques. Support vector regression (SVR) is then applied to selected features to predict the image quality score. The fifteen newly proposed feature selection algorithms are evaluated using eight different NR-IQA techniques over three commonly used image quality assessment databases. Experimental results show that the proposed feature selection algorithms not only reduce the number of features but also improve the performance of NR-IQA techniques. Moreover, features selection algorithms based on SROCC and its combination with LCC, KCC and RMSE perform better in comparison to other proposed algorithms.},
journal = {Applied Intelligence},
month = oct,
pages = {3482–3501},
numpages = {20},
keywords = {Computational intelligence, Feature extraction, Feature selection, No-reference image quality assessment, Perceived quality}
}

@inproceedings{10.1007/978-3-319-93040-4_3,
author = {Zhang, Wei and Bose, Shiladitya and Kobeissi, Said and Tomko, Scott and Challis, Chris},
title = {Efficient Feature Selection Framework for&nbsp;Digital Marketing Applications},
year = {2018},
isbn = {978-3-319-93039-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-93040-4_3},
doi = {10.1007/978-3-319-93040-4_3},
abstract = {Digital marketing strategies can help businesses achieve better Return on Investment (ROI). Big data and predictive modelling are key to identifying these specific customers. Yet the very rich and mostly irrelevant attributes(features) will adversely affect the predictive modelling performance, both computationally and qualitatively. So selecting relevant features is a crucial task for marketing applications. The feature selection process is very time consuming due to the large amount of data and high dimensionality of features. In this paper, we propose to reduce the computation time through regularizing the feature search process using expert knowledge. We also combine the regularized search with a generative filtering step, so we can address potential problems with the regularized search and further speed up the process. In addition, a progressive sampling and coarse to fine selection framework is built to further lower the space and time requirements.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part III},
pages = {28–39},
numpages = {12},
keywords = {Feature Selection Framework, Feature Search Process, Generative Filtering Step, Progressive Sampling, Forward Search Algorithm},
location = {Melbourne, VIC, Australia}
}

@inproceedings{10.1145/3167918.3167951,
author = {Pham, Ngoc Tu and Foo, Ernest and Suriadi, Suriadi and Jeffrey, Helen and Lahza, Hassan Fareed M},
title = {Improving performance of intrusion detection system using ensemble methods and feature selection},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167951},
doi = {10.1145/3167918.3167951},
abstract = {The main task of an intrusion detection system (IDS) is to detect anomalous behaviors from both within and outside the network system, and there have been increasing studies applying machine learning in this area. The limitations of using a single classifier in the classification of normal traffic and anomalies (attacks) led to the idea of building hybrid or ensemble models which are more complicated but provide higher accuracy and lower false alarm rate (FAR). The aim of this paper is to improve the performance of IDS by using ensemble methods and feature selection. The ensemble models were built based on the two ensemble techniques, Bagging and Boosting, with the tree-based algorithms as the base classifier. The proposed models were then evaluated using NSL-KDD datasets. The experimental results showed that the bagging ensemble model with J48 as the base classifier produced the best performance in terms of both classification accuracy and FAR when working with the subset of 35 selected features.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {2},
numpages = {6},
keywords = {ensemble, feature selection, intrusion detection system},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@article{10.1007/s10462-017-9581-3,
author = {Adams, Stephen and Beling, Peter A.},
title = {A survey of feature selection methods for Gaussian mixture models and hidden Markov models},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9581-3},
doi = {10.1007/s10462-017-9581-3},
abstract = {Feature selection is the process of reducing the number of collected features to a relevant subset of features and is often used to combat the curse of dimensionality. This paper provides a review of the literature on feature selection techniques specifically designed for Gaussian mixture models (GMMs) and hidden Markov models (HMMs), two common parametric latent variable models. The primary contribution of this work is the collection and grouping of feature selection methods specifically designed for GMMs and for HMMs. An additional contribution lies in outlining the connections between these two groups of feature selection methods. Often, feature selection methods for GMMs and HMMs are treated as separate topics. In this survey, we propose that methods developed for one model can be adapted to the other model. Further, we find that the number of feature selection methods for GMMs outweighs the number of methods for HMMs and that the proportion of methods for HMMs that require supervised data is larger than the proportion of GMM methods that require supervised data. We conclude that further research into unsupervised feature selection methods for HMMs is required and that established methods for GMMs could be adapted to HMMs. It should be noted that feature selection can also be referred to as dimensionality reduction, variable selection, attribute selection, and variable subset reduction. In this paper, we make a distinction between dimensionality reduction and feature selection. Dimensionality reduction, which we do not consider, is any process that reduces the number of features used in a model and can include methods that transform features in order to reduce the dimensionality. Feature selection, by contrast, is a specific form of dimensionality reduction that eliminates feature as inputs into the model. The primary difference is that dimensionality reduction can still require the collection of all the data sources in order to transform and reduce the feature set, while feature selection eliminates the need to collect the irrelevant data sources.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {1739–1779},
numpages = {41},
keywords = {Hidden Markov model, Gaussian mixture model, Feature selection}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Automated software testing, Highly-configurable systems, Test case prioritization, Variability}
}

@article{10.1016/j.eswa.2017.01.044,
author = {Goswami, Saptarsi and Das, Amit Kumar and Chakrabarti, Amlan and Chakraborty, Basabi},
title = {A feature cluster taxonomy based feature selection technique},
year = {2017},
issue_date = {August 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {79},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.01.044},
doi = {10.1016/j.eswa.2017.01.044},
abstract = {FCTFS works in both autonomous and user guided mode.The defined taxonomy helps in arriving at optimal number of good quality clusters.Feature elimination due to irrelevance and redundancy is clearly isolated.It is faster than traditional search based methods.Yields superior results compared to some state of the art methods over 24 data sets. Feature subset selection is basically an optimization problem for choosing the most important features from various alternatives in order to facilitate classification or mining problems. Though lots of algorithms have been developed so far, none is considered to be the best for all situations and researchers are still trying to come up with better solutions. In this work, a flexible and user-guided feature subset selection algorithm, named as FCTFS (Feature Cluster Taxonomy based Feature Selection) has been proposed for selecting suitable feature subset from a large feature set. The proposed algorithm falls under the genre of clustering based feature selection techniques in which features are initially clustered according to their intrinsic characteristics following the filter approach. In the second step the most suitable feature is selected from each cluster to form the final subset following a wrapper approach. The two stage hybrid process lowers the computational cost of subset selection, especially for large feature data sets. One of the main novelty of the proposed approach lies in the process of determining optimal number of feature clusters. Unlike currently available methods, which mostly employ a trial and error approach, the proposed method characterises and quantifies the feature clusters according to the quality of the features inside the clusters and defines a taxonomy of the feature clusters. The selection of individual features from a feature cluster can be done judiciously considering both the relevancy and redundancy according to users intention and requirement. The algorithm has been verified by simulation experiments with different bench mark data set containing features ranging from 10 to more than 800 and compared with other currently used feature selection algorithms. The simulation results prove the superiority of our proposal in terms of model performance, flexibility of use in practical problems and extendibility to large feature sets. Though the current proposal is verified in the domain of unsupervised classification, it can be easily used in case of supervised classification.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {76–89},
numpages = {14},
keywords = {Redundancy, Feature selection, Feature cluster, Entropy, Coefficient of variation}
}

@article{10.1007/s00521-018-3455-8,
author = {Mostafa, Sheikh Shanawaz and Morgado-Dias, Fernando and Ravelo-Garc\'{\i}a, Antonio G.},
title = {Comparison of SFS and mRMR for oximetry feature selection in obstructive sleep apnea detection},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3455-8},
doi = {10.1007/s00521-018-3455-8},
abstract = {Obstructive sleep apnea is a disorder characterized by pauses in respiration during sleep. Due to this disturbance in breathing, there is a decrease in the oxygen saturation (SpO2) level. Thus, SpO2 can be used as a source of information for the automatic detection of apnea. Several solutions exist in the literature where different features are used. To find a better discriminant capacity, a subset of few features that obtains higher accuracy with the proper classifier is needed. To face this challenge, this work compares two different feature selection methods. The first one is a filter method named minimum redundancy maximum relevance, and the other one is called sequential forward search. These methods are tested with different classifiers. Two public datasets with 8 and 25 subjects are used to test and compare the performances of the different feature selection methods. A set of features for each classifier is obtained, and the results are compared with the previous work. The results found in this work show a good performance with respect to the state of the art and present a good option for apnea screening with low resources.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {15711–15731},
numpages = {21},
keywords = {Classification, Feature section, mRMR, SFS, Sleep apnea, SpO2}
}

@article{10.1007/s11042-019-7370-5,
author = {Atallah, Dalia M. and Badawy, Mohammed and El-Sayed, Ayman and Ghoneim, Mohamed A.},
title = {Predicting kidney transplantation outcome based on hybrid feature selection and KNN classifier},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7370-5},
doi = {10.1007/s11042-019-7370-5},
abstract = {Kidney transplantation outcome prediction is very significant and doesn't require emphasis. This will grant the selection of the best available kidney donor and the best immunosuppressive treatment for patients. Survival prediction before treatment could simplify patient's decision making and boost survival by altering clinical practice. This paper proposes a new novel prediction method based on data mining techniques to predict five-year graft survival after transplantation. This new proposed prediction method composes of three stages: data preparation stage (DPS), feature selection stage (FSS), and prediction stage (PS). The new proposed prediction method merges information gain with na\"{\i}ve Bayes and k-nearest neighbor. Initially, it uses information gain to select the essential features, uses na\"{\i}ve Bayes to select the most essential features. These two methods are combined in a new hybrid feature selection method which chooses the minimum number of features that produce highest accuracy. Finally, it uses k-nearest neighbor for graft survival prediction classification. The proposed prediction method has been evaluated against recent techniques. Experimental results have proven that the proposed prediction method outperforms the recent techniques as it attains the maximum accuracy and F-measure with minimal errors. This prediction method can also be used in other transplant datasets.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {20383–20407},
numpages = {25},
keywords = {Feature selection, Information gain, K-nearest neighbor, Kidney transplantation, Na\"{\i}ve Bayes}
}

@article{10.1016/j.patcog.2016.05.018,
author = {Paul, Saurabh and Magdon-Ismail, Malik and Drineas, Petros},
title = {Feature selection for linear SVM with provable guarantees},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {60},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.05.018},
doi = {10.1016/j.patcog.2016.05.018},
abstract = {We give two provably accurate feature-selection techniques for the linear SVM. The algorithms run in deterministic and randomized time respectively. Our algorithms can be used in an unsupervised or supervised setting. The supervised approach is based on sampling features from support vectors. We prove that the margin in the feature space is preserved to within \'{z}-relative error of the margin in the full feature space in the worst-case. In the unsupervised setting, we also provide worst-case guarantees of the radius of the minimum enclosing ball, thereby ensuring comparable generalization as in the full feature space and resolving an open problem posed in Dasgupta et al. (2007) 7. We present extensive experiments on real-world datasets to support our theory and to demonstrate that our method is competitive and often better than prior state-of-the-art, for which there are no known provable guarantees. HighlightsWe give two provably accurate feature-selection techniques for the linear SVM.Algorithms can be used in supervised or unsupervised setting.We prove margin is preserved to within ε-relative error in the full feature space.In unsupervised case, we provide worst-case guarantees of margin and radius of minimum enclosing ball.Extensive experiments demonstrate that our method is competitive and often better than prior art.},
journal = {Pattern Recogn.},
month = dec,
pages = {205–214},
numpages = {10},
keywords = {Sampling, Linear SVM, Feature Selection}
}

@article{10.1007/s10462-017-9556-4,
author = {Melo, Andr\'{e} and Paulheim, Heiko},
title = {Local and global feature selection for multilabel classification with binary relevance},
year = {2019},
issue_date = {January   2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9556-4},
doi = {10.1007/s10462-017-9556-4},
abstract = {Multilabel classification has become increasingly important for various use cases. Amongst the existing multilabel classification methods, problem transformation approaches, such as Binary Relevance, Pruned Problem Transformation, and Classifier Chains, are some of the most popular, since they break a global multilabel classification problem into a set of smaller binary or multiclass classification problems. Transformation methods enable the use of two different feature selection approaches: local, where the selection is performed independently for each of the transformed problems, and global, where the selection is performed on the original dataset, meaning that all local classifiers work on the same set of features. While global methods have been widely researched, local methods have received little attention so far. In this paper, we compare those two strategies on one of the most straight forward transformation approaches, i.e., Binary Relevance. We empirically compare their performance on various flat and hierarchical multilabel datasets of different application domains. We show that local outperforms global feature selection in terms of classification accuracy, without drawbacks in runtime performance.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {33–60},
numpages = {28},
keywords = {Transformation methods, Multilabel classification, Local feature selection, Global feature selection, Binary relevance}
}

@article{10.1016/j.asoc.2019.105980,
author = {Prasad, Mahendra and Tripathi, Sachin and Dahal, Keshav},
title = {An efficient feature selection based Bayesian and Rough set approach for intrusion detection},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {87},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105980},
doi = {10.1016/j.asoc.2019.105980},
journal = {Appl. Soft Comput.},
month = feb,
numpages = {14},
keywords = {Bayes theorem, Rough set theory, Feature selection, Dataset realism, CICIDS2017 dataset evaluation, Intrusion detection system}
}

@inproceedings{10.1145/2984043.2998540,
author = {Braz, Larissa},
title = {An approach to compile configurable systems with #ifdefs based on impact analysis},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984043.2998540},
doi = {10.1145/2984043.2998540},
abstract = {Configurable systems typically use #ifdefs to denote variability. Generating and compiling all configurations may be time-consuming. An alternative consists of using variability-aware parsers, such as TypeChef. However, they may not scale. We propose a change-centric approach to compile configurable systems with #ifdefs by analyzing only configurations impacted by a code change. We implemented it in a tool called CHECKCONFIGMX. We perform an empirical study to evaluate 3,913 transformations applied to the 14 largest files of BusyBox, Apache HTTPD, and Expat configurable systems. CHECKCONFIGMX finds 595 compilation errors of 20 types introduced by 41 developers in 214 commits (5.46% of the analyzed transformations). In our study, it reduces by at least 50% (an average of 99%) the effort of evaluating the analyzed transformations by comparing with the exhaustive approach without considering a feature model.},
booktitle = {Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
pages = {51–52},
numpages = {2},
keywords = {#ifdef, conditional compilation, configurable systems, impact analysis},
location = {Amsterdam, Netherlands},
series = {SPLASH Companion 2016}
}

@article{10.3233/JIFS-200937,
author = {Liu, Xuning and Zhang, Guoying and Zhang, Zixian},
title = {A novel hybrid feature selection and modified KNN prediction model for coal and gas outbursts},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200937},
doi = {10.3233/JIFS-200937},
abstract = {The feature selection of influencing factors of coal and gas outbursts is of great significance for presenting the most discriminative features and improving prediction performance of a classifier, the paper presents an effective hybrid feature selection and modified outbursts classifier framework which aims at solving exiting coal and gas outbursts prediction problems. First, a measurement standard based on maximum information coefficient(MIC) is employed to identify the wide correlations between two variables; Second, based on a ranking procedure using non-dominated sorting genetic algorithm(NSGAII), maximum relevance minimum redundancy(MRMR) algorithm is subsequently performed to find out candidate feature set highly related to the class label and uncorrelated with each other; Third, random forest(RF) is employed to search the optimal feature subset from the candidate feature set, then the optimal feature subset that influences the classification performance of coal and gas outbursts is obtained; Finally, an improved classifier model has been proposed that combines gradient boosting decision tree(GBDT) and k-nearest neighbor(KNN) for outbursts prediction. In the modified classifier model, the GBDT is utilized to assign different weights to features, then the weighted features are input into the KNN to verify the effectiveness of proposed method on coal and gas outbursts dataset. The experimental results conclude that our proposed scheme is effective in the number of feature and prediction accuracy when compared with other related state-of-the-art prediction models based on feature selection for coal and gas outbursts.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7671–7691},
numpages = {21},
keywords = {Coal and gas outbursts, Maximum information coefficient, Non-dominated sorting genetic algorithm, Maximum relevance minimum redundancy, Random forest, Gradient boosting decision tree, K-nearest neighbor}
}

@article{10.1155/2021/9947059,
author = {Liu, Xiaodong and Li, Tong and Zhang, Runzi and Wu, Di and Liu, Yongheng and Yang, Zhen and Sciancalepore, Savio},
title = {A GAN and Feature Selection-Based Oversampling Technique for Intrusion Detection},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/9947059},
doi = {10.1155/2021/9947059},
abstract = {In recent years, there have been numerous cyber security issues that have caused considerable damage to the society. The development of efficient and reliable Intrusion Detection Systems (IDSs) is an effective countermeasure against the growing cyber threats. In modern high-bandwidth, large-scale network environments, traditional IDSs suffer from a high rate of missed and false alarms. Researchers have introduced machine learning techniques into intrusion detection with good results. However, due to the scarcity of attack data, such methods’ training sets are usually unbalanced, affecting the analysis performance. In this paper, we survey and analyze the design principles and shortcomings of existing oversampling methods. Based on the findings, we take the perspective of imbalance and high dimensionality of datasets in the field of intrusion detection and propose an oversampling technique based on Generative Adversarial Networks (GAN) and feature selection. Specifically, we model the complex high-dimensional distribution of attacks based on Gradient Penalty Wasserstein GAN (WGAN-GP) to generate additional attack samples. We then select a subset of features representing the entire dataset based on analysis of variance, ultimately generating a rebalanced low-dimensional dataset for machine learning training. To evaluate the effectiveness of our proposal, we conducted experiments based on the NSL-KDD, UNSW-NB15, and CICIDS-2017 datasets. The experimental results show that our method can effectively improve the detection performance of machine learning models and outperform the baselines.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {15}
}

@article{10.1016/j.knosys.2017.02.013,
author = {Das, Asit K and Das, Sunanda and Ghosh, Arka},
title = {Ensemble feature selection using bi-objective genetic algorithm},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.013},
doi = {10.1016/j.knosys.2017.02.013},
abstract = {An ensemble parallel processing bi-objective genetic algorithm based feature selection method is proposed.Rough set theory and Mutual information gain are used to select informative data removing the vague one.Parallel processing in genetic algorithm reduces time complexity.The method is compared with the existing state-of-the-art methods using suitable datasets.Classification accuracy and statistical measures outperforms that of other state-of-the-art methods. Feature selection problem in data mining is addressed here by proposing a bi-objective genetic algorithm based feature selection method. Boundary region analysis of rough set theory and multivariate mutual information of information theory are used as two objective functions in the proposed work, to select only precise and informative data from the data set. Data set is sampled with replacement strategy and the method is applied to determine non-dominated feature subsets from each sampled data set. Finally, ensemble of such bi-objective genetic algorithm based feature selectors is developed with the help of parallel implementations to produce much generalized feature subset. In fact, individual feature selector outputs are aggregated using a novel dominance based principle to produce final feature subset. Proposed work is validated using repository especially for feature selection datasets as well as on UCI machine learning repository datasets and the experimental results are compared with related state of art feature selection methods to show effectiveness of the proposed ensemble feature selection method.},
journal = {Know.-Based Syst.},
month = may,
pages = {116–127},
numpages = {12},
keywords = {Evolutionary optimization, Feature selection, Genetic algorithm, Mutual information, Rough set theory, Supervised learning}
}

@article{10.1016/j.cose.2019.101645,
author = {Wang, Meng and Lu, Yiqin and Qin, Jiancheng},
title = {A dynamic MLP-based DDoS attack detection method using feature selection and feedback},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {88},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2019.101645},
doi = {10.1016/j.cose.2019.101645},
journal = {Comput. Secur.},
month = jan,
numpages = {14},
keywords = {Network security, Multilayer perceptrons, Intrusion detection, Feature selection, DDoS attacks}
}

@inproceedings{10.1145/3206185.3206193,
author = {Brito, Ricardo and Fong, Simon and Wu, Yaoyang and Deb, Suash},
title = {A Novel Algorithm for Neural Network Architecture Generation, Parameter Optimization and Feature Selection: ParFeatArch Generator},
year = {2018},
isbn = {9781450364126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3206185.3206193},
doi = {10.1145/3206185.3206193},
abstract = {In this paper, we propose ParFeatArch Generator, a new algorithm for generating Neural Network architectures with optimal features and parameters through Particle Swarm Optimization. Selecting the best architecture for a Neural Network is usually done through a trial and error process, in which the number of layers is selected usually based on previous experience and then the network is trained and tested. When using Neural Networks as classifiers in feature selection algorithms, usually the number of layers in the Neural Network is selected prior to using the Neural Network as a classifier to the feature selection algorithm. In this work we propose a new generative algorithm called ParFeatArch Generator, which is based on PSO and combines the feature selection process with the Neural Network architecture selection process and parameter optimization in one algorithm which generates the Neural Network topology with optimal parameters while at the same time performs feature selection and evaluates the Neural Network topology to determine its quality. With the proposed algorithm, given a dataset, it is possible to end up with the optimal features on the dataset and with an optimal Neural Network classifier with optimal parameters for such features.},
booktitle = {Proceedings of the 2nd International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {17–23},
numpages = {7},
keywords = {Particle Swarm Optimization (PSO), Neural Networks, Generative Algorithms, Feature Selection},
location = {Phuket, Thailand},
series = {ISMSI '18}
}

@inproceedings{10.1007/978-3-030-29726-8_10,
author = {Verma, Ghanshyam and Jha, Alokkumar and Rebholz-Schuhmann, Dietrich and Madden, Michael G.},
title = {Ranked MSD: A New Feature Ranking and Feature Selection Approach for Biomarker Identification},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_10},
doi = {10.1007/978-3-030-29726-8_10},
abstract = {In the era of big data when a huge amount of data is continuously being generated, it is common for situations to arise where the number of samples is much smaller than the number of features (variables) per sample. This phenomenon is often found in biomedical domains, where we may have relatively few patients, compared to the amount of data per patient. For example, gene expression data typically has between 10,000 and 60,000 features per sample. A separate issue arises from the “right to explanation” found in the European General Data Protection Regulation (GDPR), which may prevent the use of black-box models in applications where explainability is required. In such situations, there is a need for robust algorithms which can identify the relevant features from experimental data by discarding irrelevant ones, yielding a simpler subset that facilitates explanation. To address these needs, we have developed a new algorithm for feature ranking and feature selection, named Ranked MSD. We have tested our proposed approach on two real-world gene expression data sets, both of which relate to respiratory viral infections. This Ranked MSD feature selection algorithm is able to reduce the feature set size from 12,023 genes (features) to 65 genes on the first data set and from 20,737 genes to 31 genes on the second data set, in both cases without any significant loss in disease prediction accuracy. In an alternative configuration, our proposed algorithm is able to identify a small subset of features that gives better accuracy than that of the full feature set. Our proposed algorithm can also identify important biomarkers (genes) with their importance score for a particular disease and the identified top-ranked biomarkers can play a vital role in drug discovery and precision medicine.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {147–167},
numpages = {21},
keywords = {Machine learning, Respiratory viral infection, Feature ranking, Feature selection, Classification, Explainable AI},
location = {Canterbury, United Kingdom}
}

@inproceedings{10.1145/2993236.2993254,
author = {Al-Hajjaji, Mustafa and Meinicke, Jens and Krieter, Sebastian and Schr\"{o}ter, Reimar and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {Tool demo: testing configurable systems with FeatureIDE},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993254},
doi = {10.1145/2993236.2993254},
abstract = {Most software systems are designed to provide custom functionality using configuration options. Testing such systems is challenging as running tests of a single configuration is often not sufficient, because defects may appear in other configurations. Ideally, all configurations of a software system should be tested, which is usually not applicable in practice due to the combinatorial explosion with respect to the configuration options. Multiple sampling strategies aim to reduce the set of tested configurations to a feasible amount, such as T-wise sampling, random configurations, and user-defined configurations. However, these strategies are often not applied in practice as they require manual effort or a specialized testing framework. Within our tool FeatureIDE, we integrate all aforementioned strategies and reduce the manual effort by automating the process of generating and testing configurations. Furthermore, we provide support for unit testing to avoid redundant test executions and for variability-aware testing. With this extension of FeatureIDE, we aim to make recent testing techniques for configurable systems applicable in practice.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {173–177},
numpages = {5},
keywords = {Prioritization, T-Wise Sampling, Testing},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.eswa.2020.113237,
author = {Niu, Tong and Wang, Jianzhou and Lu, Haiyan and Yang, Wendong and Du, Pei},
title = {Developing a deep learning framework with two-stage feature selection for multivariate financial time series forecasting},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {148},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113237},
doi = {10.1016/j.eswa.2020.113237},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {17},
keywords = {Deep learning, Multivariate financial time series, Forecasting, Feature selection, Multi-objective optimization}
}

@article{10.1145/2877204,
author = {Zhang, Ce and Kumar, Arun and R\'{e}, Christopher},
title = {Materialization Optimizations for Feature Selection Workloads},
year = {2016},
issue_date = {April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0362-5915},
url = {https://doi.org/10.1145/2877204},
doi = {10.1145/2877204},
abstract = {There is an arms race in the data management industry to support statistical analytics. Feature selection, the process of selecting a feature set that will be used to build a statistical model, is widely regarded as the most critical step of statistical analytics. Thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature selection language and a supporting prototype system that builds on top of current industrial R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analogue in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of datasets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new trade-off space across multiple R backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection.},
journal = {ACM Trans. Database Syst.},
month = feb,
articleno = {2},
numpages = {32},
keywords = {Feature selection, R, declarative language, machine learning, materialization, optimization, statistical analytics}
}

@article{10.1016/j.asoc.2019.105498,
author = {Hafiz, Faizal and Swain, Akshya and Naik, Chirag and Patel, Nitish},
title = {Efficient feature selection of power quality events using two dimensional (2D) particle swarms},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105498},
doi = {10.1016/j.asoc.2019.105498},
journal = {Appl. Soft Comput.},
month = aug,
numpages = {14},
keywords = {Classification, Dimensionality reduction, Feature selection, Particle swarm optimization, Pattern recognition, Power quality}
}

@article{10.1016/j.asoc.2019.105777,
author = {Sabando, Mar\'{\i}a Virginia and Ponzoni, Ignacio and Soto, Axel J.},
title = {Neural-based approaches to overcome feature selection and applicability domain in drug-related property prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105777},
doi = {10.1016/j.asoc.2019.105777},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {14},
keywords = {Neural networks, QSAR modeling, Model interpretability, Applicability domain, Feature selection}
}

@inproceedings{10.1145/3431943.3431955,
author = {WANG, LI and YAN, ZHENXIONG and LIU, YANJUN},
title = {Temporal-Spatial-Frequency Feature Selection of Brain-Computer Interface Based on BQPSO},
year = {2021},
isbn = {9781450388658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3431943.3431955},
doi = {10.1145/3431943.3431955},
abstract = {The electroencephalography (EEG) signals can be identified and translated into control commands by brain-computer interface (BCI) systems. To improve the recognition results of the EEG signals, a temporal-spatial-frequency feature selection model based on binary quantum particle swarm optimization (BQPSO) is proposed. The signals are firstly divided into six segments according to time, and then they are bandpass filtered into six different frequency ranges, respectively. Temporal-spatial-frequency features are extracted by common spatial pattern (CSP). After selecting by BQPSO, the optimized features are classified by extreme learning machine. Two different data sets are used to validate the proposed model, and their average classification results are 84.7% and 81.4%, respectively. Compared with other feature selection algorithms, our proposed model achieves the best results. Better classification results can be obtained by the appropriate feature selection algorithm.},
booktitle = {Proceedings of the 2020 9th International Conference on Bioinformatics and Biomedical Science},
pages = {71–76},
numpages = {6},
keywords = {Temporal-spatial-frequency, Extreme learning machine, Electroencephalogram (EEG), Brain-computer interface (BCI), Binary quantum particle swarm optimization (BQPSO)},
location = {Xiamen, China},
series = {ICBBS '20}
}

@article{10.1007/s10115-017-1140-3,
author = {Bol\'{o}n-Canedo, V. and Rego-Fern\'{a}ndez, D. and Peteiro-Barral, D. and Alonso-Betanzos, A. and Guijarro-Berdi\~{n}as, B. and S\'{a}nchez-Maro\~{n}o, N.},
title = {On the scalability of feature selection methods on high-dimensional data},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {56},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1140-3},
doi = {10.1007/s10115-017-1140-3},
abstract = {Lately, derived from the explosion of high dimensionality, researchers in machine learning became interested not only in accuracy, but also in scalability. Although scalability of learning methods is a trending issue, scalability of feature selection methods has not received the same amount of attention. This research analyzes the scalability of state-of-the-art feature selection methods, belonging to filter, embedded and wrapper approaches. For this purpose, several new measures are presented, based not only on accuracy but also on execution time and stability. The results on seven classical artificial datasets are presented and discussed, as well as two cases study analyzing the particularities of microarray data and the effect of redundancy. Trying to check whether the results can be generalized, we included some experiments with two real datasets. As expected, filters are the most scalable feature selection approach, being INTERACT, ReliefF and mRMR the most accurate methods.},
journal = {Knowl. Inf. Syst.},
month = aug,
pages = {395–442},
numpages = {48},
keywords = {Scalability, High-dimensionality, Feature selection, Big data}
}

@article{10.1007/s00034-020-01429-3,
author = {Alex, Starlet Ben and Mary, Leena and Babu, Ben P.},
title = {Attention and Feature Selection for Automatic Speech Emotion Recognition Using Utterance and Syllable-Level Prosodic Features},
year = {2020},
issue_date = {Nov 2020},
publisher = {Birkhauser Boston Inc.},
address = {USA},
volume = {39},
number = {11},
issn = {0278-081X},
url = {https://doi.org/10.1007/s00034-020-01429-3},
doi = {10.1007/s00034-020-01429-3},
abstract = {This work attempts to recognize emotions from human speech using prosodic information represented by variations in duration, energy, and fundamental frequency (F0) values. For this, the speech signal is first automatically segmented into syllables. Prosodic features at the utterance&nbsp;(15 features) and syllable level&nbsp;(10 features) are extracted using the syllable boundaries and trained separately using deep neural network&nbsp;classifiers. The effectiveness of the proposed approach is demonstrated on German speech corpus-EMOTional Sensitivity ASistance System&nbsp;(EmotAsS)&nbsp;for people with disabilities, the dataset used for the Interspeech 2018 Atypical Affect Sub-Challenge. The initial set of prosodic features on evaluation yields an unweighted average recall&nbsp;(UAR)&nbsp;of 30.15%. A fusion of the decision scores of these features with spectral features gives a UAR of 36.71%. This paper also employs methods like attention mechanism and feature selection using resampling-based recursive feature elimination&nbsp;(RFE)&nbsp;to enhance system performance. Implementing attention and feature selection followed by a score-level fusion improves the UAR to 36.83% and 40.96% for prosodic features and overall fusion, respectively. The fusion of the scores of the best individual system of the Atypical Affect Sub-Challenge and the proposed system provides a UAR&nbsp;(43.71%) above the best test result reported. The effectiveness of the proposed system has also been demonstrated on the Interactive Emotional Dyadic Motion Capture&nbsp;(IEMOCAP) database with a UAR of 63.83%.},
journal = {Circuits Syst. Signal Process.},
month = nov,
pages = {5681–5709},
numpages = {29},
keywords = {Score-level fusion, Feature selection, Attention mechanism, Syllabification, Prosodic features, Automatic emotion recognition&nbsp;(AER)}
}

@article{10.1016/j.patcog.2019.04.024,
author = {Yin, Qiyue and Zhang, Junge and Wu, Shu and Li, Hexi},
title = {Multi-view clustering via joint feature selection and partially constrained cluster label learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.04.024},
doi = {10.1016/j.patcog.2019.04.024},
journal = {Pattern Recogn.},
month = sep,
pages = {380–391},
numpages = {12},
keywords = {Multi-view clustering, Feature selection, Prior information, Cluster indicator}
}

@inproceedings{10.1007/978-3-030-04221-9_4,
author = {Yang, Hui and Zhu, Yingying and Huang, Qiang},
title = {A Multi-indicator Feature Selection for CNN-Driven Stock Index Prediction},
year = {2018},
isbn = {978-3-030-04220-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-04221-9_4},
doi = {10.1007/978-3-030-04221-9_4},
abstract = {Stock index prediction is regarded as a challenging task due to the phenomena of non-linearity and random drift in trends of stock indices. In practical applications, different indicator features have significant impact when predicting stock index. In addition, different technical indicators which contained in the same matrix will interfere with each other when convolutional neural network (CNN) is applied to feature extraction. To solve the above problem, this paper suggests a multi-indicator feature selection for stock index prediction based on a multi-channel CNN structure, named MI-CNN framework. In this method, candidate indicators are selected by maximal information coefficient feature selection (MICFS) approach, to ensure the correlation with stock movements while reduce redundancy between different indicators. Then an effective CNN structure without sub-sampling is designed to extract abstract features of each indicator, avoiding mutual interference between different indicators. Extensive experiments support that our proposed method performs well on different stock indices and achieves higher returns than the benchmark in trading simulations, providing good potential for further research in a wide range of financial time series prediction with deep learning based approaches.},
booktitle = {Neural Information Processing: 25th International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13–16, 2018, Proceedings, Part V},
pages = {35–46},
numpages = {12},
keywords = {Stock index prediction, Feature selection, Maximal information coefficient, Convolutional neural networks},
location = {Siem Reap, Cambodia}
}

@inproceedings{10.1007/978-3-030-58112-1_9,
author = {Fu, Guoxia and Sun, Chaoli and Tan, Ying and Zhang, Guochen and Jin, Yaochu},
title = {A Surrogate-Assisted Evolutionary Algorithm with Random Feature Selection for Large-Scale Expensive Problems},
year = {2020},
isbn = {978-3-030-58111-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58112-1_9},
doi = {10.1007/978-3-030-58112-1_9},
abstract = {When optimizing large-scale problems an evolutionary algorithm typically requires a substantial number of fitness evaluations to discover a good approximation to the global optimum. This is an issue when the problem is also computationally expensive. Surrogate-assisted evolutionary algorithms have shown better performance on high-dimensional problems which are no larger than 200 dimensions. However, it is very difficult to train sufficiently accurate surrogate models for a large-scale optimization problem due to the lack of training data. In this paper, a random feature selection technique is utilized to select decision variables from the original large-scale optimization problem to form a number of sub-problems, whose dimension may differ to each other, at each generation. The population employed to optimize the original large-scale optimization problem is updated by sequentially optimizing each sub-problem assisted by a surrogate constructed for this sub-problem. A new candidate solution of the original problem is generated by replacing the decision variables of the best solution found so far with those of the sub-problem that has achieved the best approximated fitness among all sub-problems. This new solution is then evaluated using the original expensive problem and used to update the best solution. In order to evaluate the performance of the proposed method, we conduct the experiments on 15 CEC’2013 benchmark problems and compare to some state-of-the-art algorithms. The experimental results show that the proposed method is more effective than the state-of-the-art algorithms, especially on problems that are partially separable or non-separable.},
booktitle = {Parallel Problem Solving from Nature – PPSN XVI: 16th International Conference, PPSN 2020, Leiden, The Netherlands, September 5-9, 2020, Proceedings, Part I},
pages = {125–139},
numpages = {15},
keywords = {Random feature selection, Expensive problems, Surrogate models, Large-scale optimization problems},
location = {Leiden, The Netherlands}
}

@inproceedings{10.1007/978-3-030-29911-8_14,
author = {Wang, Hengliang and Li, Yuan and Zhao, Chenfei and Mu, Kedian},
title = {An Approach with Low Redundancy to Network Feature Selection Based on Multiple Order Proximity},
year = {2019},
isbn = {978-3-030-29910-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29911-8_14},
doi = {10.1007/978-3-030-29911-8_14},
abstract = {Most models for unsupervised network feature selection use first-order proximity and reconstruction loss together as a guiding principle in the selection process. However, the first-order proximity is very sparse and insufficient in most cases. Moreover, redundant features, which can significantly hamper the performance of many machine learning algorithms, have seldom been taken into account. To address these issues, we propose an unsupervised network feature selection model called Multiple order proximity and feature Diversity guiding network Feature Selection model (MDFS), which uses multiple order proximity and feature diversity to guide the selection process. We use multi-order proximities based on the random walk model to capture linkage information between nodes. Moreover, we use an auto-encoder to capture the content information of nodes. As a last step, we design a redundancy loss to alleviate selecting highly-overlapping features. Experiment results on two real-world network datasets show the competitive ability of our model to select high-quality features among state-of-the-art models.},
booktitle = {PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26–30, 2019, Proceedings, Part II},
pages = {175–187},
numpages = {13},
keywords = {Feature diversity, Multiple order proximity, Feature selection, Network data},
location = {Cuvu, Yanuka Island, Fiji}
}

@article{10.1007/s00521-017-3092-7,
author = {El-Tarhouni, Wafa and Boubchir, Larbi and Elbendak, Mosa and Bouridane, Ahmed},
title = {Multispectral palmprint recognition using Pascal coefficients-based LBP and PHOG descriptors with random sampling},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-3092-7},
doi = {10.1007/s00521-017-3092-7},
abstract = {Local binary pattern (LBP) algorithm and its variants have been used extensively to analyse the local textural features of digital images with great success. Numerous extensions of LBP descriptors have been suggested, focusing on improving their robustness to noise and changes in image conditions. In our research, inspired by the concepts of LBP feature descriptors and a random sampling subspace, we propose an ensemble learning framework, using a variant of LBP constructed from Pascal's coefficients of n-order and referred to as a multiscale local binary pattern. To address the inherent overfitting problem of linear discriminant analysis, PCA was applied to the training samples. Random sampling was used to generate multiple feature subsets. In addition, in this work, we propose a new feature extraction technique that combines the pyramid histogram of oriented gradients and LBP, where the features are concatenated for use in the classification. Its performance in recognition was evaluated using the Hong Kong Polytechnic University database. Extensive experiments unmistakably show the superiority of the proposed approach compared to state-of-the-art techniques.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {593–603},
numpages = {11},
keywords = {Ensemble learning framework, Multiscale local binary patterns, Multispectral palmprint recognition, Pyramid histogram of oriented gradients}
}

@inproceedings{10.1145/2889160.2891036,
author = {Medeiros, Fl\'{a}vio},
title = {Safely evolving preprocessor-based configurable systems},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2891036},
doi = {10.1145/2889160.2891036},
abstract = {Since the 70s, the C preprocessor is still widely used in practice in a numbers of projects, including Apache, Linux, and Libssh, to tailor systems to different platforms. To better understand the C preprocessor challenges, we conducted 40 interviews and a survey among 202 developers. We found that developers deal with three common problems: configuration-related bugs, combinatorial testing, and code comprehension. To safely evolve preprocessor-based configurable systems, we proposed strategies to detect preprocessor-related bugs and bad smells, and a set of 16 refactorings to remove bad smells. To better deal with exponential configuration spaces, we compared 10 sampling algorithms with respect to effort (i.e., number of configurations to test) and bug-detection capabilities (i.e., number of bugs detected in the sampled configurations). Based on the results, we proposed a sampling algorithm with a useful balance between effort and bug-detection capability. By evaluating the proposed solution using 40 popular projects, we found 131 preprocessor-related bugs and more than 5K opportunities to apply the refactorings in practice.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {668–670},
numpages = {3},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.jvcir.2017.09.016,
author = {Hou, Xiaodan and Zhang, Tao and Ji, Lei and Wu, Yunda},
title = {Combating highly imbalanced steganalysis with small training samples using feature selection,},
year = {2017},
issue_date = {November 2017},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {49},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2017.09.016},
doi = {10.1016/j.jvcir.2017.09.016},
abstract = {Considering a particular highly imbalanced steganalysis with small training samples.Providing a systematic comparison of eight feature selection metrics.Evaluating the efficiency of each metric.Examining the performance of three types of methods and their combinations.Investigating the effect of several factors on the performance of feature selection. We consider a particular paradigm of steganalysis, namely, highly imbalanced steganalysis with small training samples, in which the cover images always significantly outnumber the stego ones. Researchers have rigorously studied sampling and learning algorithms as well as feature selection approaches to the class imbalance problem, but the research in the steganalysis domain is rare. This study provides a systematic comparison of eight feature selection metrics and of three types of methods developed for the imbalanced data classification problem in the steganalysis domain. Each metric is compared across three different classifiers and four steganalytic features. The efficiency of the metrics is evaluated to determine which performs best with minimal features selected. The performance of the three types of methods and their combinations is examined. Moreover, we also investigate the effect of feature dimensionality, sample number and imbalance degree on the performance of feature selection inresolving imbalanced image steganalysis.},
journal = {J. Vis. Comun. Image Represent.},
month = nov,
pages = {243–256},
numpages = {14},
keywords = {Class imbalance, Feature selection, Learning algorithms, Sampling, Steganalysis}
}

@article{10.1007/s11042-018-6155-6,
author = {Pathak, Yadunath and Arya, K. V. and Tiwari, Shailendra},
title = {Feature selection for image steganalysis using levy flight-based grey wolf optimization},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {2},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6155-6},
doi = {10.1007/s11042-018-6155-6},
abstract = {Image steganalysis is the process of detecting the availability of hidden messages in the cover images. Therefore, it may be considered as a classification problem which categorizes an image either into a cover images or a stego image. Feature selection is one of the important phases of image steganalysis which can increase its computational efficiency and performance. In this paper, a novel levy flight-based grey wolf optimization has been introduced which is used to select the prominent features for steganalysis algorithm from a set of original features. For the same, SPAM and AlexNet have been used to generate the high dimensional features. Furthermore, the random forest classifier is used to classify the images over selected features into cover images and stego images. The experimental results show that the proposed levy flight-based grey wolf optimization shows preferable convergence precision and effectively reduces the irrelevant and redundant features while maintaining the high classification accuracy as compared to other feature selection methods.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {1473–1494},
numpages = {22},
keywords = {Swarm intelligence, Image steganalysis, Grey wolf optimization, Feature selection}
}

@article{10.1016/j.eswa.2019.06.044,
author = {Zhang, Yong and Cheng, Shi and Shi, Yuhui and Gong, Dun-wei and Zhao, Xinchao},
title = {Cost-sensitive feature selection using two-archive multi-objective artificial bee colony algorithm},
year = {2019},
issue_date = {Dec 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.06.044},
doi = {10.1016/j.eswa.2019.06.044},
journal = {Expert Syst. Appl.},
month = dec,
pages = {46–58},
numpages = {13},
keywords = {Differential evolution, Particle swarm optimization, Multi-objective optimization, Artificial bee colony algorithm, Cost-sensitive feature selection}
}

@article{10.1007/s00500-017-2714-4,
author = {Yadav, Shweta and Ekbal, Asif and Saha, Sriparna},
title = {Feature selection for entity extraction from multiple biomedical corpora: A PSO-based approach},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {20},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-017-2714-4},
doi = {10.1007/s00500-017-2714-4},
abstract = {Entity extraction is an important step in biomedical text mining. Among many other challenges, there are two very crucial issues, viz. determining the most applicable feature set so that the model can be precise and less complex, and adapting the system across multiple benchmark corpora. In this paper, we propose a novel method for feature selection using the search capability of particle swarm optimization. The compact feature set used for training the classifier yields much better results when compared to the baseline model, which was developed with a complete set of features. A large number of features suitable for named entity recognition task from biomedical domain are also developed in the current paper. The complete set of features is implemented by studying the properties of datasets and from the domain knowledge. We have used conditional random field, a robust classifier as the underlying learning algorithm which has shown success in solving similar kinds of problems. Our experiments on multiple benchmark corpora yield the level of performance which are at par the state-of-the-art techniques.},
journal = {Soft Comput.},
month = oct,
pages = {6881–6904},
numpages = {24},
keywords = {Particle swarm optimization (PSO), Feature selection, Entity extraction, Condition random field}
}

@article{10.1504/ijcat.2020.112686,
author = {Chatterjee, Rajdeep and Chatterjee, Ankita},
title = {Orthogonal matching pursuit-based feature selection for motor-imagery EEG signal classification},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {64},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.112686},
doi = {10.1504/ijcat.2020.112686},
abstract = {This paper focuses on a framework that uses a small number of features to obtain high-quality classification accuracy of left/right-hand movement motor-imagery EEG signal. Motor-imagery EEG signal has been filtered, and suitable features are extracted using a temporal sliding window-based approach. These extracted features from overlapping and non-overlapping approaches are further compared based on three different types of feature extraction techniques: band power, wavelet energy entropy, and adaptive autoregressive model. The overlapping segments with wavelet energy entropy provide the best classification accuracy over other alternatives. The obtained classification accuracy is 91.43%, the highest ever reported accuracy for BCI Competition II data set III. Subsequently, Orthogonal Matching Pursuit (OMP) technique is used to select the subset of most discriminating features from the entire feature-set. It reduces the computation cost but still retains the quality of the classification results with only 1.43% information loss (that is, 90% classification accuracy), whereas the features-set size reduction is 75% for the same. It is found that the wavelet energy entropy technique performs consistently well in all the variants of our experiments and obtains a mean accuracy difference of 0.95% only.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {403–414},
numpages = {11},
keywords = {motor-imagery, orthogonal matching pursuit, ensemble learning, EEG, brain-computer interfaces}
}

@article{10.1016/j.patcog.2017.09.033,
author = {Xia, Xinghua and Song, Xiaoyu and Luan, Fangun and Zheng, Jungang and Chen, Zhili and Ma, Xiaofu},
title = {Discriminative feature selection for on-line signature verification},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.09.033},
doi = {10.1016/j.patcog.2017.09.033},
abstract = {On-line test signatures are aligned effectively to reference templates based on Gaussian mixture model before verification.Discriminative features are selected based on full factorial experiment design among consistent feature candidates.An alternative method of discriminative feature selection based on optimal orthogonal experiment design is presented to improve the efficiency.Features are not matched by DTW directly, but they are matched with the location constraints instead, which are inherent in two matching signature curves. On-line handwritten signatures are collected as real-time dynamical signals which are written on collective devices by users. Since individuals have different writing habits, consistent and discriminative features should be selected to distinguish genuine signatures from forged signatures. In this paper, two methods, which are based on full factorial experiment design and optimal orthogonal experiment design, are proposed for selecting discriminative features among candidates. To improve the robustness, consistency of feature is analyzed at first, and more consistent features are selected as candidates for discriminative feature selection. To reduce the influences of fluctuations caused by internal and external writing environments changes before verification, signatures are effectively aligned to their reference templates based on Gaussian mixture model. A modified dynamic time warping with signature curve constraint is presented for verification to improve the efficiency. Comprehensive experiments are implemented based on the data of the open access databases MCYT and SVC2004 Task2. Experimental results verify the effectiveness and robustness of our proposed methods.},
journal = {Pattern Recogn.},
month = feb,
pages = {422–433},
numpages = {12},
keywords = {Discriminative feature selection, Factorial experiment design, On-line signature verification, Orthogonal experiment design, Signature alignment, Signature curve constraint}
}

@article{10.1016/j.knosys.2017.09.006,
author = {Zhou, Peng and Hu, Xuegang and Li, Peipei and Wu, Xindong},
title = {Online feature selection for high-dimensional class-imbalanced data},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {136},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.09.006},
doi = {10.1016/j.knosys.2017.09.006},
abstract = {When tackling high dimensionality in data mining, online feature selection which deals with features flowing in one by one over time, presents more advantages than traditional feature selection methods. However, in real-world applications, such as fraud detection and medical diagnosis, the data is high-dimensional and highly class imbalanced, namely there are many more instances of some classes than others. In such cases of class imbalance, existing online feature selection algorithms usually ignore the small classes which can be important in these applications. It is hence a challenge to learn from high-dimensional and class imbalanced data in an online manner. Motivated by this, we first formalize the problem of online streaming feature selection for class imbalanced data, and then present an efficient online feature selection framework regarding the dependency between condition features and decision classes. Meanwhile, we propose a new algorithm of Online Feature Selection based on the Dependency in K nearest neighbors, called K-OFSD. In terms of Neighborhood Rough Set theory, K-OFSD uses the information of nearest neighbors to select relevant features which can get higher separability between the majority class and the minority class. Finally, experimental studies on seven high-dimensional and class imbalanced data sets show that our algorithm can achieve better performance than traditional feature selection methods with the same numbers of features and state-of-the-art online streaming feature selection algorithms in an online manner.},
journal = {Know.-Based Syst.},
month = nov,
pages = {187–199},
numpages = {13},
keywords = {Online feature selection, Neighborhood rough set, High dimensional, Class imbalance}
}

@inproceedings{10.1007/978-3-030-61401-0_50,
author = {Lucas, Thiago Jos\'{e} and Tojeiro, Carlos Alexandre Carvalho and Pires, Rafael Gon\c{c}alves and da Costa, Kelton Augusto Pontara and Papa, Jo\~{a}o Paulo},
title = {Machine Learning for Web Intrusion Detection: A Comparative Analysis of Feature Selection Methods mRMR and PFI},
year = {2020},
isbn = {978-3-030-61400-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61401-0_50},
doi = {10.1007/978-3-030-61401-0_50},
abstract = {Select from the best features in a complex dataset that is a critical task for machine learning algorithms. This work presents a comparative analysis between two resource selection techniques: Minimum Redundancy Maximum Relevance (mRMR) and Permutation Feature Important (PFI). The application of PFI to the dataset in issue is unusual. The dataset used in the experiments is HTTP CSIC 2010, which shows great results with the mRMR observed in a related work
[22]. Our PFI tests resulted in a selection of features best suited for machine learning methods and the best results for an accuracy of 97% with logistic regression and Bayes Point Machine, 98% with Support Vector Machine, and 99.9% using an artificial neural network.},
booktitle = {Artificial Intelligence and Soft Computing: 19th International Conference, ICAISC 2020, Zakopane, Poland, October 12-14, 2020, Proceedings, Part I},
pages = {535–546},
numpages = {12},
keywords = {Intrusion detection, Machine learning, Feature selection},
location = {Zakopane, Poland}
}

@inproceedings{10.1007/978-3-030-40124-5_8,
author = {Shi, Zhenwei and Zhang, Chong and Compter, Inge and Verduin, Maikel and Hoeben, Ann and Eekers, Danielle and Dekker, Andre and Wee, Leonard},
title = {A Feature-Pooling and Signature-Pooling Method for Feature Selection for Quantitative Image Analysis: Application to a Radiomics Model for Survival in Glioma},
year = {2019},
isbn = {978-3-030-40123-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-40124-5_8},
doi = {10.1007/978-3-030-40124-5_8},
abstract = {We proposed a pooling-based radiomics feature selection method and showed how it would be applied to the clinical question of predicting one-year survival in 130 patients treated for glioma by radiotherapy. The method combines filter, wrapper and embedded selection in a comprehensive process to identify useful features and build them into a potentially predictive signature. The results showed that non-invasive CT radiomics were able to moderately predict overall survival and predict WHO tumour grade. This study reveals an associative inter-relationship between WHO tumour grade, CT-based radiomics and survival, that could be clinically relevant.},
booktitle = {Radiomics and Radiogenomics in Neuro-Oncology: First International Workshop, RNO-AI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {70–80},
numpages = {11},
keywords = {Glioma, Feature selection, Quantitative imaging feature},
location = {Shenzhen, China}
}

@article{10.1016/j.ins.2017.09.028,
author = {Hancer, Emrah and Xue, Bing and Zhang, Mengjie and Karaboga, Dervis and Akay, Bahriye},
title = {Pareto front feature selection based on artificial bee colony optimization},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {422},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2017.09.028},
doi = {10.1016/j.ins.2017.09.028},
abstract = {Feature selection has two major conflicting aims, i.e., to maximize the classification performance and to minimize the number of selected features to overcome the curse of dimensionality. To balance their trade-off, feature selection can be handled as a multi-objective problem. In this paper, a feature selection approach is proposed based on a new multi-objective artificial bee colony algorithm integrated with non-dominated sorting procedure and genetic operators. Two different implementations of the proposed approach are developed: ABC with binary representation and ABC with continuous representation. Their performance are examined on 12 benchmark datasets and the results are compared with those of linear forward selection, greedy stepwise backward selection, two single objective ABC algorithms and three well-known multi-objective evolutionary computation algorithms. The results show that the proposed approach with the binary representation outperformed the other methods in terms of both the dimensionality reduction and the classification accuracy.},
journal = {Inf. Sci.},
month = jan,
pages = {462–479},
numpages = {18},
keywords = {Artificial bee colony, Classification, Feature selection, Multi-objective optimization}
}

@inproceedings{10.1145/3185089.3185131,
author = {Myo, Win Win and Wettayaprasit, Wiphada and Aiyarak, Pattara},
title = {A Noble Feature Selection Method for Human Activity Recognition using Linearly Dependent Concept (LDC)},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185131},
doi = {10.1145/3185089.3185131},
abstract = {Human physical activity recognition process using mobile phones is very complicated with many extracted features in which some features are irrelevant or redundant. Removing irrelevant or redundant features is not only reducing the dataset size but also saving the time consuming task. Hence, a reason to pick out the effective and useful features is our main study. We propose a noble feature selection technique using Linearly Dependent Concept (LDC). Our proposed work attempts a new feature selection method on UCI-HAR dataset. For classification, we use the feed forward neural network and compare the performance result with the original dataset. The goal of our study is not only to find an effective and useful features set from the original dataset but also to be better performance than original dataset. Finally, the experimental result of proposed method gives 2.7% more accuracy and reduces the relative error up to 2.67% of the original dataset.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {173–177},
numpages = {5},
keywords = {Sensor, Neural Network, Mobile Phone, Linearly Dependent, Human Activity Recognition, Feature Selection},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@article{10.1016/j.eswa.2019.113133,
author = {Cura, Tunchan},
title = {Use of support vector machines with a parallel local search algorithm for data classification and feature selection},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113133},
doi = {10.1016/j.eswa.2019.113133},
journal = {Expert Syst. Appl.},
month = may,
numpages = {17},
keywords = {Support vector machines, Feature selection, Classification, Heuristic, Machine learning}
}

@article{10.1007/s00521-016-2214-y,
author = {Zhu, Bing and Niu, Yongge and Xiao, Jin and Baesens, Bart},
title = {A new transferred feature selection algorithm for customer identification},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-016-2214-y},
doi = {10.1007/s00521-016-2214-y},
abstract = {Class imbalance brings great challenges to feature selection in customer identification, and most of the current feature selection approaches cannot produce good prediction on the minority class. A number of studies have attempted to solve this issue by using resampling techniques. However, resampling techniques only use the in-domain information and they cannot achieve good performance when the imbalance is caused by the absolute rarity of the minority class. In this paper, we focus on the issue of feature selection with class imbalance caused by absolute rarity. By introducing the idea of transfer learning, we develop a transferred feature selection method based on the group method of data handling neural networks. The proposed ensemble neural network extracts information of similar customers from related domains to deal with the information scarcity of the minority class in the target domain. Experiments are done on a real-world application using data from a cigarette company. The results indicate that the new method gives better predictive performance than other benchmark feature selection methods, especially in terms of the predictive accuracy of the minority high-value customers. At the same time, the new algorithm can help to identify important features that distinguish high-value customers from low-value ones.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {2593–2603},
numpages = {11},
keywords = {Transfer learning, Group method of data handling, Feature selection, Customer identification, Class imbalance, Absolute rarity}
}

@article{10.1016/j.jss.2019.03.012,
author = {Ni, Chao and Chen, Xiang and Wu, Fangfang and Shen, Yuxiang and Gu, Qing},
title = {An empirical study on pareto based multi-objective feature selection for software defect prediction},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.012},
doi = {10.1016/j.jss.2019.03.012},
journal = {J. Syst. Softw.},
month = jun,
pages = {215–238},
numpages = {24},
keywords = {xx-xx, xx-xx, Software defect prediction, Search based software engineering, Feature selection, Multi-Objective optimization, Empirical study}
}

@article{10.3103/S0146411619060051,
author = {Yingying Feng and Zhao, Shasha and Liu, Hui},
title = {Target Tracking Based on Multi Feature Selection Fusion Compensation in Monitoring Video},
year = {2019},
issue_date = {Nov 2019},
publisher = {Allerton Press, Inc.},
address = {USA},
volume = {53},
number = {6},
issn = {0146-4116},
url = {https://doi.org/10.3103/S0146411619060051},
doi = {10.3103/S0146411619060051},
journal = {Autom. Control Comput. Sci.},
month = nov,
pages = {522–531},
numpages = {10},
keywords = {image segmentation, moving background, target tracking, video monitoring, feature selection}
}

@article{10.1007/s11042-020-09638-3,
author = {Malakar, Samir and Ghosh, Manosij and Chaterjee, Agneet and Bhowmik, Showmik and Sarkar, Ram},
title = {Offline music symbol recognition using Daisy feature and quantum Grey wolf optimization based feature selection},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09638-3},
doi = {10.1007/s11042-020-09638-3},
abstract = {Handwritten music symbol recognition is considered by the research fraternity as a critical research problem. It becomes more critical when the symbols are collected from handwritten music sheets in offline mode. Most of the research findings, available in the literature, have tried to recognize the said symbols using various shape based features. But this approach limits system performance when we dealt with lookalike symbols such as half note, eight note and quarter note. To encounter this, in the present work we have used a texture based feature descriptor, called Daisy, for the said purpose. Though Daisy descriptor yields reasonably good recognition accuracy, but it generates a high dimensional feature vector. Hence, in this work, Quantum concept inspired Grey Wolf Optimization, named as QGWO, has been applied to select optimal feature subset from this high dimensional feature vector. We have applied the proposed method on six different standard music symbol datasets that include HOMUS, Capitan_score_uniform, Capitan_score_non-uniform, Forn\'{e}s, Rebelo_real and Rebelo_synthetic datasets. On these datasets we have achieved recognition accuracies 93.07%, 99.22%, 99.20%, 99.49% and 100.00% respectively with 39.63%, 49.75%, 42.50%, 67.62%, 54.37% and 71.25% of actual feature dimension (i.e., 800) respectively. Additionally, we have compared our results with some state-of-the-art methods along with two recent deep learning based models, and it has been found that the present approach outperforms those.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {32011–32036},
numpages = {26},
keywords = {Feature selection, Quantum Grey wolf optimization, Daisy descriptor, Music symbol recognition}
}

@inproceedings{10.1109/ICSE.2017.64,
author = {Souto, Sabrina and d'Amorim, Marcelo and Gheyi, Rohit},
title = {Balancing soundness and efficiency for practical testing of configurable systems},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.64},
doi = {10.1109/ICSE.2017.64},
abstract = {Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system - GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {632–642},
numpages = {11},
keywords = {configuration, sampling, testing},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3316615.3318223,
author = {Ndirangu, Dalton and Mwangi, Waweru and Nderu, Lawrence},
title = {An Ensemble Filter Feature Selection Method and Outlier Detection Method for Multiclass Classification},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3318223},
doi = {10.1145/3316615.3318223},
abstract = {Feature selection methods facilitate removal of irrelevant attributes. Ineffective features may contain outliers that degrade performance of classifiers. We propose an ensemble filter base feature selection technique for multiclass classification. The technique combines results of four selection methods to create an ensemble list. The study uses a red wine dataset drawn from UC Irvine machine learning data repository and WEKA, a collection of machine learning algorithms for data mining tasks. The multiclass red wine dataset is binarized using WekaMulticlassClassifier utilizing the 1against 1 with pairwise coupling decomposing scheme. Using random forest algorithm and root mean square error values, a learning curve is generated that establishes an optimal ensemble sub-list. Outliers are detected using the Tukey statistical method. The proposed ensemble method outperformed the single feature methods. The study concludes by showing that unnecessary features and presence of outliers degrades classifiers performance. We recommend further studies on the effect of gradual selective removal of outliers on classification.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {373–379},
numpages = {7},
keywords = {Classifiers, Ensemble, Features, Outliers, Performance},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@article{10.1016/j.neucom.2016.03.101,
author = {Emary, E. and Zawbaa, Hossam M. and Hassanien, Aboul Ella},
title = {Binary ant lion approaches for feature selection},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.03.101},
doi = {10.1016/j.neucom.2016.03.101},
abstract = {In this paper, binary variants of the ant lion optimizer (ALO) are proposed and used to select the optimal feature subset for classification purposes in wrapper-mode. ALO is one of the recently bio-inspired optimization techniques that imitates the hunting process of ant lions. Moreover, ALO balances exploration and exploitation using a single operator that can adaptively searches the domain of solutions for the optimal solution. Binary variants introduced here are performed using two different approaches. The first approach takes only the inspiration of ALO operators and makes the corresponding binary operators. In the second approach, the native ALO is applied while its continuous steps are threshold using suitable threshold function after squashing them. The proposed approaches for binary ant lion optimizer (BALO) are utilized in the feature selection domain for finding feature subset that maximizing the classification performance while minimizing the number of selected features. The proposed binary algorithms were compared to three common optimization algorithms hired in this domain namely particle swarm optimizer (PSO), genetic algorithms (GAs), binary bat algorithm (BBA), as well as the native ALO. A set of assessment indicators is used to evaluate and compare the different methods over 21 data sets from the UCI repository. Results prove the capability of the proposed binary algorithms to search the feature space for optimal feature combinations regardless of the initialization and the used stochastic operators.},
journal = {Neurocomput.},
month = nov,
pages = {54–65},
numpages = {12},
keywords = {Ant lion optimizer, Binary ant lion optimizer, Bio-inspired optimization, Feature selection}
}

@inproceedings{10.1007/978-3-030-31624-2_8,
author = {Xu, Jin and Zhang, Chengzhi and Ma, Shutian},
title = {Ensemble System for Identification of Cited Text Spans: Based on Two Steps of Feature Selection},
year = {2019},
isbn = {978-3-030-31623-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31624-2_8},
doi = {10.1007/978-3-030-31624-2_8},
abstract = {CL-SciSumm Shared Task proposed a novel approach which is to generate scientific summary based on cited text spans (CTS) in target paper. This mechanism requires identifying CTS from reference paper according to citation sentence (citance) firstly. Therefore, CTS identification has then arisen the attention of many scholars since identified sentences will finally be aggregated for summary generation. Prior studies viewed this task as a text classification problem and feature selection is one key step for modeling the linkage between CTS and citance. Since most studies have paved the work by building features arbitrarily and applying them directly to model training. There is a lack of investigation to evaluate the effectiveness of features. Performance variation caused by different classifiers are barely taken into consideration as well. To further improve the performance of CTS identification, this paper builds an ensemble system based on two steps of feature selection. In the first step, we construct a set of features and do correlation analysis to select those which are higher-correlated with CTS. The second step is responsible for assigning several basic classifiers (SVM, Decision Tree and Logistic Regression) with their best performing feature sets. Experimental results demonstrate that our proposed systems can surpass the previous best performing one.},
booktitle = {Information Retrieval: 25th China Conference, CCIR 2019, Fuzhou, China, September 20–22, 2019, Proceedings},
pages = {95–107},
numpages = {13},
keywords = {Cited text spans, Feature selection, Negative sampling, Text classification, Ensemble system},
location = {Fuzhou, China}
}

@inproceedings{10.1145/3386052.3386072,
author = {Luo, Peiqi and Kang, Guixia and Xu, Xin},
title = {A Novel Feature Selection and Classification Method of Alzheimer's Disease based on Multi-features in MRI},
year = {2020},
isbn = {9781450376761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386052.3386072},
doi = {10.1145/3386052.3386072},
abstract = {In this paper, we describe a novel machine learning method for classifying Alzheimer's disease (AD), Mild cognitive impairment (MCI) and Normal Control (NC) subjects based on structural MRI. We first extracted features from MRI scans, including cortical volumes, cortical thicknesses, subcortical volumes, and hippocampal subfields volumes. Then a new feature selection method combining the support vector machine-recursive feature elimination (SVM-RFE), maximal-relevance-minimal-redundancy (mRMR) and random forest (RF) was proposed to select the optimal subsets among all these features. Finally, the SVM classifier was used for AD/MCI/NC classification by 10-fold cross-validation. We applied the proposed method to the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, and the experimental results show a high degree of accuracy, sensitivity and specificity, which are superior to some other state-of-the-art approaches.},
booktitle = {Proceedings of the 2020 10th International Conference on Bioscience, Biochemistry and Bioinformatics},
pages = {114–119},
numpages = {6},
keywords = {Alzheimer's disease, Classification, Diagnosis, Feature selection, Mild cognitive impairment, Structural MRI},
location = {Kyoto, Japan},
series = {ICBBB '20}
}

@article{10.1007/s10462-015-9428-8,
author = {Diao, Ren and Shen, Qiang},
title = {Nature inspired feature selection meta-heuristics},
year = {2015},
issue_date = {October   2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {44},
number = {3},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-015-9428-8},
doi = {10.1007/s10462-015-9428-8},
abstract = {Many strategies have been exploited for the task of feature selection, in an effort to identify more compact and better quality feature subsets. A number of evaluation metrics have been developed recently that can judge the quality of a given feature subset as a whole, rather than assessing the qualities of individual features. Effective techniques of stochastic nature have also emerged, allowing good quality solutions to be discovered without resorting to exhaustive search. This paper provides a comprehensive review of the most recent methods for feature selection that originated from nature inspired meta-heuristics, where the more classic approaches such as genetic algorithms and ant colony optimisation are also included for comparison. A good number of the reviewed methodologies have been significantly modified in the present, in order to systematically support generic subset-based evaluators and higher dimensional problems. Such modifications are carried out because the original studies either work exclusively with certain subset evaluators (e.g., rough set-based methods), or are limited to specific problem domains. A total of ten different algorithms are examined, and their mechanisms and work flows are summarised in an unified manner. The performance of the reviewed approaches are compared using high dimensional, real-valued benchmark data sets. The selected feature subsets are also used to build classification models, in an effort to further validate their efficacies.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {311–340},
numpages = {30},
keywords = {Dimensionality reduction, Feature selection, Nature inspired optimisation, Stochastic search}
}

@article{10.1016/j.ins.2015.07.041,
author = {Garc\'{\i}a-Torres, Miguel and G\'{o}mez-Vela, Francisco and Meli\'{a}n-Batista, Bel\'{e}n and Moreno-Vega, J. Marcos},
title = {High-dimensional feature selection via feature grouping},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {326},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.07.041},
doi = {10.1016/j.ins.2015.07.041},
abstract = {We introduce the concept of predominant group based on the idea of Markov blanket to identify groups of correlated features.We propose a greedy strategy (GreedyPGG) that groups features based on the concept of predominant groups.We propose a VNS metaheuristic that uses the GreedyPGG strategy to reduce the dimensionality in high-dimensional data.Results show that VNS finds smaller subsets of features without degrading the predictive model. In recent years, advances in technology have led to increasingly high-dimensional datasets. This increase of dimensionality along with the presence of irrelevant and redundant features make the feature selection process challenging with respect to efficiency and effectiveness. In this context, approximate algorithms are typically applied since they provide good solutions in a reasonable time. On the other hand, feature grouping has arisen as a powerful approach to reduce dimensionality in high-dimensional data. Recently, some authors have focused their attention on developing methods that combine feature grouping and feature selection to improve the model. In this paper, we propose a feature selection strategy that utilizes feature grouping to increase the effectiveness of the search. As feature selection strategy, we propose a Variable Neighborhood Search (VNS) metaheuristic. Then, we propose to group the input space into subsets of features by using the concept of Markov blankets. To the best of our knowledge, this is the first time in which the Markov blanket is used for grouping features. We test the performance of VNS by conducting experiments on several high-dimensional datasets from two different domains: microarray and text mining. We compare VNS with popular and competitive techniques. Results show that VNS is a competitive strategy capable of finding a small size of features with similar predictive power than that obtained with other algorithms used in this study.},
journal = {Inf. Sci.},
month = jan,
pages = {102–118},
numpages = {17},
keywords = {Feature grouping, Feature selection, High dimensionality, Metaheuristic}
}

@article{10.1155/2020/8875404,
author = {Farahani, Gholamreza and Mostarda, Leonardo},
title = {Feature Selection Based on Cross-Correlation for the Intrusion Detection System},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/8875404},
doi = {10.1155/2020/8875404},
abstract = {One of the important issues in the computer networks is security. Therefore, trusted communication of information in computer networks is a critical point. To have a safe communication, it is necessary that, in addition to the prevention mechanisms, intrusion detection systems (IDSs) are used. There are various approaches to utilize intrusion detection, but any of these systems is not complete. In this paper, a new cross-correlation-based feature selection (CCFS) method is proposed and compared with the cuttlefish algorithm (CFA) and mutual information-based feature selection (MIFS) features with use of four different classifiers: support vector machine (SVM), naive Bayes (NB), decision tree (DT), and K-nearest neighbor (KNN). The experimental results on the KDD Cup 99, NSL-KDD, AWID, and CIC-IDS2017 datasets show that the proposed method has a better performance in accuracy, precision, recall, and F1-score criteria in comparison with the other two methods in different classifiers. Also, the results on different classifiers show that the usage of the DT classifier for the proposed method is the best.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {17}
}

@article{10.1016/j.neucom.2015.06.083,
author = {Emary, E. and Zawbaa, Hossam M. and Hassanien, Aboul Ella},
title = {Binary grey wolf optimization approaches for feature selection},
year = {2016},
issue_date = {Jan 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {172},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.06.083},
doi = {10.1016/j.neucom.2015.06.083},
journal = {Neurocomput.},
month = jan,
pages = {371–381},
numpages = {11},
keywords = {Grey wolf optimization, Binary grey wolf optimization, Feature selection, Bio-inspired optimization, Evolutionary computation}
}

@article{10.5555/2627435.2638579,
author = {Tan, Mingkui and Tsang, Ivor W. and Wang, Li},
title = {Towards ultrahigh dimensional feature selection for big data},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {In this paper, we present a new adaptive feature scaling scheme for ultrahigh-dimensional feature selection on Big Data, and then reformulate it as a convex semi-infinite programming (SIP) problem. To address the SIP, we propose an efficient feature generating paradigm. Different from traditional gradient-based approaches that conduct optimization on all input features, the proposed paradigm iteratively activates a group of features, and solves a sequence of multiple kernel learning (MKL) subproblems. To further speed up the training, we propose to solve the MKL subproblems in their primal forms through a modified accelerated proximal gradient approach. Due to such optimization scheme, some efficient cache techniques are also developed. The feature generating paradigm is guaranteed to converge globally under mild conditions, and can achieve lower feature selection bias. Moreover, the proposed method can tackle two challenging tasks in feature selection: 1) group-based feature selection with complex structures, and 2) nonlinear feature selection with explicit feature mappings. Comprehensive experiments on a wide range of synthetic and real-world data sets of tens of million data points with O(1014) features demonstrate the competitive performance of the proposed method over state-of-the-art feature selection methods in terms of generalization performance and training effciency.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1371–1429},
numpages = {59},
keywords = {big data, feature generation, feature selection, multiple kernel learning, nonlinear feature selection, ultrahigh dimensionality}
}

@article{10.1007/s11042-019-7354-5,
author = {Li, Jinyan and Fong, Simon and Liu, Lian-sheng and Dey, Nilanjan and Ashour, Amira S. and Moraru, Luminiundefineda},
title = {Dual feature selection and rebalancing strategy using metaheuristic optimization algorithms in X-ray image datasets},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7354-5},
doi = {10.1007/s11042-019-7354-5},
abstract = {The imbalance and multi-dimension are two common problems in the medical image datasets, which affect the performances of the image processing procedures. The traditional methods to solve these two problems are notoriously difficult. Accordingly, this work employed metaheuristic methods to optimize the rebalancing process of the imbalanced class distribution for further use in the feature selection procedure for dimensionality reduction for the medical X-ray image datasets. Different metaheuristic algorithms were used to maximize the parameter values of the rebalancing and feature selection phases to preprocess the datasets. The proposed work devised a multi-objective optimization strategy in the process of the metaheuristic algorithms search to solve the problem of dual imbalanced dataset and feature selection. Afterward, a comparative study of the proposed optimized approach with the conventional methods was conducted to evaluate the proposed method performance. The results established the superiority of the proposed method to overcome the imbalanced and multi-dimensional problem. The proposed method generated a reasonable number of minority class samples and selected a sensible subset of features to ultimately obtain a very extraordinary accuracy with great credibility from a negative value of kappa and a false high accuracy. It produced higher credibility and correctness classification performance in the practical problem of medical X-ray images compared to other algorithms. Feature selection with Random-SMOTE (RSMOTE) using the self-adaptive Bat algorithm is superior to the optimization using particle swarm optimization. The proposed method using the Bat algorithm achieved 94.6% classification accuracy with 0.883 Kappa value using the lung X-ray first dataset.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {20913–20933},
numpages = {21},
keywords = {Meta-heuristic, Medical X-ray image, Rebalancing, Feature selection, Dynamic multi-objective, Bat algorithm}
}

@article{10.1007/s10489-016-0852-5,
author = {Maldonado, Sebasti\'{a}n and Montoya, Ricardo and L\'{o}pez, Julio},
title = {Embedded heterogeneous feature selection for conjoint analysis: A SVM approach using L1 penalty},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {46},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-016-0852-5},
doi = {10.1007/s10489-016-0852-5},
abstract = {This paper presents a novel embedded feature selection approach for Support Vector Machines (SVM) in a choice-based conjoint context. We extend the L1-SVM formulation and adapt the RFE-SVM algorithm to conjoint analysis to encourage sparsity in consumer preferences. This sparsity can be attributed to consumers being selective about the attributes they consider when evaluating alternatives in choice tasks. Given limited individual data in choice-based conjoint, we control for heterogeneity by pooling information across consumers and shrinking the individual weights of the relevant attributes towards a population mean. We tested our approach through an extensive simulation study that shows that the proposed approach can capture the sparseness implied by irrelevant attributes. We also illustrate the characteristics and use of our approach on two real-world choice-based conjoint data sets. The results show that the proposed method has better predictive accuracy than competitive approaches, and that it provides additional information at an individual level. Implications for product design decisions are discussed.},
journal = {Applied Intelligence},
month = jun,
pages = {775–787},
numpages = {13},
keywords = {Conjoint analysis, Feature selection, L1 norm, Support vector machines}
}

@article{10.5555/3288339.3288348,
author = {Shafiq, Muhammad and Yu, Xiangzhan and Bashir, Ali Kashif and Chaudhry, Hassan Nazeer and Wang, Dawei},
title = {A machine learning approach for feature selection traffic classification using security analysis},
year = {2018},
issue_date = {Oct 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {10},
issn = {0920-8542},
abstract = {Class imbalance has become a big problem that leads to inaccurate traffic classification. Accurate traffic classification of traffic flows helps us in security monitoring, IP management, intrusion detection, etc. To address the traffic classification problem, in literature, machine learning (ML) approaches are widely used. Therefore, in this paper, we also proposed an ML-based hybrid feature selection algorithm named WMI_AUC that make use of two metrics: weighted mutual information (WMI) metric and area under ROC curve (AUC). These metrics select effective features from a traffic flow. However, in order to select robust features from the selected features, we proposed robust features selection algorithm. The proposed approach increases the accuracy of ML classifiers and helps in detecting malicious traffic. We evaluate our work using 11 well-known ML classifiers on the different network environment traces datasets. Experimental results showed that our algorithms achieve more than 95% flow accuracy results.},
journal = {J. Supercomput.},
month = oct,
pages = {4867–4892},
numpages = {26},
keywords = {Class imbalance, Feature selection, Machine learning, Network traffic classification, Security}
}

@inproceedings{10.1145/3106426.3106440,
author = {Alharbi, Abdullah Semran and Li, Yuefeng and Xu, Yue},
title = {Topical term weighting based on extended random sets for relevance feature selection},
year = {2017},
isbn = {9781450349512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106426.3106440},
doi = {10.1145/3106426.3106440},
abstract = {It is challenging to discover relevant features from long documents that describe user information needs due to the nature of text where synonymy, polysemy noise, and high dimensionality are inherited problems. Traditional feature selection methods could not effectively deal with these problems, because they assume that documents describe one topic only. Topic-based techniques, such as Latent Dirichlet Allocation (LDA), relax this assumption. They have been developed on the basis that a document can exhibit multiple hidden topics. However, LDA does not show encouraging results in selecting relevant features, because LDA calculates the weight of terms based on their local documents and does not generalise it globally at the collection level. So as to address this problem, we propose an innovative and effective extended random set model to generalise LDA weight for local document terms. The model is used as a weighting scheme for topical terms. It can assign a more discriminately accurate weight to these terms based on their appearance in LDA topics and relevant documents. The experimental results, based on the standard RCV1 dataset, TREC topics, and five standard performance measures, show that the proposed model significantly outperforms eight state-of-the-art baseline models in information filtering.},
booktitle = {Proceedings of the International Conference on Web Intelligence},
pages = {654–661},
numpages = {8},
keywords = {extended random set, feature selection, latent dirichlet allocation, term weighting, text mining},
location = {Leipzig, Germany},
series = {WI '17}
}

@article{10.1007/s10115-017-1121-6,
author = {Pecli, Antonio and Cavalcanti, Maria Claudia and Goldschmidt, Ronaldo},
title = {Automatic feature selection for supervised learning in link prediction applications: a comparative study},
year = {2018},
issue_date = {July      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {56},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1121-6},
doi = {10.1007/s10115-017-1121-6},
abstract = {For the last years, a considerable amount of attention has been devoted to the research about the link prediction (LP) problem in complex networks. This problem tries to predict the likelihood of an association between two not interconnected nodes in a network to appear in the future. One of the most important approaches to the LP problem is based on supervised machine learning (ML) techniques for classification. Although many works have presented promising results with this approach, choosing the set of features (variables) to train the classifiers is still a major challenge. In this article, we report on the effects of three different automatic variable selection strategies (Forward, Backward and Evolutionary) applied to the feature-based supervised learning approach in LP applications. The results of the experiments show that the use of these strategies does lead to better classification models than classifiers built with the complete set of variables. Such experiments were performed over three datasets (Microsoft Academic Network, Amazon and Flickr) that contained more than twenty different features each, including topological and domain-specific ones. We also describe the specification and implementation of the process used to support the experiments. It combines the use of the feature selection strategies, six different classification algorithms (SVM, K-NN, na\"{\i}ve Bayes, CART, random forest and multilayer perceptron) and three evaluation metrics (Precision, F-Measure and Area Under the Curve). Moreover, this process includes a novel ML voting committee inspired approach that suggests sets of features to represent data in LP applications. It mines the log of the experiments in order to identify sets of features frequently selected to produce classification models with high performance. The experiments showed interesting correlations between frequently selected features and datasets.},
journal = {Knowl. Inf. Syst.},
month = jul,
pages = {85–121},
numpages = {37},
keywords = {Binary classification, Complex network analysis, Feature selection, Link prediction}
}

@article{10.1007/s10586-016-0635-0,
author = {Zhao, Long and Jiang, Linfeng and Dong, Xiangjun},
title = {Supervised feature selection method via potential value estimation},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-016-0635-0},
doi = {10.1007/s10586-016-0635-0},
abstract = {Feature selection is an important step dealing with high dimensional data. In order to select categories related features, the importance of feature need to be measured. The existing importance measure algorithms can't reflect different distributions of data space and have poor interpretabilities. In this paper, a new feature weight calculation method via potential value estimation is proposed. The potential values indicate different data distributions in different dimensions. The quality of data points is another parameter needed to calculate the potential value of the data points in data field. The quality of the data points is related to the density and the type of the surrounding points. At the same time, the extraction of important features should not only consider the distribution of the feature itself but also consider the correlation with other features or categories. This method adopts the $$S_{w}$$Sw (potential value within class) and $$S_{b} $$Sb(potential value between different classes) to calculate the information entropy of each feature. The representative features have been selected to structure classifier. In order to accelerate the speed of operation, different grids are divided with different dimensions. By estimating the potential value of different data points on the same dimension, the correlation between feature and label is evaluated. After a series of analysis and experiments, the proposed method has been proved has overall classification accuracy with the fewest features. The effect of dimensionality reduction is significantly higher than FRGDF and the other manual information methods.},
journal = {Cluster Computing},
month = dec,
pages = {2039–2049},
numpages = {11},
keywords = {Divided grids, Feature selection, Importance measure, Potential value}
}

@article{10.1016/j.compbiomed.2021.104862,
author = {Maurya, Ritesh and Pathak, Vinay Kumar and Burget, Radim and Dutta, Malay Kishore},
title = {Automated detection of bioimages using novel deep feature fusion algorithm and effective high-dimensional feature selection approach},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104862},
doi = {10.1016/j.compbiomed.2021.104862},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {20},
keywords = {Convolutional neural networks, Bioimage classification, Transfer learning, Evolutionary algorithms, Feature fusion, Pre-trained CNNs}
}

@inproceedings{10.5555/3305381.3305565,
author = {Kale, Satyen and Karnin, Zohar and Liang, Tengyuan and P\'{a}l, D\'{a}vid},
title = {Adaptive feature selection: computationally efficient online sparse linear regression under RIP},
year = {2017},
publisher = {JMLR.org},
abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1780–1788},
numpages = {9},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{10.1016/j.jbi.2014.11.013,
author = {Kamkar, Iman and Gupta, Sunil Kumar and Phung, Dinh and Venkatesh, Svetha},
title = {Stable feature selection for clinical prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {53},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2014.11.013},
doi = {10.1016/j.jbi.2014.11.013},
abstract = {Display Omitted We model new application of Tree-Lasso for stable feature selection in healthcare.Tree-Lasso finds more stable features compared to other feature selection methods.Tree-Lasso results in better prediction accuracy compared to other methods.The features selected by Tree-Lasso are consistent with those used by clinicians. Modern healthcare is getting reshaped by growing Electronic Medical Records (EMR). Recently, these records have been shown of great value towards building clinical prediction models. In EMR data, patients' diseases and hospital interventions are captured through a set of diagnoses and procedures codes. These codes are usually represented in a tree form (e.g. ICD-10 tree) and the codes within a tree branch may be highly correlated. These codes can be used as features to build a prediction model and an appropriate feature selection can inform a clinician about important risk factors for a disease. Traditional feature selection methods (e.g. Information Gain, T-test, etc.) consider each variable independently and usually end up having a long feature list. Recently, Lasso and related l 1 -penalty based feature selection methods have become popular due to their joint feature selection property. However, Lasso is known to have problems of selecting one feature of many correlated features randomly. This hinders the clinicians to arrive at a stable feature set, which is crucial for clinical decision making process. In this paper, we solve this problem by using a recently proposed Tree-Lasso model. Since, the stability behavior of Tree-Lasso is not well understood, we study the stability behavior of Tree-Lasso and compare it with other feature selection methods. Using a synthetic and two real-world datasets (Cancer and Acute Myocardial Infarction), we show that Tree-Lasso based feature selection is significantly more stable than Lasso and comparable to other methods e.g. Information Gain, ReliefF and T-test. We further show that, using different types of classifiers such as logistic regression, naive Bayes, support vector machines, decision trees and Random Forest, the classification performance of Tree-Lasso is comparable to Lasso and better than other methods. Our result has implications in identifying stable risk factors for many healthcare problems and therefore can potentially assist clinical decision making for accurate medical prognosis.},
journal = {J. of Biomedical Informatics},
month = feb,
pages = {277–290},
numpages = {14},
keywords = {Classification, Feature selection, Feature stability, Lasso, Tree-Lasso}
}

@article{10.1016/j.neucom.2016.12.045,
author = {Jimnez, F. and Snchez, G. and Garca, J.M. and Sciavicco, G. and Miralles, L.},
title = {Multi-objective evolutionary feature selection for online sales forecasting},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {234},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.12.045},
doi = {10.1016/j.neucom.2016.12.045},
abstract = {Sales forecasting uses historical sales figures, in association with products characteristics and peculiarities, to predict short-term or long-term future performance in a business, and it can be used to derive sound financial and business plans. By using publicly available data, we build an accurate regression model for online sales forecasting obtained via a novel feature selection methodology composed by the application of the multi-objective evolutionary algorithm ENORA (Evolutionary NOn-dominated Radial slots based Algorithm) as search strategy in a wrapper method driven by the well-known regression model learner Random Forest. Our proposal integrates feature selection for regression, model evaluation, and decision making, in order to choose the most satisfactory model according to an a posteriori process in a multi-objective context. We test and compare the performances of ENORA as multi-objective evolutionary search strategy against a standard multi-objective evolutionary search strategy such as NSGA-II (Non-dominated Sorted Genetic Algorithm), against a classical backward search strategy such as RFE (Recursive Feature Elimination), and against the original data set.},
journal = {Neurocomput.},
month = apr,
pages = {75–92},
numpages = {18},
keywords = {Feature selection, Multi-objective evolutionary algorithms, Online sales forecasting, Random forest, Regression model}
}

@article{10.1007/s10458-014-9268-y,
author = {Loscalzo, Steven and Wright, Robert and Yu, Lei},
title = {Predictive feature selection for genetic policy search},
year = {2015},
issue_date = {September 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {5},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-014-9268-y},
doi = {10.1007/s10458-014-9268-y},
abstract = {Automatic learning of control policies is becoming increasingly important to allow autonomous agents to operate alongside, or in place of, humans in dangerous and fast-paced situations. Reinforcement learning (RL), including genetic policy search algorithms, comprise a promising technology area capable of learning such control policies. Unfortunately, RL techniques can take prohibitively long to learn a sufficiently good control policy in environments described by many sensors (features). We argue that in many cases only a subset of available features are needed to learn the task at hand, since others may represent irrelevant or redundant information. In this work, we propose a predictive feature selection framework that analyzes data obtained during execution of a genetic policy search algorithm to identify relevant features on-line. This serves to constrain the policy search space and reduces the time needed to locate a sufficiently good policy by embedding feature selection into the process of learning a control policy. We explore this framework through an instantiation called predictive feature selection embedded in neuroevolution of augmenting topology (NEAT), or PFS-NEAT. In an empirical study, we demonstrate that PFS-NEAT is capable of enabling NEAT to successfully find good control policies in two benchmark environments, and show that it can outperform three competing feature selection algorithms, FS-NEAT, FD-NEAT, and SAFS-NEAT, in several variants of these environments.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = sep,
pages = {754–786},
numpages = {33},
keywords = {Dimensionality reduction, Feature selection, Genetic policy search, Reinforcement learning}
}

@inproceedings{10.1145/2588555.2593678,
author = {Zhang, Ce and Kumar, Arun and R\'{e}, Christopher},
title = {Materialization optimizations for feature selection workloads},
year = {2014},
isbn = {9781450323765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2588555.2593678},
doi = {10.1145/2588555.2593678},
abstract = {There is an arms race in the data management industry to support analytics, in which one critical step is feature selection, the process of selecting a feature set that will be used to build a statistical model. Analytics is one of the biggest topics in data management, and feature selection is widely regarded as the most critical step of analytics; thus, we argue that managing the feature selection process is a pressing data management challenge. We study this challenge by describing a feature-selection language and a supporting prototype system that builds on top of current industrial, R-integration layers. From our interactions with analysts, we learned that feature selection is an interactive, human-in-the-loop process, which means that feature selection workloads are rife with reuse opportunities. Thus, we study how to materialize portions of this computation using not only classical database materialization optimizations but also methods that have not previously been used in database optimization, including structural decomposition methods (like QR factorization) and warmstart. These new methods have no analog in traditional SQL systems, but they may be interesting for array and scientific database applications. On a diverse set of data sets and programs, we find that traditional database-style approaches that ignore these new opportunities are more than two orders of magnitude slower than an optimal plan in this new tradeoff space across multiple R-backends. Furthermore, we show that it is possible to build a simple cost-based optimizer to automatically select a near-optimal execution plan for feature selection.},
booktitle = {Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data},
pages = {265–276},
numpages = {12},
keywords = {feature selection, materialization, statistical analytics},
location = {Snowbird, Utah, USA},
series = {SIGMOD '14}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.asoc.2017.07.003,
author = {Masood, M.K. and Soh, Yeng Chai and Jiang, Chaoyang},
title = {Occupancy estimation from environmental parameters using wrapper and hybrid feature selection},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {60},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.07.003},
doi = {10.1016/j.asoc.2017.07.003},
abstract = {A strategy to estimate indoor occupancy from environmental sensors is presented.Two novel feature selection algorithms are proposed: one is a wrapper (WRANK-ELM) and the other a filterwrapper hybrid (RIG-ELM).Experimental results from an office space are presented.Both algorithms achieve excellent occupancy estimation accuracy, higher than past work.The algorithms are computationally efficient, much more so than the past work. Occupancy information is essential to facilitate demand-driven operations of air-conditioning and mechanical ventilation (ACMV) systems. Environmental sensors are increasingly being explored as cost effective and non-intrusive means to obtain the occupancy information. This requires the extraction and selection of useful features from the sensor data. In past works, feature selection has generally been implemented using filter-based approaches. In this work, we introduce the use of wrapper and hybrid feature selection for better occupancy estimation. To achieve a fast computation time, we introduce a ranking-based incremental search in our algorithms, which is more efficient than the exhaustive search used in past works. For wrapper feature selection, we propose the WRANK-ELM, which searches an ordered list of features using the extreme learning machine (ELM) classifier. For hybrid feature selection, we propose the RIG-ELM, which is a filterwrapper hybrid that uses the relative information gain (RIG) criterion for feature ranking and the ELM for the incremental search. We present experimental results in an office space with a multi-sensory network to validate the proposed algorithms.},
journal = {Appl. Soft Comput.},
month = nov,
pages = {482–494},
numpages = {13},
keywords = {ELM, Hybrid feature selection, Occupancy estimation, Wrapper}
}

@article{10.1016/j.knosys.2017.03.002,
author = {Wang, Shiping and Wang, Han},
title = {Unsupervised feature selection via low-rank approximation and structure learning},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {124},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.002},
doi = {10.1016/j.knosys.2017.03.002},
abstract = {Feature selection is an important research topic in machine learning and computer vision in that it can reduce the dimensionality of input data and improve the performance of learning algorithms. Low-rank approximation techniques can well exploit the low-rank property of input data, which coincides with the internal consistency of dimensionality reduction. In this paper, we propose an efficient unsupervised feature selection algorithm, which incorporates low-rank approximation as well as structure learning. First, using the self-representation of data matrix, we formalize the feature selection problem as a matrix factorization with low-rank constraints. This matrix factorization formulation also embeds structure learning regularization as well as a sparse regularized term. Second, we present an effective technique to approximate low-rank constraints and propose a convergent algorithm in a batch mode. This technique can serve as an algorithmic framework for general low-rank recovery problems as well. Finally, the proposed algorithm is validated in twelve publicly available datasets from machine learning repository. Extensive experimental results demonstrate that the proposed method is capable to achieve competitive performance compared to existing state-of-the-art feature selection methods in terms of clustering performance.},
journal = {Know.-Based Syst.},
month = may,
pages = {70–79},
numpages = {10},
keywords = {Feature selection, Low-rank approximation, Machine learning, Structure learning, Unsupervised learning}
}

@article{10.1016/j.neucom.2016.10.062,
author = {Wang, Shuqin and Wei, Jinmao},
title = {Feature selection based on measurement of ability to classify subproblems},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {224},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.10.062},
doi = {10.1016/j.neucom.2016.10.062},
abstract = {Feature selection is important and necessary especially for processing large scale data. Existing feature selection methods generally compute a discriminant value with respect to class variable for a feature to indicate its classification ability. Such a scalar value can hardly reveal the multi-faceted classification abilities of a feature for the different subproblems in a classification task. In this paper, an effective way is proposed for feature selection based on measurement of ability to classify subproblems and discrimination structure complementarity of features. The classification abilities of a feature for different subproblems are calculated respectively. Hence for the feature, a discrimination structure vector representing its classification abilities for all subproblems can be obtained. In feature selection, the features, which can individually classify as many subproblems as possible, are firstly evaluated and selected. Subsequently, their complementary features are selectively chosen, which can complementarily classify the subproblems that the selected features cannot classify. Two algorithms are designed for progressively selecting features, by firstly eliminating irrelevant features and then abandoning redundant features based on discrimination structure complementarity. The proposed algorithms are compared with some related methods for feature selection on some open gene expression datasets and UCI datasets. Experimental results demonstrate the effectiveness of the proposed method.},
journal = {Neurocomput.},
month = feb,
pages = {155–165},
numpages = {11},
keywords = {Discrimination structure complementarity, Feature selection, Fisher's discriminant ratio}
}

@inproceedings{10.1007/978-3-030-30493-5_42,
author = {Mbuvha, Rendani and Boulkaibet, Illyes and Marwala, Tshilidzi},
title = {Bayesian Automatic Relevance Determination for Feature Selection in Credit Default Modelling},
year = {2019},
isbn = {978-3-030-30492-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30493-5_42},
doi = {10.1007/978-3-030-30493-5_42},
abstract = {This work develops a neural network based global model interpretation mechanism - the Bayesian Neural Network with Automatic Relevance Determination (BNN-ARD) for feature selection in credit default modelling. We compare the resulting selected important features to those obtained from the Random Forest (RF) and Gradient Tree Boosting (GTB). We show by re-training the models on the identified important features that the predictive quality of the features obtained from the BNN-ARD is similar to that of the GTB and outperforms those of RF in terms of the predictive performance of the retrained models.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings},
pages = {420–425},
numpages = {6},
keywords = {Bayesian, Neural networks, Hybrid Monte Carlo, Credit default modelling, Automatic Relevance Determination},
location = {Munich, Germany}
}

@inproceedings{10.1145/3167918.3167963,
author = {Estivill-Castro, Vladimir and Lombardi, Matteo and Marani, Alessandro},
title = {Improving binary classification of web pages using an ensemble of feature selection algorithms},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167963},
doi = {10.1145/3167918.3167963},
abstract = {A well-known method to produce accurate predictive models is to apply algorithms for feature selection and feature reduction. These algorithms describe an item with a subset of its attributes that is expected to be the smallest possible without compromising the actual representation of the object, and consequently the entire classification. However, different feature-selection algorithms have different potentially complementary properties each only collecting some aspects of the feature set. Hence the resulting subset of attributes may significantly vary from one feature-selection approach to another. Each method has different effects on the accuracy of the classification. In this contribution, we combine feature-selection algorithms with the intention of recognising the purpose of a web-page. That is, we propose a framework for building an ensemble of feature selection algorithms, to merge their outcomes into a single score and thus achieving a comprehensive analysis of the feature set. We evaluated our proposal against traditional feature-selection and feature-reduction algorithms in a binary classification task of web pages. Our dataset consists of more than 400 pages labelled by educators as either relevant or not relevant for teaching purposes. We also evaluate the impact of the combination across several classifiers. Our results show that our framework outperforms current algorithms, allowing for a much faster and yet reliable classification of web pages in all the different scenarios tested. We expect that our findings will contribute to improving the performance of web classifiers, particularly when running on-the-fly and in real-time.},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {17},
numpages = {10},
keywords = {feature selection, information retrieval, web-page classification},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@article{10.1016/j.neucom.2019.01.017,
author = {Gonz\'{a}lez, Jes\'{u}s and Ortega, Julio and Damas, Miguel and Mart\'{\i}n-Smith, Pedro and Gan, John Q.},
title = {A new multi-objective wrapper method for feature selection – Accuracy and stability analysis for BCI},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {333},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.01.017},
doi = {10.1016/j.neucom.2019.01.017},
journal = {Neurocomput.},
month = mar,
pages = {407–418},
numpages = {12},
keywords = {BCI, EEG, Motor imagery, Feature selection, Multi-objective problem, Evolutionary algorithm, Classification, Stability, Ensemble}
}

@article{10.3233/JIFS-191721,
author = {Antony Rosewelt, L. and Arokia Renjit, J.},
title = {A content recommendation system for effective e-learning using embedded feature selection and fuzzy DT based CNN},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-191721},
doi = {10.3233/JIFS-191721},
abstract = {This paper proposes a new content recommendation system which combines the newly proposed embedded feature selection method and the new Fuzzy Temporal Logic based Decision Tree incorporated Convolutional Neural Network classifier. The newly proposed embedded feature selection called Fuzzy Decision Tree and Weighted Gini-Index based Feature Selection Algorithm (FDTWGI-FSA) that contains the existing incorporated the Fuzzy Decision Tree (FDT) and the Weighted Gini-index based Feature Selection Algorithm (WGIFSA) for getting optimized feature subset. Moreover, an enhanced CNN and Fuzzy Temporal Decision Tree for performing the deep learning process which is able to identify the exact e-content from the huge volume of data with the help of the recommended features by the proposed embedded feature selection method. The exact e-content can be identified after performing the five-layer network structure for extracting the relevant features and it also can be classified by applying the Fuzzy Temporal Decision Tree for the e-learners. Finally, the proposed content recommendation system provides exact content to the e-learners according to their level of understanding and it also satisfies them by providing the exact high level contents. The experiments have been conducted for evaluating the proposed content recommendation system and compared with the existing classifier including the standard CNN.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {795–808},
numpages = {14},
keywords = {Classification, deep learning, feature selection (FS), fuzzy logic, weighted genetic algorithm (WGA)}
}

@inproceedings{10.1007/978-3-030-28377-3_28,
author = {Gwetu, Mandlenkosi Victor and Tapamo, Jules-Raymond and Viriri, Serestina},
title = {Exploring the Impact of Purity Gap Gain on the Efficiency and Effectiveness of Random Forest Feature Selection},
year = {2019},
isbn = {978-3-030-28376-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-28377-3_28},
doi = {10.1007/978-3-030-28377-3_28},
abstract = {The Random Forest (RF) classifier has the capacity to facilitate both wrapper and embedded feature selection through the Mean Decrease Accuracy (MDA) and Mean Decrease Impurity (MDI) methods, respectively. MDI is known to be biased towards predictor variables with multiple values whilst MDA is stable in this regard. As such, MDA is the predominantly preferred option for RF-based feature selection, despite its higher computational overhead in comparison to MDI. This research seeks to simultaneously reduce the computational overhead and improve the effectiveness of RF feature selection. We propose two improvements to the MDI method to overcome its shortcomings. The first is using our proposed Purity Gap Gain (PGG) measure which has an emphasis on computational efficiency, as an alternative to the Gini Importance (GI) metric. The second is incorporating a Relative Mean Decrease Impurity (RMDI) score, which aims to offset the bias towards multi-valued predictor variables through random feature value permutations. Experiments are conducted on UCI datasets to establish the impact of PGG and RMDI on RF performance.},
booktitle = {Computational Collective Intelligence: 11th International Conference, ICCCI 2019, Hendaye, France, September 4–6, 2019, Proceedings, Part I},
pages = {340–352},
numpages = {13},
keywords = {Random Forest, Mean Decrease Accuracy, Mean Decrease Impurity, Purity Gap Gain, Relative Mean Decrease Impurity},
location = {Hendaye, France}
}

@article{10.1007/s00500-019-04167-0,
author = {Sivasankar, E. and Selvi, C. and Mahalakshmi, S.},
title = {Rough set-based feature selection for credit risk prediction using weight-adjusted boosting ensemble method},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04167-0},
doi = {10.1007/s00500-019-04167-0},
abstract = {With the tremendous development of financial institutions, credit risk prediction (CRP) plays an essential role in granting loans to customers and helps them to minimize their loss because credit approval sometimes results in massive financial loss. So extra attention is needed to identify risky customer. Researchers have designed complex CRP models using artificial intelligence (AI) and statistical techniques to support the financial institutions to take correct business decisions. Though there are various statistical and AI methods available, the recent literature shows that the ensemble-based CRP model provides improved prediction results than single classifier system. The small increase in the performance of CRP model could result in a significant improvement in the profit of financial institutions and banks. This work proposes a weight-adjusted boosting ensemble method (WABEM) using rough set (RS)-based feature selection (FS) technique with the balancing and regression-based preprocessing called RS_RFS-WABEM. Regression is used to fill missing value in the records to improve the performance of CRP. Three credit datasets (Australia, German and Japanese) are chosen to validate the feasibility and effectiveness of the proposed ensemble method. The trade-off between the uncertainty and imprecise probability of the proposed classifier model is evaluated using the performance measures such as accuracy and area under the curve. Experimental results show that the proposed ensemble method performs better than other base and ensemble classifier methods.},
journal = {Soft Comput.},
month = mar,
pages = {3975–3988},
numpages = {14},
keywords = {Credit risk prediction (CRP), Preprocessing, Feature selection (FS), Missing value, Regression, Ensemble method, Friedman Test}
}

@article{10.1016/j.asoc.2019.105545,
author = {Fatemi Bushehri, S.M.M. and Zarchi, Mohsen Sardari},
title = {An expert model for self-care problems classification using probabilistic neural network and feature selection approach},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105545},
doi = {10.1016/j.asoc.2019.105545},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {11},
keywords = {Soft computing, Self-care classification, PNN classifier, Feature selection, Rule extraction, ICF-CY, Genetic algorithm}
}

@inproceedings{10.1007/978-3-030-61609-0_7,
author = {Tokovarov, Mikhail},
title = {Convolutional Neural Networks with Reusable Full-Dimension-Long Layers for Feature Selection and Classification of Motor Imagery in EEG Signals},
year = {2020},
isbn = {978-3-030-61608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61609-0_7},
doi = {10.1007/978-3-030-61609-0_7},
abstract = {In the present article the author addresses the task of classification of motor imagery in EEG signals by proposing innovative architecture of neural network. Despite all the successes of deep learning, neural networks of significant depth could not ensure better performance compared to shallow architectures. The approach presented in the article employs this idea, making use of yet shallower, but productive architecture. The main idea of the proposed architecture is based on three points: full-dimension-long ‘valid’ convolutions, dense connections - combination of layer’s input and output and layer reuse. Another aspect addressed in the paper is related to interpretable machine learning. Interpretability is extremely important in medicine, where decisions must be taken on the basis of solid arguments and clear reasons. Being shallow, the architecture could be used for feature selection by interpreting the layers’ weights, which allows understanding of the knowledge about the data cumulated in the network’s layers. The approach, based on a fuzzy measure, allows using Choquet integral to aggregate the knowledge generated in the layer weights and understanding which features (EEG electrodes) provide the most essential information. The approach allows lowering feature number from 64 to 14 with an insignificant drop of accuracy (less than a percentage point).},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2020: 29th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 15–18, 2020, Proceedings, Part I},
pages = {79–91},
numpages = {13},
keywords = {Motor imagery, Feature selection, Convolutional neural network, Reusable convolutions},
location = {Bratislava, Slovakia}
}

@article{10.4018/IJMCMC.2017070107,
author = {Wang, Hui and Guo, Li Li and Lin, Yun},
title = {Modulation Recognition of Digital Multimedia Signal Based on Data Feature Selection},
year = {2017},
issue_date = {July 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {3},
issn = {1937-9412},
url = {https://doi.org/10.4018/IJMCMC.2017070107},
doi = {10.4018/IJMCMC.2017070107},
abstract = {Automatic modulation recognition is very important for the receiver design in the broadband multimedia communication system, and the reasonable signal feature extraction and selection algorithm is the key technology of Digital multimedia signal recognition. In this paper, the information entropy is used to extract the single feature, which are power spectrum entropy, wavelet energy spectrum entropy, singular spectrum entropy and Renyi entropy. And then, the feature selection algorithm of distance measurement and Sequential Feature SelectionSFS are presented to select the optimal feature subset. Finally, the BP neural network is used to classify the signal modulation. The simulation result shows that the four-different information entropy can be used to classify different signal modulation, and the feature selection algorithm is successfully used to choose the optimal feature subset and get the best performance.},
journal = {Int. J. Mob. Comput. Multimed. Commun.},
month = jul,
pages = {90–111},
numpages = {22},
keywords = {Feature Selection, Information Entropy, Neural Network, Signal Recognition}
}

@article{10.1016/j.chb.2014.11.007,
author = {Zhao, Jie and Wang, Xueya and Jin, Peiquan},
title = {Feature selection for event discovery in social media},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {PB},
issn = {0747-5632},
url = {https://doi.org/10.1016/j.chb.2014.11.007},
doi = {10.1016/j.chb.2014.11.007},
abstract = {We analyze five feature selection methods for event discovery in social media.We present an improved feature selection method.We conduct comparative experiments on a real dataset using various metrics.We discuss the choosing of appropriate feature selection methods for microblogs. Microblog as one kind of typical social media has many research implications in social event discovery and social-media-based e-learning and collaborative learning. At present, researchers usually employ feature-based classification approaches to detect social events in microblogs. However, it is very common to get different results when different features are used in event discovery. Therefore, it has been a critical issue how to select appropriate features for event discovery in microblogs. In this paper, we analyze five different feature selection methods and present an improved method for selecting features for microblog-based event discovery. We compare all the methods on a real microblog dataset in terms of various metrics including precision, recall, and F-measure. And finally we discuss the best feature selection method for the event discovery in microblogs. To the best of our knowledge, there are no such comparative studies on feature selection for event discovery in social media, and this paper is expected to offer some useful references for the future research and applications on the event discovery in microblogs.},
journal = {Comput. Hum. Behav.},
month = oct,
pages = {903–909},
numpages = {7},
keywords = {Algorithm comparison, Event discovery, Feature selection, Microblog}
}

@article{10.1016/j.neunet.2015.03.005,
author = {Connor, Patrick and Hollensen, Paul and Krigolson, Olav and Trappenberg, Thomas},
title = {A biological mechanism for Bayesian feature selection},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {67},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.03.005},
doi = {10.1016/j.neunet.2015.03.005},
abstract = {Biological systems are capable of learning that certain stimuli are valuable while ignoring the many that are not, and thus perform feature selection. In machine learning, one effective feature selection approach is the least absolute shrinkage and selection operator (LASSO) form of regularization, which is equivalent to assuming a Laplacian prior distribution on the parameters. We review how such Bayesian priors can be implemented in gradient descent as a form of weight decay, which is a biologically plausible mechanism for Bayesian feature selection. In particular, we describe a new prior that offsets or "raises" the Laplacian prior distribution. We evaluate this alongside the Gaussian and Cauchy priors in gradient descent using a generic regression task where there are few relevant and many irrelevant features. We find that raising the Laplacian leads to less prediction error because it is a better model of the underlying distribution. We also consider two biologically relevant online learning tasks, one synthetic and one modeled after the perceptual expertise task of Krigolson et al. (2009). Here, raising the Laplacian prior avoids the fast erosion of relevant parameters over the period following training because it only allows small weights to decay. This better matches the limited loss of association seen between days in the human data of the perceptual expertise task. Raising the Laplacian prior thus results in a biologically plausible form of Bayesian feature selection that is effective in biologically relevant contexts.},
journal = {Neural Netw.},
month = jul,
pages = {121–130},
numpages = {10},
keywords = {Bayesian prior, Feature selection, LASSO, Online learning, Regularization, Weight decay}
}

@article{10.1016/j.asoc.2018.02.003,
author = {Alijla, Basem O. and Lim, Chee Peng and Wong, Li-Pei and Khader, Ahamad Tajudin and Al-Betar, Mohammed Azmi},
title = {An ensemble of intelligent water drop algorithm for feature selection optimization problem},
year = {2018},
issue_date = {Apr 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {65},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.02.003},
doi = {10.1016/j.asoc.2018.02.003},
journal = {Appl. Soft Comput.},
month = apr,
pages = {531–541},
numpages = {11},
keywords = {Intelligent water drops, Optimization, Swarm intelligence, Feature selection, Motion detection, Motor fault detection}
}

@article{10.1016/j.procs.2016.09.369,
author = {Cai, Fuyu and Wang, Hao and Tang, Xiaoqin and Emmerich, Michael and Verbeek, Fons J.},
title = {Fuzzy Criteria in Multi-objective Feature Selection for Unsupervised Learning},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2016.09.369},
doi = {10.1016/j.procs.2016.09.369},
abstract = {Feature selection in which most informative variables are selected for model generation is an important step in pattern recognition. Here, one often tries to optimize multiple criteria such as discriminating power of the descriptor, performance of model and cardinality of a subset. In this paper we propose a fuzzy criterion in multi-objective unsupervised feature selection by applying the hybridized filter-wrapper approach (FC-MOFS). These formulations allow for an efficient way to pick features from a pool and to avoid misunderstanding of overlapping features via crisp clustered learning in a conventional multi-objective optimization procedure. Moreover, the optimization problem is solved by using non-dominated sorting genetic algorithm, type two (NSGA-II). The performance of the proposed approach is then examined on six benchmark datasets from multiple disciplines and different numbers of features. Systematic comparisons of the proposed method and representative non-fuzzified approaches are illustrated in this work. The experimental studies show a superior performance of the proposed approach in terms of accuracy and feasibility.},
journal = {Procedia Comput. Sci.},
month = dec,
pages = {51–58},
numpages = {8},
keywords = {feature selection, fuzzy criteria, multi-objective optimization, unsupervised learning}
}

@article{10.1016/j.future.2019.01.048,
author = {Chatterjee, Rajdeep and Maitra, Tanmoy and Hafizul Islam, SK and Hassan, Mohammad Mehedi and Alamri, Atif and Fortino, Giancarlo},
title = {A novel machine learning based feature selection for motor imagery EEG signal classification in Internet of medical things environment},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {98},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.01.048},
doi = {10.1016/j.future.2019.01.048},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {419–434},
numpages = {16},
keywords = {BCI, Classification, Discernibility, EEG, Feature selection, Fuzzy set, IoMT, Machine learning}
}

@article{10.1007/s00500-018-3373-9,
author = {Tang, Jian and Qiao, Junfei and Liu, Zhuo and Zhou, Xiaojie and Yu, Gang and Zhao, Jianjun},
title = {Optimized ensemble modeling based on feature selection using simple sphere criterion for multi-scale mechanical frequency spectrum},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {16},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-018-3373-9},
doi = {10.1007/s00500-018-3373-9},
abstract = {Several parameters of industrial processes are indirectly measured by multi-scale mechanical frequency spectrum. Selecting suitable mechanical sub-signals and relevant frequency spectral features for different process parameters remains an open issue. This study proposes a new optimized ensemble model based on feature selection using simple sphere criterion (SSC). Mechanical signals are adaptively decomposed and transformed into frequency spectral data with different timescales. These spectral data are fed into adaptive multi-scale spectral feature selection and modeling framework, in which local-scale frequency spectral features are adaptively selected with concurrent projection to latent structures and SSC based on unscaled data. The optimized ensemble model is constructed with selective information fusion strategy based on reduced frequency spectral data. The feature selection and model learning parameters are jointly selected. Simulation results based on the mechanical vibration and acoustic signals of an experimental laboratory-scale ball mill show the effectiveness of the proposed scheme.},
journal = {Soft Comput.},
month = aug,
pages = {7263–7278},
numpages = {16},
keywords = {Concurrent projection to latent structure (CPLS), Multi-scale mechanical frequency spectrum, Optimized ensemble modeling, Selective information fusion, Simple sphere criterion (SSC)}
}

@inproceedings{10.1145/3233547.3233631,
author = {Hsieh, Min-Wei and Ohwada, Hayato and Chen, Sheng-I},
title = {Practical Feature Selection for Lung Cancer Gene Detection},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233631},
doi = {10.1145/3233547.3233631},
abstract = {Lung cancer is the leading cause of cancer death in many countries. Interstitial lung disease (ILD), it is not cancer though, affects people more severely than many kinds of cancer. While ILD and lung cancer often occur concomitantly, the cause is still unclear. We intend to find the key factor that make patients suffer from both ILD and lung cancer instead of only lung cancer.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {522},
numpages = {1},
keywords = {cancer, data analytics, gene, machine learning},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1007/978-3-030-66187-8_26,
author = {Guru, D. S. and Vinay Kumar, N.},
title = {Clustering of Interval Valued Data Through Interval Valued Feature Selection: Filter Based Approaches},
year = {2019},
isbn = {978-3-030-66186-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66187-8_26},
doi = {10.1007/978-3-030-66187-8_26},
abstract = {In this paper, a problem of selectively choosing a few best interval valued features out of several available features is addressed in an Un-supervised environment. Various models belonging to two categories viz., models which transform interval data to crisp and models which accomplish feature selection through clustering of interval valued features are explored for clustering of interval valued data. Extensive experimentation is conducted on two standard benchmarking datasets using suitable symbolic clustering algorithms. The experimental results show that the approaches presented outperform the state-of-the-art models in terms of correct rand index score and number of features selected.},
booktitle = {Mining Intelligence and Knowledge Exploration: 7th International Conference, MIKE 2019, Goa, India, December 19–22, 2019, Proceedings},
pages = {270–285},
numpages = {16},
keywords = {Symbolic data, Interval valued data, Mutual similarity value, Feature ranking criteria, Symbolic clustering},
location = {Goa, India}
}

@article{10.1504/ijbis.2019.099524,
author = {Sivasankar, E. and Vijaya, J.},
title = {A study of feature selection techniques for predicting customer retention in telecommunication sector},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {31},
number = {1},
issn = {1746-0972},
url = {https://doi.org/10.1504/ijbis.2019.099524},
doi = {10.1504/ijbis.2019.099524},
abstract = {Feature selection is the process of eliminating irrelevant features from the dataset, while maintaining acceptable classification accuracy. The selected features play an important role which can directly influence the effectiveness of the resulting classification. In this paper, a methodology is proposed consisting of two phases, attributes selection and classification based on the attributes selected. Phase one uses a filter and wrapper method for attribute selection with random over-sampling (Ros) through which the size of attributes set and misclassification error can be reduced. In the second phase, the selected attributes are taken as inputs by classification techniques like decision trees (DT), K-nearest neighbour (KNN), support vector machine (SVM), naive Bayes (NB) and artificial neural network (ANN). Finally, true churn, false churn, specificity and accuracy are measured to evaluate the efficiency of the proposed system and it is found that the above mentioned methodology performs well ahead for churn prediction and suits well for the telecommunication sector.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {1–26},
numpages = {25},
keywords = {churn prediction, random over sampling, feature selection, filter method, wrapper method, decision trees, k-nearest neighbour, KNN, support vector machine, SVM, naive Bayes, artificial neural network, ANN}
}

@article{10.1016/j.neucom.2015.05.105,
author = {Qian, Wenbin and Shu, Wenhao},
title = {Mutual information criterion for feature selection from incomplete data},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {168},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.05.105},
doi = {10.1016/j.neucom.2015.05.105},
abstract = {Feature selection is an important preprocessing step in machine learning and data mining, and feature criterion arises a key issue in the construction of feature selection algorithms. Mutual information is one of the widely used criteria in feature selection, which determines the relevance between features and target classes. Some mutual information-based feature selection algorithms have been extensively studied, but less effort has been made to investigate the feature selection issue in incomplete data. In this paper, combined with the tolerance information granules in rough sets, the mutual information criterion is provided for evaluating candidate features in incomplete data, which not only utilizes the largest mutual information with the target class but also takes into consideration the redundancy between selected features. We first validate the feasibility of the mutual information. Then an effective mutual information-based feature selection algorithm with forward greedy strategy is developed in incomplete data. To further accelerate the feature selection process, the selection of candidate features is implemented in a dwindling object set. Compared with existing feature selection algorithms, the experimental results on different real data sets show that the proposed algorithm is more effective for feature selection in incomplete data at most cases.},
journal = {Neurocomput.},
month = nov,
pages = {210–220},
numpages = {11},
keywords = {Feature selection, Incomplete data, Mutual information, Rough sets, Uncertainty measure}
}

@inproceedings{10.1007/978-3-030-13709-0_6,
author = {Vivas, Sebasti\'{a}n and Cobos, Carlos and Mendoza, Martha},
title = {Covering Arrays to Support the Process of Feature Selection in the Random Forest Classifier},
year = {2018},
isbn = {978-3-030-13708-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13709-0_6},
doi = {10.1007/978-3-030-13709-0_6},
abstract = {The Random Forest (RF) algorithm consists of an assembly of base decision trees, constructed from Bootstrap subsets of the original dataset. Each subset is a sample of instances (rows) by a random subset of features (variables or columns) of the original dataset to be classified. In RF, pruning is not applied in the generation of base trees and in the classification process of a new record, each tree issues a vote enabling the selected class to be defined, as that with the most votes. Bearing in mind that in the state of the art it is defined that random feature selection for constructing the Bootstrap subsets decreases the quality of the results achieved with RF, in this work the integration of covering arrays (CA) in RF is proposed to solve this situation, in an algorithm called RFCA. In RFCA, the number N of rows of the CA defines the lowest number of base trees that require to be generated in RF and each row of the CA defines the features that each Bootstrap subset will use in the creation of each tree. To evaluate the new proposal, 32 datasets available in the UCI repository are used and compared with the RF available in Weka. The experiments show that the use of a CA of strength 2 to 7 obtains promising results in terms of accuracy.},
booktitle = {Machine Learning, Optimization, and Data Science: 4th International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018, Revised Selected Papers},
pages = {64–76},
numpages = {13},
keywords = {Classification, Random Forest, Covering arrays, Feature selection},
location = {Volterra, Italy}
}

@article{10.4018/IJRSDA.2018070101,
author = {Subudhi, Sharmila and Panigrahi, Suvasini},
title = {Detection of Automobile Insurance Fraud Using Feature Selection and Data Mining Techniques},
year = {2018},
issue_date = {July 2018},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {2334-4598},
url = {https://doi.org/10.4018/IJRSDA.2018070101},
doi = {10.4018/IJRSDA.2018070101},
abstract = {This article presents a novel approach for fraud detection in automobile insurance claims by applying various data mining techniques. Initially, the most relevant attributes are chosen from the original dataset by using an evolutionary algorithm based feature selection method. A test set is then extracted from the selected attribute set and the remaining dataset is subjected to the Possibilistic Fuzzy C-Means PFCM clustering technique for the undersampling approach. The 10-fold cross validation method is then used on the balanced dataset for training and validating a group of Weighted Extreme Learning Machine WELM classifiers generated from various combinations of WELM parameters. Finally, the test set is applied on the best performing model for classification purpose. The efficacy of the proposed system is illustrated by conducting several experiments on a real-world automobile insurance defraud dataset. Besides, a comparative analysis with another approach justifies the superiority of the proposed system.},
journal = {Int. J. Rough Sets Data Anal.},
month = jul,
pages = {1–20},
numpages = {20},
keywords = {Automobile Insurance Claim Records, Classification, Evolutionary Algorithm, Feature Selection, Fraud Detection, Possibilistic Fuzzy C-Means Clustering, Undersampling, Weighted Extreme Learning Machine}
}

@article{10.1155/2020/2394948,
author = {Wang, Yimeng and Zhang, Yunqi and Zhang, Guangchen},
title = {Credit Risk Assessment for Small and Microsized Enterprises Using Kernel Feature Selection-Based Multiple Criteria Linear Optimization Classifier: Evidence from China},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/2394948},
doi = {10.1155/2020/2394948},
abstract = {Credit risk assessment has gained increasing marked attention in the recent years by researchers, financial institutions, and banks, especially for small and microsized enterprises. Evidence shows that the core of small and microsized enterprises’ credit risk assessment is to construct a scientific credit risk indicator system, and the key is to establish an effective credit risk prediction model. Therefore, we analyze the factors that influence the credit risk of Chinese small and microsized enterprises and then construct a comprehensive credit risk indicator system by adding behaviour information, supervision information, and policy information. Furthermore, we improve the multiple criteria linear optimization classifier (MCLOC) by introducing the one-norm kernel feature selection and thereby establish the kernel feature selection-based multiple criteria linear optimization classifier (KFS-MCLOC). As for experiments, we use real business data from a Chinese commercial bank to test the performance of these models. The results show that (1) the proposed KFS-MCLOC has greater advantages in predictive accuracy, interpretability, and stability than other models; (2) the KFS-MCLOC selects 10 features from 53 original features and gives selected features their weight automatically; (3) the features selected by the KFS-MCLOC are further verified and compared by the features selected by the logistic regression model with stepwise parameter, and the indicators of “quick ratio; net operating cash flow; enterprises’ abnormal times of water, electricity, and tax fee; overdue days of enterprises’ loans; and mortgage and pledge status” are proved to be the most influencing credit risk factors.},
journal = {Complex.},
month = jan,
numpages = {16}
}

@article{10.1016/j.compeleceng.2016.02.009,
author = {Zainuddin, Zarita and Lai, Kee Huong and Ong, Pauline},
title = {An enhanced harmony search based algorithm for feature selection},
year = {2016},
issue_date = {July 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {53},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2016.02.009},
doi = {10.1016/j.compeleceng.2016.02.009},
abstract = {A feature selection approach is proposed using the harmony search algorithm.A new harmony memory initialization is adopted and dynamic parameters are used.The proposed method and other metaheuristic algorithms give comparable performance.The enhanced harmony search algorithm outperforms the standard algorithm. Feature selection is a well-studied problem in the areas of pattern recognition and artificial intelligence. Apart from reducing computational cost and time, a good feature subset is also imperative in improving the classification accuracy of automated classifiers. In this work, a wrapper-based feature selection approach is proposed using the evolutionary harmony search algorithm, whereas the classifiers used are the wavelet neural networks. The metaheuristic algorithm is able to find near-optimal solutions within a reasonable amount of iterations. The modifications are accomplished in two ways-initialization of harmony memory and improvisation of solutions. The proposed algorithm is tested and verified using UCI benchmark data sets, as well as two real life binary classification problems, namely epileptic seizure detection and prediction. The simulation results show that the standard harmony search algorithm and other similar metaheuristic algorithms give comparable performance. In addition, the enhanced harmony search algorithm outperforms the standard harmony search algorithm. Display Omitted},
journal = {Comput. Electr. Eng.},
month = jul,
pages = {143–162},
numpages = {20},
keywords = {Epileptic seizure classification, Epileptic seizure prediction, Feature selection, Harmony search, Wavelet neural networks}
}

@inproceedings{10.5555/3382225.3382310,
author = {Yao, Mengfan and Chelmis, Charalampos and Zois, Daphney-Stavroula},
title = {Cyberbullying detection on instagram with optimal online feature selection},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Cyberbullying has emerged as a large-scale societal problem that demands accurate methods for its detection in an effort to mitigate its detrimental consequences. While automated, data-driven techniques for analyzing and detecting cyberbullying incidents have been developed, the scalability of existing approaches has largely been ignored. At the same time, the complexities underlying cyberbullying behavior (e.g., social context and changing language) make the automatic identification of "the best subset of features" to use challenging. We address this gap by formulating cyberbullying detection as a sequential hypothesis testing problem. Based on this formulation, we propose a novel algorithm to drastically reduce the number of features used in classification. We demonstrate the utility, scalability and responsiveness of our approach using a real-world dataset from Instagram, the online social media platform with the highest percentage of users reporting experiencing cyberbullying. Our approach improves recall by a staggering 700%, while at the same time reducing the average number of features by up to 99.82% compared to state-of-the-art supervised cyberbullying detection methods, learning approaches that require weak supervision, and traditional offline feature selection and dimensionality reduction techniques.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {401–408},
numpages = {8},
keywords = {classification, cyberharassment, online social media, optimization algorithm, selection process},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1155/2021/3597051,
author = {Huang, Chengyuan and Versaci, Mario},
title = {Feature Selection and Feature Stability Measurement Method for High-Dimensional Small Sample Data Based on Big Data Technology},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/3597051},
doi = {10.1155/2021/3597051},
abstract = {With the rapid development of artificial intelligence in recent years, the research on image processing, text mining, and genome informatics has gradually deepened, and the mining of large-scale databases has begun to receive more and more attention. The objects of data mining have also become more complex, and the data dimensions of mining objects have become higher and higher. Compared with the ultra-high data dimensions, the number of samples available for analysis is too small, resulting in the production of high-dimensional small sample data. High-dimensional small sample data will bring serious dimensional disasters to the mining process. Through feature selection, redundancy and noise features in high-dimensional small sample data can be effectively eliminated, avoiding dimensional disasters and improving the actual efficiency of mining algorithms. However, the existing feature selection methods emphasize the classification or clustering performance of the feature selection results and ignore the stability of the feature selection results, which will lead to unstable feature selection results, and it is difficult to obtain real and understandable features. Based on the traditional feature selection method, this paper proposes an ensemble feature selection method, Random Bits Forest Recursive Clustering Eliminate (RBF-RCE) feature selection method, combined with multiple sets of basic classifiers to carry out parallel learning and screen out the best feature classification results, optimizes the classification performance of traditional feature selection methods, and can also improve the stability of feature selection. Then, this paper analyzes the reasons for the instability of feature selection and introduces a feature selection stability measurement method, the Intersection Measurement (IM), to evaluate whether the feature selection process is stable. The effectiveness of the proposed method is verified by experiments on several groups of high-dimensional small sample data sets.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {12}
}

@inproceedings{10.1145/2791405.2791423,
author = {Singhal, Vanika and Singh, Preety},
title = {Correlation based Feature Selection for Diagnosis of Acute Lymphoblastic Leukemia},
year = {2015},
isbn = {9781450333610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791405.2791423},
doi = {10.1145/2791405.2791423},
abstract = {Acute Lymphoblastic Leukemia (ALL) is a type of cancer characterized by increase in abnormal white blood cells in the blood or bone marrow. This paper presents a methodology to detect ALL automatically using shape features of the lymphocyte cell extracted from its image. We apply Correlation based Feature Selection technique to find a prominent set of features which can be used to predict a lymphocyte cell as normal or blast. The experiments are performed on 260 blood microscopic images of lymphocyte and an accuracy of 92.30% is obtained with a set of sixteen features.},
booktitle = {Proceedings of the Third International Symposium on Women in Computing and Informatics},
pages = {5–9},
numpages = {5},
keywords = {Acute Lymphoblastic Leukemia, Correlation based Feature Selection, Shape features, blast, classification},
location = {Kochi, India},
series = {WCI '15}
}

@inproceedings{10.1145/2884781.2884793,
author = {Medeiros, Fl\'{a}vio and K\"{a}stner, Christian and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Apel, Sven},
title = {A comparison of 10 sampling algorithms for configurable systems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884793},
doi = {10.1145/2884781.2884793},
abstract = {Almost every software system provides configuration options to tailor the system to the target platform and application scenario. Often, this configurability renders the analysis of every individual system configuration infeasible. To address this problem, researchers have proposed a diverse set of sampling algorithms. We present a comparative study of 10 state-of-the-art sampling algorithms regarding their fault-detection capability and size of sample sets. The former is important to improve software quality and the latter to reduce the time of analysis. In a nutshell, we found that sampling algorithms with larger sample sets are able to detect higher numbers of faults, but simple algorithms with small sample sets, such as most-enabled-disabled, are the most efficient in most contexts. Furthermore, we observed that the limiting assumptions made in previous work influence the number of detected faults, the size of sample sets, and the ranking of algorithms. Finally, we have identified a number of technical challenges when trying to avoid the limiting assumptions, which questions the practicality of certain sampling algorithms.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {643–654},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.3233/JIFS-169323,
author = {Qi, Chengming and Hu, Lishuan and Yu, Xin and Guirao, Juan L.G. and Gao, Wei},
title = {A framework of multiple kernel ensemble learning for classification using two-stage feature selection method},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {33},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169323},
doi = {10.3233/JIFS-169323},
abstract = {Feature selection aims at selecting a feature subset that has the most discriminative information and preserve most of characteristics from original features in HyperSpectral Image (HSI) classification. This paper proposes a two-stage feature selection method based on Mutual Information (MI) and Jeffries-Matusita (J-M) measure. In first stage, we select a feature subset with minimal redundancy maximal relevance criteria. In second stage, we select further a feature subset from which obtained in first stage by maximizing J-M distance. Multiple Kernel Learning (MKL) and Ensemble Learning (EL) are promising family of machine learning algorithms and have been applied extensively in HSI classification. Many MKL methods often formulate the problem as an optimization task. To avoid solving the complicated optimization problem, this paper presents an ensemble learning framework, SMKB (Stochastic Multiple Kernel Boosting), which applies Adaptive Boosting (AdaBoost) and stochastic approach to learning multiple kernel-based classifier for multi-class classification problem. We examine empirical performance of proposed approach on benchmark hyperspectral classification data set in comparison with various state-of-the-art algorithms. Experimental results demonstrate that the proposed method obtains better feature subsets and is more effective and efficient than classical methods.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {2737–2747},
numpages = {11},
keywords = {Feature selection, hyperspectral classification, Jeffries-Matusita measure, Multiple Kernel Learning, Mutual Information}
}

@inproceedings{10.1145/2623330.2623611,
author = {Nguyen, Xuan Vinh and Chan, Jeffrey and Romano, Simone and Bailey, James},
title = {Effective global approaches for mutual information based feature selection},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623611},
doi = {10.1145/2623330.2623611},
abstract = {Most current mutual information (MI) based feature selection techniques are greedy in nature thus are prone to sub-optimal decisions. Potential performance improvements could be gained by systematically posing MI-based feature selection as a global optimization problem. A rare attempt at providing a global solution for the MI-based feature selection is the recently proposed Quadratic Programming Feature Selection (QPFS) approach. We point out that the QPFS formulation faces several non-trivial issues, in particular, how to properly treat feature `self-redundancy' while ensuring the convexity of the objective function. In this paper, we take a systematic approach to the problem of global MI-based feature selection. We show how the resulting NP-hard global optimization problem could be efficiently approximately solved via spectral relaxation and semi-definite programming techniques. We experimentally demonstrate the efficiency and effectiveness of these novel feature selection frameworks.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {512–521},
numpages = {10},
keywords = {feature selection, global optimization, mutual information, semi-definite programming, spectral relaxation},
location = {New York, New York, USA},
series = {KDD '14}
}

@article{10.1016/j.patcog.2015.01.023,
author = {Jiang, Feng and Sui, Yuefei and Zhou, Lin},
title = {A relative decision entropy-based feature selection approach},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.01.023},
doi = {10.1016/j.patcog.2015.01.023},
abstract = {Rough set theory has been proven to be an effective tool for feature selection. To avoid the exponential computation in exhaustive methods, many heuristic feature selection algorithms have been proposed in rough sets. However, these algorithms still suffer from high computational cost. In this paper, we propose a novel heuristic feature selection algorithm (called FSMRDE) in rough sets. To measure the significance of features in FSMRDE, we propose a new model of relative decision entropy, which is an extension of Shannon s information entropy in rough sets. Moreover, to test the effectiveness of FSMRDE, we apply it to intrusion detection and other application domains. Experimental results show that by using the relative decision entropy-based feature significance as heuristic information, FSMRDE is efficient for feature selection. In particular, FSMRDE is able to achieve good scalability for large data sets. HighlightsWe proposed a novel heuristic feature selection algorithm in rough sets.We presented a new information entropy model - relative decision entropy.We proved that relative decision entropy is monotonic with respect to the partial order of partitions.We applied our feature selection algorithm to intrusion detection.The effectiveness of our algorithm was shown on KDD-99 data set and some other data sets.},
journal = {Pattern Recogn.},
month = jul,
pages = {2151–2163},
numpages = {13},
keywords = {Feature selection, Feature significance, Relative decision entropy, Rough sets, Roughness, The degree of dependency}
}

@article{10.1016/j.engappai.2016.10.008,
author = {Moayedikia, Alireza and Ong, Kok-Leong and Boo, Yee Ling and Yeoh, William GS and Jensen, Richard},
title = {Feature selection for high dimensional imbalanced class data using harmony search},
year = {2017},
issue_date = {January 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {57},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2016.10.008},
doi = {10.1016/j.engappai.2016.10.008},
abstract = {Misclassification costs of minority class data in real-world applications can be very high. This is a challenging problem especially when the data is also high in dimensionality because of the increase in overfitting and lower model interpretability. Feature selection is recently a popular way to address this problem by identifying features that best predict a minority class. This paper introduces a novel feature selection method call SYMON which uses symmetrical uncertainty and harmony search. Unlike existing methods, SYMON uses symmetrical uncertainty to weigh features with respect to their dependency to class labels. This helps to identify powerful features in retrieving the least frequent class labels. SYMON also uses harmony search to formulate the feature selection phase as an optimisation problem to select the best possible combination of features. The proposed algorithm is able to deal with situations where a set of features have the same weight, by incorporating two vector tuning operations embedded in the harmony search process. In this paper, SYMON is compared against various benchmark feature selection algorithms that were developed to address the same issue. Our empirical evaluation on different micro-array data sets using G-Mean and AUC measures confirm that SYMON is a comparable or a better solution to current benchmarks.},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
pages = {38–49},
numpages = {12},
keywords = {Feature selection, Harmony search, High-dimensionality, Imbalanced class, Symmetrical uncertainty}
}

@article{10.5555/2946645.3053464,
author = {Lefakis, Leonidas and Fleuret, Fran\c{c}ois},
title = {Jointly informative feature selection made tractable by Gaussian modeling},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {We address the problem of selecting groups of jointly informative, continuous, features in the context of classification and propose several novel criteria for performing this selection. The proposed class of methods is based on combining a Gaussian modeling of the feature responses with derived bounds on and approximations to their mutual information with the class label. Furthermore, specific algorithmic implementations of these criteria are presented which reduce the computational complexity of the proposed feature selection algorithms by up to two-orders of magnitude. Consequently we show that feature selection based on the joint mutual information of features and class label is in fact tractable; this runs contrary to prior works that largely depend on marginal quantities. An empirical evaluation using several types of classifiers on multiple data sets show that this class of methods outperforms state-of-the-art baselines, both in terms of speed and classification accuracy.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {6314–6352},
numpages = {39},
keywords = {entropy, feature selection, mixture of gaussians, mutual information}
}

@article{10.1016/j.jbi.2017.11.005,
author = {Lee, Shin-Jye and Xu, Zhaozhao and Li, Tong and Yang, Yun},
title = {A novel bagging C4.5 algorithm based on wrapper feature selection for supporting wise clinical decision making},
year = {2018},
issue_date = {Feb 2018},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {78},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2017.11.005},
doi = {10.1016/j.jbi.2017.11.005},
journal = {J. of Biomedical Informatics},
month = feb,
pages = {144–155},
numpages = {12},
keywords = {Ensemble learning, Sampling method, Bagging algorithm, C4.5 decision tree, Wrapper feature selection}
}

@inproceedings{10.1007/978-3-030-31605-1_1,
author = {Chen, Shin-Fu and Chakraborty, Goutam and Li, Li-Hua},
title = {Feature Selection on Credit Risk Prediction for Peer-to-Peer Lending},
year = {2018},
isbn = {978-3-030-31604-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31605-1_1},
doi = {10.1007/978-3-030-31605-1_1},
abstract = {Lending plays a key role in economy from early civilization. One of the most important issue in lending business is to measure the risk that the borrower will default or delay in loan payment. This is called credit risk. After Lehman shock in 2008–2009, big banks increased verification for lending operation to reduce risk. As borrowing from established financial institutions is getting harder, social lending also called Peer-to-Peer (P2P) lending, is becoming the popular trend. Because the client information at P2P lending is not sufficient as in traditional financial system, big data and machine learning become the default methods for analyzing credit risk. However, cost of computation and the problem of training the classifier with imbalance data affect the quality of result. This paper proposes a machine learning model with feature selection to measure credit risk of individual borrower on P2P lending. Based on our experimental results, we showed that the credit risk prediction for P2P lending can be improved using Logistic Regression in addition to proper feature selection.},
booktitle = {New Frontiers in Artificial Intelligence: JSAI-IsAI 2018 Workshops, JURISIN, AI-Biz, SKL, LENLS, IDAA, Yokohama, Japan, November 12–14, 2018, Revised Selected Papers},
pages = {5–18},
numpages = {14},
keywords = {P2P lending, Credit risk, Minimum Redundancy Maximum Relevance (mRMR), Least Absolute Shrinkage and Selection Operator (LASSO), Logistic Regression},
location = {Yokohama, Japan}
}

@article{10.5555/3121409.3121410,
author = {Zhang, Baichuan and Mohammed, Noman and Dave, Vachik S. and Al Hasan, Mohammad},
title = {Feature Selection for Classification under Anonymity Constraint},
year = {2017},
issue_date = {April 2017},
publisher = {IIIA-CSIC},
address = {Bellaterra, Catalonia, ESP},
volume = {10},
number = {1},
issn = {1888-5063},
abstract = {Over the last decade, proliferation of various online platforms and their increasing adoption by billions of users have heightened the privacy risk of a user enormously. In fact, security researchers have shown that sparse microdata containing information about online activities of a user although anonymous, can still be used to disclose the identity of the user by cross-referencing the data with other data sources. To preserve the privacy of a user, in existing works several methods (k-anonymity, l-diversity, differential privacy) are proposed for ensuring that a dataset bears small identity disclosure risk. However, the majority of these methods modify the data in isolation, without considering their utility in subsequent knowledge discovery tasks, which makes these datasets less informative. In this work, we consider labeled data that are generally used for classification, and propose two methods for feature selection considering two goals: first, on the reduced feature set the data has small disclosure risk, and second, the utility of the data is preserved for performing a classification task. Experimental results on various real-world datasets show that the method is effective and useful in practice.},
journal = {Trans. Data Privacy},
month = apr,
pages = {1–25},
numpages = {25}
}

@article{10.3233/KES-140293,
author = {Liu, Shuang and Zhao, Qiang and Wu, Xiang},
title = {Feature selection based on partition clustering},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-140293},
doi = {10.3233/KES-140293},
abstract = {Feature selection plays an important role in data mining, machine learning and pattern recognition, especially for large scale data with high dimensions. Many selection techniques have been proposed during past years. Their general purposes are to exploit certain metric to measure the relevance or irrelevance between different features of data for certain task, and then select fewer features without deteriorating discriminative capability. Each technique, however, has not absolutely better performance than others' for all kinds of data, due to the data characterized by incorrectness, incompleteness, inconsistency, and diversity. Based on this fact, this paper put forward to a new scheme based on partition clustering for feature selection, which is a special preprocessing procedure and independent of selection techniques. Experimental results carried out on UCI data sets show that the performance achieved by our proposed scheme is better than selection techniques without using this scheme in most cases.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = apr,
pages = {135–142},
numpages = {8},
keywords = {Clustering, Data Preprocessing, Feature Selection, Methodology, Partition Clustering}
}

@article{10.1016/j.compbiomed.2017.10.004,
author = {Martn-Gonzlez, Sofa and Navarro-Mesa, Juan L. and Juli-Serd, Gabriel and Kraemer, Jan F. and Wessel, Niels and Ravelo-Garca, Antonio G.},
title = {Heart rate variability feature selection in the presence of sleep apnea},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2017.10.004},
doi = {10.1016/j.compbiomed.2017.10.004},
abstract = {We introduce a sleep apnea characterization and classification approach based on a Heart Rate Variability (HRV) feature selection process, thus focusing on the characterization of the underlying process from a cardiac rate point of view. Therefore, we introduce linear and nonlinear variables, namely Cepstrum Coefficients (CC), Filterbanks (Fbank) and Detrended Fluctuation Analysis (DFA). Logistic Regression, Linear Discriminant Analysis and Quadratic Discriminant Analysis were used for classification purposes.The experiments were carried out using two databases. We achieved a per-segment accuracy of 84.76% (sensitivity=81.45%, specificity=86.82%, AUC=0.92) in the Apnea-ECG Physionet database, whereas in the HuGCDN2014 database, provided by the Dr. Negrn University Hospital (Las Palmas de Gran Canaria, Spain), the best results were: accuracy=81.96%, sensitivity=70.95%, specificity=85.47%, AUC=0.87. The former results were comparable or better than those obtained by other methods for the same database in the recent literature.We have concluded that the selected features that best characterize the underlying process are common to both databases. This supports the fact that the conclusions reached are potentially generalizable. The best results were obtained when the three kinds of features were jointly used. Another notable fact is the small number of features needed to describe the phenomenon. Results suggest that the two first Fbanks, the first CC and the first DFA coefficient are the variables that best describe the RR pattern in OSA and, therefore, are especially relevant to extract discriminative information for apnea screening purposes. Furthering the understanding of sleep apnea characterization based on cardiac rate.Novel combination of linear and nonlinear variables for characterization.Potentially generalizable results backed by two different databases.Best results obtained when three types of features are jointly used.Only 3 features to reach comparable or better results than other authors.},
journal = {Comput. Biol. Med.},
month = dec,
pages = {47–58},
numpages = {12},
keywords = {Cepstrum, Detrended Fluctuation Analysis, Feature selection, Filter bank, Heart rate variability, Single-lead ECG, Sleep apnea}
}

@inproceedings{10.1007/978-3-662-44851-9_34,
author = {Paul, Saurabh and Drineas, Petros},
title = {Deterministic Feature Selection for Regularized Least Squares Classification},
year = {2014},
isbn = {978-3-662-44850-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-44851-9_34},
doi = {10.1007/978-3-662-44851-9_34},
abstract = {We introduce a deterministic sampling based feature selection technique for regularized least squares classification. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We perform experiments on synthetic and real-world datasets, namely a subset of TechTC-300 datasets, to support our theory. Experimental results indicate that the proposed method performs better than the existing feature selection methods.},
booktitle = {Machine Learning and Knowledge Discovery in Databases},
pages = {533–548},
numpages = {16},
keywords = {Feature Selection, Sampling, Regularized Least Squares Classification},
location = {Nancy
France}
}

@article{10.1016/j.asoc.2019.105866,
author = {Aladeemy, Mohammed and Adwan, Linda and Booth, Amy and Khasawneh, Mohammad T. and Poranki, Srikanth},
title = {New feature selection methods based on opposition-based learning and self-adaptive cohort intelligence for predicting patient no-shows},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105866},
doi = {10.1016/j.asoc.2019.105866},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {19},
keywords = {Artificial intelligence, Machine learning, Feature selection, Opposition-based learning, Metaheuristic, Cohort intelligence, Health care, Classification}
}

@article{10.1016/j.ins.2013.12.029,
author = {Banerjee, Monami and Pal, Nikhil R.},
title = {Feature selection with SVD entropy: Some modification and extension},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {264},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.12.029},
doi = {10.1016/j.ins.2013.12.029},
abstract = {Many approaches have been developed for dimensionality reduction. These approaches can broadly be categorized into supervised and unsupervised methods. In case of supervised dimensionality reduction, for any input vector the target value is known, which can be a class label also. In a supervised approach, our objective is to select a subset of features that has adequate discriminating power to predict the target value. This target value for an input vector is absent in case of an unsupervised approach. In an unsupervised scheme, we mainly try to find a subset that can capture the inherent ''structure'' of the data, such as the neighborhood relation or the cluster structure. In this work, we first study a Singular Value Decomposition (SVD) based unsupervised feature selection approach proposed by Varshavsky et al. Then we propose a modification of this method to improve its performance. An SVD-entropy based supervised feature selection algorithm is also developed in this paper. Performance evaluation of the algorithms is done on altogether 13 benchmark and one Synthetic data sets. The quality of the selected features is assessed using three indices: Sammon's Error (SE), Cluster Preservation Index (CPI) and MisClassification Error (MCE) using a 1-Nearest Neighbor (1-NN) classifier. Besides showing the improvement of the modified unsupervised scheme over the existing one, we have also made a comparative study of the modified unsupervised and the proposed supervised algorithms with one well-known unsupervised and two popular supervised feature selection methods respectively. Our results reveal the effectiveness of the proposed algorithms in selecting relevant features.},
journal = {Inf. Sci.},
month = apr,
pages = {118–134},
numpages = {17},
keywords = {Entropy, Feature selection, Singular Value Decomposition}
}

@article{10.1016/j.compag.2017.01.007,
author = {Bermejo, Sergio},
title = {Ensembles of wrappers for automated feature selection in fish age classification},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2017.01.007},
doi = {10.1016/j.compag.2017.01.007},
abstract = {A novel feature selection method based on an ensemble of wrappers is proposed.It has been applied for automatically select features in fish age classification.The feature subsets are particularly noticeable given current biological findings.The classification results outperform those based on manual feature selection. In feature selection, the most important features must be chosen so as to decrease the number thereof while retaining their discriminatory information. Within this context, a novel feature selection method based on an ensemble of wrappers is proposed and applied for automatically select features in fish age classification. The effectiveness of this procedure using an Atlantic cod database has been tested for different powerful statistical learning classifiers. The subsets based on few features selected, e.g. otolith weight and fish weight, are particularly noticeable given current biological findings and practices in fishery research and the classification results obtained with them outperforms those of previous studies in which a manual feature selection was performed.},
journal = {Comput. Electron. Agric.},
month = mar,
pages = {27–32},
numpages = {6},
keywords = {Atlantic cod otoliths, Automated fish age classification, Feature selection, Nearest neighbor classifiers, Statistical pattern recognition, Support vector machines}
}

@article{10.1007/s00521-018-3655-2,
author = {Veredas, Francisco J. and Urda, Daniel and Subirats, Jos\'{e} L. and Cant\'{o}n, Francisco R. and Aledo, Juan C.},
title = {Combining feature engineering and feature selection to improve the prediction of methionine oxidation sites in proteins},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3655-2},
doi = {10.1007/s00521-018-3655-2},
abstract = {Methionine is a proteinogenic amino acid that can be post-translationally modified. It is now well established that reactive oxygen species can oxidise methionine residues within living cells. For a long time, it has been thought that such a modification represents merely an inevitable damage derived from aerobic metabolism. However, several authors have begun to contemplate a possible role for this methionine modification in cell signalling. During the last years, a number of proteomic studies have been carried out with the purpose of detecting proteins containing oxidised methionines. Although these proteomic works allow to pinpoint those methionines being oxidised, they are also arduous, expensive and time-consuming. For these reasons, computational approaches aimed at predicting methionine oxidation sites in proteins become an appealing alternative. In the current work, we address methionine oxidation prediction by combining computational intelligence methods with feature engineering and feature selection techniques to improve the efficacy of several machine learning models, while reducing the number of input characteristics needed to get high accuracy rates. We compare random forests, support vector machines, neural networks and flexible discriminant analysis models. Random forests give the best AUC (0.8124±0.0334) and accuracy rates (0.7590±0.0551) by using only a reduced set of 16 characteristics. These results surpass the outcomes of previous works. In addition, we present an end-user script that has been developed to take a protein ID as an input and return a list with the oxidation state of all the methionine residues found in the analysed protein. Finally, to illustrate the applicability of this tool, we have selected the human α1-antitrypsin protein as a case study. This protein was selected because it was not present among the set of proteins used to build up the predictive models but the protein has been well characterised experimentally in terms of methionine oxidation. The prediction returned by our script fully matches the empirical evidence. Out of the nine methionine residues found in this protein, our model predicts the oxidation of only two of them, M351 and M358, which have been reported, on the base of mass spectrometry analyses, to be particularly susceptible to oxidation.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {323–334},
numpages = {12},
keywords = {Protein prediction, Post-translational modification, Methionine oxidation, Predictive computational model}
}

@article{10.1016/j.neucom.2016.09.005,
author = {Daassi-Gnaba, Hela and Oussar, Yacine and Merlan, Maria and Ditchi, Thierry and Gron, Emmanuel and Hol, Stphane},
title = {Wood moisture content prediction using feature selection techniques and a kernel method},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {237},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.005},
doi = {10.1016/j.neucom.2016.09.005},
abstract = {Wood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (MC) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting MC from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the MC of wood. The relationship between the reflection coefficient measurements and the MC is studied. Based upon this relationship, MC predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible. HighlightsThe prediction of moisture content for two wood chips species using the wood dielectric property is studied.Nonlinear models are built to predict the reflection coefficient values from frequencies.Those reflection coefficients are used as input variables of a moisture content predictive model designed using Least Squares Support Vector Machines (LS-SVM) technique and feature selection methods.Numerical experiments using real world data show the effectiveness of the proposed methodology that requires a limited computational power.},
journal = {Neurocomput.},
month = may,
pages = {79–91},
numpages = {13},
keywords = {Feature selection, LS-SVM, Moisture content, Nonlinear regression}
}

@article{10.1177/0165551515613226,
author = {Onan, Aytu\u{g} and Koruko\u{g}lu, Serdar},
title = {A feature selection model based on genetic rank aggregation for text sentiment classification},
year = {2017},
issue_date = {2 2017},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {43},
number = {1},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551515613226},
doi = {10.1177/0165551515613226},
abstract = {Sentiment analysis is an important research direction of natural language processing, text mining and web mining which aims to extract subjective information in source materials. The main challenge encountered in machine learning method-based sentiment classification is the abundant amount of data available. This amount makes it difficult to train the learning algorithms in a feasible time and degrades the classification accuracy of the built model. Hence, feature selection becomes an essential task in developing robust and efficient classification models whilst reducing the training time. In text mining applications, individual filter-based feature selection methods have been widely utilized owing to their simplicity and relatively high performance. This paper presents an ensemble approach for feature selection, which aggregates the several individual feature lists obtained by the different feature selection methods so that a more robust and efficient feature subset can be obtained. In order to aggregate the individual feature lists, a genetic algorithm has been utilized. Experimental evaluations indicated that the proposed aggregation model is an efficient method and it outperforms individual filter-based feature selection methods on sentiment classification.},
journal = {J. Inf. Sci.},
month = feb,
pages = {25–38},
numpages = {14},
keywords = {Feature selection, rank aggregation, sentiment classification}
}

@article{10.1016/j.compbiomed.2016.03.004,
author = {Luo, Jing and Feng, Zuren and Zhang, Jun and Lu, Na},
title = {Dynamic frequency feature selection based approach for classification of motor imageries},
year = {2016},
issue_date = {August 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {75},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.03.004},
doi = {10.1016/j.compbiomed.2016.03.004},
abstract = {Electroencephalography (EEG) is one of the most popular techniques to record the brain activities such as motor imagery, which is of low signal-to-noise ratio and could lead to high classification error. Therefore, selection of the most discriminative features could be crucial to improve the classification performance. However, the traditional feature selection methods employed in brain-computer interface (BCI) field (e.g. Mutual Information-based Best Individual Feature (MIBIF), Mutual Information-based Rough Set Reduction (MIRSR) and cross-validation) mainly focus on the overall performance on all the trials in the training set, and thus may have very poor performance on some specific samples, which is not acceptable. To address this problem, a novel sequential forward feature selection approach called Dynamic Frequency Feature Selection (DFFS) is proposed in this paper. The DFFS method emphasized the importance of the samples that got misclassified while only pursuing high overall classification performance. In the DFFS based classification scheme, the EEG data was first transformed to frequency domain using Wavelet Packet Decomposition (WPD), which is then employed as the candidate set for further discriminatory feature selection. The features are selected one by one in a boosting manner. After one feature being selected, the importance of the correctly classified samples based on the feature will be decreased, which is equivalent to increasing the importance of the misclassified samples. Therefore, a complement feature to the current features could be selected in the next run. The selected features are then fed to a classifier trained by random forest algorithm. Finally, a time series voting-based method is utilized to improve the classification performance. Comparisons between the DFFS-based approach and state-of-art methods on BCI competition IV data set 2b have been conducted, which have shown the superiority of the proposed algorithm.},
journal = {Comput. Biol. Med.},
month = aug,
pages = {45–53},
numpages = {9},
keywords = {Brain computer interface, Classification, Feature selection, Motor imagery}
}

@article{10.1016/j.engappai.2013.08.016,
author = {Liu, Rong and Shi, Yong},
title = {Spatial distance join based feature selection},
year = {2013},
issue_date = {November, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {10},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2013.08.016},
doi = {10.1016/j.engappai.2013.08.016},
abstract = {A Spatial Distance Join (SDJ) based feature selection method (SDJ-FS) is developed to extend the concept of Correlation Fractal Dimension (CFD) to handle both feature relevance and redundancy jointly for supervised feature selection problems. The Pair-count Exponents (PCEs) for the SDJ between different classes and that of the entire dataset (i.e., the CFD of the dataset) are proposed respectively as feature relevance and redundancy measures. For the SDJ-FS method, an efficient divide-count approach of backward elimination property is designed for the calculation of the SDJ based feature quality (relevance and redundancy) measures. The extensive evaluations on both synthetic and benchmark datasets demonstrate the capability of SDJ-FS in identification of feature subsets of high relevance and low redundancy, along with the favorable performance of SDJ-FS over other reference feature selection methods (including those based on CFD). The success of SDJ-FS shows that, SDJ provides a good framework for the extension of CFD to supervised feature selection problems and offers a new view point for feature selection researches.},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
pages = {2597–2607},
numpages = {11},
keywords = {Divide-count, Feature selection, Fractal dimension, Pair-count exponent, Spatial distance join}
}

@inproceedings{10.5555/2999792.2999794,
author = {Maung, Crystal and Schweitzer, Haim},
title = {Pass-efficient unsupervised feature selection},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The goal of unsupervised feature selection is to identify a small number of important features that can represent the data. We propose a new algorithm, a modification of the classical pivoted QR algorithm of Businger and Golub, that requires a small number of passes over the data. The improvements are based on two ideas: keeping track of multiple features in each pass, and skipping calculations that can be shown not to affect the final selection. Our algorithm selects the exact same features as the classical pivoted QR algorithm, and has the same favorable numerical stability. We describe experiments on real-world datasets which sometimes show improvements of several orders of magnitude over the classical algorithm. These results appear to be competitive with recently proposed randomized algorithms in terms of pass efficiency and run time. On the other hand, the randomized algorithms may produce more accurate features, at the cost of small probability of failure.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {1628–1636},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.5555/2969033.2969223,
author = {Zhou, Yingbo and Porwal, Utkarsh and Zhang, Ce and Ngo, Hung and Nguyen, XuanLong and R\'{e}, Christopher and Govindaraju, Venu},
title = {Parallel feature selection inspired by group testing},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {This paper presents a parallel feature selection method for classification that scales up to very high dimensions and large data sizes. Our original method is inspired by group testing theory, under which the feature selection procedure consists of a collection of randomized tests to be performed in parallel. Each test corresponds to a subset of features, for which a scoring function may be applied to measure the relevance of the features in a classification task. We develop a general theory providing sufficient conditions under which true features are guaranteed to be correctly identified. Superior performance of our method is demonstrated on a challenging relation extraction task from a very large data set that have both redundant features and sample size in the order of millions. We present comprehensive comparisons with state-of-the-art feature selection methods on a range of data sets, for which our method exhibits competitive performance in terms of running time and accuracy. Moreover, it also yields substantial speedup when used as a pre-processing step for most other existing methods.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3554–3562},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@inproceedings{10.1145/2970276.2970322,
author = {Meinicke, Jens and Wong, Chu-Pan and K\"{a}stner, Christian and Th\"{u}m, Thomas and Saake, Gunter},
title = {On essential configuration complexity: measuring interactions in highly-configurable systems},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970322},
doi = {10.1145/2970276.2970322},
abstract = {Quality assurance for highly-configurable systems is challenging due to the exponentially growing configuration space. Interactions among multiple options can lead to surprising behaviors, bugs, and security vulnerabilities. Analyzing all configurations systematically might be possible though if most options do not interact or interactions follow specific patterns that can be exploited by analysis tools. To better understand interactions in practice, we analyze program traces to characterize and identify where interactions occur on control flow and data. To this end, we developed a dynamic analysis for Java based on variability-aware execution and monitor executions of multiple small to medium-sized programs. We find that the essential configuration complexity of these programs is indeed much lower than the combinatorial explosion of the configuration space indicates. However, we also discover that the interaction characteristics that allow scalable and complete analyses are more nuanced than what is exploited by existing state-of-the-art quality assurance strategies.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {483–494},
numpages = {12},
keywords = {Configurable Software, Feature Interaction, Variability-Aware Execution},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.5555/3155562.3155625,
author = {Jamshidi, Pooyan and Siegmund, Norbert and Velez, Miguel and K\"{a}stner, Christian and Patel, Akshay and Agarwal, Yuvraj},
title = {Transfer learning for performance modeling of configurable systems: an exploratory analysis},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = {Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.},
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {497–508},
numpages = {12},
keywords = {Performance analysis, transfer learning},
location = {Urbana-Champaign, IL, USA},
series = {ASE '17}
}

@article{10.1016/j.ijar.2013.04.003,
author = {Min, Fan and Hu, Qinghua and Zhu, William},
title = {Feature selection with test cost constraint},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {55},
number = {1},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2013.04.003},
doi = {10.1016/j.ijar.2013.04.003},
abstract = {Feature selection is an important preprocessing step in machine learning and data mining. In real-world applications, costs, including money, time and other resources, are required to acquire the features. In some cases, there is a test cost constraint due to limited resources. We shall deliberately select an informative and cheap feature subset for classification. This paper proposes the feature selection with test cost constraint problem for this issue. The new problem has a simple form while described as a constraint satisfaction problem (CSP). Backtracking is a general algorithm for CSP, and it is efficient in solving the new problem on medium-sized data. As the backtracking algorithm is not scalable to large datasets, a heuristic algorithm is also developed. Experimental results show that the heuristic algorithm can find the optimal solution in most cases. We also redefine some existing feature selection problems in rough sets, especially in decision-theoretic rough sets, from the viewpoint of CSP. These new definitions provide insight to some new research directions.},
journal = {Int. J. Approx. Reasoning},
month = jan,
pages = {167–179},
numpages = {13},
keywords = {Backtracking algorithm, Constraint satisfaction problem, Cost-sensitive learning, Decision-theoretic rough sets, Feature selection, Heuristic algorithm}
}

@inproceedings{10.5555/3466184.3466429,
author = {Vahdat, Kimia and Shashaani, Sara},
title = {Simulation optimization based feature selection, a study on data-driven optimization with input uncertainty},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {In machine learning, removing uninformative or redundant features from a dataset can significantly improve the construction, analysis, and interpretation of the prediction models, especially when the set of collected features is extensive. We approach this challenge with simulation optimization over a high dimensional binary space in place of the classic greedy search in forward or backward selection or regularization methods. We use genetic algorithms to generate scenarios, bootstrapping to estimate the contribution of the intrinsic and extrinsic noise and sampling strategies to expedite the procedure. By including the uncertainty from the input data in the measurement of the estimators' variability, the new framework obtains robustness and efficiency. Our results on a simulated dataset exhibit improvement over state-of-the-art accuracy, interpretability, and reliability. Our proposed framework provides insight for leveraging Monte Carlo methodology in probabilistic data-driven modeling and analysis.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2149–2160},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1007/s10115-015-0901-0,
author = {Benabdeslem, Khalid and Elghazel, Haytham and Hindawi, Mohammed},
title = {Ensemble constrained Laplacian score for efficient and robust semi-supervised feature selection},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-015-0901-0},
doi = {10.1007/s10115-015-0901-0},
abstract = {In this paper, we propose an efficient and robust approach for semi-supervised feature selection, based on the constrained Laplacian score. The main drawback of this method is the choice of the scant supervision information, represented by pairwise constraints. In fact, constraints are proven to have some noise which may deteriorate learning performance. In this work, we try to override any negative effects of constraint set by the variation of their sources. This is achieved by an ensemble technique using both a resampling of data (bagging) and a random subspace strategy. Experiments on high-dimensional datasets are provided for validating the proposed approach and comparing it with other representative feature selection methods.},
journal = {Knowl. Inf. Syst.},
month = dec,
pages = {1161–1185},
numpages = {25},
keywords = {Constraints, Ensemble methods, Feature selection, Semi-supervised context}
}

@article{10.5555/3163585.3163725,
author = {Nakisa, Bahareh and Rastgoo, Mohammad Naim and Tjondronegoro, Dian and Chandran, Vinod},
title = {Evolutionary computation algorithms for feature selection of EEG-based emotion recognition using mobile sensors},
year = {2018},
issue_date = {March 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0957-4174},
abstract = {A review of state-of-the-art feature extraction methods from electroencephalogram signals.A new framework using evolutionary algorithms to find the most optimal features set and channels.Comprehensive experimental results based on two public datasets and one newly collected dataset. There is currently no standard or widely accepted subset of features to effectively classify different emotions based on electroencephalogram (EEG) signals. While combining all possible EEG features may improve the classification performance, it can lead to high dimensionality and worse performance due to redundancy and inefficiency. To solve the high-dimensionality problem, this paper proposes a new framework to automatically search for the optimal subset of EEG features using evolutionary computation (EC) algorithms. The proposed framework has been extensively evaluated using two public datasets (MAHNOB, DEAP) and a new dataset acquired with a mobile EEG sensor. The results confirm that EC algorithms can effectively support feature selection to identify the best EEG features and the best channels to maximize performance over a four-quadrant emotion classification problem. These findings are significant for informing future development of EEG-based emotion classification because low-cost mobile EEG sensors with fewer electrodes are becoming popular for many new applications.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {143–155},
numpages = {13},
keywords = {EEG signals, Emotion classification, Evolutionary computation algorithms, Feature selection}
}

@article{10.1016/j.patcog.2017.05.008,
author = {Golay, Jean and Leuenberger, Michael and Kanevski, Mikhail},
title = {Feature selection for regression problems based on the Morisita estimator of intrinsic dimension},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {70},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.05.008},
doi = {10.1016/j.patcog.2017.05.008},
abstract = {A new supervised filter for regression problems is proposed.The filter uses the newly introduced Morisita estimator of intrinsic dimension.The filter distinguishes between relevant, irrelevant and redundant features.The filter is comprehensively validated using real and simulated datasets.A generic methodology for validating and comparing filters is suggested. Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results, and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed.},
journal = {Pattern Recogn.},
month = oct,
pages = {126–138},
numpages = {13},
keywords = {Data mining, Feature selection, Intrinsic dimension, Measure of relevance, Morisita index}
}

@inproceedings{10.1007/978-3-030-29911-8_12,
author = {Yamada, Shinichi and Neshatian, Kourosh},
title = {Comparison of Embedded and Wrapper Approaches for Feature Selection in Support Vector Machines},
year = {2019},
isbn = {978-3-030-29910-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29911-8_12},
doi = {10.1007/978-3-030-29911-8_12},
abstract = {Feature selection methods are generally divided into three categories: filter, wrapper and embedded approaches. In terms of learning performance, the filter approach is typically inferior compared to the other two because it does not use the target learning algorithm. The embedded and wrapper approaches are both considered high-performing. In this paper we compare the embedded and the wrapper approaches in the context of Support Vector Machines (SVMs). In the wrapper category, we compare well-known algorithms such as Genetic Algorithm (GA), Forward and Backward selection, and a new binary Particle Swarm Optimization (PSO) algorithm. For an embedded approach we devise a new heuristic algorithm based on Multiple Kernel Learning.},
booktitle = {PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26–30, 2019, Proceedings, Part II},
pages = {149–161},
numpages = {13},
keywords = {Binary Particle Swarm Optimization, Genetic Algorithm, Multiple Kernel Learning, Support Vector Machine},
location = {Cuvu, Yanuka Island, Fiji}
}

@article{10.1016/j.eswa.2018.01.046,
author = {Rejer, Izabela and Twardochleb, Michal},
title = {Gamers involvement detection from EEG data with cGAAM A method for feature selection for clustering},
year = {2018},
issue_date = {July 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.01.046},
doi = {10.1016/j.eswa.2018.01.046},
abstract = {A genetic algorithm controlled by unsupervised classification was introduced.A joined-approach for clustering and feature selection was proposed.Three EEG features differentiating levels of players involvement were identified.The predominance of the proposed approach over other methods was shown. This paper reports the results of an experiment to identify EEG patterns specific to different levels of player involvement when playing a video game. To obtain unbiased results, we based our patterns on both raw EEG data and expert knowledge. We used a three-step procedure to identify patterns. First, we looked for clusters in the reduced feature space extracted from EEG data. Next, we assigned experts interpretations to the clusters. Finally, we analysed relations between features used to form the clusters and the class labels provided by experts. The most challenging part of the procedure was feature selection simultaneous with unsupervised classification. To accomplish this task, we developed a new approach for simultaneous feature selection and clustering based on modified GAAM (genetic algorithm with aggressive mutation). When the cGAAM algorithm was applied to EEG data, it returned the feature subsets that (a) were highly consistent across subjects and (b) provided 50% more compact clusters than clusters built over the feature subsets returned by a forward selection search strategy.The main cognitive outcome of EEG signal analysis was a set of patterns differentiating players involvement in a game. Conclusions included: 1. For a majority of subjects the most discriminative features were activity in the theta band in the left and right frontal areas, and activity in the delta band in the left frontal area; 2. All three features significantly differentiated between low and high, or medium and high engagement; 3. All subjects showed positive correlations between selected feature values and levels of engagement.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {196–204},
numpages = {9},
keywords = {Brain activity patterns, Clustering, EEG signal analysis, Feature selection, Game player involvement recognition, Genetic algorithm}
}

@article{10.1145/3191747,
author = {Kamminga, Jacob W. and Le, Duc V. and Meijers, Jan Pieter and Bisby, Helena and Meratnia, Nirvana and Havinga, Paul J.M.},
title = {Robust Sensor-Orientation-Independent Feature Selection for Animal Activity Recognition on Collar Tags},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191747},
doi = {10.1145/3191747},
abstract = {Fundamental challenges faced by real-time animal activity recognition include variation in motion data due to changing sensor orientations, numerous features, and energy and processing constraints of animal tags. This paper aims at finding small optimal feature sets that are lightweight and robust to the sensor's orientation. Our approach comprises four main steps. First, 3D feature vectors are selected since they are theoretically independent of orientation. Second, the least interesting features are suppressed to speed up computation and increase robustness against overfitting. Third, the features are further selected through an embedded method, which selects features through simultaneous feature selection and classification. Finally, feature sets are optimized through 10-fold cross-validation. We collected real-world data through multiple sensors around the neck of five goats. The results show that activities can be accurately recognized using only accelerometer data and a few lightweight features. Additionally, we show that the performance is robust to sensor orientation and position. A simple Naive Bayes classifier using only a single feature achieved an accuracy of 94 % with our empirical dataset. Moreover, our optimal feature set yielded an average of 94 % accuracy when applied with six other classifiers. This work supports embedded, real-time, energy-efficient, and robust activity recognition for animals.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = mar,
articleno = {15},
numpages = {27},
keywords = {Animal Activity Recognition, Decision Tree, Embedded Systems, Machine Learning, Naive Bayes, Sensor Orientation}
}

@article{10.1155/2020/8864315,
author = {Chu, Xianghua and Li, Shuxiang and Gao, Da and Zhao, Wei and Cui, Jianshuang and Huang, Linya and Yang, Zhile},
title = {A Binary Superior Tracking Artificial Bee Colony with Dynamic Cauchy Mutation for Feature Selection},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/8864315},
doi = {10.1155/2020/8864315},
abstract = {This paper aims to propose an improved learning algorithm for feature selection, termed as binary superior tracking artificial bee colony with dynamic Cauchy mutation (BSTABC-DCM). To enhance exploitation capacity, a binary learning strategy is proposed to enable each bee to learn from the superior individuals in each dimension. A dynamic Cauchy mutation is introduced to diversify the population distribution. Ten datasets from UCI repository are adopted as test problems, and the average results of cross-validation of BSTABC-DCM are compared with other seven popular swarm intelligence metaheuristics. Experimental results demonstrate that BSTABC-DCM could obtain the optimal classification accuracy and select the best representative features for the UCI problems.},
journal = {Complex.},
month = jan,
numpages = {13}
}

@inproceedings{10.1007/978-3-319-92639-1_6,
author = {Martinez-de-Pison, F. J. and Gonzalez-Sendino, R. and Ferreiro, J. and Fraile, E. and Pernia-Espinoza, A.},
title = {GAparsimony: An R Package for Searching Parsimonious Models by Combining Hyperparameter Optimization and Feature Selection},
year = {2018},
isbn = {978-3-319-92638-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-92639-1_6},
doi = {10.1007/978-3-319-92639-1_6},
abstract = {Nowadays, there is an increasing interest in automating KDD processes. Thanks to the increasing power and costs reduction of computation devices, the search of best features and model parameters can be solved with different meta-heuristics. Thus, researchers can be focused in other important tasks like data wrangling or feature engineering. In this contribution, GAparsimony R package is presented. This library implements GA-PARSIMONY methodology that has been published in previous journals and HAIS conferences. The objective of this paper is to show how to use GAparsimony for searching accurate parsimonious models by combining feature selection, hyperparameter optimization, and parsimonious model search. Therefore, this paper covers the cautions and considerations required for finding a robust parsimonious model by using this package and with a regression example that can be easily adapted for another problem, database or algorithm.},
booktitle = {Hybrid Artificial Intelligent Systems: 13th International Conference, HAIS 2018, Oviedo, Spain, June 20-22, 2018, Proceedings},
pages = {62–73},
numpages = {12},
keywords = {GA-PARSIMONY, Hyperparameter optimization, Feature selection, Parsimonious model, Genetic algorithms},
location = {Oviedo, Spain}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Bug prediction, Feature selection, Software Quality},
location = {Bangalore, India},
series = {ISEC '15}
}

@article{10.1016/j.compbiomed.2013.11.019,
author = {Zhang, Zhancheng and Dong, Jun and Luo, Xiaoqing and Choi, Kup-Sze and Wu, Xiaojun},
title = {Heartbeat classification using disease-specific feature selection},
year = {2014},
issue_date = {March, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {46},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2013.11.019},
doi = {10.1016/j.compbiomed.2013.11.019},
abstract = {Automatic heartbeat classification is an important technique to assist doctors to identify ectopic heartbeats in long-term Holter recording. In this paper, we introduce a novel disease-specific feature selection method which consists of a one-versus-one (OvO) features ranking stage and a feature search stage wrapped in the same OvO-rule support vector machine (SVM) binary classifier. The proposed method differs from traditional approaches in that it focuses on the selection of effective feature subsets for distinguishing a class from others by making OvO comparison. The electrocardiograms (ECG) from the MIT-BIH arrhythmia database (MIT-BIH-AR) are used to evaluate the proposed feature selection method. The ECG features adopted include inter-beat and intra-beat intervals, amplitude morphology, area morphology and morphological distance. Following the recommendation of the Advancement of Medical Instrumentation (AAMI), all the heartbeat samples of MIT-BIH-AR are grouped into four classes, namely, normal or bundle branch block (N), supraventricular ectopic (S), ventricular ectopic (V) and fusion of ventricular and normal (F). The division of training and testing data complies with the inter-patient schema. Experimental results show that the average classification accuracy of the proposed feature selection method is 86.66%, outperforming those methods without feature selection. The sensitivities for the classes N, S, V and F are 88.94%, 79.06%, 85.48% and 93.81% respectively, and the corresponding positive predictive values are 98.98%, 35.98%, 92.75% and 13.74% respectively. In terms of geometric means of sensitivity and positive predictivity, the proposed method also demonstrates better performance than other state-of-the-art feature selection methods.},
journal = {Comput. Biol. Med.},
month = mar,
pages = {79–89},
numpages = {11},
keywords = {Disease specific, Feature selection, Heartbeat classification, Support vector machine}
}

@inproceedings{10.1145/2818869.2818896,
author = {Zhao, Long and Wang, Shuliang and Lin, Yi},
title = {A new Feature Selection method for face recognition based on general data field},
year = {2015},
isbn = {9781450337359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818869.2818896},
doi = {10.1145/2818869.2818896},
abstract = {Feature selection is an important step when building a classifier for face recognition. It is difficult to classify the high dimensional and small sample data sets such as face data sets pose. Because the high dimensions increase the risk of over fitting and the small samples decrease the accuracy. A new feature selection method for face recognition based on general data field is proposed in this paper. This method adopts the Sw (potential value within class) and Sb (potential value between different classes) to calculate the information entropy of each feature. The representative features have been selected to structure classifier. Well known feature selection techniques for face data sets are implemented and compared with our present method to show its effectiveness. The experiments show that our algorithm effectively reduces the dimensionality of face data sets and keeps the classifier performance.},
booktitle = {Proceedings of the ASE BigData &amp; SocialInformatics 2015},
articleno = {26},
numpages = {5},
keywords = {Face recognition, feature selection, general data field, information entropy, potential value},
location = {Kaohsiung, Taiwan},
series = {ASE BD&amp;SI '15}
}

@article{10.1007/s00521-015-1924-x,
author = {Bacciu, Davide},
title = {Unsupervised feature selection for sensor time-series in pervasive computing applications},
year = {2016},
issue_date = {July      2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {5},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-015-1924-x},
doi = {10.1007/s00521-015-1924-x},
abstract = {The paper introduces an efficient feature selection approach for multivariate time-series of heterogeneous sensor data within a pervasive computing scenario. An iterative filtering procedure is devised to reduce information redundancy measured in terms of time-series cross-correlation. The algorithm is capable of identifying nonredundant sensor sources in an unsupervised fashion even in presence of a large proportion of noisy features. In particular, the proposed feature selection process does not require expert intervention to determine the number of selected features, which is a key advancement with respect to time-series filters in the literature. The characteristic of the prosed algorithm allows enriching learning systems, in pervasive computing applications, with a fully automatized feature selection mechanism which can be triggered and performed at run time during system operation.
 A comparative experimental analysis on real-world data from three pervasive computing applications is provided, showing that the algorithm addresses major limitations of unsupervised filters in the literature when dealing with sensor time-series. Specifically, it is presented an assessment both in terms of reduction of time-series redundancy and in terms of preservation of informative features with respect to associated supervised learning tasks.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {1077–1091},
numpages = {15},
keywords = {Echo state networks, Feature selection, Multivariate time-series, Pervasive computing, Wireless sensor networks}
}

@article{10.1016/j.knosys.2015.11.010,
title = {Cost-sensitive feature selection using random forest},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {95},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.11.010},
doi = {10.1016/j.knosys.2015.11.010},
abstract = {Feature selection aims to select a small subset of informative features that contain most of the information related to a given task. Existing feature selection methods often assume that all the features have the same cost. However, in many real world applications, different features may have different costs (e.g., different tests a patient might take in medical diagnosis). Ignoring the feature cost may produce good feature subsets in theory but they can not be used in practice. In this paper, we propose a random forest-based feature selection algorithm that incorporates the feature cost into the base decision tree construction process to produce low-cost feature subsets. In particular, when constructing a base tree, a feature is randomly selected with a probability inversely proportional to its associated cost. We evaluate the proposed method on a number of UCI datasets and apply it to a medical diagnosis problem where the real feature costs are estimated by experts. The experimental results demonstrate that our feature-cost-sensitive random forest (FCS-RF) is able to select a low-cost subset of informative features and achieves better performance than other state-of-art feature selection methods in real-world problems.},
journal = {Know.-Based Syst.},
month = mar,
pages = {1–11},
numpages = {11}
}

@article{10.1007/s00530-014-0390-0,
author = {Song, Xiaonan and Zhang, Jianguang and Han, Yahong and Jiang, Jianmin},
title = {Semi-supervised feature selection via hierarchical regression for web image classification},
year = {2016},
issue_date = {February  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-014-0390-0},
doi = {10.1007/s00530-014-0390-0},
abstract = {Feature selection is an important step for large-scale image data analysis, which has been proved to be difficult due to large size in both dimensions and samples. Feature selection firstly eliminates redundant and irrelevant features and then chooses a subset of features that performs as efficient as the complete set. Generally, supervised feature selection yields better performance than unsupervised feature selection because of the utilization of labeled information. However, labeled data samples are always expensive to obtain, which constraints the performance of supervised feature selection, especially for the large web image datasets. In this paper, we propose a semi-supervised feature selection algorithm that is based on a hierarchical regression model. Our contribution can be highlighted as: (1) Our algorithm utilizes a statistical approach to exploit both labeled and unlabeled data, which preserves the manifold structure of each feature type. (2) The predicted label matrix of the training data and the feature selection matrix are learned simultaneously, making the two aspects mutually benefited. Extensive experiments are performed on three large-scale image datasets. Experimental results demonstrate the better performance of our algorithm, compared with the state-of-the-art algorithms.},
journal = {Multimedia Syst.},
month = feb,
pages = {41–49},
numpages = {9},
keywords = {Feature selection, Multi-class classification, Semi-supervised learning}
}

@article{10.1016/j.ins.2021.06.096,
author = {Jain, Rahi and Xu, Wei},
title = {RHDSI: A novel dimensionality reduction based algorithm on high dimensional feature selection with interactions},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {574},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.06.096},
doi = {10.1016/j.ins.2021.06.096},
journal = {Inf. Sci.},
month = oct,
pages = {590–605},
numpages = {16},
keywords = {Interaction terms, High-dimensional data, Feature selection, Dimensionality reduction, Machine learning, Regression}
}

@article{10.1007/s10044-015-0524-9,
author = {Settouti, Nesma and Chikh, Mohamed Amine and Barra, Vincent},
title = {A new feature selection approach based on ensemble methods in semi-supervised classification},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-015-0524-9},
doi = {10.1007/s10044-015-0524-9},
abstract = {In computer aided medical system, many practical classification applications are confronted to the massive multiplication of collection and storage of data, this is especially the case in areas such as the prediction of medical test efficiency, the classification of tumors and the detection of cancers. Data with known class labels (labeled data) can be limited but unlabeled data (with unknown class labels) are more readily available. Semi-supervised learning deals with methods for exploiting the unlabeled data in addition to the labeled data to improve performance on the classification task. In this paper, we consider the problem of using a large amount of unlabeled data to improve the efficiency of feature selection in large dimensional datasets, when only a small set of labeled examples is available. We propose a new semi-supervised feature evaluation method called Optimized co-Forest for Feature Selection (OFFS) that combines ideas from co-forest and the embedded principle of selecting in Random Forest based by the permutation of out-of-bag set. We provide empirical results on several medical and biological benchmark datasets, indicating an overall significant improvement of OFFS compared to four other feature selection approaches using filter, wrapper and embedded manner in semi-supervised learning. Our method proves its ability and effectiveness to select and measure importance to improve the performance of the hypothesis learned with a small amount of labeled samples by exploiting unlabeled samples.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {673–686},
numpages = {14},
keywords = {Co-forest, Ensemble methods, Feature selection, Large datasets, Medical diagnosis, Random Forest, Semi-supervised learning}
}

@inproceedings{10.1145/2801081.2801091,
author = {Emary, E. and Zawbaa, Hossam M. and Ghany, Kareem Kamal A. and Hassanien, Aboul Ella and Parv, B.},
title = {Firefly Optimization Algorithm for Feature Selection},
year = {2015},
isbn = {9781450333351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801081.2801091},
doi = {10.1145/2801081.2801091},
abstract = {In this paper, a system for feature selection based on firefly algorithm (FFA) optimization is proposed. Data sets ordinarily includes a huge number of attributes, with irrelevant and redundant attributes. Redundant and irrelevant attributes might reduce the classification accuracy because of the large search space. The main goal of attribute reduction is to choose a subset of relevant attributes from a huge number of available attributes to obtain comparable or even better classification accuracy from using all attributes. A system for feature selection is proposed in this paper using a modified version of the firefly algorithm (FFA) optimization. The modified FFA algorithm adaptively balance the exploration and exploitation to quickly find the optimal solution. FFA is a new evolutionary computation technique, inspired by the flash lighting process of fireflies. The FFA can quickly search the feature space for optimal or near-optimal feature subset minimizing a given fitness function. The proposed fitness function used incorporate both classification accuracy and feature reduction size. The proposed system was tested on eighteen data sets and proves advance over other search methods as particle swarm optimization (PSO) and genetic algorithm (GA) optimizers commonly used in this context using different evaluation indicators.},
booktitle = {Proceedings of the 7th Balkan Conference on Informatics Conference},
articleno = {26},
numpages = {7},
location = {Craiova, Romania},
series = {BCI '15}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.cmpb.2016.07.029,
author = {Shrivastava, Prashant and Shukla, Anupam and Vepakomma, Praneeth and Bhansali, Neera and Verma, Kshitij},
title = {A survey of nature-inspired algorithms for feature selection to identify Parkinson's disease},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.07.029},
doi = {10.1016/j.cmpb.2016.07.029},
abstract = {We perform a comparative analysis of nature inspired-algorithms for feature selection to aid the classification of affected Parkinson's patients from the rest.Feature selection was applied to datasets of gait and speech of Parkinson's patients.Binary Bat Algorithm outperformed traditional techniques like Particle Swarm Optimization (PSO), Genetic Algorithm and Modified Cuckoo Search Algorithm. Background and ObjectivesParkinson's disease is a chronic neurological disorder that directly affects human gait. It leads to slowness of movement, causes muscle rigidity and tremors. Analyzing human gait serves to be useful in studies aiming at early recognition of the disease. In this paper we perform a comparative analysis of various nature inspired algorithms to select optimal features/variables required for aiding in the classification of affected patients from the rest. MethodsFor the experiments, we use a real life dataset of 166 people containing both healthy controls and affected people. Following the optimal feature selection process, the dataset is then classified using a neural network. Results and ConclusionsThe experimental results show Binary Bat Algorithm outperformed traditional techniques like Particle Swarm Optimization (PSO), Genetic Algorithm and Modified Cuckoo Search Algorithm with a competitive recognition rate on the dataset of selected features. We compare this through different criteria like cross-validated accuracies, true positive rates, false positive rates, positive predicted values and negative predicted values.},
journal = {Comput. Methods Prog. Biomed.},
month = feb,
pages = {171–179},
numpages = {9},
keywords = {Bat algorithm, Feature selection, Gait, Parkinson's}
}

@article{10.1007/s11761-016-0202-9,
author = {Kumar, Lov and Krishna, Aneesh and Rath, Santanu Ku.},
title = {The impact of feature selection on maintainability prediction of service-oriented applications},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {2},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-016-0202-9},
doi = {10.1007/s11761-016-0202-9},
abstract = {Service-oriented development methodologies are very often considered for distributed system development. The quality of service-oriented computing can be best assessed by the use of software metrics that are considered to design the prediction model. Feature selection technique is a process of selecting a subset of features that may lead to build improved prediction models. Feature selection techniques can be broadly classified into two subclasses such as feature ranking and feature subset selection technique. In this study, eight different types of feature ranking and four different types of feature subset selection techniques have been considered for improving the performance of a prediction model focusing on maintainability criterion. The performance of these feature selection techniques is evaluated using support vector machine with different types of kernels over a case study, i.e., five different versions of eBay Web service. The performances are measured using accuracy and F-measure value. The results show that maintainability of the service-oriented computing paradigm can be predicted by using object-oriented metrics. The results also show that it is possible to find a small subset of object-oriented metrics which helps to predict maintainability with higher accuracy and also reduces the value of misclassification errors.},
journal = {Serv. Oriented Comput. Appl.},
month = jun,
pages = {137–161},
numpages = {25},
keywords = {Feature selection techniques, Kernel function, Maintainability, Object-oriented metrics, SVM, Service-oriented computing}
}

@article{10.1016/j.procs.2018.05.195,
author = {Nagpal, Arpita and Singh, Vijendra},
title = {A Feature Selection Algorithm Based on Qualitative Mutual Information for Cancer Microarray Data},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.195},
doi = {10.1016/j.procs.2018.05.195},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {244–252},
numpages = {9},
keywords = {Feature Selection, Mutual Information, Random Forest, Microarray}
}

@inproceedings{10.5555/3298483.3298564,
author = {Liu, Meng and Xu, Chang and Luo, Yong and Xu, Chao and Wen, Yonggang and Tao, Dacheng},
title = {Cost-sensitive feature selection via F-measure optimization reduction},
year = {2017},
publisher = {AAAI Press},
abstract = {Feature selection aims to select a small subset from the high-dimensional features which can lead to better learning performance, lower computational complexity, and better model readability. The class imbalance problem has been neglected by traditional feature selection methods, therefore the selected features will be biased towards the majority classes. Because of the superiority of F-measure to accuracy for imbalanced data, we propose to use F-measure as the performance measure for feature selection algorithms. As a pseudo-linear function, the optimization of F-measure can be achieved by minimizing the total costs. In this paper, we present a novel cost-sensitive feature selection (CSFS) method which optimizes F-measure instead of accuracy to take class imbalance issue into account. The features will be selected according to optimal F-measure classifier after solving a series of cost-sensitive feature selection sub-problems. The features selected by our method will fully represent the characteristics of not only majority classes, but also minority classes. Extensive experimental results conducted on synthetic, multi-class and multi-label datasets validate the efficiency and significance of our feature selection method.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {2252–2258},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {configurable systems, mapping study, safety, security, software product line engineering},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.neucom.2017.02.025,
author = {de Assis Boldt, Francisco and Rauber, Thomas W. and Varejo, Flvio M.},
title = {Cascade Feature Selection and ELM for automatic fault diagnosis of the Tennessee Eastman process},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {239},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.025},
doi = {10.1016/j.neucom.2017.02.025},
abstract = {This work presents the concept of Cascade Feature Selection to combine feature selection methods. Fast and weak methods, like ranking, are placed on the top of the cascade to reduce the dimensionality of the initial feature set. Thus, strong and computationally demanding methods, placed on the bottom of the cascade, have to deal with less features. Three cascade combinations are tested with the Extreme Learning Machine as the underlying classification architecture. The Tennessee Eastman chemical process simulation software and one high-dimensional data set are used as sources of the benchmark data. Experimental results suggest that the cascade arrangement can produce smaller final feature subsets, expending less time, with higher classification performances than a feature selection based on a Genetic Algorithm. Many works in the literature have proposed mixed methods with specific combination strategies. The main contribution of this work is a concept able to combine any existent method using a single strategy. Provided that the Cascade Feature Selection requirements are fulfilled, the combinations might reduce the time to select features or increase the classification performance of the classifiers trained with the selected features.},
journal = {Neurocomput.},
month = may,
pages = {238–248},
numpages = {11},
keywords = {Extreme Learning Machine, Fault diagnosis, Feature selection, Tennessee Eastman process}
}

@inproceedings{10.1145/2833258.2833262,
author = {Le, Huong Thanh and Van Tran, Luan and Nguyen, Xuan Hoai and Nguyen, Thi Hien},
title = {Optimizing Genetic Algorithm in Feature Selection for Named Entity Recognition},
year = {2015},
isbn = {9781450338431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833258.2833262},
doi = {10.1145/2833258.2833262},
abstract = {This paper proposes some strategies to reduce the running time of genetic algorithms used in a feature selection task for the problem of named entity recognition. They include: (i) reduction of population size during the evolution process of the genetic algorithm; (ii) parallelization of the fitness computation; and (iii) use of progressive sampling for calculating the optimal sample size of the training data. Maximum Entropy algorithm is then used, as a test classifier, to compute the accuracy of the named entity recognition system with the reduced feature sets identified by the genetic algorithm. Experimental results show that our improved genetic algorithm run three time faster than the standard genetic algorithm, while the accuracy of the named entity recognition system (using Maximum Entropy) on the induced feature subset does not decrease. In addition, the feature subset induced by our improved genetic algorithm is much smaller than the original feature set and has helped Maximum Entropy to achieve higher accuracy than the original one.},
booktitle = {Proceedings of the 6th International Symposium on Information and Communication Technology},
pages = {11–16},
numpages = {6},
keywords = {Feature Selection, Genetic Algorithm, Maximum Entropy, Named Entity Recognition, Progressive Sampling},
location = {Hue City, Viet Nam},
series = {SoICT '15}
}

@inproceedings{10.1145/2801948.2802026,
author = {Koumpouri, Athanasia and Mporas, Iosif and Megalooikonomou, Vasileios},
title = {Feature selection for improving opinion identification from web authors' posts},
year = {2015},
isbn = {9781450335515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801948.2802026},
doi = {10.1145/2801948.2802026},
abstract = {In the present article, we address the problem of automatic opinion identification of web users from movie reviews. Specifically, relying on six well-known machine learning algorithms, we investigate the effectiveness of feature selection in the improvement of the accuracy of opinion identification. The feature ranking is performed over a set of statistical, part-of-speech tagging and language model based features. In the experiments, we employed classification models based on decision trees, support vector machines and lazy-learning algorithms. The experimental evaluation performed on the publicly available Polarity Dataset v2.0 demonstrated that feature selection significantly improves the accuracy of opinion identification regardless of the type of machine learning algorithm used.},
booktitle = {Proceedings of the 19th Panhellenic Conference on Informatics},
pages = {117–122},
numpages = {6},
keywords = {feature ranking, feature selection, opinion classification, opinion mining, sentiment analysis},
location = {Athens, Greece},
series = {PCI '15}
}

@article{10.1016/j.asoc.2015.07.046,
author = {P\'{e}rez-Rodr\'{\i}guez, Javier and Arroyo-Pe\~{n}a, Alexis Germ\'{a}n and Garc\'{\i}a-Pedrajas, Nicol\'{a}s},
title = {Simultaneous instance and feature selection and weighting using evolutionary computation},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.07.046},
doi = {10.1016/j.asoc.2015.07.046},
abstract = {HighlightsThe paper presents a framework for simultaneous instance and feature selection and weighting.An extensive comparison of all the possible combinations is carried out.The proposal is also studied in class-imbalanced datasets. Current research is constantly producing an enormous amount of information, which presents a challenge for data mining algorithms. Many of the problems in some of the most relevant research areas, such as bioinformatics, security and intrusion detection or text mining, involve large or huge datasets. Data mining algorithms are seriously challenged by these datasets. One of the most common methods to handle large datasets is data reduction. Among others, feature and instance selection are arguably the most commonly used methods for data reduction. Conversely, feature and instance weighting focus on improving the performance of the data mining task.Due to the different aims of these four methods, instance and feature selection and weighting, they can be combined to improve the performance of the data mining methods used. In this paper, a general framework for combining these four tasks is presented, and a comprehensive study of the usefulness of the 15 possible combinations is performed.Using a large set of 80 problems, a study of the behavior of all possible combinations in classification performance, data reduction and execution time is carried out. These factors are also studied using 60 class-imbalanced datasets.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {416–443},
numpages = {28},
keywords = {Class-imbalanced datasets, Feature selection, Feature weighting, Instance selection, Instance weighting}
}

@article{10.1016/j.procs.2017.08.227,
author = {Trabelsi, Marwa and Meddouri, Nida and Maddouri, Mondher},
title = {A New Feature Selection Method for Nominal Classifier based on Formal Concept Analysis},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.227},
doi = {10.1016/j.procs.2017.08.227},
abstract = {The high dimension of data makes difficult to train and test many classification methods. This work aims to present a new filter Feature Selection Method, called H-Ratio, which can identify pertinent features from data. This method improves results of two previous works focusing on nominal classifiers based on Formals Concepts Analysis. The evaluation of H-Ratio shows that this method performs nominal classifiers processing. Our method has an error rate of 5% (~7% relative improvement over a supervised classification method).},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {186–194},
numpages = {9},
keywords = {Feature Selection Methods, Formal Concept Analysis, Nominal Classifiers, Supervised Classification}
}

@article{10.1016/j.eswa.2013.09.047,
author = {Lin, Fengyi and Liang, Deron and Yeh, Ching-Chiang and Huang, Jui-Chieh},
title = {Novel feature selection methods to financial distress prediction},
year = {2014},
issue_date = {April, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.09.047},
doi = {10.1016/j.eswa.2013.09.047},
abstract = {Financially distressed prediction (FDP) has been a widely and continually studied topic in the field of corporate finance. One of the core problems to FDP is to design effective feature selection algorithms. In contrast to existing approaches, we propose an integrated approach to feature selection for the FDP problem that embeds expert knowledge with the wrapper method. The financial features are categorized into seven classes according to their financial semantics based on experts' domain knowledge surveyed from literature. We then apply the wrapper method to search for ''good'' feature subsets consisting of top candidates from each feature class. For concept verification, we compare several scholars' models as well as leading feature selection methods with the proposed method. Our empirical experiment indicates that the prediction model based on the feature set selected by the proposed method outperforms those models based on traditional feature selection methods in terms of prediction accuracy.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2472–2483},
numpages = {12},
keywords = {Feature selection, Financial distress prediction, Genetic algorithm, Integrated prediction model, Wrappers}
}

@inproceedings{10.1109/ICSE43902.2021.00100,
author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00100},
doi = {10.1109/ICSE43902.2021.00100},
abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1072–1084},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1162/NECO_a_00816,
author = {Paul, Saurabh and Drineas, Petros},
title = {Feature selection for ridge regression with provable guarantees},
year = {2016},
issue_date = {April 2016},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {28},
number = {4},
issn = {0899-7667},
url = {https://doi.org/10.1162/NECO_a_00816},
doi = {10.1162/NECO_a_00816},
abstract = {We introduce single-set spectral sparsification as a deterministic sampling-based feature selection technique for regularized least-squares classification, which is the classification analog to ridge regression. The method is unsupervised and gives worst-case guarantees of the generalization power of the classification function after feature selection with respect to the classification function obtained using all features. We also introduce leverage-score sampling as an unsupervised randomized feature selection method for ridge regression. We provide risk bounds for both single-set spectral sparsification and leverage-score sampling on ridge regression in the fixed design setting and show that the risk in the sampled space is comparable to the risk in the full-feature space. We perform experiments on synthetic and real-world data sets; a subset of TechTC-300 data sets, to support our theory. Experimental results indicate that the proposed methods perform better than the existing feature selection methods.},
journal = {Neural Comput.},
month = apr,
pages = {716–742},
numpages = {27}
}

@article{10.3233/IDA-160186,
author = {Maldonado, Sebasti\'{a}n and Armelini, Guillermo and Guevara, C. Angelo},
title = {Assessing university enrollment and admission efforts via hierarchical classification and feature selection},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {4},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-160186},
doi = {10.3233/IDA-160186},
abstract = {Recruiting prospective students efficiently and effectively is a very important challenge for universities, mainly because of the increasing competition and the relevance of enrollment-generated revenues. This work provides an intelligent system for modeling the student enrollment decisions problem. A nested logit classifier was constructed to predict which prospective students will eventually enroll in different Bachelor degree programs of a small-sized, private Chilean university. Feature selection is performed to identify the key features that influence the student decisions, such as socio-demographic variables (gender, age, school type, among others), admission efforts, and admission test results. Our results suggest that on-campus activities are far more productive than career fairs and other efforts performed off campus, demonstrating the importance of bringing prospective students to the university. Furthermore, variables such as gender, school type, and declared university and Bachelor degree program preferences are shown to be relevant in successfully modeling the student’s choice of university.},
journal = {Intell. Data Anal.},
month = jan,
pages = {945–962},
numpages = {18},
keywords = {Hierarchical classification, university enrollment, feature selection, analytics, nested logit}
}

@article{10.1109/TCBB.2021.3066597,
author = {Dey, Lopamudra and Mukhopadhyay, Anirban},
title = {Compact Genetic Algorithm-Based Feature Selection for Sequence-Based Prediction of Dengue–Human Protein Interactions},
year = {2021},
issue_date = {July-Aug. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3066597},
doi = {10.1109/TCBB.2021.3066597},
abstract = {Dengue Virus (DENV) infection is one of the rapidly spreading mosquito-borne viral infections in humans. Every year, around 50 million people get affected by DENV infection, resulting in 20,000 deaths. Despite the recent experiments focusing on dengue infection to understand its functionality in the human body, several functionally important DENV-human protein-protein interactions (PPIs) have remained unrecognized. This article presents a model for predicting new DENV-human PPIs by combining different sequence-based features of human and dengue proteins like the amino acid composition, dipeptide composition, conjoint triad, pseudo amino acid composition, and pairwise sequence similarity between dengue and human proteins. A Learning vector quantization (LVQ)-based Compact Genetic Algorithm (CGA) model is proposed for feature subset selection. CGA is a probabilistic technique that simulates the behavior of a Genetic Algorithm (GA) with lesser memory and time requirements. Prediction of DENV-human PPIs is performed by the weighted Random Forest (RF) technique as it is found to perform better than other classifiers. We have predicted 1013 PPIs between 335 human proteins and 10 dengue proteins. All predicted interactions are validated by literature filtering, GO-based assessment, and KEGG Pathway enrichment analysis. This study will encourage the identification of potential targets for more effective anti-dengue drug discovery.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {2137–2148},
numpages = {12}
}

@article{10.1007/s10844-017-0446-7,
author = {Lango, Mateusz and Stefanowski, Jerzy},
title = {Multi-class and feature selection extensions of Roughly Balanced Bagging for imbalanced data},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {1},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-017-0446-7},
doi = {10.1007/s10844-017-0446-7},
abstract = {Roughly Balanced Bagging is one of the most efficient ensembles specialized for class imbalanced data. In this paper, we study its basic properties that may influence its good classification performance. We experimentally analyze them with respect to bootstrap construction, deciding on the number of component classifiers, their diversity, and ability to deal with the most difficult types of the minority examples. Then, we introduce two generalizations of this ensemble for dealing with a higher number of attributes and for adapting it to handle multiple minority classes. Experiments with synthetic and real life data confirm usefulness of both proposals.},
journal = {J. Intell. Inf. Syst.},
month = feb,
pages = {97–127},
numpages = {31},
keywords = {Class imbalance, Feature selection, Multiple imbalanced classes, Roughly balanced bagging, Types of minority examples}
}

@article{10.1016/j.neucom.2012.04.039,
author = {Yin, Liuzhi and Ge, Yong and Xiao, Keli and Wang, Xuehua and Quan, Xiaojun},
title = {Feature selection for high-dimensional imbalanced data},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.04.039},
doi = {10.1016/j.neucom.2012.04.039},
abstract = {Given its importance, the problem of classification in imbalanced data has attracted great attention in recent years. However, few efforts have been made to develop feature selection techniques for the classification of imbalanced data. This paper thus fills this critical void by introducing two approaches for the feature selection of high-dimensional imbalanced data. To this end, after introducing three traditional methods, we study and illustrate the challenges of feature selection in imbalanced data with Bayesian learning. Indeed, we reveal that the samples in the larger classes have a dominant influence on these feature selection methods. However, the samples in rare classes are essential for the learning performances of rare classes. Based on these observations, we provide a new feature selection approach based on class decomposition. Specifically, we partition the large classes into relatively smaller pseudo-subclasses and generate the pseudo-class labels accordingly. Feature selection is then performed on the new decomposed data for computing the goodness measurement of features. In addition, we also introduce a Hellinger distance-based method for feature selection. Hellinger distance is a measure of distribution divergence, which is strongly skew insensitive as the class prior information is not involved for computing the distance. Finally, we theoretically show the effectiveness of these two approaches with Bayesian learning on synthetic data. We also test and compare the performances of the proposed feature-selection methods on some real-world data sets. The experimental results show that both decomposition-based and Hellinger distance-based methods can outperform existing feature-selection methods with a clear margin on imbalanced data.},
journal = {Neurocomput.},
month = apr,
pages = {3–11},
numpages = {9},
keywords = {AUC, F-measure, Feature selection, Hellinger distance, Imbalanced data}
}

@article{10.1007/s10772-015-9294-4,
author = {Milton, A. and Selvi, S. Tamil},
title = {Four-stage feature selection to recognize emotion from speech signals},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {4},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-015-9294-4},
doi = {10.1007/s10772-015-9294-4},
abstract = {Feature selection plays an important role in emotion recognition from speech signals because it improves the classification accuracy by choosing the best uncorrelated features. In wrapper method of feature selection, the features are evaluated by a classifier. Features of large dimension will increase the computational complexity of the classifier, and further it will affect the training of classifiers which needs inverse of covariance matrix. We propose a four-stage feature selection method which avoids the problem of curse of dimensionality by the principle of divide and conquer. In the proposed method, the dimension of the feature vector is shortened at any stage in a way that the classifiers, whose training is affected by the large feature dimension, can also be used to evaluate the features. Experimental results show that the four-stage feature selection method improves classification accuracy. Another method to improve classification accuracy is evolved by bringing together several classifiers with a fusion technique. Class-specific multiple classifiers scheme is one such method that improves classification accuracy by combining optimum performance feature set and classifier for each emotional class. In this work, we improve the performance of the class-specific multiple classifiers scheme by embedding the proposed feature selection method in its structure.},
journal = {Int. J. Speech Technol.},
month = dec,
pages = {505–520},
numpages = {16},
keywords = {Human computer interaction, Multiple classifiers, Multistage feature selection, Speech emotion recognition}
}

@article{10.5555/2781902.2782171,
author = {Chen, Gang and Chen, Jin},
title = {A novel wrapper method for feature selection and its applications},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {159},
number = {C},
issn = {0925-2312},
abstract = {This paper introduces a wrapper method, namely cosine similarity measure support vector machines (CSMSVM), to eliminate irrelevant or redundant features during classifier construction by introducing the cosine distance into support vector machines (SVM). Traditionally, feature selection approaches typically extract features and learn SVM parameters independently or in the attribute space, which might result in a loss of information related to classification process or lead to the increase of classification error when introduce the kernel SVM. The proposed CSMSVM framework, however, jointly performs feature selection, SVM parameter learning and remove low relevance features by optimizing the shape of an anisotropic RBF kernel in feature space. Moreover, the Bayesian interpretation of the novel methodology reveals its Bayesian character, which builds the proposed method on solid theory foundation, and the iteration algorithm, which is proposed to optimize the feature weight, has achieved to maximize the maximum a posterior (MAP). Comparing the novel method with well-known feature selection techniques with experiments, CSMSVM outperformed the other methodologies in improving the pattern recognition accuracy with fewer features. An excellent feature selection method combined with SVM has been proposed.An efficient optimization algorithm has been proposed for non-convex problem.Features have been reduced without affecting the performance of SVM.Bayesian interpretation of the novel method have been revealed.},
journal = {Neurocomput.},
month = jul,
pages = {219–226},
numpages = {8},
keywords = {Bayesian interpretation, Cosine similarity measure, Feature selection, Support vector machines}
}

@article{10.1016/j.ins.2014.01.008,
author = {Peralta, Billy and Soto, Alvaro},
title = {Embedded local feature selection within mixture of experts},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {269},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2014.01.008},
doi = {10.1016/j.ins.2014.01.008},
abstract = {A useful strategy to deal with complex classification scenarios is the ''divide and conquer'' approach. The mixture of experts (MoE) technique makes use of this strategy by jointly training a set of classifiers, or experts, that are specialized in different regions of the input space. A global model, or gate function, complements the experts by learning a function that weighs their relevance in different parts of the input space. Local feature selection appears as an attractive alternative to improve the specialization of experts and gate function, particularly, in the case of high dimensional data. In general, subsets of dimensions, or subspaces, are usually more appropriate to classify instances located in different regions of the input space. Accordingly, this work contributes with a regularized variant of MoE that incorporates an embedded process for local feature selection using L"1 regularization. Experiments using artificial and real-world datasets provide evidence that the proposed method improves the classical MoE technique, in terms of accuracy and sparseness of the solution. Furthermore, our results indicate that the advantages of the proposed technique increase with the dimensionality of the data.},
journal = {Inf. Sci.},
month = jun,
pages = {176–187},
numpages = {12},
keywords = {Embedded feature selection, Local feature selection, Mixture of experts, Regularization}
}

@article{10.1109/TCBB.2018.2833482,
author = {Brankovic, Aida and Hosseini, Marjan and Piroddi, Luigi},
title = {A Distributed Feature Selection Algorithm Based on Distance Correlation with an Application to Microarrays},
year = {2019},
issue_date = {Nov.-Dec. 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2833482},
doi = {10.1109/TCBB.2018.2833482},
abstract = {DNA microarray datasets are characterized by a large number of features with very few samples, which is a typical cause of overfitting and poor generalization in the classification task. Here, we introduce a novel feature selection (FS) approach which employs the distance correlation (dCor) as a criterion for evaluating the dependence of the class on a given feature subset. The dCor index provides a reliable dependence measure among random vectors of arbitrary dimension, without any assumption on their distribution. Moreover, it is sensitive to the presence of redundant terms. The proposed FS method is based on a probabilistic representation of the feature subset model, which is progressively refined by a repeated process of model extraction and evaluation. A key element of the approach is a distributed optimization scheme based on a vertical partitioning of the dataset, which alleviates the negative effects of its unbalanced dimensions. The proposed method has been tested on several microarray datasets, resulting in quite compact and accurate models obtained at a reasonable computational cost.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1802–1815},
numpages = {14}
}

@article{10.1007/s11042-013-1529-2,
author = {Kiktova-Vozarikova, Eva and Juhar, Jozef and Cizmar, Anton},
title = {Feature selection for acoustic events detection},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-013-1529-2},
doi = {10.1007/s11042-013-1529-2},
abstract = {The paper deals with the detection of abnormal situations via captured sound processing. Different settings of feature extraction algorithms were realized and evaluated. Chosen feature sets were used for building the effective parametric representation for gun shots and breaking glass. This way two types of high dimensional feature supervectors were created in regard to the best individual settings of each feature extraction algorithm. For improving the recognition rate Minimum Redundancy Maximum Relevance (MRMR) and Joint Mutual Information (JMI) feature selection algorithms were also applied. They were used for the selection of superior features and for the creation of n-dimensional feature supervectors. The investigation of the appropriate dimension of feature supervectors was performed too. The framework for recognition of potentially dangerous acoustic events such as breaking glass and gun shots, based on the MRMR and JMI selected feature supervector through Hidden Markov Models based classification is proposed in the paper.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {4213–4233},
numpages = {21},
keywords = {Acoustic event, JMI, MRMR, Supervector}
}

@article{10.1016/j.envsoft.2017.12.001,
author = {Meyer, Hanna and Reudenbach, Christoph and Hengl, Tomislav and Katurji, Marwan and Nauss, Thomas},
title = {Improving performance of spatio-temporal machine learning models using forward feature selection and target-oriented validation},
year = {2018},
issue_date = {Mar 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2017.12.001},
doi = {10.1016/j.envsoft.2017.12.001},
journal = {Environ. Model. Softw.},
month = mar,
pages = {1–9},
numpages = {9},
keywords = {Cross-validation, Feature selection, Over-fitting, Random forest, Spatio-temporal, Target-oriented validation}
}

@article{10.1007/s10844-014-0324-5,
author = {Pan, Feng and Song, Guangwei and Gan, Xiaobing and Gu, Qiwei},
title = {Consistent feature selection and its application to face recognition},
year = {2014},
issue_date = {October   2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {43},
number = {2},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-014-0324-5},
doi = {10.1007/s10844-014-0324-5},
abstract = {In this paper we consider feature selection for face recognition using both labeled and unlabeled data. We introduce the weighted feature space in which the global separability between different classes is maximized and the local similarity of the neighboring data points is preserved. By integrating the global and local structures, a general optimization framework is formulated. We propose a simple solution to this problem, avoiding the matrix eigen-decomposition procedure which is often computationally expensive. Experimental results demonstrate the efficacy of our approach and confirm that utilizing labeled and unlabeled data together does help feature selection with small number of labeled samples.},
journal = {J. Intell. Inf. Syst.},
month = oct,
pages = {307–321},
numpages = {15},
keywords = {Eigen-decomposition, Feature selection, Laplacian matrix, Pattern recognition}
}

@article{10.1016/j.ins.2014.05.042,
author = {Bol\'{o}n-Canedo, V. and S\'{a}nchez-Maro\~{n}o, N. and Alonso-Betanzos, A. and Ben\'{\i}tez, J. M. and Herrera, F.},
title = {A review of microarray datasets and applied feature selection methods},
year = {2014},
issue_date = {October, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {282},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2014.05.042},
doi = {10.1016/j.ins.2014.05.042},
abstract = {Microarray data classification is a difficult challenge for machine learning researchers due to its high number of features and the small sample sizes. Feature selection has been soon considered a de facto standard in this field since its introduction, and a huge number of feature selection methods were utilized trying to reduce the input dimensionality while improving the classification performance. This paper is devoted to reviewing the most up-to-date feature selection methods developed in this field and the microarray databases most frequently used in the literature. We also make the interested reader aware of the problematic of data characteristics in this domain, such as the imbalance of the data, their complexity, or the so-called dataset shift. Finally, an experimental evaluation on the most representative datasets using well-known feature selection methods is presented, bearing in mind that the aim is not to provide the best feature selection method, but to facilitate their comparative study by the research community.},
journal = {Inf. Sci.},
month = oct,
pages = {111–135},
numpages = {25},
keywords = {Dataset shift, Feature selection, Microarray data, Unbalanced data}
}

@article{10.1155/2020/8890477,
author = {Qi, Yingji and Ding, Feng and Xu, Fangzhou and Yang, Jimin and Lo Bosco, Giosu\`{e}},
title = {Channel and Feature Selection for a Motor Imagery-Based BCI System Using Multilevel Particle Swarm Optimization},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-5265},
url = {https://doi.org/10.1155/2020/8890477},
doi = {10.1155/2020/8890477},
abstract = {Brain-computer interface (BCI) is a communication and control system linking the human brain and computers or other electronic devices. However, irrelevant channels and misleading features unrelated to tasks limit classification performance. To address these problems, we propose an efficient signal processing framework based on particle swarm optimization (PSO) for channel and feature selection, channel selection, and feature selection. Modified Stockwell transforms were used for a feature extraction, and multilevel hybrid PSO-Bayesian linear discriminant analysis was applied to optimization and classification. The BCI Competition III dataset I was used here to confirm the superiority of the proposed scheme. Compared to a method without optimization (89% accuracy), the best classification accuracy of the PSO-based scheme was 99% when less than 10.5% of the original features were used, the test time was reduced by more than 90%, and it achieved Kappa values and F-score of 0.98 and 98.99%, respectively, and better signal-to-noise ratio, thereby outperforming existing algorithms. The results show that the channel and feature selection scheme can accelerate the speed of convergence to the global optimum and reduce the training time. As the proposed framework can significantly improve classification performance, effectively reduce the number of features, and greatly shorten the test time, it can serve as a reference for related real-time BCI application system research.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {11}
}

@article{10.1016/j.neucom.2016.07.026,
author = {Solorio-Fern\'{a}ndez, Sa\'{u}l and Carrasco-Ochoa, J. Ariel and Mart\'{\i}nez-Trinidad, Jos\'{e} Fco.},
title = {A new hybrid filter-wrapper feature selection method for clustering based on ranking},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {214},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.07.026},
doi = {10.1016/j.neucom.2016.07.026},
abstract = {Feature selection is a common task in areas such as Pattern Recognition, Data Mining, and Machine Learning since it can help to improve prediction quality, reduce computation time and build more understandable models. Although feature selection for supervised classification has been widely studied, feature selection in the absence of class labels, namely feature selection for clustering or unsupervised feature selection, has been less addressed. Most existing unsupervised feature selection approaches suffer from the called "Bias of Criterion Values to Dimension," which arises when feature subsets with different cardinality are evaluated by an internal evaluation clustering criterion. In this paper, we introduce a new hybrid filter-wrapper method for clustering, which combines the spectral feature selection framework using the Laplacian Score ranking and a modified Calinski-Harabasz index. The proposed method in the filter stage sorts the features according to their relevance, while in the wrapper stage, through our modified Calinski-Harabasz index that takes into account the cardinality of the feature subsets under evaluation, evaluates the features considering them as a subset rather than individually by using two well-known selection strategies. Experiments on different datasets show that the proposed method alleviates the "Bias of Criterion Values to Dimension" and, identifies and selects more relevant features than those selected by other reported hybrid filter-wrapper feature selection methods for clustering. Additionally, we also contrast our results against other filter and wrapper methods of the state-of-the-art.},
journal = {Neurocomput.},
month = nov,
pages = {866–880},
numpages = {15},
keywords = {Feature ranking, Feature selection for clustering, Laplacian score, Weighted normalized Calinski-Harabasz index}
}

@article{10.1007/s10489-015-0725-3,
author = {Cerrada, Mariela and S\'{a}nchez, Ren\'{e}-Vinicio and Pacheco, Fannia and Cabrera, Diego and Zurita, Grover and Li, Chuan},
title = {Hierarchical feature selection based on relative dependency for gear fault diagnosis},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {44},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-015-0725-3},
doi = {10.1007/s10489-015-0725-3},
abstract = {Feature selection is an important aspect under study in machine learning based diagnosis, that aims to remove irrelevant features for reaching good performance in the diagnostic systems. The behaviour of diagnostic models could be sensitive with regard to the amount of features, and significant features can represent the problem better than the entire set. Consequently, algorithms to identify these features are valuable contributions. This work deals with the feature selection problem through attribute clustering. The proposed algorithm is inspired by existing approaches, where the relative dependency between attributes is used to calculate dissimilarity values. The centroids of the created clusters are selected as representative attributes. The selection algorithm uses a random process for proposing centroid candidates, in this way, the inherent exploration in random search is included. A hierarchical procedure is proposed for implementing this algorithm. In each level of the hierarchy, the entire set of available attributes is split in disjoint sets and the selection process is applied on each subset. Once the significant attributes are proposed for each subset, a new set of available attributes is created and the selection process runs again in the next level. The hierarchical implementation aims to refine the search space in each level on a reduced set of selected attributes, while the computational time-consumption is improved also. The approach is tested with real data collected from a test bed, results show that the diagnosis precision by using a Random Forest based classifier is over 98 % with only 12 % of the attributes from the available set.},
journal = {Applied Intelligence},
month = apr,
pages = {687–703},
numpages = {17},
keywords = {Attribute clustering, Feature selection, Gear fault diagnosis, Relative dependency, Rough sets}
}

@article{10.1016/j.eswa.2015.07.058,
author = {Gunduz, Hakan and Cataltepe, Zehra},
title = {Borsa Istanbul (BIST) daily prediction using financial news and balanced feature selection},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {22},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.07.058},
doi = {10.1016/j.eswa.2015.07.058},
abstract = {The direction of Borsa Istanbul 100 Index (BIST100) open prices is predicted.A feature selection method, called Balanced Mutual Information (BMI) is proposed.BMI is able to deal with the class imbalance problem through oversampling.BMI is compared with Mutual Information and Chi-square based feature selection.BMI achieves higher macro-averaged F-measure than the other methods using less features. In this paper, a novel method is proposed to predict the direction of Borsa Istanbul (BIST) 100 Index (BIST100) open prices using the news articles released, as well as the price data, from the day before. Although English news articles have been used for market-prediction before, to the best of our knowledge, Turkish news articles together with prices have not yet been used to predict the Turkish markets. Turkish text mining techniques are applied on news articles to form feature vectors for each trading day. The feature vectors are assigned three labels based on the direction of the price change from the closing price of the day before and whether the change is significant. News articles are represented using high dimensional features, some of which could be noisy or irrelevant for prediction. There is also the scarcity of training data. Therefore, this study incorporates feature selection methods to select features that could improve classification performance. By its nature, significant positive or negative changes in stock price happen much less than non-significant changes, resulting in an imbalanced data set. Most feature selection methods in literature aim to reduce the classification accuracy. However, for imbalanced datasets, other measures, such as macro-averaged F-measure need to be considered. The paper proposes a feature selection methods that is able to deal with the class imbalance problem through oversampling of the minority classes and consideration of an ensemble of selected features. In order to decide on importance of features, as the relevance criterion for each feature, the proposed methodology uses mutual information which can detect nonlinear dependencies between variables. Therefore, the proposed feature selection method is called Balanced Mutual Information (BMI) feature selection method. Experiments were performed based on news articles provided by two different news sources: Public Disclosure Platform of BIST and financial news websites. It was shown that, using Balanced Mutual Information feature selection method, the significant changes in the BIST100 Index were predicted with an accuracy of 0.74 and a macro-averaged F-measure of 0.68. The BMI feature selection method was compared with Mutual Information and Chi-square based feature selection methods and it was found out that BMI method results in higher performance using a smaller number of features.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {9001–9011},
numpages = {11},
keywords = {BIST, Borsa Istanbul, Feature selection, Stock prediction, Unbalanced data}
}

@article{10.1016/j.neucom.2016.12.036,
author = {Cao, Peng and Liu, Xiaoli and Zhang, Jian and Zhao, Dazhe and Huang, Min and Zaiane, Osmar},
title = {2,1 norm regularized multi-kernel based joint nonlinear feature selection and over-sampling for imbalanced data classification},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {234},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.12.036},
doi = {10.1016/j.neucom.2016.12.036},
abstract = {High dimensionality and classification of imbalanced data sets are two of the most interesting machine learning challenges. Both issues have been independently studied in the literature. In order to simultaneously explore the both issues of feature selection and oversampling, we efficiently combine two different methodological approaches in an unified kernel framework. Specifically, we proposed a novel 2,1 norm balanced multiple kernel feature selection (2,1 MKFS), and designed a proximal based optimization algorithm for efficiently learning the model. Moreover, multiple kernel oversampling (MKOS) was developed to generate synthetic instances in the optimal kernel space induced by 2,1 MKFS, so as to compensate for the class imbalanced distribution. Our experimental results on multiple UCI data and two real medical application demonstrate that jointly operating nonlinear feature selection and oversampling with 2,1 norm multi-kernel learning framework (2,1 MKFSOS) can lead to a promising classification performance. HighlightsProposed multi-kernel framework to nonlinear feature selection and oversampling.Regularized multiple kernel with l21norm to feature selection for imbalanced data.The over-sampling operate in kernel space induced by l21MKFS for minority instances.Evaluating the method on UCI datasets, and two real medical clinical applications.},
journal = {Neurocomput.},
month = apr,
pages = {38–57},
numpages = {20},
keywords = {Classification, Feature selection, Imbalanced data learning, Multi-kernel learning, Proximal method}
}

@article{10.1007/s11634-014-0168-4,
author = {Liu, Zhiliang and Zhao, Xiaomin and Zuo, Ming J. and Xu, Hongbing},
title = {Feature selection for fault level diagnosis of planetary gearboxes},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {4},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-014-0168-4},
doi = {10.1007/s11634-014-0168-4},
abstract = {Feature selection is critical to maintain high performance of classification-based fault diagnosis with a large feature size. In this paper, we propose a criterion to evaluate features effectiveness by class separability that is defined on cosine similarity in the kernel space of the Gaussian radial basis function. We develop a feature selection algorithm accordingly using the proposed criterion together with sequential backward selection and a feature re-ranking mechanism. We then employ the proposed feature selection algorithm to determine fault-sensitive features and select them for fault level diagnosis of planetary gearboxes. The experimental results demonstrate that the proposed algorithm can effectively reduce the feature size and improve accuracy of fault level diagnosis simultaneously.},
journal = {Adv. Data Anal. Classif.},
month = dec,
pages = {377–401},
numpages = {25},
keywords = {62H30, 68T10, 93C85, Class separability, Cosine similarity, Fault diagnosis, Feature selection, Planetary gearbox}
}

@article{10.1016/j.patcog.2014.11.010,
author = {Freeman, Cecille and Kuli\'{c}, Dana and Basir, Otman},
title = {An evaluation of classifier-specific filter measure performance for feature selection},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.11.010},
doi = {10.1016/j.patcog.2014.11.010},
abstract = {Feature selection is an important part of classifier design. There are many possible methods for searching and evaluating feature subsets, but little consensus on which methods are best. This paper examines a number of filter-based feature subset evaluation measures with the goal of assessing their performance with respect to specific classifiers.This work tests 16 common filter measures for use with K-nearest neighbors and support vector machine classifiers. The measures are tested on 20 real and 20 artificial data sets, which are designed to probe for specific challenges. The strengths and weaknesses of each measure are discussed with respect to the specific challenges and correlation with classifier accuracy. The results highlight several challenging problems with a number of common filter measures.The results indicate that the best filter measure is classifier-specific. K-nearest neighbors classifiers work well with subset-based RELIEF, correlation feature selection or conditional mutual information maximization, whereas Fisher s interclass separability criterion and conditional mutual information maximization work better for support vector machines. Despite the large number and variety of feature selection measures proposed in the literature, no single measure is guaranteed to outperform the others, even within a single classifier, and the overall performance of a feature selection method cannot be characterized independently of the subsequent classifier. HighlightsCompare common feature selection filter measures for use with specific classifiers.Many tested filter measures do not reliably predict classifier accuracy.Some measures have specific problems that cause them to select unsuitable features.Best feature selection filter measure is classifier specific.},
journal = {Pattern Recogn.},
month = may,
pages = {1812–1826},
numpages = {15},
keywords = {Classification, Feature selection, Filter measures}
}

@article{10.1016/j.knosys.2016.07.035,
author = {Sun, Shiquan and Peng, Qinke and Zhang, Xiaokang},
title = {Global feature selection from microarray data using Lagrange multipliers},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {110},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.07.035},
doi = {10.1016/j.knosys.2016.07.035},
abstract = {In microarray-based gene expression analysis, thousands of genes are involved to monitor their expression levels under a particular condition. In fact, however, only few of them are highly expressed, which has been proven by Golub et\'{z}al. How to identify these discriminative genes effectively is a significant challenge to risk assessment, diagnosis, prognostication in growing cancer incidence and mortality.In this paper, we present a global feature selection method based on semidefinite programming model which is relaxed from the quadratic programming model with maximizing feature relevance and minimizing feature redundancy. The main advantage of relaxation is that the matrix in mathematical model only requires symmetric matrix rather than positive (or semi) definite matrix. In semidefinite programming model, each feature has one constraint condition to restrict the objective function of feature selection problem. Herein, another trick in this paper is that we utilize Lagrange multiplier as proxy measurement to identify the discriminative features instead of solving a feasible solution for the original max-cut problem. The proposed method is compared with several popular feature selection methods on seven microarray data sets. The results demonstrate that our method outperforms the others on most data sets, especially for the two hard feature selection data sets, Beast(Wang) and Medulloblastoma.},
journal = {Know.-Based Syst.},
month = oct,
pages = {267–274},
numpages = {8},
keywords = {Gene selection, High-dimensional and small sample size, Lagrange multiplier, Max-cut, Semidefinite programming}
}

@article{10.1007/s10618-013-0320-3,
author = {Mavroeidis, Dimitrios and Marchiori, Elena},
title = {Feature selection for k-means clustering stability: theoretical analysis and an algorithm},
year = {2014},
issue_date = {July      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-013-0320-3},
doi = {10.1007/s10618-013-0320-3},
abstract = {Stability of a learning algorithm with respect to small input perturbations is an important property, as it implies that the derived models are robust with respect to the presence of noisy features and/or data sample fluctuations. The qualitative nature of the stability property enhardens the development of practical, stability optimizing, data mining algorithms as several issues naturally arise, such as: how "much" stability is enough, or how can stability be effectively associated with intrinsic data properties. In the context of this work we take into account these issues and explore the effect of stability maximization in the continuous (PCA-based) k-means clustering problem. Our analysis is based on both mathematical optimization and statistical arguments that complement each other and allow for the solid interpretation of the algorithm's stability properties. Interestingly, we derive that stability maximization naturally introduces a tradeoff between cluster separation and variance, leading to the selection of features that have a high cluster separation index that is not artificially inflated by the features variance. The proposed algorithmic setup is based on a Sparse PCA approach, that selects the features that maximize stability in a greedy fashion. In our study, we also analyze several properties of Sparse PCA relevant to stability that promote Sparse PCA as a viable feature selection mechanism for clustering. The practical relevance of the proposed method is demonstrated in the context of cancer research, where we consider the problem of detecting potential tumor biomarkers using microarray gene expression data. The application of our method to a leukemia dataset shows that the tradeoff between cluster separation and variance leads to the selection of features corresponding to important biomarker genes. Some of them have relative low variance and are not detected without the direct optimization of stability in Sparse PCA based k-means. Apart from the qualitative evaluation, we have also verified our approach as a feature selection method for  $$k$$ k -means clustering using four cancer research datasets. The quantitative empirical results illustrate the practical utility of our framework as a feature selection mechanism for clustering.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {918–960},
numpages = {43},
keywords = {Clustering, Feature selection, Sparse PCA, Stability}
}

@inproceedings{10.1145/2905055.2905122,
author = {Trivedi, Shrawan Kumar and Dey, Shubhamoy},
title = {A Comparative Study of Various Supervised Feature Selection Methods for Spam Classification},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905122},
doi = {10.1145/2905055.2905122},
abstract = {Classification of the spam from bunch of the email files is a challenging research area in text mining domain. However, machine learning based approaches are widely experimented in the literature with enormous success. For excellent learning of the classifiers, few numbers of informative features are important. This researh presents a comparative study between various supervised feature selection methods such as Document Frequency (DF), Chi-Squared (χ2), Information Gain (IG), Gain Ratio (GR), Relief F (RF), and One R (OR). Two corpuses (Enron and SpamAssassin) are selected for this study where enron is main corpus and spamAssassin is used for validation of the results. Bayesian Classifier is taken to classify the given corpuses with the help of features selected by above feature selection techniques. Results of this study shows that RF is the excellent feature selection technique amongst other in terms of classification accuracy and false positive rate whereas DF and X2 were not so effective methods. Bayesian classifier has proven its worth in this study in terms of good performance accuracy and low false positives.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {64},
numpages = {6},
keywords = {Bayesian Classifier, Classification Accuracy, F- Value, False Positive Rate, Feature selection, Spam classification},
location = {Udaipur, India},
series = {ICTCS '16}
}

@article{10.1016/j.neucom.2015.07.057,
author = {Yong, Zhang and Dun-wei, Gong and Wan-qiu, Zhang},
title = {Feature selection of unreliable data using an improved multi-objective PSO algorithm},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {171},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.057},
doi = {10.1016/j.neucom.2015.07.057},
abstract = {Due to the influence of environment, data obtained in real world are not completely reliable sometimes. This paper focuses on tackling the feature selection problem with unreliable data. First, the problem is formulated as an multi-objective optimization one with two objectives: the reliability and the classification accuracy. Then, an effective multi-objective feature selection algorithm based on bare-bones particle swarm optimization is proposed by incorporating two new operators. One is a reinforced memory strategy, which is designed to overcome the degradation phenomenon of particles. Another is a hybrid mutation, which is designed to improve the search ability of the proposed algorithm. Finally, two state-of-the-art multi-objective optimization algorithms are also applied to this kind of problem, and comparison results suggest that the proposed algorithm is highly competitive for the feature selection problem with unreliable data.},
journal = {Neurocomput.},
month = jan,
pages = {1281–1290},
numpages = {10},
keywords = {Classification, Feature selection, Particle swarm optimization, Unreliable data}
}

@article{10.1016/j.ins.2015.10.002,
author = {Li, Fachao and Zhang, Zan and Jin, Chenxia},
title = {Feature selection with partition differentiation entropy for large-scale data sets},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {329},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.10.002},
doi = {10.1016/j.ins.2015.10.002},
abstract = {Feature selection, especially for large data sets, is a challenging problem in areas such as pattern recognition, machine learning and data mining. With the development of data collection and storage technologies, the data has become bigger than ever, thus making it difficult for learning from large data sets with traditional methods. In this paper, we introduce the partition differentiation entropy from the viewpoint of partition in rough sets to measure the significance and uncertainty of attributes, and present a feature selection method for large-scale data sets based on the information-theoretical measurement of attribute significance. Given a large-scale decision information system, the proposed method first divides it into small sub information systems according to the decision classes. Then by computing partition differentiation entropy in the sub-systems, the partition differentiation entropy of the attribute subset in the original decision information system is obtained. Accordingly, the important features are selected based on the value of partition differentiation entropy. The experimental results show that the idea of the proposed method is feasible and valid.},
journal = {Inf. Sci.},
month = feb,
pages = {690–700},
numpages = {11},
keywords = {Attributes significance, Feature selection, Large-scale data sets, Partition differentiation entropy, Uncertainty}
}

@article{10.1016/j.ins.2016.05.025,
author = {Zhao, Hong and Wang, Ping and Hu, Qinghua},
title = {Cost-sensitive feature selection based on adaptive neighborhood granularity with multi-level confidence},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {366},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.05.025},
doi = {10.1016/j.ins.2016.05.025},
abstract = {Neighborhood rough set model is considered as one of the effective granular computing models in dealing with numerical data. This model is now widely discussed in feature selection and rule learning. However, there is no theoretical analysis on the issue of neighborhood granularity selection, the influence of sampling resolution, test and misclassification costs on modeling. In this paper, we design an adaptive neighborhood rough set model according to data precision and develop a fast backtracking algorithm for neighborhood rough sets based cost-sensitive feature selection by considering the trade-off between test costs and misclassification costs. In the proposed model, the neighborhood granularity, based on the 3\'{z} rule of statistics, is adaptive to data precision that is described by the multi-level confidence of the feature subsets. Our experiments, thoroughly performed on 12 datasets, demonstrate the effectiveness of the model and the efficiency of the backtracking algorithm.},
journal = {Inf. Sci.},
month = oct,
pages = {134–149},
numpages = {16},
keywords = {Cost-sensitive learning, Feature selection, Granular computing, Neighborhood granularity, Neighborhood rough sets}
}

@article{10.1016/j.compbiomed.2015.08.010,
author = {Drot\'{a}r, P. and Gazda, J. and Sm\'{e}kal, Z.},
title = {An experimental comparison of feature selection methods on two-class biomedical datasets},
year = {2015},
issue_date = {November 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2015.08.010},
doi = {10.1016/j.compbiomed.2015.08.010},
abstract = {Feature selection is a significant part of many machine learning applications dealing with small-sample and high-dimensional data. Choosing the most important features is an essential step for knowledge discovery in many areas of biomedical informatics. The increased popularity of feature selection methods and their frequent utilisation raise challenging new questions about the interpretability and stability of feature selection techniques. In this study, we compared the behaviour of ten state-of-the-art filter methods for feature selection in terms of their stability, similarity, and influence on prediction performance. All of the experiments were conducted on eight two-class datasets from biomedical areas. While entropy-based feature selection appears to be the most stable, the feature selection techniques yielding the highest prediction performance are minimum redundance maximum relevance method and feature selection based on Bhattacharyya distance. In general, univariate feature selection techniques perform similarly to or even better than more complex multivariate feature selection techniques with high-dimensional datasets. However, with more complex and smaller datasets multivariate methods slightly outperform univariate techniques. HighlightsTen feature selection methods are compared using stability and similarity measures.Univariate FS perform better than multivariate FS for high dimensional datasets.Multivariate FS slightly outperform univariate FS for complex and smaller datasets.Most stable appears to be entropy based FS.FS yielding the highest prediction performance are MRMR and Bhattacharyya distance.},
journal = {Comput. Biol. Med.},
month = nov,
pages = {1–10},
numpages = {10},
keywords = {Classification performance, Feature selection, Multivariate FS, Stability, Univariate FS}
}

@article{10.1016/j.comcom.2012.04.012,
author = {Zhang, Hongli and Lu, Gang and Qassrawi, Mahmoud T. and Zhang, Yu and Yu, Xiangzhan},
title = {Feature selection for optimizing traffic classification},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {12},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2012.04.012},
doi = {10.1016/j.comcom.2012.04.012},
abstract = {Machine learning (ML) algorithms have been widely applied in recent traffic classification. However, due to the imbalance in the number of traffic flows, ML based classifiers are prone to misclassify flows as the traffic type that occupies the majority of flows on the Internet. To address the problem, a novel feature selection metric named Weighted Symmetrical Uncertainty (WSU) is proposed. We design a hybrid feature selection algorithm named WSU_AUC, which prefilters most of features with WSU metric and further uses a wrapper method to select features for a specific classifier with Area Under roc Curve (AUC) metric. Additionally, to overcome the impacts of dynamic traffic flows on feature selection, we propose an algorithm named SRSF that Selects the Robust and Stable Features from the results achieved by WSU_AUC. We evaluate our approaches using three classifiers on the traces captured from entirely different networks. Experimental results obtained by our algorithms are promising in terms of true positive rate (TPR) and false positive rate (FPR). Moreover, our algorithms can achieve &gt;94% flow accuracy and &gt;80% byte accuracy on average.},
journal = {Comput. Commun.},
month = jul,
pages = {1457–1471},
numpages = {15},
keywords = {Class imbalance, Feature selection, Robust features, Traffic classification}
}

@inproceedings{10.1145/3143434.3143456,
author = {Hosni, Mohamed and Idri, Ali and Abran, Alain},
title = {Investigating heterogeneous ensembles with filter feature selection for software effort estimation},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143456},
doi = {10.1145/3143434.3143456},
abstract = {Ensemble Effort Estimation (EEE) consists on predicting the software development effort by combining more than one single estimation technique. EEE has recently been investigated in software development effort estimation (SDEE) in order to improve the estimation accuracy. The overall results suggested that the EEE yield better prediction accuracy than single techniques. On the other hand, feature selection (FS) methods have been used in the area of SDEE for the purpose of reducing the dimensionality of a dataset size by eliminating the irrelevant and redundant features. Thus, the SDEE techniques are trained on a dataset with relevant features which can lead to improving the accuracy of their estimations. This paper aims at investigating the impact of two Filter feature selection methods: Correlation based Feature Selection (CFS) and RReliefF on the estimation accuracy of Heterogeneous (HT) ensembles. Four machine learning techniques (K-Nearest Neighbor, Support Vector Regression, Multilayer Perceptron and Decision Trees) were used as base techniques for the HT ensembles of this study. We evaluate the accuracy of these HT ensembles when their base techniques were trained on datasets preprocessed by the two feature selection methods. The HT ensembles use three combination rules: average, median, and inverse ranked weighted mean. The evaluation was carried out by means of eight unbiased accuracy measures through the leave-one-out-cross validation (LOOCV) technique over six datasets. The overall results suggest that all the attributes of most datasets used are relevant for building an accurate predictive technique since the ensembles constructed without features selection outperformed in general the ones using features selection. As for the combination rule, the median generally produces better results than the other two used in this empirical study.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {207–220},
numpages = {14},
keywords = {accuracy, ensemble effort estimation, features selection, filter, machine learning},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1007/978-3-319-47955-2_31,
author = {Homci, M\'{a}rcia and Chagas, Paulo and Miranda, Brunelli and Freire, Jean and Vi\'{e}gas, Raimundo and Pires, Yomara and Meiguins, Bianchi and Morais, Jefferson},
title = {A New Strategy Based on Feature Selection for Fault Classification in Transmission Lines},
year = {2016},
isbn = {978-3-319-47954-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47955-2_31},
doi = {10.1007/978-3-319-47955-2_31},
abstract = {The transmission lines are the element most susceptible to faults on power systems, and the short circuit faults are the worst type of faults than can happen on this element. In order to avoid further problems due to these faults, a fault diagnostic is necessary, and the use of front ends is required. However, the selection process for choosing the front ends is not a simple one because it behaves differently for each. Therefore, this paper presents a new front end, called Concat front end, which integrates other front ends, such as wavelet, raw and Root Mean Square. Furthermore, we have applied feature selection techniques based on filter in order to decrease the dimension of the input data. Thus, we used the following classifiers: neural network, K-nearest neighbor, Random Forest and support vector machine. We used a public dataset called UFPAFaults to train and test the classifiers. As a result, the concatenation of front ends, on most cases, had achieved the lowest error rates. In addition, the feature selection techniques applied showed that it is possible to get higher accuracy using less features on the process.},
booktitle = {Advances in Artificial Intelligence - IBERAMIA 2016: 15th Ibero-American Conference on AI, San Jos\'{e}, Costa Rica, November 23-25, 2016, Proceedings},
pages = {376–387},
numpages = {12},
keywords = {Short-circuit fault, Transmission lines, Front ends, Feature selection, Machine learning algorithms},
location = {San Jos\'{e}, Costa Rica}
}

@article{10.1016/j.knosys.2008.08.002,
author = {Tsai, Chih-Fong},
title = {Feature selection in bankruptcy prediction},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {22},
number = {2},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2008.08.002},
doi = {10.1016/j.knosys.2008.08.002},
abstract = {For many corporations, assessing the credit of investment targets and the possibility of bankruptcy is a vital issue before investment. Data mining and machine learning techniques have been applied to solve the bankruptcy prediction and credit scoring problems. As feature selection is an important step to select more representative data from a given dataset in data mining to improve the final prediction performance, it is unknown that which feature selection method is better. Therefore, this paper aims at comparing five well-known feature selection methods used in bankruptcy prediction, which are t-test, correlation matrix, stepwise regression, principle component analysis (PCA) and factor analysis (FA) to examine their prediction performance. Multi-layer perceptron (MLP) neural networks are used as the prediction model. Five related datasets are used in order to provide a reliable conclusion. Regarding the experimental results, the t-test feature selection method outperforms the other ones by the two performance measurements.},
journal = {Know.-Based Syst.},
month = mar,
pages = {120–127},
numpages = {8},
keywords = {Bankruptcy prediction, Data mining, Feature selection, Neural networks}
}

@phdthesis{10.5555/2231215,
author = {Deng, Houtao},
advisor = {Runger, George C.},
title = {System complexity reduction via feature selection},
year = {2011},
isbn = {9781124605876},
publisher = {Arizona State University},
address = {USA},
abstract = {This dissertation transforms a set of system complexity reduction problems to feature selection problems. Three systems are considered: classification based on association rules, network structure learning, and time series classification. Furthermore, two variable importance measures are proposed to reduce the feature selection bias in tree models. Associative classifiers can achieve high accuracy, but the combination of many rules is difficult to interpret. Rule condition subset selection (RCSS) methods for associative classification are considered. RCSS aims to prune the rule conditions into a subset via feature selection. The subset then can be summarized into rule-based classifiers. Experiments show that classifiers after RCSS can substantially improve the classification interpretability without loss of accuracy. An ensemble feature selection method is proposed to learn Markov blankets for either discrete or continuous networks (without linear, Gaussian assumptions). The method is compared to a Bayesian local structure learning algorithm and to alternative feature selection methods in the causal structure learning problem. Feature selection is also used to enhance the interpretability of time series classification. Existing time series classification algorithms (such as nearest-neighbor with dynamic time warping measures) are accurate but difficult to interpret. This research leverages the time-ordering of the data to extract features, and generates an effective and efficient classifier referred to as a time series forest (TSF). The computational complexity of TSF is only linear in the length of time series, and interpretable features can be extracted. These features can be further reduced, and summarized for even better interpretability. Lastly, two variable importance measures are proposed to reduce the feature selection bias in tree-based ensemble models. It is well known that bias can occur when predictor attributes have different numbers of values. Two methods are proposed to solve the bias problem. One uses an out-of-bag sampling method called OOBForest, and the other, based on the new concept of a partial permutation test, is called a pForest. Experimental results show the existing methods are not always reliable for multi-valued predictors, while the proposed methods have advantages.},
note = {AAI3452830}
}

@article{10.1016/j.eswa.2021.115678,
author = {Shafizadeh-Moghadam, Hossein},
title = {Fully component selection: An efficient combination of feature selection and principal component analysis to increase model performance},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115678},
doi = {10.1016/j.eswa.2021.115678},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {7},
keywords = {High dimensional data, Dimension reduction, Random forest, Spectroscopic data, Principal component analysis}
}

@article{10.1016/j.engappai.2015.08.003,
author = {Lee, Sang-Hong},
title = {Feature selection based on the center of gravity of BSWFMs using NEWFM},
year = {2015},
issue_date = {October 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2015.08.003},
doi = {10.1016/j.engappai.2015.08.003},
abstract = {Feature selection has commonly been used to remove irrelevant features and improve classification performance. Some of features are irrelevant to the learning process; therefore to remove these irrelevant features not only decreases training and testing times, but can also improve learning accuracy. This study proposes a novel supervised feature selection method based on the bounded sum of weighted fuzzy membership functions (BSWFM) and Euclidean distances between their centers of gravity for decreasing the computational load and improving accuracy by removing irrelevant features. This study compares the performance of a neural network with a weighted fuzzy membership function (NEWFM) without and with the proposed feature selection method. The superiority of the NEWFM with feature selection over NEWFM without feature selection was demonstrated using three experimental datasets from the UCI Machine Learning Repository: Statlog Heart, Parkinsons and Ionosphere. 13 features, 22 features, and 34 features were used as inputs for the NEWFM without feature selection and these resulted in performance accuracies of 85.6%, 86.2% and 91.2%, respectively, using Statlog Heart, Parkinsons and Ionosphere datasets. 10 minimum features, 4 minimum features and 25 minimum features were used as inputs for the NEWFM with feature selection and these resulted in performance accuracies of 87.4%, 88.2%, and 92.6%, respectively, using Statlog Heart, Parkinsons and Ionosphere datasets. The results show that NEWFM with feature selection performed better than NEWFM without feature selection.},
journal = {Eng. Appl. Artif. Intell.},
month = oct,
pages = {482–487},
numpages = {6},
keywords = {Center of gravity, Euclidean distance, Feature selection, Fuzzy neural networks}
}

@article{10.1016/j.knosys.2012.10.001,
author = {Sun, Xin and Liu, Yanheng and Xu, Mantao and Chen, Huiling and Han, Jiawei and Wang, Kunhao},
title = {Feature selection using dynamic weights for classification},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2012.10.001},
doi = {10.1016/j.knosys.2012.10.001},
abstract = {Feature selection aims at finding a feature subset that has the most discriminative information from the original feature set. In this paper, we firstly present a new scheme for feature relevance, interdependence and redundancy analysis using information theoretic criteria. Then, a dynamic weighting-based feature selection algorithm is proposed, which not only selects the most relevant features and eliminates redundant features, but also tries to retain useful intrinsic groups of interdependent features. The primary characteristic of the method is that the feature is weighted according to its interaction with the selected features. And the weight of features will be dynamically updated after each candidate feature has been selected. To verify the effectiveness of our method, experimental comparisons on six UCI data sets and four gene microarray datasets are carried out using three typical classifiers. The results indicate that our proposed method achieves promising improvement on feature selection and classification accuracy.},
journal = {Know.-Based Syst.},
month = jan,
pages = {541–549},
numpages = {9},
keywords = {Classification, Feature selection, Filter method, Information criterion, Machine learning}
}

@inproceedings{10.1007/978-3-030-30484-3_3,
author = {Dash, Tirtharaj and Srinivasan, Ashwin and Joshi, Ramprasad S. and Baskar, A.},
title = {Discrete Stochastic Search and Its Application to Feature-Selection for Deep Relational Machines},
year = {2019},
isbn = {978-3-030-30483-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30484-3_3},
doi = {10.1007/978-3-030-30484-3_3},
abstract = {We use a model for discrete stochastic search in which one or more objects (“targets”) are to be found by a search over n locations (“boxes”), where n is infinitely large. Each box has some probability that it contains a target, resulting in a distribution H over boxes. We model the search for the targets as a stochastic procedure that draws boxes using some distribution S. We derive first a general expression on the expected number of misses E[Z] made by the search procedure in terms of H and S. We then obtain an expression for an optimal distribution S∗ to minimise E[Z]. This results in a relation between: the entropy of H and the KL-divergence between H and S∗. This result induces a 2-partitions over the boxes consisting of those boxes with H probability greater than 1n and the rest. We use this result to devise a stochastic search procedure for the practical situation when H is unknown. We present results from simulations that agree with theoretical predictions; and demonstrate that the expected misses by the optimal seeker decreases as the entropy of H decreases, with a maximum obtained for uniform H. Finally, we demonstrate applications of this stochastic search procedure with a coarse assumption about H. The theoretical results and the procedure are applicable to stochastic search over any aspect of machine learning that involves a discrete search-space: for example, choice over features, structures or discretized parameter-selection. In this work, the procedure is used to select features for Deep Relational Machines (DRMs) which are Deep Neural Networks (DNNs) defined in terms of domain-specific knowledge and built with features selected from large, potentially infinite-attribute space. Empirical results obtained across over 70 real-world datasets show that using the stochastic search procedure results in significantly better performances than the state-of-the-art.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part II},
pages = {29–45},
numpages = {17},
keywords = {Deep Neural Network, Inductive Logic Programming, Relational learning, Stochastic search, Infinite-attribute space},
location = {Munich, Germany}
}

@inproceedings{10.1145/2908446.2908460,
author = {Mazaar, Hussein and Emary, Eid and Onsi, Hoda},
title = {Ensemble Based-Feature Selection on Human Activity Recognition},
year = {2016},
isbn = {9781450340625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908446.2908460},
doi = {10.1145/2908446.2908460},
abstract = {The paper presents an approach for ensemble-based feature selection in human activity recognition. The goal is to select an important discriminating features to recognize the human activities in videos and removing the irrelevant redundant features. The features are extracted based on spatiotemporal orientation energy and template matching. Due to robust and accurate ensemble models with low variability and biases, Gradient Boosting and Random Forest are applied to identify the relevant features. Support Vector Machine with linear kernel is used to classify the activities. The experiments have tested on KTH dataset. The results show an improvement in accuracy (better by 1.51%) and the features are reduced by 99.2%. The Comparisons to related works were given.},
booktitle = {Proceedings of the 10th International Conference on Informatics and Systems},
pages = {81–87},
numpages = {7},
keywords = {Gradient Boosting, Human Action Recognition, Random Forest, Spatiotemporal Orientation Energy, Support Vector Machine, Template Matching},
location = {Giza, Egypt},
series = {INFOS '16}
}

@article{10.1007/s10844-013-0295-y,
author = {Chen, Kun-Huang and Chen, Li-Fei and Su, Chao-Ton},
title = {A new particle swarm feature selection method for classification},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {42},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-013-0295-y},
doi = {10.1007/s10844-013-0295-y},
abstract = {Searching for an optimal feature subset from a high-dimensional feature space is an NP-complete problem; hence, traditional optimization algorithms are inefficient when solving large-scale feature selection problems. Therefore, meta-heuristic algorithms are extensively adopted to solve such problems efficiently. This study proposes a regression-based particle swarm optimization for feature selection problem. The proposed algorithm can increase population diversity and avoid local optimal trapping by improving the jump ability of flying particles. The data sets collected from UCI machine learning databases are used to evaluate the effectiveness of the proposed approach. Classification accuracy is used as a criterion to evaluate classifier performance. Results show that our proposed approach outperforms both genetic algorithms and sequential search algorithms.},
journal = {J. Intell. Inf. Syst.},
month = jun,
pages = {507–530},
numpages = {24},
keywords = {Feature selection, Genetic algorithms, Particle swarm optimization, Regression, Sequential search algorithms}
}

@inproceedings{10.1007/978-3-030-03928-8_38,
author = {Villegas, Jorge and Cobos, Carlos and Mendoza, Martha and Herrera-Viedma, Enrique},
title = {Feature Selection Using Sampling with Replacement, Covering Arrays and Rule-Induction Techniques to Aid Polarity Detection in Twitter Sentiment Analysis},
year = {2018},
isbn = {978-3-030-03927-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03928-8_38},
doi = {10.1007/978-3-030-03928-8_38},
abstract = {One of the main tasks in analyzing sentiment on Twitter is polarity detection – i.e. the classification of ‘tweets’ in terms of feelings, opinions and attitudes expressed. Polarity detection on Twitter by means of machine learning methods is generally affected by the use of irrelevant, redundant, noisy or correlated features, especially when a high-dimensional representation is used in the feature set. There is thus a need for a selection method that removes those features that render the classification algorithm inefficient. In this work, we propose a feature selection method based on the concept of bagging, with two important modifications: (i) the use of covering arrays to support the process of building bootstrap samples; and (ii) the use of the results of rule-induction techniques (JRIP, C4.5, CART or others) to generate the reduced representation of tweets with the features selected. The experimental results show that on using the method proposed, we obtain similar or better results than those obtained with the original representation (this comprising a set of 91 features used in research related to polarity detection in Twitter), bringing the possibility of simpler and faster process models. A subset of features is thereby identified that can facilitate improvements in future polarity detection proposals on Twitter.},
booktitle = {Advances in Artificial Intelligence – IBERAMIA 2018: 16th Ibero-American Conference on AI, Trujillo, Peru, November 13–16, 2018, Proceedings},
pages = {467–480},
numpages = {14},
keywords = {Sentiment analysis, Polarity detection, Covering arrays, Feature selection, Twitter},
location = {Trujillo, Peru}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Aging Related Bugs, Empirical Software Engineering, Feature Selection Techniques, Imbalance Learning, Machine Learning, Predictive Modeling, Software Maintenance, Source Code Metrics},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3194452.3194468,
author = {Li, Xueyan},
title = {Image Authenticity Decision Based on Random Sample Consensus and Circular Feature Selection},
year = {2018},
isbn = {9781450364195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194452.3194468},
doi = {10.1145/3194452.3194468},
abstract = {In order to reduce the complexity of forgery detection algorithm and improve the accuracy, this paper proposes an image forgery detection algorithm based on DCT coupled random sample consensus optimization. First of all, the initial image is divided into sub-blocks of uniform size and DCT coefficients for each block is obtained through DCT to represent each blocks; then, circular feature screening mechanism is established to extract four features of the block, thereby reducing the feature dimension of each block. Finally, each eigenvector is ordered in a lexicographical manner and prior threshold is used to match the feature, reduce the image block false matching rate optimized by random sample consensus, thus completing the image authenticity for decision making. Experimental results show that, compared with the current image forgery detection algorithm, this algorithm has better robustness, efficiency and accuracy, and good detection effects on the fuzzy and noise forgery.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Artificial Intelligence},
pages = {91–97},
numpages = {7},
keywords = {Circular feature selection, Discrete cosine transform, Image forgery detection, Prior threshold, Random sample consensus optimization, lexicographical ordering},
location = {Chengdu, China},
series = {ICCAI '18}
}

@article{10.1016/j.knosys.2020.106684,
author = {Hu, Jiao and Chen, Huiling and Heidari, Ali Asghar and Wang, Mingjing and Zhang, Xiaoqin and Chen, Ying and Pan, Zhifang},
title = {Orthogonal learning covariance matrix for defects of grey wolf optimizer: Insights, balance, diversity, and feature selection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106684},
doi = {10.1016/j.knosys.2020.106684},
journal = {Know.-Based Syst.},
month = feb,
numpages = {41},
keywords = {Grey wolf optimizer, Swarm intelligence, Efficiency, Performance, Defect, Feature selection}
}

@article{10.1016/j.cmpb.2016.01.020,
author = {Nalband, Saif and Sundar, Aditya and Prince, A. Amalin and Agarwal, Anita},
title = {Feature selection and classification methodology for the detection of knee-joint disorders},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.01.020},
doi = {10.1016/j.cmpb.2016.01.020},
abstract = {HighlightsWe proposed RQA, ApEn, SampEn and wavelet based energy as feature extraction techniques.We have proposed feature selection algorithm to extract the most significant and relevant features.We have used LS-SVM and random forest as classifiers.Performance among feature selection algorithms are compared. Vibroarthographic (VAG) signals emitted from the knee joint disorder provides an early diagnostic tool. The nonstationary and nonlinear nature of VAG signal makes an important aspect for feature extraction. In this work, we investigate VAG signals by proposing a wavelet based decomposition. The VAG signals are decomposed into sub-band signals of different frequencies. Nonlinear features such as recurrence quantification analysis (RQA), approximate entropy (ApEn) and sample entropy (SampEn) are extracted as features of VAG signal. A total of twenty-four features form a vector to characterize a VAG signal. Two feature selection (FS) techniques, apriori algorithm and genetic algorithm (GA) selects six and four features as the most significant features. Least square support vector machines (LS-SVM) and random forest are proposed as classifiers to evaluate the performance of FS techniques. Results indicate that the classification accuracy was more prominent with features selected from FS algorithms. Results convey that LS-SVM using the apriori algorithm gives the highest accuracy of 94.31% with false discovery rate (FDR) of 0.0892. The proposed work also provided better classification accuracy than those reported in the previous studies which gave an accuracy of 88%. This work can enhance the performance of existing technology for accurately distinguishing normal and abnormal VAG signals. And the proposed methodology could provide an effective non-invasive diagnostic tool for knee joint disorders.},
journal = {Comput. Methods Prog. Biomed.},
month = apr,
pages = {94–104},
numpages = {11},
keywords = {Apriori algorithm, Biomedical signal processing, Feature selection, Genetic algorithm, Vibroarthographic signal, Wavelets}
}

@article{10.1016/j.jpdc.2019.12.001,
author = {Soheili, Majid and Eftekhari-Moghadam, Amir Masoud},
title = {DQPFS: Distributed quadratic programming based feature selection for big data},
year = {2020},
issue_date = {Apr 2020},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {138},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.12.001},
doi = {10.1016/j.jpdc.2019.12.001},
journal = {J. Parallel Distrib. Comput.},
month = apr,
pages = {1–14},
numpages = {14},
keywords = {Big data, Apache Spark, Feature selection, Feature ranking, Quadratic programming}
}

@article{10.1007/s10844-014-0317-4,
author = {Lamirel, Jean-Charles and Cuxac, Pascal and Chivukula, Aneesh Sreevallabh and Hajlaoui, Kafil},
title = {Optimizing text classification through efficient feature selection based on quality metric},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {3},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-014-0317-4},
doi = {10.1007/s10844-014-0317-4},
abstract = {Feature maximization is a cluster quality metric which favors clusters with maximum feature representation as regard to their associated data. In this paper we show that a simple adaptation of such metric can provide a highly efficient feature selection and feature contrasting model in the context of supervised classification. The method is experienced on different types of textual datasets. The paper illustrates that the proposed method provides a very significant performance increase, as compared to state of the art methods, in all the studied cases even when a single bag of words model is exploited for data description. Interestingly, the most significant performance gain is obtained in the case of the classification of highly unbalanced, highly multidimensional and noisy data, with a high degree of similarity between the classes.},
journal = {J. Intell. Inf. Syst.},
month = dec,
pages = {379–396},
numpages = {18},
keywords = {Clustering quality index, Feature maximization, Feature selection, Supervised learning, Text, Unbalanced data}
}

@article{10.1016/j.neucom.2014.03.053,
author = {Forsati, Rana and Moayedikia, Alireza and Jensen, Richard and Shamsfard, Mehrnoush and Meybodi, Mohammad Reza},
title = {Enriched ant colony optimization and its application in feature selection},
year = {2014},
issue_date = {October, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {142},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.03.053},
doi = {10.1016/j.neucom.2014.03.053},
abstract = {This paper presents a new variant of ant colony optimization (ACO), called enRiched Ant Colony Optimization (RACO). This variation tries to consider the previously traversed edges in the earlier executions to adjust the pheromone values appropriately and prevent premature convergence. Feature selection (FS) is the task of selecting relevant features or disregarding irrelevant features from data. In order to show the efficacy of the proposed algorithm, RACO is then applied to the feature selection problem. In the RACO-based feature selection (RACOFS) algorithm, it might be assumed that the proposed algorithm considers later features with a higher priority. Hence in another variation, the algorithm is integrated with a capability local search procedure to demonstrate that this is not the case. The modified RACO algorithm is able to find globally optimal solutions but suffers from entrapment in local optima. Hence, in the third variation, the algorithm is integrated with a local search procedure to tackle this problem by searching the vicinity of the globally optimal solution. To demonstrate the effectiveness of the proposed algorithms, experiments were conducted using two measures, kappa statistics and classification accuracy, on several standard datasets. The comparisons were made with a wide variety of other swarm-based algorithms and other feature selection methods. The results indicate that the proposed algorithms have superiorities over competitors.},
journal = {Neurocomput.},
month = oct,
pages = {354–371},
numpages = {18},
keywords = {Ant colony optimization, Feature selection, Hybrid algorithms, Swarm intelligence}
}

@inproceedings{10.1145/3205651.3208267,
author = {Xuan, Jifeng and Gu, Yongfeng and Ren, Zhilei and Jia, Xiangyang and Fan, Qingna},
title = {Genetic configuration sampling: learning a sampling strategy for fault detection of configurable systems},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208267},
doi = {10.1145/3205651.3208267},
abstract = {A highly-configurable system provides many configuration options to diversify application scenarios. The combination of these configuration options results in a large search space of configurations. This makes the detection of configuration-related faults extremely hard. Since it is infeasible to exhaust every configuration, several methods are proposed to sample a subset of all configurations to detect hidden faults. Configuration sampling can be viewed as a process of repeating a pre-defined sampling action to the whole search space, such as the one-enabled or pair-wise strategy.In this paper, we propose genetic configuration sampling, a new method of learning a sampling strategy for configuration-related faults. Genetic configuration sampling encodes a sequence of sampling actions as a chromosome in the genetic algorithm. Given a set of known configuration-related faults, genetic configuration sampling evolves the sequence of sampling actions and applies the learnt sequence to new configuration data. A pilot study on three highly-configurable systems shows that genetic configuration sampling performs well among nine sampling strategies in comparison.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1624–1631},
numpages = {8},
keywords = {configuration sampling, fault detection, genetic improvement, highly-configurable systems, software configurations},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1155/2018/5812872,
author = {Sharaf, Ahmed I. and El-Soud, Mohamed Abu and El-Henawy, Ibrahim M. and Louis, A. K.},
title = {An Automated Approach for Epilepsy Detection Based on Tunable Q-Wavelet and Firefly Feature Selection Algorithm},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-4188},
url = {https://doi.org/10.1155/2018/5812872},
doi = {10.1155/2018/5812872},
abstract = {Detection of epileptic seizures using an electroencephalogram (EEG) signals is a challenging task that requires a high level of skilled neurophysiologists. Therefore, computer-aided detection provides an asset to the neurophysiologist in interpreting the EEG. This paper introduces a novel approach to recognize and classify the epileptic seizure and seizure-free EEG signals automatically by an intelligent computer-aided method. Moreover, the prediction of the preictal phase of the epilepsy is proposed to assist the neurophysiologist in the clinic. The proposed method presents two perspectives for the EEG signal processing to detect and classify the seizures and seizure-free signals. The first perspectives consider the EEG signal as a nonlinear time series. A tunable Q-wavelet is applied to decompose the signal into smaller segments called subbands. Then a chaotic, statistical, and power spectrum features sets are extracted from each subband. The second perspectives process the EEG signal as an image; hence the gray-level co-occurrence matrix is determined from the image to obtain the textures of contrast, correlation, energy, and homogeneity. Due to a large number of features obtained, a feature selection algorithm based on firefly optimization was applied. The firefly optimization reduces the original set of features and generates a reduced compact set. A random forest classifier is trained for the classification and prediction of the seizures and seizure-free signals. Afterward, a dataset from the University of Bonn, Germany, is used for benchmarking and evaluation. The proposed approach provided a significant result compared with other recent work regarding accuracy, recall, specificity, F-measure, and Matthew’s correlation coefficient.},
journal = {Journal of Biomedical Imaging},
month = jan,
numpages = {12}
}

@article{10.1504/ijcat.2019.100297,
author = {Jayanthi, R. and Florence, M. Lilly},
title = {Improved Bayesian regularisation using neural networks based on feature selection for software defect prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {60},
number = {3},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2019.100297},
doi = {10.1504/ijcat.2019.100297},
abstract = {Demand for software-based applications has grown drastically in various real-time applications. However, software testing schemes have been developed which include manual and automatic testing. Manual testing requires human effort and chances of error may still affect the quality of software. To overcome this issue, automatic software testing techniques based on machine learning techniques have been developed. In this work, we focus on the machine learning scheme for early prediction of software defects using Levenberg-Marquardt algorithm (LM), Back Propagation (BP) and Bayesian Regularisation (BR) techniques. Bayesian regularisation achieves better performance in terms of bug prediction. However, this performance can be enhanced further. Hence, we developed a novel approach for attribute selection-based feature selection technique to improve the performance of BR classification. An extensive study is carried out with the PROMISE repository where we considered KC1 and JM1 datasets. Experimental study shows that the proposed approach achieves better performance in predicting the defects in software.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {225–241},
numpages = {16},
keywords = {defect prediction model, machine learning techniques, software defect prediction, software metrics, gradient descent optimisation, gradient-based approach, feature subset selection, cross entropy error function, adaptive computation process}
}

@inproceedings{10.1145/1835804.1835848,
author = {Cai, Deng and Zhang, Chiyuan and He, Xiaofei},
title = {Unsupervised feature selection for multi-cluster data},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835848},
doi = {10.1145/1835804.1835848},
abstract = {In many data analysis tasks, one is often confronted with very high dimensional data. Feature selection techniques are designed to find the relevant feature subset of the original features which can facilitate clustering, classification and retrieval. In this paper, we consider the feature selection problem in unsupervised learning scenario, which is particularly difficult due to the absence of class labels that would guide the search for relevant information. The feature selection problem is essentially a combinatorial optimization problem which is computationally expensive. Traditional unsupervised feature selection methods address this issue by selecting the top ranked features based on certain scores computed independently for each feature. These approaches neglect the possible correlation between different features and thus can not produce an optimal feature subset. Inspired from the recent developments on manifold learning and L1-regularized models for subset selection, we propose in this paper a new approach, called Multi-Cluster Feature Selection (MCFS), for unsupervised feature selection. Specifically, we select those features such that the multi-cluster structure of the data can be best preserved. The corresponding optimization problem can be efficiently solved since it only involves a sparse eigen-problem and a L1-regularized least squares problem. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {333–342},
numpages = {10},
keywords = {clustering, feature selection, unsupervised},
location = {Washington, DC, USA},
series = {KDD '10}
}

@article{10.1016/j.neucom.2017.02.053,
author = {Mursalin, Md and Zhang, Yuan and Chen, Yuehui and Chawla, Nitesh V},
title = {Automated epileptic seizure detection using improved correlation-based feature selection with random forest classifier},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {241},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.053},
doi = {10.1016/j.neucom.2017.02.053},
abstract = {Analysis of electroencephalogram (EEG) signal is crucial due to its non-stationary characteristics, which could lead the way to proper detection method for the treatment of patients with neurological abnormalities, especially for epilepsy. The performance of EEG-based epileptic seizure detection relies largely on the quality of selected features from an EEG data that characterize seizure activity. This paper presents a novel analysis method for detecting epileptic seizure from EEG signal using Improved Correlation-based Feature Selection method (ICFS) with Random Forest classifier (RF). The analysis involves, first applying ICFS to select the most prominent features from the time domain, frequency domain, and entropy based features. An ensemble of Random Forest (RF) classifiers is then learned on the selected set of features. The experimental results demonstrate that the proposed method shows better performance compared to the conventional Correlation-based method and also outperforms some other state-of-the-art methods of epileptic seizure detection using the same benchmark EEG dataset.},
journal = {Neurocomput.},
month = jun,
pages = {204–214},
numpages = {11},
keywords = {Correlation-based Feature Selection (CFS), Discrete Wavelet transformation (DWT), Electroencephalogram (EEG), Improved Correlation-based Feature Selection (ICFS), Random Forest (RF)}
}

@article{10.1016/j.ins.2021.01.020,
author = {Salesi, Sadegh and Cosma, Georgina and Mavrovouniotis, Michalis},
title = {TAGA: Tabu Asexual Genetic Algorithm embedded in a filter/filter feature selection approach for high-dimensional data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {565},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.01.020},
doi = {10.1016/j.ins.2021.01.020},
journal = {Inf. Sci.},
month = jul,
pages = {105–127},
numpages = {23}
}

@article{10.1016/j.comnet.2018.01.007,
author = {Shi, Hongtao and Li, Hongping and Zhang, Dan and Cheng, Chaqiu and Cao, Xuanxuan},
title = {An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification},
year = {2018},
issue_date = {Feb 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2018.01.007},
doi = {10.1016/j.comnet.2018.01.007},
journal = {Comput. Netw.},
month = feb,
pages = {81–98},
numpages = {18},
keywords = {Feature selection, Deep learning, Multi-class imbalance, Concept drift, Machine learning, Traffic classification}
}

@article{10.1109/TCBB.2021.3099068,
author = {Manduchi, Elisabetta and Le, Trang T. and Fu, Weixuan and Moore, Jason H.},
title = {Genetic Analysis of Coronary Artery Disease Using Tree-Based Automated Machine Learning Informed By Biology-Based Feature Selection},
year = {2021},
issue_date = {May-June 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3099068},
doi = {10.1109/TCBB.2021.3099068},
abstract = {Machine Learning (ML) approaches are increasingly being used in biomedical applications. Important challenges of ML include choosing the right algorithm and tuning the parameters for optimal performance. Automated ML (AutoML) methods, such as Tree-based Pipeline Optimization Tool (TPOT), have been developed to take some of the guesswork out of ML thus making this technology available to users from more diverse backgrounds. The goals of this study were to assess applicability of TPOT to genomics and to identify combinations of single nucleotide polymorphisms (SNPs) associated with coronary artery disease (CAD), with a focus on genes with high likelihood of being good CAD drug targets. We leveraged public functional genomic resources to group SNPs into biologically meaningful sets to be selected by TPOT. We applied this strategy to data from the U.K. Biobank, detecting a strikingly recurrent signal stemming from a group of 28 SNPs. Importance analysis of these SNPs uncovered functional relevance of the top SNPs to genes whose association with CAD is supported in the literature and other resources. Furthermore, we employed game-theory based metrics to study SNP contributions to individual-level TPOT predictions and discover distinct clusters of well-predicted CAD cases. The latter indicates a promising approach towards precision medicine.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1379–1386},
numpages = {8}
}

@article{10.1007/s00521-019-04331-5,
author = {Barushka, Aliaksandr and Hajek, Petr},
title = {Spam detection on social networks using cost-sensitive feature selection and ensemble-based regularized deep neural networks},
year = {2020},
issue_date = {May 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04331-5},
doi = {10.1007/s00521-019-04331-5},
abstract = {Spam detection on social networks is increasingly important owing to the rapid growth of social network user base. Sophisticated spam filters must be developed to deal with this complex problem. Traditional machine learning approaches such as neural networks, support vector machines and Na\"{\i}ve Bayes classifiers are not effective enough to process and utilize complex features present in high-dimensional data on social network spam. Moreover, the traditional objective criteria of social network spam filters cannot cope with different costs assigned to type I and type II errors. To overcome these problems, here we propose a novel cost-sensitive approach to social network spam filtering. The proposed approach is composed of two stages. In the first stage, multi-objective evolutionary feature selection is used to minimize both the misclassification cost of the proposed model and the number of attributes necessary for spam filtering. Then, the approach uses cost-sensitive ensemble learning techniques with regularized deep neural networks as base learners. We demonstrate that this approach is effective for social network spam filtering on two benchmark datasets. We also show that the proposed approach outperforms other popular algorithms used in social network spam filtering, such as random forest, Na\"{\i}ve Bayes or support vector machines.},
journal = {Neural Comput. Appl.},
month = may,
pages = {4239–4257},
numpages = {19},
keywords = {Neural network, Social networks, Regularization, Ensemble learning, Misclassification cost}
}

@article{10.1016/j.eswa.2012.11.016,
author = {Peteiro-Barral, D. and Bol\'{o}N-Canedo, V. and Alonso-Betanzos, A. and Guijarro-Berdi\~{n}As, B. and S\'{a}Nchez-Maro\~{n}O, N.},
title = {Toward the scalability of neural networks through feature selection},
year = {2013},
issue_date = {June, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.11.016},
doi = {10.1016/j.eswa.2012.11.016},
abstract = {In the past few years, the bottleneck for machine learning developers is not longer the limited data available but the algorithms inability to use all the data in the available time. For this reason, researches are now interested not only in the accuracy but also in the scalability of the machine learning algorithms. To deal with large-scale databases, feature selection can be helpful to reduce their dimensionality, turning an impracticable algorithm into a practical one. In this research, the influence of several feature selection methods on the scalability of four of the most well-known training algorithms for feedforward artificial neural networks (ANNs) will be analyzed over both classification and regression tasks. The results demonstrate that feature selection is an effective tool to improve scalability.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {2807–2816},
numpages = {10},
keywords = {Feature selection, High dimensional datasets, Machine learning, Neural networks}
}

@inproceedings{10.1145/2506583.2506600,
author = {Shu, Le and Ma, Tianyang and Latecki, Longin Jan},
title = {Stable Feature Selection with Minimal Independent Dominating Sets},
year = {2013},
isbn = {9781450324342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506583.2506600},
doi = {10.1145/2506583.2506600},
abstract = {In this paper, we focus on stable selection of relevant features. The main contribution is a novel framework for selecting most informative features which can preserve the linear combination property of the original feature space. We propose a novel formulation of this problem as selection of a minimal independent dominating set (MIDS). MIDS of a feature graph is a smallest subset such that no two of its nodes are connected and all other nodes are connected to at least one node in it. In this way, the diversity and coverage of the original feature space can be preserved.Furthermore, the proposed MIDS framework complements standard feature selection algorithms like SVM-RFE, stability lasso and ensemble SVM RFE. When these algorithms are applied to feature subsets selected by MIDS as opposed to all the input features, they select more stable features and achieve better prediction accuracy, as our experimental results clearly demonstrate.},
booktitle = {Proceedings of the International Conference on Bioinformatics, Computational Biology and Biomedical Informatics},
pages = {450–457},
numpages = {8},
keywords = {Feature Selection, Minimum Independent Dominating Sets, Stability},
location = {Wshington DC, USA},
series = {BCB'13}
}

@article{10.1016/j.patcog.2008.10.028,
author = {Liu, Huawen and Sun, Jigui and Liu, Lei and Zhang, Huijie},
title = {Feature selection with dynamic mutual information},
year = {2009},
issue_date = {July, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {7},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.10.028},
doi = {10.1016/j.patcog.2008.10.028},
abstract = {Feature selection plays an important role in data mining and pattern recognition, especially for large scale data. During past years, various metrics have been proposed to measure the relevance between different features. Since mutual information is nonlinear and can effectively represent the dependencies of features, it is one of widely used measurements in feature selection. Just owing to these, many promising feature selection algorithms based on mutual information with different parameters have been developed. In this paper, at first a general criterion function about mutual information in feature selector is introduced, which can bring most information measurements in previous algorithms together. In traditional selectors, mutual information is estimated on the whole sampling space. This, however, cannot exactly represent the relevance among features. To cope with this problem, the second purpose of this paper is to propose a new feature selection algorithm based on dynamic mutual information, which is only estimated on unlabeled instances. To verify the effectiveness of our method, several experiments are carried out on sixteen UCI datasets using four typical classifiers. The experimental results indicate that our algorithm achieved better results than other methods in most cases.},
journal = {Pattern Recogn.},
month = jul,
pages = {1330–1339},
numpages = {10},
keywords = {Classification, Feature selection, Filter method, Mutual information}
}

@article{10.5555/2445637.2445942,
author = {Chen, Bolun and Chen, Ling and Chen, Yixin},
title = {Efficient ant colony optimization for image feature selection},
year = {2013},
issue_date = {June, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
number = {6},
issn = {0165-1684},
abstract = {Feature selection (FS) is an important task which can significantly affect the performance of image classification and recognition. In this paper, we present a feature selection algorithm based on ant colony optimization (ACO). For n features, existing ACO-based feature selection methods need to traverse a complete graph with O(n^2) edges. However, we propose a novel algorithm in which the artificial ants traverse on a directed graph with only O(2n) arcs. The algorithm incorporates the classification performance and feature set size into the heuristic guidance, and selects a feature set with small size and high classification accuracy. We perform extensive experiments on two large image databases and 15 non-image datasets to show that our proposed algorithm can obtain higher processing speed as well as better classification accuracy using a smaller feature set than other existing methods.},
journal = {Signal Process.},
month = jun,
pages = {1566–1576},
numpages = {11},
keywords = {Ant colony optimization, Dimensionality reduction, Feature selection, Image classification}
}

@article{10.5555/1756006.1859900,
author = {Rodriguez-Lujan, Irene and Huerta, Ramon and Elkan, Charles and Cruz, Carlos Santa},
title = {Quadratic Programming Feature Selection},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {Identifying a subset of features that preserves classification accuracy is a problem of growing importance, because of the increasing size and dimensionality of real-world data sets. We propose a new feature selection method, named Quadratic Programming Feature Selection (QPFS), that reduces the task to a quadratic optimization problem. In order to limit the computational complexity of solving the optimization problem, QPFS uses the Nystr\"{o}m method for approximate matrix diagonalization. QPFS is thus capable of dealing with very large data sets, for which the use of other methods is computationally expensive. In experiments with small and medium data sets, the QPFS method leads to classification accuracy similar to that of other successful techniques. For large data sets, QPFS is superior in terms of computational efficiency.},
journal = {J. Mach. Learn. Res.},
month = aug,
pages = {1491–1516},
numpages = {26}
}

@article{10.1016/j.eswa.2014.07.052,
author = {Chernbumroong, Saisakul and Cang, Shuang and Yu, Hongnian},
title = {Maximum relevancy maximum complementary feature selection for multi-sensor activity recognition},
year = {2015},
issue_date = {January 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {1},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.07.052},
doi = {10.1016/j.eswa.2014.07.052},
abstract = {We propose a feature selection algorithm using MRMC.Show that MRMC provides a good result comparing to the 3 popular algorithms.The complementary measure improves the performance of the Clamping algorithm.Evaluate the proposed algorithm on 2 well-defined problems and 5 real life data sets. In the multi-sensor activity recognition domain, the input space is often large and contains irrelevant and overlapped features. It is important to perform feature selection in order to select the smallest number of features which can describe the outputs. This paper proposes a new feature selection algorithms using the maximal relevance and maximal complementary (MRMC) based on neural networks. Unlike other feature selection algorithms that are based on relevance and redundancy measurements, the idea of how a feature complements to the already selected features is utilized. The proposed algorithm is evaluated on two well-defined problems and five real world data sets. The data sets cover different types of data i.e. real, integer and category and sizes i.e. small to large set of features. The experimental results show that the MRMC can select a smaller number of features while achieving good results. The proposed algorithm can be applied to any type of data, and demonstrate great potential for the data set with a large number of features.},
journal = {Expert Syst. Appl.},
month = jan,
pages = {573–583},
numpages = {11},
keywords = {Activity recognition, Feature selection, Mutual information, Neural networks}
}

@article{10.1007/s00371-014-0984-8,
author = {Zhan, Jin and Su, Zhuo and Wu, Hefeng and Luo, Xiaonan},
title = {Robust tracking via discriminative sparse feature selection},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {0178-2789},
url = {https://doi.org/10.1007/s00371-014-0984-8},
doi = {10.1007/s00371-014-0984-8},
abstract = {In this paper, we propose a novel generative tracking approach based on discriminative sparse feature selection. The sparse features are the discriminative sparse representation of samples, which are achieved by learning a compact and discriminative dictionary. Besides the target templates, the proposed approach also incorporates the close-background templates to approximate the partial variations. We learn the dictionary and a classifier together, and search the tracking result with the maximum similarity and the minimal reconstruction error criterion using the discrimination of sparse features. In addition, we resample the close-background templates and update the dictionary in an adaptive way during tracking. Experimental results on several challenging video sequences demonstrate that the proposed approach has more favorable performance than the state-of-the-art approaches.},
journal = {Vis. Comput.},
month = may,
pages = {575–588},
numpages = {14},
keywords = {Discriminative sparse feature, Object tracking, Sparse representation, Template dictionary}
}

@inproceedings{10.1145/2491411.2491459,
author = {Kim, Chang Hwan Peter and Marinov, Darko and Khurshid, Sarfraz and Batory, Don and Souto, Sabrina and Barros, Paulo and D'Amorim, Marcelo},
title = {SPLat: lightweight dynamic analysis for reducing combinatorics in testing configurable systems},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491459},
doi = {10.1145/2491411.2491459},
abstract = {Many programs can be configured through dynamic and/or static selection of configuration variables. A software product line (SPL), for example, specifies a family of programs where each program is defined by a unique combination of features. Systematically testing SPL programs is expensive as it can require running each test against a combinatorial number of configurations. Fortunately, a test is often independent of many configuration variables and need not be run against every combination. Configurations that are not required for a test can be pruned from execution. This paper presents SPLat, a new way to dynamically prune irrelevant configurations: the configurations to run for a test can be determined during test execution by monitoring accesses to configuration variables. SPLat achieves an optimal reduction in the number of configurations and is lightweight compared to prior work that used static analysis and heavyweight dynamic execution. Experimental results on 10 SPLs written in Java show that SPLat substantially reduces the total test execution time in many cases. Moreover, we demonstrate the scalability of SPLat by applying it to a large industrial code base written in Ruby on Rails.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {Automated testing, Configurable Systems, Efficiency, Software Product Lines},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1007/s00500-015-1942-8,
author = {Bostani, Hamid and Sheikhan, Mansour},
title = {Hybrid of binary gravitational search algorithm and mutual information for feature selection in intrusion detection systems},
year = {2017},
issue_date = {May       2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {9},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-015-1942-8},
doi = {10.1007/s00500-015-1942-8},
abstract = {Intrusion detection systems (IDSs) play an important role in the security of computer networks. One of the main challenges in IDSs is the high-dimensional input data analysis. Feature selection is a solution to overcoming this problem. This paper presents a hybrid feature selection method using binary gravitational search algorithm (BGSA) and mutual information (MI) for improving the efficiency of standard BGSA as a feature selection algorithm. The proposed method, called MI-BGSA, used BGSA as a wrapper-based feature selection method for performing global search. Moreover, MI approach was integrated into the BGSA, as a filter-based method, to compute the feature---feature and the feature---class mutual information with the aim of pruning the subset of features. This strategy found the features considering the least redundancy to the selected features and also the most relevance to the target class. A two-objective function based on maximizing the detection rate and minimizing the false positive rate was defined as a fitness function to control the search direction of the standard BGSA. The experimental results on the NSL-KDD dataset showed that the proposed method can reduce the feature space dramatically. Moreover, the proposed algorithm found better subset of features and achieved higher accuracy and detection rate as compared to the some standard wrapper-based and filter-based feature selection methods.},
journal = {Soft Comput.},
month = may,
pages = {2307–2324},
numpages = {18},
keywords = {Anomaly detection, Binary gravitational search algorithm, Feature selection, Hybrid model, Intrusion detection system, Mutual information}
}

@article{10.1016/j.matcom.2019.07.011,
author = {Garc\'{\i}a-Nieto, P.J. and Garc\'{\i}a-Gonzalo, E. and Fern\'{a}ndez, J.R. Alonso and Mu\~{n}iz, C. D\'{\i}az},
title = {Modeling of the algal atypical increase in La Barca reservoir using the DE optimized least square support vector machine approach with feature selection},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {166},
number = {C},
issn = {0378-4754},
url = {https://doi.org/10.1016/j.matcom.2019.07.011},
doi = {10.1016/j.matcom.2019.07.011},
journal = {Math. Comput. Simul.},
month = dec,
pages = {461–480},
numpages = {20},
keywords = {Least square support vector machines (LS-SVM), Differential evolution (DE), Algal abnormal productivity in reservoirs, Feature selection, Regression analysis}
}

@article{10.1007/s10844-013-0243-x,
author = {Li, Bing and Chow, Tommy W. and Huang, Di},
title = {A novel feature selection method and its application},
year = {2013},
issue_date = {October   2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {41},
number = {2},
issn = {0925-9902},
url = {https://doi.org/10.1007/s10844-013-0243-x},
doi = {10.1007/s10844-013-0243-x},
abstract = {In this paper, a novel feature selection method based on rough sets and mutual information is proposed. The dependency of each feature guides the selection, and mutual information is employed to reduce the features which do not favor addition of dependency significantly. So the dependency of the subset found by our method reaches maximum with small number of features. Since our method evaluates both definitive relevance and uncertain relevance by a combined selection criterion of dependency and class-based distance metric, the feature subset is more relevant than other rough sets based methods. As a result, the subset is near optimal solution. In order to verify the contribution, eight different classification applications are employed. Our method is also employed on a real Alzheimer's disease dataset, and finds a feature subset where classification accuracy arrives at 81.3 %. Those present results verify the contribution of our method.},
journal = {J. Intell. Inf. Syst.},
month = oct,
pages = {235–268},
numpages = {34},
keywords = {Alzheimer's disease, Class-based distance metric, Feature selection, Mutual information, Rough sets}
}

@article{10.1016/j.cviu.2016.02.003,
author = {Yang, Jing and Zhang, Kaihua and Liu, Qingshan},
title = {Robust object tracking by online Fisher discrimination boosting feature selection},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {153},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2016.02.003},
doi = {10.1016/j.cviu.2016.02.003},
abstract = {A novel online Fisher discrimination boosting feature selection method is proposed for tracking.A particle filtering framework with the context information around the particles is exploited to enhance the robustness of tracking.It outperforms 29 representative algorithms in the CVPR2013 tracking benchmark. Large appearance changes in visual tracking affect the tracking performance severely. To address this challenge, in this paper we develop an effective appearance model with the highly discriminative features. We propose an online Fisher discrimination boosting feature selection mechanism, which selects features that reduce the with-in scatter while enlarging the between-class scatter, thereby enhancing the discriminative capability between the target and background. Moreover, we utilize a particle filtering framework for visual tracking, in which the weights of candidate particles take into account the context information around the particles, thereby enhancing the robustness of tracking. In order to increase efficiency, a coarse-to-fine search strategy is exploited to efficiently and accurately locate the target. Extensive experiments on the CVPR2013 tracking benchmark demonstrate the competitive performance of our algorithm over other representative algorithms in terms of accuracy and robustness.},
journal = {Comput. Vis. Image Underst.},
month = dec,
pages = {100–108},
numpages = {9},
keywords = {Boosting, Fisher discrimination, Particle filter, Visual tracking}
}

@article{10.5555/2873819.2873855,
author = {Ben Brahim, Afef and Limam, Mohamed},
title = {A hybrid feature selection method based on instance learning and cooperative subset search},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {69},
number = {C},
issn = {0167-8655},
abstract = {A hybrid feature selection method is proposed for classification in small sample size data sets.The filter step is based on instance learning taking advantage of the small sample size of data.A few candidate feature subsets are generated since their number corresponds to the number of instances.Cooperative feature subset search is proposed with a classifier algorithm for the wrapper step.The proposed method improves classification accuracy and stability of feature selection. The problem of selecting the most useful features from thousands of candidates in a low sample size data set arises in many areas of modern sciences. Feature subset selection is a key problem in such data mining classification tasks. In practice, it is very common to use filter methods. However, they ignore the correlations between genes which are prevalent in gene expression data. On the other hand, standard wrapper algorithms cannot be applied because of their complexity. Additionally, existing methods are not specially conceived to handle the small sample size of the data which is one of the main causes of feature selection instability. In order to deal with these issues, we propose a new hybrid, filter wrapper, approach based on instance learning. Its main challenge is that it converts the problem of the small sample size to a tool that allows choosing only a few subsets of features in a filter step. A cooperative subset search, CSS, is then proposed with a classifier algorithm to represent an evaluation system of wrappers. Our method is experimentally tested and compared with state-of-the-art algorithms based on several high-dimensional low sample size cancer datasets. Results show that our proposed approach outperforms other methods in terms of accuracy and stability of the selected subset.},
journal = {Pattern Recogn. Lett.},
month = jan,
pages = {28–34},
numpages = {7},
keywords = {Classification, Feature selection, Hybrid, Small sample size, Stability}
}

@article{10.3233/JIFS-152073,
author = {Gollou, Abbas Rahimi and Ghadimi, Noradin},
title = {A new feature selection and hybrid forecast&nbsp;engine for day-ahead price forecasting of&nbsp;electricity markets},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-152073},
doi = {10.3233/JIFS-152073},
abstract = {In this paper, a new feature selection and forecast engine is presented for day ahead prediction of electricity prices, which are so valuable for both producers and consumers in the new competitive electric power markets. In a competitive electricity market, forecast of energy prices is a key information for the market participants. However, price signal usually has a complex behavior due to its nonlinearity, non-stationary, and time variance. Also, an appropriate feature selection is crucial for accurate forecasting. In this paper, a two-step approach that identifies a set of candidate features based on the data characteristics proposed and then selects a subset of them using correlation and instance-based feature selection methods, applied in a systematic way. Then, a combination of wavelet transform (WT) and a hybrid forecast method is presented based on neural network (NN) and an optimization algorithms. The proposed method is examined on PJM electricity market and compared with some of the most recent price forecast methods. These comparisons illustrate effectiveness of the proposed strategy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {4031–4045},
numpages = {15},
keywords = {Neural network, price forecast, feature selection, hybrid forecast engine}
}

@inproceedings{10.1145/3424978.3425090,
author = {Shakhgeldyan, Karina and Geltser, Boris and Rublev, Vladislav and Shirobokov, Basil and Geltser, Dan and Kriger, Alexandra},
title = {Feature Selection Strategy for Intrahospital Mortality Prediction after Coronary Artery Bypass Graft Surgery on an Unbalanced Sample},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425090},
doi = {10.1145/3424978.3425090},
abstract = {The aim of the study is to develop models of intrahospital mortality (IHM) prediction on an unbalanced sample of patients with coronary artery disease (CAD) post coronary artery bypass graft (CABG) surgery. Methods. Models for IHM prediction were built following the analysis of 866 electronic case histories based on the analysis of CAD patients, revascularized with the CABG operation. The patient cohort consisted of two groups. The first included 35 (4%) patients who died within the first 30 days after CABG, the second - 831 (96%) patients with a favorable operation outcome. We analyzed 99 factors, including the results of clinical, laboratory and instrumental studies obtained before CABG. For feature compilation, classical filtering and model selection methods were used (wrapper method). The primary drawback to applying a classical approach was the unbalanced sample as one cohort only consisted of 4% of subjects. In that case, it was not possible to apply the cross-validation procedure with three types of samples, standard quality metrics and multi-category factors. Results. Features searching approach using the multi-stage selection procedure, which combined the validation of predefined predictors, filtering methods and multifactor model development based on logistic regression, random forest (RF) and artificial neural networks (ANNs) was proposed. The models' accuracy was evaluated by a combined quality metric. RF and ANNs based models allowed not only to build more accurate forecasting tools, but also assisted in verifying five additional IHM predictors.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {108},
numpages = {7},
keywords = {Evaluation metrics, Filter features selection method, Unbalanced sampling, Wrapper features selection method},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.5555/2908286.2908300,
author = {Han, Yahong and Zhang, Jianguang and Xu, Zhongwen and Yu, Shoou-I},
title = {Discriminative multi-task feature selection},
year = {2013},
publisher = {AAAI Press},
abstract = {The effectiveness of supervised feature selection degrades in low training data scenarios. We propose to alleviate this problem by augmenting per-task feature selection with joint feature selection over multiple tasks. Our algorithm builds on the assumption that different tasks have shared structure which could be utilized to cope with data sparsity. The proposed trace-ratio based model not only selects discriminative features for each task, but also finds features which are discriminative over all tasks. Extensive experiment on different data sets demonstrates the effectiveness of our algorithm in low training data scenarios.},
booktitle = {Proceedings of the 17th AAAI Conference on Late-Breaking Developments in the Field of Artificial Intelligence},
pages = {41–43},
numpages = {3},
series = {AAAIWS'13-17}
}

@article{10.4018/IJAMC.2018040103,
author = {Belattar, Khadidja and Mostefai, Sihem and Draa, Amer},
title = {A Hybrid GA-LDA Scheme for Feature Selection in Content-Based Image Retrieval},
year = {2018},
issue_date = {April 2018},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {2},
issn = {1947-8283},
url = {https://doi.org/10.4018/IJAMC.2018040103},
doi = {10.4018/IJAMC.2018040103},
abstract = {Feature selection is an important pre-processing technique in the pattern recognition domain. This article proposes a hybridization between Genetic Algorithm GA and the Linear Discriminant Analysis LDA for solving the feature selection problem in Content-Based Image Retrieval CBIR applied to dermatological images. In the first step, we preprocess and segment the input image, then we derive color and texture features characterizing healthy skin and the segmented skin lesion. At this stage, a binary GA is used to evolve chromosome subsets whose fitness is evaluated by a Logistic Regression classifier. The optimal identified features are then used to feed LDA for a CBIR system, based on a K-Nearest Neighbor classification. To assess the proposed approach, the authors have opted for a K-fold cross validation method on a database of 1097 images of melanomas and other skin lesions. As a result, the authors obtained a reduced number of features and an improved CBDIR system compared to PCA, LDA and ICA methods.},
journal = {Int. J. Appl. Metaheuristic Comput.},
month = apr,
pages = {48–71},
numpages = {24},
keywords = {Content-Based Image Retrieval, Cross Validation, Genetic Algorithm, K-Nearest Neighbor Classification, Linear Discriminant Analysis, Logistic Regression}
}

@article{10.1007/s10115-013-0722-y,
author = {Xiao, Jin and Xiao, Yi and Huang, Anqiang and Liu, Dunhu and Wang, Shouyang},
title = {Feature-selection-based dynamic transfer ensemble model for customer churn prediction},
year = {2015},
issue_date = {April     2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {43},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-013-0722-y},
doi = {10.1007/s10115-013-0722-y},
abstract = {Customer churn prediction is one of the key steps to maximize the value of customers for an enterprise. It is difficult to get satisfactory prediction effect by traditional models constructed on the assumption that the training and test data are subject to the same distribution, because the customers usually come from different districts and may be subject to different distributions in reality. This study proposes a feature-selection-based dynamic transfer ensemble (FSDTE) model that aims to introduce transfer learning theory for utilizing the customer data in both the target and related source domains. The model mainly conducts a two-layer feature selection. In the first layer, an initial feature subset is selected by GMDH-type neural network only in the target domain. In the second layer, several appropriate patterns from the source domain to target training set are selected, and some features with higher mutual information between them and the class variable are combined with the initial subset to construct a new feature subset. The selection in the second layer is repeated several times to generate a series of new feature subsets, and then, we train a base classifier in each one. Finally, a best base classifier is selected dynamically for each test pattern. The experimental results in two customer churn prediction datasets show that FSDTE can achieve better performance compared with the traditional churn prediction strategies, as well as three existing transfer learning strategies.},
journal = {Knowl. Inf. Syst.},
month = apr,
pages = {29–51},
numpages = {23},
keywords = {Customer churn prediction, Feature selection, GMDH-type neural network, Transfer ensemble model, Transfer learning}
}

@article{10.1016/j.eswa.2016.11.024,
author = {Pacheco, Fannia and Cerrada, Mariela and S\'{a}nchez, Ren\'{e}-Vinicio and Cabrera, Diego and Li, Chuan and Valente de Oliveira, Jos\'{e}},
title = {Attribute clustering using rough set theory for feature selection in fault severity classification of rotating machinery},
year = {2017},
issue_date = {April 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {71},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.11.024},
doi = {10.1016/j.eswa.2016.11.024},
abstract = {A novel algorithm is proposed for unsupervised feature selection.The algorithm efficacy is evaluated through the accuracy of several classifiers.Adequate attributes are effectively selected for several case studies.The proposal presents better results than other attribute clustering algorithms.The proposal provides similar results to supervised feature selection approaches. Features extracted from real world applications increase dramatically, while machine learning methods decrease their performance given the previous scenario, and feature reduction is required. Particularly, for fault diagnosis in rotating machinery, the number of extracted features are sizable in order to collect all the available information from several monitored signals. Several approaches lead to data reduction using supervised or unsupervised strategies, where the supervised ones are the most reliable and its main disadvantage is the beforehand knowledge of the fault condition. This work proposes a new unsupervised algorithm for feature selection based on attribute clustering and rough set theory. Rough set theory is used to compute similarities between features through the relative dependency. The clustering approach combines classification based on distance with clustering based on prototype to group similar features, without requiring the number of clusters as an input. Additionally, the algorithm has an evolving property that allows the dynamic adjustment of the cluster structure during the clustering process, even when a new set of attributes feeds the algorithm. That gives to the algorithm an incremental learning property, avoiding a retraining process. These properties define the main contribution and significance of the proposed algorithm. Two fault diagnosis problems of fault severity classification in gears and bearings are studied to test the algorithm. Classification results show that the proposed algorithm is able to select adequate features as accurate as other feature selection and reduction approaches.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {69–86},
numpages = {18},
keywords = {Attribute clustering, Fault severity classification, Feature selection, Rotating machinery, Rough set}
}

@article{10.1016/j.eswa.2017.06.030,
author = {Aladeemy, Mohammed and Tutun, Salih and Khasawneh, Mohammad T.},
title = {A new hybrid approach for feature selection and support vector machine model selection based on self-adaptive cohort intelligence},
year = {2017},
issue_date = {December 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.06.030},
doi = {10.1016/j.eswa.2017.06.030},
abstract = {The limitations of cohort intelligence algorithm are identified.A new variation of cohort intelligence algorithm is proposed.The proposed algorithm employs self-adaptive scheme and mutation operator.A new hybrid approach for feature selection and SVM model selection is proposed.The proposed algorithm outperforms the original cohort intelligence algorithm. This research proposes a new hybrid approach for feature selection and Support Vector Machine (SVM) model selection based on a new variation of Cohort Intelligence (CI) algorithm. Feature selection can improve the accuracy of classification algorithms and reduce their computation complexity by removing the irrelevant and redundant features. SVM is a classification algorithm that has been used in many areas, such as bioinformatics and pattern recognition. However, the classification accuracy of SVM depends mainly on tuning its hyperparameters (i.e., SVM model selection). This paper presents a framework that is comprised of the following two major components. First, Self-Adaptive Cohort Intelligence (SACI) algorithm is proposed, which is a new variation of the emerging metaheuristic algorithm, Cohort Intelligence (CI). Second, SACI is integrated with SVM resulting in a new hybrid approach referred to as SVMSACI for simultaneous feature selection and SVM model selection. SACI differs from CI by employing tournament-based mutation and self-adaptive scheme for sampling interval and mutation rate. Furthermore, SACI is both real-coded and binary-coded, which makes it directly applicable to both binary and continuous domains. The performance of SACI for feature selection and SVM model selection was examined using ten benchmark datasets from the literature and compared with those of CI and five well-known metaheuristics, namely, Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Differential Evolution (DE) and Artificial Bee Colony (ABC). The comparative results demonstrate that SACI outperformed CI and comparable to or better than the other compared metaheuristics in terms of the SVM classification accuracy and dimensionality reduction. In addition, SACI requires less tuning efforts as the number of its control parameters is less than those of the other compared metaheuristics due to adopting the self-adaptive scheme in SACI. Finally, this research suggests employing more efficient methods for high-dimensional or large datasets due to the relatively high training time required by search strategies based on metaheuristics when applied to such datasets.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {118–131},
numpages = {14},
keywords = {Classification, Cohort intelligence, Feature selection, Metaheuristic, SVM}
}

@inproceedings{10.5555/2540128.2540327,
author = {Han, Yahong and Yang, Yi and Zhou, Xiaofang},
title = {Co-regularized ensemble for feature selection},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {Supervised feature selection determines feature relevance by evaluating feature's correlation with the classes. Joint minimization of a classifier's loss function and an l2,1-norm regularization has been shown to be effective for feature selection. However, the appropriate feature subset learned from different classifiers' loss function may be different. Less effort has been made on improving the performance of feature selection by the ensemble of different classifiers' criteria and take advantages of them. Furthermore, for the cases when only a few labeled data per class are available, over-fitting would be a potential problem and the performance of each classifier is restrained. In this paper, we add a joint l2,1-norm on multiple feature selection matrices to ensemble different classifiers' loss function into a joint optimization framework. This added co-regularization term has twofold role in enhancing the effect of regularization for each criterion and uncovering common irrelevant features. The problem of over-fitting can be alleviated and thus the performance of feature selection is improved. Extensive experiment on different data types demonstrates the effectiveness of our algorithm.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {1380–1386},
numpages = {7},
location = {Beijing, China},
series = {IJCAI '13}
}

@article{10.1504/IJBIC.2016.081326,
author = {Sahoo, Anita and Chandra, Satish},
title = {Improved cervix lesion classification using multi-objective binary firefly algorithm-based feature selection},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {6},
issn = {1758-0366},
url = {https://doi.org/10.1504/IJBIC.2016.081326},
doi = {10.1504/IJBIC.2016.081326},
abstract = {Cervical cancer is one of the vital and most frequent cancers, but can be cured if correctly diagnosed. This work is a novel effort towards developing a methodology for effective characterisation of cervix lesions that may assist radiologists in the diagnostic process by providing a reliable and objective discrimination of benign and malignant lesions in contrast enhanced CT-Scan images. Feature selection, which is a key stage in building such efficient classification models, is NP-hard; where, randomised algorithms do better. Since, firefly algorithm is an efficient biologically inspired randomised algorithm; here it has been utilised for optimal feature selection. This paper presents a multi-objective binary firefly algorithm for wrapper-based feature selection and utilises the selected feature subset for improved classification of cervix lesions. For experiments, contrast enhanced CT-Scan images of 22 patients have been used, where all lesions had been recommended for surgical biopsy by specialists. For characterisation of lesions, grey-level cooccurrence matrix-based texture features are extracted from two-level decomposition of wavelet coefficients. The objective function is designed to minimise the classification error and feature subset length both; making it multi-objective. With 94% accuracy in lesion classification, it has superior performance and greatly reduced execution time than multi-objective genetic algorithm-based feature selection.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {367–378},
numpages = {12},
keywords = {CT scans, benign lesions, bio-inspired computation, cervical cancer, cervix lesion characterisation, cervix lesions, computed tomography, firefly algorithm, lesion classification, malignant lesions, medical images, multi-objective optimisation, textural features, wavelet features, wrapper-based feature selection}
}

@article{10.1016/j.neucom.2014.11.098,
author = {Shi, Yinghuan and Gao, Yaozong and Liao, Shu and Zhang, Daoqiang and Gao, Yang and Shen, Dinggang},
title = {A learning-based CT prostate segmentation method via joint transductive feature selection and regression},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {173},
number = {P2},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2014.11.098},
doi = {10.1016/j.neucom.2014.11.098},
abstract = {In recent years, there has been a great interest in prostate segmentation, which is an important and challenging task for CT image guided radiotherapy. In this paper, a learning-based segmentation method via joint transductive feature selection and transductive regression is presented, which incorporates the physicians simple manual specification (only taking a few seconds), to aid accurate segmentation, especially for the case with large irregular prostate motion. More specifically, for the current treatment image, experienced physician is first allowed to manually assign the labels for a small subset of prostate and non-prostate voxels, especially in the first and last slices of the prostate regions. Then, the proposed method follows the two step: in prostate-likelihood estimation step, two novel algorithms, tLasso and wLapRLS, will be sequentially employed for transductive feature selection and transductive regression, respectively, aiming to generate the prostate-likelihood map. In multi-atlases based label fusion step, the final segmentation result will be obtained according to the corresponding prostate-likelihood map and the previous images of the same patient. The proposed method has been substantially evaluated on a real prostate CT dataset including 24 patients with 330 CT images, and compared with several state-of-the-art methods. Experimental results show that the proposed method outperforms the state-of-the-arts in terms of higher Dice ratio, higher true positive fraction, and lower centroid distances. Also, the results demonstrate that simple manual specification can help improve the segmentation performance, which is clinically feasible in real practice.},
journal = {Neurocomput.},
month = jan,
pages = {317–331},
numpages = {15},
keywords = {Feature selection, Prostate segmentation, Transductive learning}
}

@article{10.1016/j.patcog.2016.11.007,
author = {Cao, Peng and Liu, Xiaoli and Yang, Jinzhu and Zhao, Dazhe and Li, Wei and Huang, Min and Zaiane, Osmar},
title = {A multi-kernel based framework for heterogeneous feature selection and over-sampling for computer-aided detection of pulmonary nodules},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {64},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.007},
doi = {10.1016/j.patcog.2016.11.007},
abstract = {Classification plays a critical role in False Positive Reduction (FPR) in lung nodule Computer Aided Detection (CAD). To achieve effective recognition of nodule, many machine learning methods have been proposed. However, multiple heterogeneous feature subsets, high dimensional irrelevant features, as well as imbalanced distribution between the nodule and non-nodule classes typically makes this problem challenging. To solve these challenges, we proposed a multi-kernel based framework for feature selection and imbalanced data learning in Lung nodule CAD, involving multiple kernel learning with a \'{z} 2 , 1 norm regularizer for heterogeneous feature fusion and selection from the feature subset level, a multi-kernel feature selection based on pairwise similarities from the feature level, and a multi-kernel over-sampling for the imbalanced data learning. Experimental results demonstrate the effectiveness of the proposed method in terms of Geometric mean (G-mean) and Area under the ROC curve (AUC), and consistently outperform the competing methods. HighlightsProposed a unified multiple kernel framework to classify potential nodule objects.Regularized multiple kernel with l 2 , 1 \'{z}norm to fuse the heterogeneous feature subsets.Two different feature selection from heterogeneous feature subsets.Over-sampling positive instances in kernel space for imbalanced data.},
journal = {Pattern Recogn.},
month = apr,
pages = {327–346},
numpages = {20},
keywords = {Classification, False positive reduction, Feature selection, Imbalanced data learning, Lung nodule detection, Multi-kernel learning}
}

@article{10.1162/EVCO_a_00102,
author = {Garc\'{\i}a-Pedrajas, Nicol\'{a}s and de Haro-Garc\'{\i}a, Aida and P\'{e}rez-Rodr\'{\i}guez, Javier},
title = {A scalable memetic algorithm for simultaneous instance and feature selection},
year = {2014},
issue_date = {Spring 2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {22},
number = {1},
issn = {1063-6560},
url = {https://doi.org/10.1162/EVCO_a_00102},
doi = {10.1162/EVCO_a_00102},
abstract = {Instance selection is becoming increasingly relevant due to the huge amount of data that is constantly produced in many fields of research. At the same time, most of the recent pattern recognition problems involve highly complex datasets with a large number of possible explanatory variables. For many reasons, this abundance of variables significantly harms classification or recognition tasks. There are efficiency issues, too, because the speed of many classification algorithms is largely improved when the complexity of the data is reduced. One of the approaches to address problems that have too many features or instances is feature or instance selection, respectively. Although most methods address instance and feature selection separately, both problems are interwoven, and benefits are expected from facing these two tasks jointly. This paper proposes a new memetic algorithm for dealing with many instances and many features simultaneously by performing joint instance and feature selection. The proposed method performs four different local search procedures with the aim of obtaining the most relevant subsets of instances and features to perform an accurate classification. A new fitness function is also proposed that enforces instance selection but avoids putting too much pressure on removing features. We prove experimentally that this fitness function improves the results in terms of testing error. Regarding the scalability of the method, an extension of the stratification approach is developed for simultaneous instance and feature selection. This extension allows the application of the proposed algorithm to large datasets. An extensive comparison using 55 medium to large datasets from the UCI Machine Learning Repository shows the usefulness of our method. Additionally, the method is applied to 30 large problems, with very good results. The accuracy of the method for class-imbalanced problems in a set of 40 datasets is shown. The usefulness of the method is also tested using decision trees and support vector machines as classification methods.},
journal = {Evol. Comput.},
month = mar,
pages = {1–45},
numpages = {45},
keywords = {Memetic algorithms, feature selection, instance selection, scaling-up}
}

@article{10.1504/IJDMB.2018.093683,
title = {A multi-objective feature selection and classifier ensemble technique for microarray data analysis},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {2},
issn = {1748-5673},
url = {https://doi.org/10.1504/IJDMB.2018.093683},
doi = {10.1504/IJDMB.2018.093683},
abstract = {Since last few years, microarray technology has got tremendous application in many biomedical researches. Many intelligent models have been developed with different biological interpretation. This work presents a multi-objective feature selection and classifier ensemble MOFSCE technique for microarray data. MOFSCE works in two phases. The first phase is a pre-processing step where bi-objective optimisation technique is used to identify the significant genes through Pareto front. Here seven feature ranking approaches are used to develop 21 bi-objective feature selection BOFS models. The performance of BOFS model varies with different datasets. Therefore, grading system is used to identify stable BOFS model. In the second phase a classifier ensemble is build up that receives selected features from the identified BOFS model. Output of the classifiers is presented to a harmony search based functional link artificial neural network HSFLANN for decision. Performance of MOFSCE is evaluated using seven publicly available microarray datasets.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {123–160},
numpages = {38}
}

@article{10.1109/TASLP.2021.3097215,
author = {Azadi, Hamid and Akbarzadeh-T, Mohammad-R. and Kobravi, Hamid-R. and Shoeibi, Ali},
title = {Robust Voice Feature Selection Using Interval Type-2 Fuzzy AHP for Automated Diagnosis of Parkinson's Disease},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3097215},
doi = {10.1109/TASLP.2021.3097215},
abstract = {Goal: Human voice is a promising noninvasive indicator for diagnosing Parkinson's Disease (PD). It is also unique since it can be collected remotely, increasing accessibility to a wide range of underprivileged patients. However, recognizing PD's signature in the human voice is nontrivial since the available features are many, and the signal may be noisy. Methods: A new mechanism based on Interval Type-2 Fuzzy Analytical Hierarchy Process is proposed here for choosing a reduced feature set from 339 dysphonia speech features, based on five criteria of 1) Robustness, 2) Relief, 3) Minimum Redundancy and Maximum Relevance, 4) Gaussian mixture model separation, and 5) Classifier separation ability. A Least Squares Support Vector Machine then categorizes the samples as belonging to either a healthy subject or a patient with PD. The database of 47 subjects with an average age of 67 is obtained from the elderly in nursing homes and Parkinson's specialized clinics. By reducing signal quality similar to a standard phone line, we study the teleoperation prospect of the proposed technique. Results: Ten-fold cross-validation shows an overall accuracy of 95.32%(93.11%) for noiseless(noisy) conditions, with separate analysis for male, female, and both genders populations. Furthermore, Leave-One-Speaker-Out analysis yields an overall accuracy of 93.11%(84.61%) for noiseless(noisy) conditions. Conclusion: The proposed strategy offers viable remote PD diagnosis with higher accuracy for the male population. Significance: The proposed method suggests reduced feature sets that meet differing objectives of simplicity, performance, and robustness. Results could be particularly significant in PD diagnosis in remote areas.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2792–2802},
numpages = {11}
}

@inproceedings{10.1145/2513228.2513313,
author = {Trivedi, Shrawan Kumar and Dey, Shubhamoy},
title = {Effect of feature selection methods on machine learning classifiers for detecting email spams},
year = {2013},
isbn = {9781450323482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513228.2513313},
doi = {10.1145/2513228.2513313},
abstract = {This research presents the effects of using features selected by two feature selection methods i.e. Genetic Search and Greedy Stepwise Search on popular Machine Learning Classifiers like Bayesian, Naive Bayes, Support Vector Machine and Genetic Algorithm. Tests were performed on two different publicly available spam email datasets: "Enron" and "SpamAssassin". Results show that, Greedy Stepwise Search is a good method for feature selection for spam email detection. Among the Machine Learning Classifiers, Support Vector Machine has been found to be the best both in terms of accuracy and False Positive rate},
booktitle = {Proceedings of the 2013 Research in Adaptive and Convergent Systems},
pages = {35–40},
numpages = {6},
keywords = {classification accuracy, email spam classification, evolutionary algorithms, false positive rate, feature selection},
location = {Montreal, Quebec, Canada},
series = {RACS '13}
}

@inproceedings{10.1109/ICMLA.2013.13,
author = {Gao, Kehan and Khoshgoftaar, Taghi and Napolitano, Amri},
title = {Improving Software Quality Estimation by Combining Boosting and Feature Selection},
year = {2013},
isbn = {9780769551449},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2013.13},
doi = {10.1109/ICMLA.2013.13},
abstract = {The predictive accuracy of a classification modelis often affected by the quality of training data. However, there are two problems which may affect the quality of the training data: high dimensionality (too many independent attributes in a dataset) and class imbalance (many more instances of one class than the other class in a binary-classification problem). In this study, we present an iterative feature selection approach working with an ensemble learning method to solve both of these problems. The iterative feature selection approach samples the dataset k times and applies feature ranking to each sampled dataset, the k different rankings are then aggregated to create a single feature ranking. The ensemble learning method used is RUSBoost, in which random under sampling(RUS) is integrated into a boosting algorithm. The main purpose of this paper is to investigate the impact of feature selection as well as the RUSBoost approach on the classification performance in the context of software quality prediction. In the experiment, we explore six rankers, each used along with RUS in the iterative feature selection process. Following feature selection, models are built either using a plain learner or byusing the RUSBoost algorithm. We also examine the case of no feature selection and use this as the baseline for comparisons. The experimental results demonstrate that with the exception of one learner, feature selection combined with boosting provides better classification performance than when either is applied alone or when neither are applied.},
booktitle = {Proceedings of the 2013 12th International Conference on Machine Learning and Applications - Volume 01},
pages = {27–33},
numpages = {7},
keywords = {Data Sampling, Feature Selection, Performance Metric, RUSBoost, Software Quality Classification},
series = {ICMLA '13}
}

@article{10.1016/j.neucom.2019.11.071,
author = {Lim, Hyunki and Kim, Dae-Won},
title = {MFC: Initialization method for multi-label feature selection based on conditional mutual information},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {382},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.071},
doi = {10.1016/j.neucom.2019.11.071},
journal = {Neurocomput.},
month = mar,
pages = {40–51},
numpages = {12},
keywords = {Multi-label feature selection, Evolutionary algorithm, Mutual information}
}

@inproceedings{10.1145/3007669.3007701,
author = {Chen, Dongmei and Zhang, Jingcheng and Yuan, Lin},
title = {Feature Selection and Analysis of Powdery Mildew of Winter Wheat based on Multi-Temporal Satellite Imagery},
year = {2016},
isbn = {9781450348508},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3007669.3007701},
doi = {10.1145/3007669.3007701},
abstract = {The powdery mildew of wheat cause visible and obvious image changes and lead to different spectral reflections, which can be detected by the environment and disaster reduction small satellites. The information of satellites can be transformed into many kinds of indices, some of which could be sensitive and effective in studying and detecting the powdery mildew in wheat, however, it is still unknown whether all of these features are useful in evaluating and classifying the severity of the infected plants. In this paper, we try to study and analyze the relationship and functions of the features to find a more effective combination of features in estimating and evaluating the plants and fields.We studied the feature selection and analysis in the process to discuss whether they are all necessary in the classification.Based on the multi-temporal moderate resolution images, it is found that SR_T4, NDVI_T4, MSR_T4 carry the same information and that MSR_T4 is more suitable in this study. The two-stage features are reconfirmed to be powerful in the study and enrich the results of the classification. But NDVI_T42 can be eliminated and hardly has an effect on the results. However the universality of the conclusion still needs further study due to the limitation of the data dimensionality and the analysis measure selection},
booktitle = {Proceedings of the International Conference on Internet Multimedia Computing and Service},
pages = {251–254},
numpages = {4},
keywords = {feature selection, powdery mildew},
location = {Xi'an, China},
series = {ICIMCS'16}
}

@article{10.5555/2770961.2771096,
author = {Shang, Changxing and Li, Min and Feng, Shengzhong and Jiang, Qingshan and Fan, Jianping},
title = {Feature selection via maximizing global information gain for text classification},
year = {2013},
issue_date = {December 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {0950-7051},
abstract = {A novel feature selection metric called global information gain (GIG) is proposed.An efficient algorithm called maximizing global information gain (MGIG) is developed.MGIG performs better than other algorithms (IG, mRMR, JMI, DISR) in most cases.MGIG runs significantly faster than mRMR, JMI and DISR, and comparable with IG. Feature selection is a vital preprocessing step for text classification task used to solve the curse of dimensionality problem. Most existing metrics (such as information gain) only evaluate features individually but completely ignore the redundancy between them. This can decrease the overall discriminative power because one feature's predictive power is weakened by others. On the other hand, though all higher order algorithms (such as mRMR) take redundancy into account, the high computational complexity renders them improper in the text domain. This paper proposes a novel metric called global information gain (GIG) which can avoid redundancy naturally. An efficient feature selection method called maximizing global information gain (MGIG) is also given. We compare MGIG with four other algorithms on six datasets, the experimental results show that MGIG has better results than others methods in most cases. Moreover, MGIG runs significantly faster than the traditional higher order algorithms, which makes it a proper choice for feature selection in text domain.},
journal = {Know.-Based Syst.},
month = dec,
pages = {298–309},
numpages = {12},
keywords = {Distributional clustering, Feature selection, High dimensionality, Information bottleneck, Text classification}
}

@article{10.5555/3202704.3202705,
author = {Han, Bin and Ryzhov, Ilya O. and Defourny, Boris},
title = {Optimal Learning in Linear Regression with Combinatorial Feature Selection},
year = {2016},
issue_date = {November 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {28},
number = {4},
issn = {1526-5528},
abstract = {We present a new framework for sequential information collection in applications where regression is used to learn about a set of unknown parameters and alternates with optimization to design new data points. Such problems can be handled using the framework of ranking and selection R&amp;S, but traditional R&amp;S procedures will experience high computational costs when the decision space grows combinatorially. This challenge arises in many applications of business analytics; in particular, we are motivated by the problem of efficiently learning effective strategies for nonprofit fundraising. We present a value of information procedure for simultaneously learning unknown regression parameters and unknown sampling noise. We then develop approximate versions of the procedure, based on optimal quantization, that retain good performance and scale better to large problems.},
journal = {INFORMS J. on Computing},
month = nov,
pages = {721–735},
numpages = {15},
keywords = {Bayesian linear regression, optimal learning, ranking and selection, simulation optimization}
}

@inproceedings{10.1145/1982185.1982436,
author = {Hu, Yeming and Milios, Evangelos E. and Blustein, James},
title = {Interactive feature selection for document clustering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982436},
doi = {10.1145/1982185.1982436},
abstract = {Traditional document clustering techniques group similar documents without any user interaction. Although such methods minimize user effort, the clusters they generate are often not in accord with their users' conception of the document collection. In this paper we describe a new framework and experiments with it exploring how clustering might be improved by including user supervision at the level of selecting features that are used to distinguish between documents. Our features are based on the words that appear in documents (see §4.1 for details.) We conjecture that clusters better matching user expectations can be generated with user input at the feature level. In order to verify our conjecture, we propose a novel iterative framework which involves users interactively selecting the features used to cluster documents. Unlike existing semi-supervised clustering, which asks users to label constraints between documents, this framework interactively asks users to label features. The proposed method ranks all features based on the recent clusters using cluster-based feature selection and presents a list of highly ranked features to users for labeling. The feature set for next clustering iteration includes both features accepted by users and other highly ranked features. The experimental results on several real datasets demonstrate that the feature set obtained using the new interactive framework can produce clusters that better match the user's expectations. Moreover, we quantify and evaluate the effect of reweighting previously accepted features and of user effort.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1143–1150},
numpages = {8},
keywords = {document clustering, feature selection, interactive clustering, interactive feature selection, user supervision},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@article{10.1016/j.asoc.2015.10.038,
author = {Abdoos, Ali Akbar and Khorshidian Mianaei, Peyman and Rayatpanah Ghadikolaei, Mostafa},
title = {Combined VMD-SVM based feature selection method for classification of power quality events},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.10.038},
doi = {10.1016/j.asoc.2015.10.038},
abstract = {The proposed power quality detection scheme. Different types of power quality disturbances are discriminated from each other.VMD and ST are used for extraction of dominant features.SVM with simple structure and few adjustable parameters is used as the classifier core.The generalization capability and detection accuracy of the proposed method are increased by elimination of redundant features by using different feature selection methods.The start and end points of PQ events can be detected accurately. Power quality (PQ) issues have become more important than before due to increased use of sensitive electrical loads. In this paper, a new hybrid algorithm is presented for PQ disturbances detection in electrical power systems. The proposed method is constructed based on four main steps: simulation of PQ events, extraction of features, selection of dominant features, and classification of selected features. By using two powerful signal processing tools, i.e. variational mode decomposition (VMD) and S-transform (ST), some potential features are extracted from different PQ events. VMD as a new tool decomposes signals into different modes and ST also analyzes signals in both time and frequency domains. In order to avoid large dimension of feature vector and obtain a detection scheme with optimum structure, sequential forward selection (SFS) and sequential backward selection (SBS) as wrapper based methods and Gram-Schmidt orthogonalization (GSO) based feature selection method as filter based method are used for elimination of redundant features. In the next step, PQ events are discriminated by support vector machines (SVMs) as classifier core. Obtained results of the extensive tests prove the satisfactory performance of the proposed method in terms of speed and accuracy even in noisy conditions. Moreover, the start and end points of PQ events can be detected with high precision.},
journal = {Appl. Soft Comput.},
month = jan,
pages = {637–646},
numpages = {10},
keywords = {Feature selection, Pattern recognition, S-transform, Signal analysis, Support vector machines, Variational mode decomposition}
}

@article{10.1145/3459745,
author = {Ajenaghughrure, Ighoyota Ben and Sousa, Sonia Cl\'{a}udia Da Costa and Lamas, David},
title = {Psychophysiological Modeling of Trust In Technology: Influence of Feature Selection Methods},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3459745},
doi = {10.1145/3459745},
abstract = {Trust as a precursor for users' acceptance of artificial intelligence (AI) technologies that operate as a conceptual extension of humans (e.g., autonomous vehicles (AVs)) is highly influenced by users' risk perception amongst other factors. Prior studies that investigated the interplay between risk and trust perception recommended the development of real-time tools for monitoring cognitive states (e.g., trust). The primary objective of this study was to investigate a feature selection method that yields feature sets that can help develop a highly optimized and stable ensemble trust classifier model. The secondary objective of this study was to investigate how varying levels of risk perception influence users' trust and overall reliance on technology. A within-subject four-condition experiment was implemented with an AV driving game. This experiment involved 25 participants, and their electroencephalogram, electrodermal activity, and facial electromyogram psychophysiological signals were acquired. We applied wrapper, filter, and hybrid feature selection methods on the 82 features extracted from the psychophysiological signals. We trained and tested five voting-based ensemble trust classifier models using training and testing datasets containing only the features identified by the feature selection methods. The results indicate the superiority of the hybrid feature selection method over other methods in terms of model performance. In addition, the self-reported trust measurement and overall reliance of participants on the technology (AV) measured with joystick movements throughout the game reveals that a reduction in risk results in an increase in trust and overall reliance on technology.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {203},
numpages = {25},
keywords = {autonomous vehicle, classifier, machine learning, psychophysiology, trust}
}

@inproceedings{10.1145/1281192.1281220,
author = {Dasgupta, Anirban and Drineas, Petros and Harb, Boulos and Josifovski, Vanja and Mahoney, Michael W.},
title = {Feature selection methods for text classification},
year = {2007},
isbn = {9781595936097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281192.1281220},
doi = {10.1145/1281192.1281220},
abstract = {We consider feature selection for text classification both theoretically and empirically. Our main result is an unsupervised feature selection strategy for which we give worst-case theoretical guarantees on the generalization power of the resultant classification function f with respect to the classification function f obtained when keeping all the features. To the best of our knowledge, this is the first feature selection method with such guarantees. In addition, the analysis leads to insights as to when and why this feature selection strategy will perform well in practice. We then use the TechTC-100, 20-Newsgroups, and Reuters-RCV2 data sets to evaluate empirically the performance of this and two simpler but related feature selection strategies against two commonly-used strategies. Our empirical evaluation shows that the strategy with provable performance guarantees performs well in comparison with other commonly-used feature selection strategies. In addition, it performs better on certain datasets under very aggressive feature selection.},
booktitle = {Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {230–239},
numpages = {10},
keywords = {feature selection, random sampling, regularized least squares classification, text classification},
location = {San Jose, California, USA},
series = {KDD '07}
}

@article{10.1155/2021/9973277,
author = {Iraji, Mohammad Saber and Feizi-Derakhshi, Mohammad-Reza and Tanha, Jafar and Khan, Adil Mehmood},
title = {COVID-19 Detection Using Deep Convolutional Neural Networks and Binary Differential Algorithm-Based Feature Selection from X-Ray Images},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/9973277},
doi = {10.1155/2021/9973277},
abstract = {The new COVID-19 is rapidly spreading and has already claimed the lives of numerous people. The virus is highly destructive to the human lungs, and early detection is critical. As a result, this paper presents a hybrid approach based on deep convolutional neural networks that are very effective tools for image classification. The feature vectors were extracted from the images using a deep convolutional neural network, and the binary differential metaheuristic algorithm was used to select the most valuable features. The SVM classifier was then given these optimized features. For the study, a database containing images from three categories, including COVID-19, pneumonia, and a healthy category, included 1092 X-ray samples, was used. The proposed method achieved a 99.43% accuracy, a 99.16% sensitivity, and a 99.57% specificity. Our findings indicate that the proposed method outperformed recent studies on COVID-19 detection using X-ray images.},
journal = {Complex.},
month = jan,
numpages = {10}
}

@article{10.1016/j.eswa.2017.04.017,
author = {Jiang, Shancheng and Chin, Kwai-Sang and Wang, Long and Qu, Gang and Tsui, Kwok L.},
title = {Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {82},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.04.017},
doi = {10.1016/j.eswa.2017.04.017},
abstract = {A deep learning approach to model the patients demand for different key resources.Normal GA is improved for feature selection tasks by redesigning its key operators.An SAE-based pre-training process is designed to improve the initialization of DNN.The case study is implemented with real data collected from a hospital.Proposed approach can be used as a universal model for demand forecasting in hospital. A well-performed demand forecasting can provide outpatient department (OPD) managers with essential information for staff scheduling and rostering, considering the non-reservation policy of OPD in China. Based on the results reported by relevant studies, most approaches have focused on forecasting the overall amount of patient flow and ignored the demand for other key resources in OPD or similar department. Moreover, few studies have conducted feature selection before training a forecast model, which is a significant pre-processing operation of data mining and widely applied for knowledge discovery in expert and intelligent system. This study develops a novel hybrid methodology to forecast the patients demand for different key resources in OPD, by combining a new feature selection method and a deep learning approach. A modified version of genetic algorithm (MGA) is proposed for feature selection. The key operators of normal genetic algorithm are redesigned to utilize useful information provided by filter-based feature selection and feature combinations. A feedforward deep neural network is introduced as the forecast model, and the initial parameter set is generated from a stacked autoencoder-based pre-training process to overcome the optimization challenges in constructing deep architectures. In order to evaluate the performance of our methodology, it is applied to an OPD located at Northeast China. The results are compared with those obtained from combinations of other feature selection methods and demand forecasting models. Compared with GA and PCA, MGA improves the quality and efficiency of feature selection, with less selected features to get higher forecast accuracy. Pre-trained DNN optimally strengthens the advantage of MGA, compared with MLR, ARIMAX and SANN. The combination of MGA and pre-trained DNN possesses strongest predictive power among all involved combinations. Furthermore, the results of proposed methodology are crucial prerequisites for staff scheduling and resource allocation in OPD. Elite features obtained by MGA can provide practical insights on potential association between manifold feature combinations and demand variance.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {216–230},
numpages = {15},
keywords = {Deep learning, Demand forecasting in hospital, Feature selection, Modified genetic algorithm, Stacked autoencoder}
}

@article{10.1007/s11280-015-0381-x,
author = {Manek, Asha S. and Shenoy, P. Deepa and Mohan, M. Chandra and R, Venugopal K.},
title = {Aspect term extraction for sentiment analysis in large movie reviews using Gini Index feature selection method and SVM classifier},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-015-0381-x},
doi = {10.1007/s11280-015-0381-x},
abstract = {With the rapid development of the World Wide Web, electronic word-of-mouth interaction has made consumers active participants. Nowadays, a large number of reviews posted by the consumers on the Web provide valuable information to other consumers. Such information is highly essential for decision making and hence popular among the internet users. This information is very valuable not only for prospective consumers to make decisions but also for businesses in predicting the success and sustainability. In this paper, a Gini Index based feature selection method with Support Vector Machine (SVM) classifier is proposed for sentiment classification for large movie review data set. The results show that our Gini Index method has better classification performance in terms of reduced error rate and accuracy.},
journal = {World Wide Web},
month = mar,
pages = {135–154},
numpages = {20},
keywords = {Feature selection, Gini Index, Reviews, Sentiment, Support Vector Machine (SVM)}
}

@article{10.5555/3215234.3215243,
author = {Han, Bin and Ryzhov, Ilya O. and Defourny, Boris},
title = {Optimal Learning in Linear Regression with Combinatorial Feature Selection},
year = {2016},
issue_date = {November 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {28},
number = {4},
issn = {1526-5528},
abstract = {We present a new framework for sequential information collection in applications where regression is used to learn about a set of unknown parameters and alternates with optimization to design new data points. Such problems can be handled using the framework of ranking and selection R&amp;S, but traditional R&amp;S procedures will experience high computational costs when the decision space grows combinatorially. This challenge arises in many applications of business analytics; in particular, we are motivated by the problem of efficiently learning effective strategies for nonprofit fundraising. We present a value of information procedure for simultaneously learning unknown regression parameters and unknown sampling noise. We then develop approximate versions of the procedure, based on optimal quantization, that retain good performance and scale better to large problems.},
journal = {INFORMS J. on Computing},
month = nov,
pages = {721–735},
numpages = {15},
keywords = {Bayesian linear regression, optimal learning, ranking and selection, simulation optimization}
}

@article{10.1007/s10115-012-0487-8,
author = {Bol\'{o}n-Canedo, Ver\'{o}nica and S\'{a}nchez-Maro\~{n}o, Noelia and Alonso-Betanzos, Amparo},
title = {A review of feature selection methods on synthetic data},
year = {2013},
issue_date = {March     2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-012-0487-8},
doi = {10.1007/s10115-012-0487-8},
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
journal = {Knowl. Inf. Syst.},
month = mar,
pages = {483–519},
numpages = {37},
keywords = {Embedded methods, Feature selection, Filters, Synthetic datasets, Wrappers}
}

@article{10.1504/ijkesdp.2019.102851,
author = {Singh, Ajeet and Tiwari, Vikas},
title = {An optimal dimension reduction-based feature selection and classification strategy for geospatial imagery},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {2},
issn = {1755-3210},
url = {https://doi.org/10.1504/ijkesdp.2019.102851},
doi = {10.1504/ijkesdp.2019.102851},
abstract = {Driven by the explosive growth on the available data nowadays and advancement of technologies, the strong need arises for utilising and maintaining the available data. However, while building an expert prediction system, the inconsistency present in the information system, incompleteness of available knowledge base, continuous natured attribute values and noise present in the system (especially in case of spatial image data handling), are prime factors which may degrade the process of classification with available traditional methods. Our proposed construction adopts an efficient strategy for classification. Here we explore the problem of classifying remote sensing satellite images. Image data pre-processing and its categorisation refers to the labelling of individual pixel object instances into one of a number of predefined categories. Although this is usually not a much intractable task for humans, it has proved to be an extremely difficult problem for machines. We performed experimental analysis for classification using NWPU-RESISC45 dataset. Experiment result shows the improvement in classification by adopting our proposed strategy over other significant state of the art.},
journal = {Int. J. Knowl. Eng. Soft Data Paradigm.},
month = jan,
pages = {120–138},
numpages = {18},
keywords = {knowledge discovery, machine learning, classification, high-resolution satellite imagery}
}

@article{10.5555/3215222.3215223,
author = {Han, Bin and Ryzhov, Ilya O. and Defourny, Boris},
title = {Optimal Learning in Linear Regression with Combinatorial Feature Selection},
year = {2016},
issue_date = {November 2016},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {28},
number = {4},
issn = {1526-5528},
abstract = {We present a new framework for sequential information collection in applications where regression is used to learn about a set of unknown parameters and alternates with optimization to design new data points. Such problems can be handled using the framework of ranking and selection R&amp;S, but traditional R&amp;S procedures will experience high computational costs when the decision space grows combinatorially. This challenge arises in many applications of business analytics; in particular, we are motivated by the problem of efficiently learning effective strategies for nonprofit fundraising. We present a value of information procedure for simultaneously learning unknown regression parameters and unknown sampling noise. We then develop approximate versions of the procedure, based on optimal quantization, that retain good performance and scale better to large problems.},
journal = {INFORMS J. on Computing},
month = nov,
pages = {721–735},
numpages = {15},
keywords = {Bayesian linear regression, optimal learning, ranking and selection, simulation optimization}
}

@article{10.5555/2595577.2595586,
author = {Hatami, Nima and Chira, Camelia},
title = {Diverse accurate feature selection for microarray cancer diagnosis},
year = {2013},
issue_date = {July 2013},
publisher = {IOS Press},
address = {NLD},
volume = {17},
number = {4},
issn = {1088-467X},
abstract = {Gene expression microarray data provides simultaneous activity measurement of thousands of features facilitating a potential effective and reliable cancer diagnosis. An important and challenging task in microarray analysis refers to selecting the most relevant and significant genes for data cancer classification. A random subspace ensemble based method is proposed to address feature selection in gene expression cancer diagnosis. The introduced Diverse Accurate Feature Selection method relies on multiple individual classifiers built based on random feature subspaces. Each feature is assigned a score computed based on the pairwise diversity among individual classifiers and the ratio between individual and ensemble accuracies. This triggers the creation of a ranked list of features for which a final classifier is applied with an increased performance using minimum possible number of genes. Experimental results focus on the problem of gene expression cancer diagnosis based on microarray datasets publicly available. Numerical results show that the proposed method is competitive with related models from literature.},
journal = {Intell. Data Anal.},
month = jul,
pages = {697–716},
numpages = {20},
keywords = {Gene Expression Data Analysis, Multiple Classifier Systems, Multivariate Feature Selection, Pairwise Diversity, Random Subspace Ensembles}
}

@article{10.1155/2016/9569161,
author = {Yavuz, G\"{u}rcan and Aydin, Do\u{g}an},
title = {Angle modulated Artificial Bee Colony algorithms for feature selection},
year = {2016},
issue_date = {January 2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-9724},
url = {https://doi.org/10.1155/2016/9569161},
doi = {10.1155/2016/9569161},
abstract = {Optimal feature subset selection is an important and a difficult task for pattern classification, data mining, and machine intelligence applications. The objective of the feature subset selection is to eliminate the irrelevant and noisy feature in order to select optimum feature subsets and increase accuracy. The large number of features in a dataset increases the computational complexity thus leading to performance degradation. In this paper, to overcome this problem, angle modulation technique is used to reduce feature subset selection problem to four-dimensional continuous optimization problem instead of presenting the problem as a high-dimensional bit vector. To present the effectiveness of the problem presentation with angle modulation and to determine the efficiency of the proposed method, six variants of Artificial Bee Colony (ABC) algorithms employ angle modulation for feature selection. Experimental results on six high-dimensional datasets show that Angle Modulated ABC algorithms improved the classification accuracy with fewer feature subsets.},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
articleno = {7},
numpages = {1}
}

@article{10.1016/j.inffus.2016.05.002,
title = {Adaptive latent fingerprint segmentation using feature selection and random decision forest classification},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {34},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2016.05.002},
doi = {10.1016/j.inffus.2016.05.002},
abstract = {Five different categories of features are combined for latent fingerprint segmentation.Modified RELIEF based feature selection and RDF classification for improved results.A novel SIVV based metric to measure the effect of segmentation algorithm. Latent fingerprints are important evidences used by law enforcement agencies. However, current state-of-the-art for automatic latent fingerprint recognition is not as reliable as live-scan fingerprints and advancements are required in every step of the recognition pipeline. This research focuses on automatically segmenting latent fingerprints to distinguish between ridge and non-ridge patterns. There are three major contributions of this research: (i) a machine learning algorithm for combining five different categories of features for automatic latent fingerprint segmentation, (ii) a feature selection technique using modified RELIEF formulation for analyzing the influence of multiple category features on latent fingerprint segmentation, and (iii) a novel SIVV based metric to measure the effect of the segmentation algorithm without the requirement to perform the entire matching process. The image is tessellated into local patches and saliency based features along with image, gradient, ridge, and quality based features are extracted. Feature selection is performed to study the contribution of the various category features towards foreground ridge pattern representation. Using these selected features, a trained Random Decision Forest based algorithm classifies the local patches as background or foreground. The results on three publicly available databases demonstrate the efficacy of the proposed algorithm.},
journal = {Inf. Fusion},
month = mar,
pages = {1–15},
numpages = {15}
}

@article{10.1016/j.compbiomed.2017.03.002,
author = {Lu, Wei and Li, Zhe and Chu, Jinghui},
title = {A novel computer-aided diagnosis system for breast MRI based on feature selection and ensemble learning},
year = {2017},
issue_date = {April 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {83},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2017.03.002},
doi = {10.1016/j.compbiomed.2017.03.002},
abstract = {Breast cancer is a common cancer among women. With the development of modern medical science and information technology, medical imaging techniques have an increasingly important role in the early detection and diagnosis of breast cancer. In this paper, we propose an automated computer-aided diagnosis (CADx) framework for magnetic resonance imaging (MRI). The scheme consists of an ensemble of several machine learning-based techniques, including ensemble under-sampling (EUS) for imbalanced data processing, the Relief algorithm for feature selection, the subspace method for providing data diversity, and Adaboost for improving the performance of base classifiers. We extracted morphological, various texture, and Gabor features. To clarify the feature subsets' physical meaning, subspaces are built by combining morphological features with each kind of texture or Gabor feature. We tested our proposal using a manually segmented Region of Interest (ROI) data set, which contains 438 images of malignant tumors and 1898 images of normal tissues or benign tumors. Our proposal achieves an area under the ROC curve (AUC) value of 0.9617, which outperforms most other state-of-the-art breast MRI CADx systems. Compared with other methods, our proposal significantly reduces the false-positive classification rate. HighlightsThe dimensionality of the features we use is larger than most state-of-the-art methods. Various features including morphological, gabor, and several types of texture features are extracted to make a comprehensive characterization of breast masses.We select the optimal feature subset from the original feature set with Relief according to their type, which helps diminish the redundant and irrelevant features as well as take the physical meaning of features into consideration.We propose a novel ensemble learning framework based on the combination of EUS, subspace and Adaboost, which helps alleviate the data imbalance problem and improve the overall classification accuracy of the whole CADx system.},
journal = {Comput. Biol. Med.},
month = apr,
pages = {157–165},
numpages = {9},
keywords = {Breast cancer, Classification, Computer-aided diagnosis, Ensemble learning, Feature selection, MRI}
}

@article{10.1016/j.knosys.2021.107283,
author = {Ahmed, Shameem and Ghosh, Kushal Kanti and Mirjalili, Seyedali and Sarkar, Ram},
title = {AIEOU: Automata-based improved equilibrium optimizer with U-shaped transfer function for feature selection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {228},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107283},
doi = {10.1016/j.knosys.2021.107283},
journal = {Know.-Based Syst.},
month = sep,
numpages = {17},
keywords = {Meta-heuristic, Optimization, Equilibrium optimizer, Learning automata, Adaptive β-hill climbing, Feature selection, Genetic Algorithm, Particle Swarm Optimization}
}

@article{10.1016/j.patcog.2014.01.003,
author = {Tayal, Aditya and Coleman, Thomas F. and Li, Yuying},
title = {Primal explicit max margin feature selection for nonlinear support vector machines},
year = {2014},
issue_date = {June, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {47},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.01.003},
doi = {10.1016/j.patcog.2014.01.003},
abstract = {Embedding feature selection in nonlinear support vector machines (SVMs) leads to a challenging non-convex minimization problem, which can be prone to suboptimal solutions. This paper develops an effective algorithm to directly solve the embedded feature selection primal problem. We use a trust-region method, which is better suited for non-convex optimization compared to line-search methods, and guarantees convergence to a minimizer. We devise an alternating optimization approach to tackle the problem efficiently, breaking it down into a convex subproblem, corresponding to standard SVM optimization, and a non-convex subproblem for feature selection. Importantly, we show that a straightforward alternating optimization approach can be susceptible to saddle point solutions. We propose a novel technique, which shares an explicit margin variable to overcome saddle point convergence and improve solution quality. Experiment results show our method outperforms the state-of-the-art embedded SVM feature selection method, as well as other leading filter and wrapper approaches. HighlightsWe solve a primal embedded feature selection problem for nonlinear SVMs.Use smooth hinge loss and trust region algorithm for bound constrained minimization.Propose alternating optimization approach to break problem into two smaller ones.Propose explicit margin maximization to solve feature selection subproblem.Show our approach improves state-of-art and other algorithms on various data.},
journal = {Pattern Recogn.},
month = jun,
pages = {2153–2164},
numpages = {12},
keywords = {Alternating optimization, Embedded, Feature selection, Non-convex optimization, Nonlinear, Support vector machine, Trust-region method}
}

@article{10.1016/j.asoc.2015.10.037,
author = {Apolloni, Javier and Leguizam\'{o}n, Guillermo and Alba, Enrique},
title = {Two hybrid wrapper-filter feature selection algorithms applied to high-dimensional microarray experiments},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {38},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.10.037},
doi = {10.1016/j.asoc.2015.10.037},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose two new, simple, and efficient Hybrid Feature Selection techniques.We use a feature-based ranking to initialize the Binary Differential Evolution.We also propose a new fitness function influenced by the features in the population.Several statistical tests show the robustness and effectiveness of the proposals.The reducing of the size of the original set of features is larger than 99%. Microarray experiments generally deal with complex and high-dimensional samples, and in addition, the number of samples is much smaller than their dimensions. Both issues can be alleviated by using a feature selection (FS) method. In this paper two new, simple, and efficient hybrid FS algorithms, called respectively BDE- X Rank and BDE- X Rank f , are presented. Both algorithms combine a wrapper FS method based on a Binary Differential Evolution (BDE) algorithm with a rank-based filter FS method. Besides, they generate the initial population with solutions involving only a small number of features. Some initial solutions are built considering only the most relevant features regarding the filter method, and the remaining ones include only random features (to promote diversity). In the BDE- X Rank f , a new fitness function, in which the score value of a solution is influenced by the frequency of the features in the current population, is incorporated in the algorithm. The robustness of BDE- X Rank and BDE- X Rank f is shown by using four Machine Learning (ML) algorithms (NB, SVM, C4.5, and kNN). Six high-dimensional well-known data sets of microarray experiments are used to carry out an extensive experimental study based on statistical tests. This experimental analysis shows the robustness as well as the ability of both proposals to obtain highly accurate solutions at the earlier stages of BDE evolutionary process. Finally, BDE- X Rank and BDE- X Rank f are also compared against the results of nine state-of-the-art algorithms to highlight its competitiveness and the ability to successfully reduce the original feature set size by more than 99%.},
journal = {Appl. Soft Comput.},
month = jan,
pages = {922–932},
numpages = {11},
keywords = {Binary Differential Evolution, High-dimensional data set, Hybrid Feature Selection, Microarray}
}

@article{10.1016/j.ins.2012.10.006,
author = {Garc\'{\i}A-Pedrajas, Nicol\'{a}S and De Haro-Garc\'{\i}A, Aida and P\'{e}Rez-Rodr\'{\i}Guez, Javier},
title = {A scalable approach to simultaneous evolutionary instance and feature selection},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {228},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2012.10.006},
doi = {10.1016/j.ins.2012.10.006},
abstract = {An enormous amount of information is continually being produced in current research, which poses a challenge for data mining algorithms. Many of the problems in extremely active research areas, such as bioinformatics, security and intrusion detection and text mining, involve large or enormous datasets. These datasets pose serious problems for many data mining algorithms. One method to address very large datasets is data reduction. Among the most useful data reduction methods is simultaneous instance and feature selection. This method achieves a considerable reduction in the training data while maintaining, or even improving, the performance of the data-mining algorithm. However, it suffers from a high degree of scalability problems, even for medium-sized datasets. In this paper, we propose a new evolutionary simultaneous instance and feature selection algorithm that is scalable to millions of instances and thousands of features. This proposal is based on the divide-and-conquer principle combined with bookkeeping. The divide-and-conquer principle allows the execution of the algorithm in linear time. Furthermore, the proposed method is easy to implement using a parallel environment and can work without loading the entire dataset into memory. Using 50 medium-sized datasets, we will demonstrate our method's ability to match the results of state-of-the-art instance and feature selection methods while significantly reducing the time requirements. Using 13 very large datasets, we will demonstrate the scalability of our proposal to millions of instances and thousands of features.},
journal = {Inf. Sci.},
month = apr,
pages = {150–174},
numpages = {25},
keywords = {Feature selection, Instance selection, Instance-based learning, Simultaneous instance and feature selection, Very large problems}
}

@inproceedings{10.5555/2891460.2891700,
author = {Li, Haiguang and Wu, Xindong and Li, Zhao and Ding, Wei},
title = {Online group feature selection from feature streams},
year = {2013},
publisher = {AAAI Press},
abstract = {Standard feature selection algorithms deal with given candidate feature sets at the individual feature level. When features exhibit certain group structures, it is beneficial to conduct feature selection in a grouped manner. For high-dimensional features, it could be far more preferable to online generate and process features one at a time rather than wait for generating all features before learning begins. In this paper, we discuss a new and interesting problem of online group feature selection from feature streams at both the group and individual feature levels simultaneously from a feature stream. Extensive experiments on both real-world and synthetic datasets demonstrate the superiority of the proposed algorithm.},
booktitle = {Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence},
pages = {1627–1628},
numpages = {2},
location = {Bellevue, Washington},
series = {AAAI'13}
}

@article{10.1016/j.neucom.2016.02.028,
author = {Pacheco, Fannia and Valente de Oliveira, Jos\'{e} and S\'{a}nchez, Ren\'{e}-Vinicio and Cerrada, Mariela and Cabrera, Diego and Li, Chuan and Zurita, Grover and Art\'{e}s, Mariano},
title = {A statistical comparison of neuroclassifiers and feature selection methods for gearbox fault diagnosis under realistic conditions},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {194},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.02.028},
doi = {10.1016/j.neucom.2016.02.028},
abstract = {Gearboxes are crucial devices in rotating power transmission systems with applications in a variety of industries. Gearbox faults can cause catastrophic physical consequences, long equipment downtimes, and severe production costs. Several artificial neural networks, learning algorithms, and feature selection methods have been used in the diagnosis of the gearbox healthy state. Given a specific gearbox, this study investigates how these approaches compare with each other in terms of the typical fault classification accuracy but also in terms of the area under curve (AUC), where the curve refers to the precision-recall curve otherwise known as receiver operating characteristic (ROC) curve. In particular, the comparison aims at identifying whether there are statistically significant (dis)similarities among six feature selection methods, and seven pairs of neural nets with different learning rules. Genetic algorithm based, entropy based, linear discriminants, principal components, most neighbors first, and non-negative matrix factorization are the studied feature selection methods. Feed forward perceptrons, cascade forward, probabilistic nets, and radial basis function neural nets are evaluated. Six supervised and one unsupervised learning rules are considered. Both parametric and nonparametric statistical tests are employed. A ranking process is defined to elect the best approach, when available. An experimental setup was especially prepared to ensure operating conditions as realistic as possible.},
journal = {Neurocomput.},
month = jun,
pages = {192–206},
numpages = {15},
keywords = {Classification, Fault diagnosis, Feature selection, Gearbox, Neural networks, Statistic tests}
}

@article{10.1016/j.engappai.2015.06.003,
author = {Moayedikia, Alireza and Jensen, Richard and Wiil, Uffe Kock and Forsati, Rana},
title = {Weighted bee colony algorithm for discrete optimization problems with application to feature selection},
year = {2015},
issue_date = {September 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {44},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2015.06.003},
doi = {10.1016/j.engappai.2015.06.003},
abstract = {The conventional bee colony optimization (BCO) algorithm, one of the recent swarm intelligence (SI) methods, is good at exploration whilst being weak at exploitation. In order to improve the exploitation power of BCO, in this paper we introduce a novel algorithm, dubbed as weighted BCO (wBCO), that allows the bees to search in the solution space deliberately while considering policies to share the attained information about the food sources heuristically. For this purpose, wBCO considers global and local weights for each food source, where the former is the rate of popularity of a given food source in the swarm and the latter is the relevancy of a food source to a category label. To preserve diversity in the population, we embedded new policies in the recruiter selection stage to ensure that uncommitted bees follow the most similar committed ones. Thus, the local food source weighting and recruiter selection strategies make the algorithm suitable for discrete optimization problems. To demonstrate the utility of wBCO, the feature selection (FS) problem is modeled as a discrete optimization task, and has been tackled by the proposed algorithm. The performance of wBCO and its effectiveness in dealing with feature selection problem are empirically evaluated on several standard benchmark optimization functions and datasets and compared to the state-of-the-art methods, exhibiting the superiority of wBCO over the competitor approaches.},
journal = {Eng. Appl. Artif. Intell.},
month = sep,
pages = {153–167},
numpages = {15},
keywords = {Bee colony optimization, Categorical optimization, Classification, Feature selection, Weighted bee colony optimization}
}

@article{10.1007/s10044-017-0653-4,
author = {Narayanan, Barath Narayanan and Hardie, Russell C. and Kebede, Temesguen M. and Sprague, Matthew J.},
title = {Optimized feature selection-based clustering approach for computer-aided detection of lung nodules in different modalities},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-017-0653-4},
doi = {10.1007/s10044-017-0653-4},
abstract = {Early detection of pulmonary lung nodules plays a significant role in the diagnosis of lung cancer. Computed tomography (CT) and chest radiographs (CRs) are currently being used by radiologists to detect such nodules. In this paper, we present a novel cluster-based classifier architecture for lung nodule computer-aided detection systems in both modalities. We propose a novel optimized method of feature selection for both cluster and classifier components. For CRs, we make use of an independent database comprising of 160 cases with a total of 173 nodules for training purposes. Testing is implemented on a publicly available database created by the Standard Digital Image Database Project Team of the Scientific Committee of the Japanese Society of Radiological Technology (JRST). The JRST database comprises 154 CRs containing one radiologist-confirmed nodule in each. In this research, we exclude 14 cases from the JRST database that contain lung nodules in the retrocardiac and subdiaphragmatic regions of the lung. For CT scans, the analysis is based on threefold cross-validation performance on 107 cases from publicly available dataset created by Lung Image Database Consortium comprised of 280 nodules. Overall, with a specificity of 3 false positives per case/patient on average, we show a classifier performance boost of 7.7% for CRs and 5.0% for CT scans when compared to a single aggregate classifier architecture.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {559–571},
numpages = {13},
keywords = {Chest radiographs, Computed tomography, Computer-aided detection system, Lung nodules}
}

@inproceedings{10.5555/2908286.2908318,
author = {Prasad, Yamuna and Biswas, K. K. and Singla, Parag},
title = {Scaling-up quadratic programming feature selection},
year = {2013},
publisher = {AAAI Press},
abstract = {Domains such as vision, bioinformatics, web search and web rankings involve datasets where number of features is very large. Feature selection is commonly employed to deal with high dimensional data. Recently, Quadratic Programming Feature Selection (QPFS) has been shown to outperform many of the existing feature selection methods for a variety of datasets. In this paper, we propose a Sequential Minimal Optimization (SMO) based framework for QPFS. This helps in reducing the cubic computational time (in terms of data dimension) of the standard QPFS to quadratic time in practice. Further, our approach has significantly less memory requirement than QPFS. This memory saving can be critical for doing feature selection in high dimensions. The performance of our approach is demonstrated using three publicly available benchmark datasets from bioinformatics domain.},
booktitle = {Proceedings of the 17th AAAI Conference on Late-Breaking Developments in the Field of Artificial Intelligence},
pages = {95–97},
numpages = {3},
series = {AAAIWS'13-17}
}

@article{10.1016/j.cmpb.2016.08.010,
author = {Li, Xiaowei and Hu, Bin and Sun, Shuting and Cai, Hanshu},
title = {EEG-based mild depressive detection using feature selection methods and classifiers},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.08.010},
doi = {10.1016/j.cmpb.2016.08.010},
abstract = {Feature selection method GSW based on CFS combine with KNN can achieve the optimal performance for mild depression detection.FP1, FP2, F3, O2, T3 with linear features may be a good choice for portable device and auxiliary diagnosis of mild depression.With classification accuracy above 91% and AUC above 0.950, these results are better than some existing studies. Background and objectiveDepression has become a major health burden worldwide, and effectively detection of such disorder is a great challenge which requires latest technological tool, such as Electroencephalography (EEG). This EEG-based research seeks to find prominent frequency band and brain regions that are most related to mild depression, as well as an optimal combination of classification algorithms and feature selection methods which can be used in future mild depression detection. MethodsAn experiment based on facial expression viewing task (Emo_block and Neu_block) was conducted, and EEG data of 37 university students were collected using a 128 channel HydroCel Geodesic Sensor Net (HCGSN). For discriminating mild depressive patients and normal controls, BayesNet (BN), Support Vector Machine (SVM), Logistic Regression (LR), k-nearest neighbor (KNN) and RandomForest (RF) classifiers were used. And BestFirst (BF), GreedyStepwise (GSW), GeneticSearch (GS), LinearForwordSelection (LFS) and RankSearch (RS) based on Correlation Features Selection (CFS) were applied for linear and non-linear EEG features selection. Independent Samples T-test with Bonferroni correction was used to find the significantly discriminant electrodes and features. ResultsData mining results indicate that optimal performance is achieved using a combination of feature selection method GSW based on CFS and classifier KNN for beta frequency band. Accuracies achieved 92.00% and 98.00%, and AUC achieved 0.957 and 0.997, for Emo_block and Neu_block beta band data respectively. T-test results validate the effectiveness of selected features by search method GSW. Simplified EEG system with only FP1, FP2, F3, O2, T3 electrodes was also explored with linear features, which yielded accuracies of 91.70% and 96.00%, AUC of 0.952 and 0.972, for Emo_block and Neu_block respectively. ConclusionsClassification results obtained by GSW\'{z}+\'{z}KNN are encouraging and better than previously published results. In the spatial distribution of features, we find that left parietotemporal lobe in beta EEG frequency band has greater effect on mild depression detection. And fewer EEG channels (FP1, FP2, F3, O2 and T3) combined with linear features may be good candidates for usage in portable systems for mild depression detection.},
journal = {Comput. Methods Prog. Biomed.},
month = nov,
pages = {151–161},
numpages = {11},
keywords = {Classification algorithms, EEG, Mild depression, Search methods}
}

@article{10.1016/j.neucom.2016.09.123,
author = {Martn-Smith, Pedro and Ortega, Julio and Asensio-Cubero, Javier and Gan, John Q. and Ortiz, Andrs},
title = {A supervised filter method for multi-objective feature selection in EEG classification based on multi-resolution analysis for BCI},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {250},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.09.123},
doi = {10.1016/j.neucom.2016.09.123},
abstract = {This paper proposes a supervised filter method for evolutionary multi-objective feature selection for classification problems in high-dimensional feature space, which is evaluated by comparison with wrapper approaches for the same application. The filter method based on a set of label-aided utility functions is compared with wrapper approaches using the accuracy and generalization properties in the effective searching of the most adequate subset of features through an evolutionary multi-objective optimization scheme. The target application corresponds to a braincomputer interface (BCI) classification task based on linear discriminant analysis (LDA) classifiers, where the properties of multi-resolution analysis (MRA) for signal analysis in temporal and spectral domains have been used to extract features from electroencephalogram (EEG) signals. The results, corresponding to a dataset obtained from the databases of the BCI Laboratory of the University of Essex, UK, including ten subjects with three different imagery movements, have allowed us to evaluate the advantages and drawbacks of the different approaches with respect to time consumption, accuracy and generalization capabilities.},
journal = {Neurocomput.},
month = aug,
pages = {45–56},
numpages = {12},
keywords = {Braincomputer interfaces (BCI), Feature selection, Filter methods, Multi-objective optimization, Multi-resolution analysis (MRA), Wrapper-based feature selection}
}

@article{10.5555/2608507.2608509,
author = {Chen, Li-Fei and Su, Chao-Ton and Chen, Kun-Huang},
title = {An improved particle swarm optimization for feature selection},
year = {2012},
issue_date = {March 2012},
publisher = {IOS Press},
address = {NLD},
volume = {16},
number = {2},
issn = {1088-467X},
abstract = {Searching for an optimal feature subset in a high-dimensional feature space is an NP-complete problem; hence, traditional optimization algorithms are inefficient when solving large-scale feature selection problems. Therefore, meta-heuristic algorithms have been extensively adopted to solve the feature selection problem efficiently. This study proposes an improved particle swarm optimization IPSO algorithm using the opposite sign test OST. The test increases population diversity in the PSO mechanism, and avoids local optimal trapping by improving the jump ability of flying particles. Data sets collected from UCI machine learning databases are used to evaluate the effectiveness of the proposed approach. Classification accuracy is employed as a criterion to evaluate classifier performance. Results show that the proposed approach outperforms both genetic algorithms and sequential search algorithms.},
journal = {Intell. Data Anal.},
month = mar,
pages = {167–182},
numpages = {16},
keywords = {Feature Selection, Genetic Algorithms, Particle Swarm Optimization, Sequential Search Algorithms}
}

@inproceedings{10.5555/2840819.2840933,
author = {Gallardo, Ricardo Ochoa and Hu, Alan J. and Ivanov, Andr\'{e} and Mirian, Maryam S.},
title = {Reducing Post-Silicon Coverage Monitoring Overhead with Emulation and Bayesian Feature Selection},
year = {2015},
isbn = {9781467383899},
publisher = {IEEE Press},
abstract = {With increasing design complexity, post-silicon validation has become a critical problem. In pre-silicon validation, coverage is the primary metric of validation effectiveness, but in post-silicon, the lack of observability makes coverage measurement problematic. On-chip coverage monitors are a possible solution, but prior research has shown that the overhead is prohibitive for anything beyond a small number of coverage points. This paper presents a novel solution for post-silicon coverage monitoring: fully instrument the design in emulation to sample the relationships between coverage points, and then use this statistical data to choose a small set of coverage points whose coverage provides high probability that all the other coverage points are covered as well; only that small set is instrumented on silicon. To demonstrate the method, we propose a simple feature selection algorithm based on Bayesian networks to choose the small set of coverage points. In experiments emulating a non-trivial SoC, our technique reduces the number of coverage monitors by 92%, yet predicts over 98% probability that all coverage points are covered.},
booktitle = {Proceedings of the IEEE/ACM International Conference on Computer-Aided Design},
pages = {816–823},
numpages = {8},
location = {Austin, TX, USA},
series = {ICCAD '15}
}

@inproceedings{10.1145/1557019.1557084,
author = {Loscalzo, Steven and Yu, Lei and Ding, Chris},
title = {Consensus group stable feature selection},
year = {2009},
isbn = {9781605584959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1557019.1557084},
doi = {10.1145/1557019.1557084},
abstract = {Stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data. In this paper, we show that stability of feature selection has a strong dependency on sample size. We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples, and then performs feature selection by treating each consensus feature group as a single entity. Experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms. Synthetic data sets and algorithm source code are available at http://www.cs.binghamton.edu/~lyu/KDD09/.},
booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {567–576},
numpages = {10},
keywords = {ensemble, feature selection, high-dimensional data, small sample, stability},
location = {Paris, France},
series = {KDD '09}
}

@inproceedings{10.1145/3366030.3366133,
author = {Muramoto, Naoki and Shiraga, Hiromi and Shin, Kilho and Ohshima, Hiroaki},
title = {Fatten Features and Drop Wastes: Finding Repeaters' Reviews by Feature Generation and Feature Selection},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366133},
doi = {10.1145/3366030.3366133},
abstract = {In this paper, we proposed a method for determining whether a given restaurant review comment is a repeater's review, or not. We often use restaurant review sites to decide which restaurant to go to. When we read a restaurant review comment, we can know whether the reviewer is a repeater of the restaurant. If a certain restaurant has many repeaters, the restaurant must be great. However, restaurant review sites usually do not provide a "revisit rate". Therefore, we tackle a problem for determining whether a review is a repeater's review, or not. There are many sentences in a review comment that are completely not useful for determining whether the review is a repeater review, such as what was ordered, what was delicious, or how was the price. To confront such difficulties, we have taken the following approach. First, very various features are extracted from review comments so as not to miss the features that represent repeaters' reviews. Next, from the very various features, only the necessary features that really contribute to the classification is selected by a feature selection method. Finally, classification is performed using a classifier. We have implemented the proposed method using super-CWC [12], a state-of-the-art feature selection method, and SVM. The experimental results show that the proposed method is better than other methods.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {161–165},
numpages = {5},
keywords = {classification, feature selection, repeaters' reviews},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/2635868.2635915,
author = {Swanson, Jacob and Cohen, Myra B. and Dwyer, Matthew B. and Garvin, Brady J. and Firestone, Justin},
title = {Beyond the rainbow: self-adaptive failure avoidance in configurable systems},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635915},
doi = {10.1145/2635868.2635915},
abstract = {Self-adaptive software systems monitor their state and then adapt when certain conditions are met, guided by a global utility function. In prior work we developed algorithms and conducted a post-hoc analysis demonstrating the possibility of adapting to software failures by judiciously changing configurations. In this paper we present the REFRACT framework that realizes this idea in practice by building on the self-adaptive Rainbow architecture. REFRACT extends Rainbow with new components and algorithms targeting failure avoidance. We use REFRACT in a case study running four independently executing Firefox clients with 36 passing test cases and 7 seeded faults. The study show that workarounds for all but one of the seeded faults are found and the one that is not found never fails -- it is guarded from failing by a related workaround. Moreover, REFRACT finds workarounds for eight configuration-related unseeded failures from tests that were expected to pass (and did under the default configuration). Finally, the data show that when a failure and its workaround are found, configuration guards prevent the failure from appearing again. In a simulation lasting 24 hours we see over 150 guard activations and no failures with workarounds remaining beyond 16 hours.},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {377–388},
numpages = {12},
keywords = {Configurable Software, Self-Adaptive Software},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.5555/1885639.1885655,
author = {Chen, Lianping and Babar, Muhammad Ali},
title = {Variability management in software product lines: an investigation of contemporary industrial challenges},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Variability management is critical for achieving the large scale reuse promised by the software product line paradigm. It has been studied for almost 20 years. We assert that it is important to explore how well the body of knowledge of variability management solves the challenges faced by industrial practitioners, and what are the remaining and (or) emerging challenges. To gain such understanding of the challenges of variability management faced by practitioners, we have conducted an empirical study using focus group as data collection method. The results of the study highlight several technical challenges that are often faced by practitioners in their daily practices. Different from previous studies, the results also reveal and shed light on several non-technical challenges that were almost neglected by existing research.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {166–180},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1109/TCBB.2013.70,
author = {Liu, Hsi-Che and Peng, Pei-Chen and Hsieh, Tzung-Chien and Yeh, Ting-Chi and Lin, Chih-Jen and Chen, Chien-Yu and Hou, Jen-Yin and Shih, Lee-Yung and Liang, Der-Cherng},
title = {Comparison of Feature Selection Methods for Cross-Laboratory Microarray Analysis},
year = {2013},
issue_date = {May 2013},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {10},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2013.70},
doi = {10.1109/TCBB.2013.70},
abstract = {The amount of gene expression data of microarray has grown exponentially. To apply them for extensive studies, integrated analysis of cross-laboratory (cross-lab) data becomes a trend, and thus, choosing an appropriate feature selection method is an essential issue. This paper focuses on feature selection for Affymetrix (Affy) microarray studies across different labs. We investigate four feature selection methods: $(t)$-test, significance analysis of microarrays (SAM), rank products (RP), and random forest (RF). The four methods are applied to acute lymphoblastic leukemia, acute myeloid leukemia, breast cancer, and lung cancer Affy data which consist of three cross-lab data sets each. We utilize a rank-based normalization method to reduce the bias from cross-lab data sets. Training on one data set or two combined data sets to test the remaining data set(s) are both considered. Balanced accuracy is used for prediction evaluation. This study provides comprehensive comparisons of the four feature selection methods in cross-lab microarray analysis. Results show that SAM has the best classification performance. RF also gets high classification accuracy, but it is not as stable as SAM. The most naive method is $(t)$-test, but its performance is the worst among the four methods. In this study, we further discuss the influence from the number of training samples, the number of selected genes, and the issue of unbalanced data sets.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {593–604},
numpages = {12},
keywords = {Microarray data analysis, cancer, cross-laboratory experiment, feature selection}
}

@inproceedings{10.1007/978-3-030-13709-0_18,
author = {Sinoquet, Christine and Niel, Cl\'{e}ment},
title = {Ant Colony Optimization for Markov Blanket-Based Feature Selection. Application for Precision Medicine},
year = {2018},
isbn = {978-3-030-13708-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-13709-0_18},
doi = {10.1007/978-3-030-13709-0_18},
abstract = {In this work, we address feature subset selection in the case when the variables exert a null or weak marginal effect on the target variable, a situation called “pure" epistasis hereafter. We explore the Markov blanket approach, to tackle epistasis detection, and we introduce SMMB-ACO. This method combines Markov blanket learning with stochastic and ensemble features and guides the stochastic sampling process by incorporating ant colony optimization. We first analyze the impact of parameter adjustment on SMMB-ACO complexity. Then using simulated and real data, we compare SMMB-ACO with four other methods, including its former version SMMB. We show that SMMB-ACO compares well with three state-of-the-art methods and that SMMB-ACO is more stable than SMMB. On the real dataset, the detection ability of SMMB-ACO is close to that of the best approach, which is a slow method, and SMMB-ACO is the fastest algorithm behind a much less performing method.},
booktitle = {Machine Learning, Optimization, and Data Science: 4th International Conference, LOD 2018, Volterra, Italy, September 13-16, 2018, Revised Selected Papers},
pages = {217–230},
numpages = {14},
keywords = {Feature subset selection, Epistasis pattern, Bayesian network, Markov blanket, Metaheuristic, High dimensionality},
location = {Volterra, Italy}
}

@inproceedings{10.3233/978-1-61499-672-9-743,
author = {Salmer\'{o}n, Antonio and Madsen, Anders L. and Jensen, Frank and Langseth, Helge and Nielsen, Thomas D. and Ramos-L\'{o}pez, Dar\'{\i}o and Mart\'{\i}nez, Ana M. and Masegosa, Andr\'{e}s R.},
title = {Parallel filter-based feature selection based on balanced incomplete block designs},
year = {2016},
isbn = {9781614996712},
publisher = {IOS Press},
address = {NLD},
url = {https://doi.org/10.3233/978-1-61499-672-9-743},
doi = {10.3233/978-1-61499-672-9-743},
abstract = {In this paper we propose a method for scaling up filter-based feature selection in classification problems. We use the conditional mutual information as filter measure and show how the required statistics can be computed in parallel avoiding unnecessary calculations. The distribution of the calculations between the available computing units is determined based on balanced incomplete block designs, a strategy first developed within the area of statistical design of experiments. We show the scalability of our method through a series of experiments on synthetic and real-world datasets.},
booktitle = {Proceedings of the Twenty-Second European Conference on Artificial Intelligence},
pages = {743–750},
numpages = {8},
location = {The Hague, The Netherlands},
series = {ECAI'16}
}

@article{10.1007/s11277-017-4320-2,
author = {Huang, Wanwei and Zhang, Jianwei and Sun, Haiyan and Ma, Huan and Cai, Zengyu},
title = {An Anomaly Detection Method Based on Normalized Mutual Information Feature Selection and Quantum Wavelet Neural Network},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {96},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-4320-2},
doi = {10.1007/s11277-017-4320-2},
abstract = {This paper presents an anomaly detection model based on normalized mutual information feature selection (NMIFS) and quantum wavelet neural network (QWNN). The goal of the proposed model is to address the problem of determining the feature subset used to detect an anomaly in a machine learning task. In order to achieve an effective reduction for high-dimensional feature data, the NMIFS method is used to select the best feature combination from a given set of sample features. Then, the best combination of feature vectors are sent to the QWNN classifier for learning and training in the training phase, and the anomaly detection model is obtained. At the detection stage, the data is fed into the detection model and ultimately generates accurate detection results. The learning algorithm of structural risk minimization extreme learning machine is employed by the QWNN classifier to account for empirical and confidence risk. The experimental results on real abnormal data demonstrate that the NMIFS---QWNN method has higher detection accuracy and a lower false negative rate than the existing common anomaly detection methods. Furthermore, the complexity of the algorithm is low and the detection accuracy can reach up to 95.8%.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {2693–2713},
numpages = {21},
keywords = {Anomaly detection, Extreme learning machine, Normalized mutual information feature selection, Quantum wavelet neural network, Structural risk minimization}
}

@inproceedings{10.1145/2979779.2979804,
author = {Lachheta, Pawan and Bawa, Seema},
title = {Combining Synthetic Minority Oversampling Technique and Subset Feature Selection Technique For Class Imbalance Problem},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979804},
doi = {10.1145/2979779.2979804},
abstract = {Building an effective classification model when the high dimensional data is suffering from class imbalance problem is a major challenge. The problem is severe when negative samples have large percentages than positive samples. To surmount the class imbalance and high dimensionality issues in the dataset, we propose a SFS framework that comprises of SMOTE filters, which are used for balancing the datasets, as well as feature ranker for pre-processing of data. The framework is developed using R language and various R packages. Then the performance of SFS framework is evaluated and found that proposed framework outperforms than other state-of-the-art methods.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {25},
numpages = {6},
keywords = {Class Imbalance, Feature Selection, Majority Class, Minority Class},
location = {Bikaner, India},
series = {AICTC '16}
}

@article{10.5555/2567709.2567741,
author = {Dyer, Eva L. and Sankaranarayanan, Aswin C. and Baraniuk, Richard G.},
title = {Greedy feature selection for subspace clustering},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation--the identification of points that live in the same subspace--and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)--recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with l1-minimization, in this paper, we develop sufficient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide significant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2487–2517},
numpages = {31},
keywords = {hybrid linear models, low-rank approximation, nearest neighbors, sparse approximation, structured sparsity, subspace clustering, unions of subspaces}
}

@article{10.1016/j.imavis.2015.04.002,
title = {Feature selection for position estimation using an omnidirectional camera},
year = {2015},
issue_date = {July 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {39},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2015.04.002},
doi = {10.1016/j.imavis.2015.04.002},
abstract = {This paper considers visual feature selection to implement position estimation using an omnidirectional camera. The localization is based on a maximum likelihood estimation (MLE) with a map from optimally selected visual features using Gaussian process (GP) regression. In particular, the collection of selected features over a surveillance region is modeled by a multivariate GP with unknown hyperparameters. The hyperparameters are identified through the learning process by an MLE, which are used for prediction in an empirical Bayes fashion. To select features, we apply a backward sequential elimination technique in order to improve the quality of the position estimation with compressed features for efficient localization. The excellent results of the proposed algorithm are illustrated by the experimental studies with different visual features under both indoor and outdoor real-world scenarios. Display Omitted},
journal = {Image Vision Comput.},
month = jul,
pages = {1–9},
numpages = {9}
}

@article{10.1016/j.fss.2014.08.014,
author = {Zeng, Anping and Li, Tianrui and Liu, Dun and Zhang, Junbo and Chen, Hongmei},
title = {A fuzzy rough set approach for incremental feature selection on hybrid information systems},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {258},
number = {C},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2014.08.014},
doi = {10.1016/j.fss.2014.08.014},
abstract = {In real-applications, there may exist many kinds of data (e.g., boolean, categorical, real-valued and set-valued data) and missing data in an information system which is called as a Hybrid Information System (HIS). A new Hybrid Distance (HD) in HIS is developed based on the value difference metric, and a novel fuzzy rough set is constructed by combining the HD distance and the Gaussian kernel. Considering the information systems often vary with time, the updating mechanisms for attribute reduction (feature selection) are analyzed with the variation of the attribute set. Fuzzy rough set approaches for incremental feature selection on HIS are presented. Then two corresponding incremental algorithms are proposed, respectively. Finally, extensive experiments on eight datasets from UCI and an artificial dataset show that the incremental approaches significantly outperform non-incremental approaches with feature selection in the computational time.},
journal = {Fuzzy Sets Syst.},
month = jan,
pages = {39–60},
numpages = {22},
keywords = {Big data, Feature selection, Fuzzy rough sets, Hybrid information systems, Incremental learning}
}

@article{10.1016/j.asoc.2015.05.058,
author = {Maldonado, Sebasti\'{a}n and Flores, \'{A}lvaro and Verbraken, Thomas and Baesens, Bart and Weber, Richard},
title = {Profit-based feature selection using support vector machines - General framework and an application for customer retention},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.05.058},
doi = {10.1016/j.asoc.2015.05.058},
abstract = {Graphical abstractDisplay Omitted HighlightsA novel profit-based feature selection method for churn prediction with SVM is presented.A backward elimination algorithm is performed to maximize the profit of a retention campaign.Our experiments on churn prediction datasets underline the potential of the proposed approaches. Churn prediction is an important application of classification models that identify those customers most likely to attrite based on their respective characteristics described by e.g. socio-demographic and behavioral variables. Since nowadays more and more of such features are captured and stored in the respective computational systems, an appropriate handling of the resulting information overload becomes a highly relevant issue when it comes to build customer retention systems based on churn prediction models. As a consequence, feature selection is an important step of the classifier construction process. Most feature selection techniques; however, are based on statistically inspired validation criteria, which not necessarily lead to models that optimize goals specified by the respective organization. In this paper we propose a profit-driven approach for classifier construction and simultaneous variable selection based on support vector machines. Experimental results show that our models outperform conventional techniques for feature selection achieving superior performance with respect to business-related goals.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {740–748},
numpages = {9},
keywords = {Churn prediction, Customer retention, Data mining, Feature selection, Maximum profit, Support vector machines}
}

@article{10.1016/j.neucom.2012.05.001,
author = {Sun, Xin and Liu, Yanheng and Li, Jin and Zhu, Jianqi and Liu, Xuejie and Chen, Huiling},
title = {Using cooperative game theory to optimize the feature selection problem},
year = {2012},
issue_date = {November, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {97},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.05.001},
doi = {10.1016/j.neucom.2012.05.001},
abstract = {Feature selection is an important preprocessing step in machine learning and pattern recognition. Recent years, various information theoretic based measurements have been proposed to remove redundant and irrelevant features from high-dimensional data set as many as possible. One of the main disadvantages of existing filter feature selection methods is that they often ignore some features which have strong discriminatory power as a group but are weak as individuals. In this work, we propose a new framework for feature evaluation and weighting to optimize the performance of feature selection. The framework first introduces a cooperative game theoretic method based on Shapley value to evaluate the weight of each feature according to its influence to the intricate and intrinsic interrelation among features, and then provides the weighted features to feature selection algorithm. We also present a flexible feature selection scheme to employ any information criterion to our framework. To verify the effectiveness of our method, experimental comparisons on a set of UCI data sets are carried out using two typical classifiers. The results show that the proposed method achieves promising improvement on feature selection and classification accuracy.},
journal = {Neurocomput.},
month = nov,
pages = {86–93},
numpages = {8},
keywords = {Feature selection, Filter method, Machine learning, Shapley value}
}

@article{10.1109/TASLP.2017.2662232,
author = {Koizumi, Yuma and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Ohmuro, Hitoshi and Koizumi, Yuma and Niwa, Kenta and Hioka, Yusuke and Kobayashi, Kazunori and Ohmuro, Hitoshi},
title = {Informative Acoustic Feature Selection to Maximize Mutual Information for Collecting Target Sources},
year = {2017},
issue_date = {April 2017},
publisher = {IEEE Press},
volume = {25},
number = {4},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2017.2662232},
doi = {10.1109/TASLP.2017.2662232},
abstract = {An informative acoustic-feature-selection method for collecting target sources in noisy environments is proposed. Wiener filtering is a powerful framework for sound-source enhancement. For Wiener-filter estimation, statistical-mapping functions, such as deep neural network based or Gaussian mixture model based mappings, have been used. In this framework, it is essential to find informative acoustic features that provide effective cues for Wiener-filter estimation. In this study, we measured the informativeness of acoustic features using mutual information between acoustic features and supervised Wiener-filter parameters, e.g., prior signal-to-noise ratios, and developed a method for automatically selecting informative acoustic features from a large number of feature candidates. To automatically select optimum features, we derived a differentiable objective function in proportion to mutual information based on the kernel method. Since the higher order correlations between acoustic features and Wiener-filter parameters are calculated using the kernel method, the statistical dependence of these variables is accurately calculated; thus, only meaningful acoustic features are selected. Through several experiments conducted on a mock sports field, we confirmed that the signal-to-distortion ratio score improved when various types of target sources were surrounded by loud cheering noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {768–779},
numpages = {12}
}

@inproceedings{10.5555/2832747.2832781,
author = {Shin, Kilho and Angulo, Adrian Pino},
title = {A geometric theory of feature selection and distance-based measures},
year = {2015},
isbn = {9781577357384},
publisher = {AAAI Press},
abstract = {Feature selection measures are often explained by the analogy to a rule to measure the "distance" of sets of features to the "closest" ideal sets of features. An ideal feature set is such that it can determine classes uniquely and correctly. This way of explanation was just an analogy before this paper. In this paper, we show a way to map arbitrary feature sets of datasets into a common metric space, which is indexed by a real number p with 1 ≤ p ≤ ∞. Since this determines the distance between an arbitrary pair of feature sets, even if they belong to different datasets, the distance of a feature set to the closest ideal feature set can be used as a feature selection measure. Surprisingly, when p = 1, the measure is identical to the Bayesian risk, which is probably the feature selection measure that is used the most widely in the literature. For 1 &lt; p ≤ ∞, the measure is novel and has significantly different properties from the Bayesian risk. We also investigate the correlation between measurements by these measures and classification accuracy through experiments. As a result, we show that our novel measures with p &gt; 1 exhibit stronger correlation than the Bayesian risk.},
booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
pages = {3812–3819},
numpages = {8},
location = {Buenos Aires, Argentina},
series = {IJCAI'15}
}

@inproceedings{10.1007/978-3-642-55192-5_5,
author = {Taylor, Phillip and Griffths, Nathan and Bhalerao, Abhir and Popham, Thomas and Zhou, Xu and Dunoyer, Alain},
title = {Redundant Feature Selection for Telemetry Data},
year = {2013},
isbn = {9783642551918},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-55192-5_5},
doi = {10.1007/978-3-642-55192-5_5},
abstract = {Feature sets in many domains often contain many irrelevant and redundant features, both of which have a negative effect on the performance and complexity of agents that use the data [&lt;CitationRef CitationID="CR9"&gt;9&lt;/CitationRef&gt;]. Supervised feature selection aims to overcome this problem by selecting features that are highly related to the class labels, yet unrelated to each other. One proposed technique to select good features with few inter-dependencies is minimal Redundancy Maximal Relevance mRMR [&lt;CitationRef CitationID="CR12"&gt;12&lt;/CitationRef&gt;], but this can be impractical with large feature sets. In many situations, features are extracted from signal data such as vehicle telemetry, medical sensors, or financial time-series, and it is possible for feature redundancies to exist both between features extracted from the same signal intra-signal, and between features extracted from different signals inter-signal. We propose a two stage selection process to take advantage of these different types of redundancy, considering intra-signal and inter-signal redundancies separately. We illustrate the process on vehicle telemetry signal data collected in a driver distraction monitoring project. We evaluate it using several machine learning algorithms: Random Forest; Na\"{\i}ve Bayes; and C4.5 Decision Tree. Our results show that this two stage process significantly reduces the computation required because of inter-dependency calculations, while having minimal detrimental effect on the performance of the feature sets produced.},
booktitle = {Revised Selected Papers of the 9th International Workshop on Agents and Data Mining Interaction - Volume 8316},
pages = {53–65},
numpages = {13},
location = {Saint Paul, MN, USA},
series = {ADMI 2013}
}

@article{10.1016/j.ins.2010.11.008,
author = {Su, Chao-Ton and Lin, Hung-Chun},
title = {Applying electromagnetism-like mechanism for feature selection},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {5},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.11.008},
doi = {10.1016/j.ins.2010.11.008},
abstract = {In recent years, methods of feature selection have been increasingly emphasized as venues for reducing cost and shortening the length of time required for computation in data mining. This study utilizes electromagnetism-like mechanism as a wrapper approach to feature selection. Birbil and Fang proposed EM in 2003. EM uses the attraction-repulsion mechanism of the electromagnetism theory to ascertain the optimal solution. Although EM has been applied to the topic of optimization in continuous space and a small number of studies on discrete problems, it has not been applied to the subject of feature selection. In this study, EM combined with 1-nearest-neighbor (1NN) was applied for feature selection and classification. This study utilized the total force exerted on a particle and evaluated this force to determine which features are to be selected. The most crucial features were selected according to the proposed method based on the minimum miss-classification rate, which was attained through 1NN. An unknown datum was classified by 1NN based on the chosen reduced model. To estimate the effectiveness of the proposed method, a numerical experiment was conducted using several data sets with diverse sizes, features, separability, and classes. Experimental results indicated that the proposed method outperformed other well-known algorithms in not only balanced classification accuracy but also efficiency of feature selection. Lastly, this study used an actual case concerning gestational diabetes mellitus to demonstrate the workability of the proposed method.},
journal = {Inf. Sci.},
month = mar,
pages = {972–986},
numpages = {15},
keywords = {1-Nearest-neighbor (1NN), Classification, Electromagnetism theory, Electromagnetism-like mechanism (EM), Feature selection, Optimization}
}

@article{10.1007/s10916-010-9518-8,
author = {Luo, Shu-Ting and Cheng, Bor-Wen},
title = {Diagnosing Breast Masses in Digital Mammography Using Feature Selection and Ensemble Methods},
year = {2012},
issue_date = {Apr 2012},
publisher = {Plenum Press},
address = {USA},
volume = {36},
number = {2},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-010-9518-8},
doi = {10.1007/s10916-010-9518-8},
abstract = {Methods that can accurately predict breast cancer are greatly needed and good prediction techniques can help to predict breast cancer more accurately. In this study, we used two feature selection methods, forward selection (FS) and backward selection (BS), to remove irrelevant features for improving the results of breast cancer prediction. The results show that feature reduction is useful for improving the predictive accuracy and density is irrelevant feature in the dataset where the data had been identified on full field digital mammograms collected at the Institute of Radiology of the University of Erlangen-Nuremberg between 2003 and 2006. In addition, decision tree (DT), support vector machine--sequential minimal optimization (SVM-SMO) and their ensembles were applied to solve the breast cancer diagnostic problem in an attempt to predict results with better performance. The results demonstrate that ensemble classifiers are more accurate than a single classifier.},
journal = {J. Med. Syst.},
month = apr,
pages = {569–577},
numpages = {9},
keywords = {Breast cancer, Digital mammography, Ensemble classifiers, Feature selection}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Empirical studies, Software product line, Systematic literature reviews, Variability management}
}

@article{10.1016/j.artmed.2017.05.001,
author = {Ahmad, Jamal and Javed, Faisal and Hayat, Maqsood},
title = {Intelligent computational model for classification of sub-Golgi protein using oversampling and fisher feature selection methods},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {78},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2017.05.001},
doi = {10.1016/j.artmed.2017.05.001},
abstract = {Computational model is developed for Golgi proteins.PSSM-bigram and discrete methods are utilized for features.Fisher feature selection approach is applied.Three cross validation tests are examined.Obtained quite promising results than existing methods. Golgi is one of the core proteins of a cell, constitutes in both plants and animals, which is involved in protein synthesis. Golgi is responsible for receiving and processing the macromolecules and trafficking of newly processed protein to its intended destination. Dysfunction in Golgi protein is expected to cause many neurodegenerative and inherited diseases that may be cured well if they are detected effectively and timely. Golgi protein is categorized into two parts cis-Golgi and trans-Golgi. The identification of Golgi protein via direct method is very hard due to limited available recognized structures. Therefore, the researchers divert their attention toward the sequences from structures. However, owing to technological advancement, exploration of huge amount of sequences was reported in the databases. So recognition of large amount of unprocessed data using conventional methods is very difficult. Therefore, the concept of intelligence was incorporated with computational model. Intelligence based computational model obtained reasonable results, but the gap of improvement is still under consideration. In this regard, an intelligent automatic recognition model is developed in order to enhance the true classification rate of sub-Golgi proteins. In this approach, discrete and evolutionary feature extraction methods are applied on the benchmark Golgi protein datasets to excerpt salient, propound and variant numerical descriptors. After that, an oversampling technique Syntactic Minority over Sampling Technique is employed to balance the data. Hybrid spaces are also generated with combination of these feature spaces. Further, Fisher feature selection method is utilized to reduce the extra noisy and redundant features from feature vector. Finally, k-nearest neighbor algorithm is used as learning hypothesis. Three distinct cross validation tests are used to examine the stability and efficiency of the proposed model. The predicted outcomes of proposed model are better than the existing models in the literature so far. Finally, it is anticipated that the proposed model will provide the foundation to pharmaceutical industry in drug design and research community to innovate new ideas in the area of computational biology and bioinformatics.},
journal = {Artif. Intell. Med.},
month = may,
pages = {14–22},
numpages = {9},
keywords = {Bigram position specific scoring matrix, Dipeptide composition, Fisher feature selection, Golgi protein, Split pseudo amino acid composition, k-nearest neighbor}
}

@article{10.1016/j.asoc.2015.08.042,
author = {Kundu, Partha Pratim and Mitra, Sushmita},
title = {Multi-objective optimization of shared nearest neighbor similarity for feature selection},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.08.042},
doi = {10.1016/j.asoc.2015.08.042},
abstract = {Graphical abstractDisplay Omitted HighlightsA new feature selection method is proposed based on sample similarity.The SNN distance is taken into consideration for sample similarity computation.Multi-objective optimizations is employed to arrive at a consensus solutions.Comparative study with related state-of-art methods are presented here. A new unsupervised feature selection algorithm, based on the concept of shared nearest neighbor distance between pattern pairs, is developed. A multi-objective framework is employed for the preservation of sample similarity, along with dimensionality reduction of the feature space. A reduced set of samples, chosen to preserve sample similarity, serves to reduce the effect of outliers on the feature selection procedure while also decreasing computational complexity. Experimental results on six sets of publicly available data demonstrate the effectiveness of this feature selection strategy. Comparative study with related methods based on different evaluation indices have demonstrated the superiority of the proposed algorithm.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {751–762},
numpages = {12},
keywords = {Hubs, Multi-objective optimization, Nearest neighbor distance, Redundancy analysis, Sample similarity}
}

@article{10.1007/s10489-015-0714-6,
author = {Bdiri, Taoufik and Bouguila, Nizar and Ziou, Djemel},
title = {Variational Bayesian inference for infinite generalized inverted Dirichlet mixtures with feature selection and its application to clustering},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {44},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-015-0714-6},
doi = {10.1007/s10489-015-0714-6},
abstract = {We developed a variational Bayesian learning framework for the infinite generalized Dirichlet mixture model (i.e. a weighted mixture of Dirichlet process priors based on the generalized inverted Dirichlet distribution) that has proven its capability to model complex multidimensional data. We also integrate a "feature selection" approach to highlight the features that are most informative in order to construct an appropriate model in terms of clustering accuracy. Experiments on synthetic data as well as real data generated from visual scenes and handwritten digits datasets illustrate and validate the proposed approach.},
journal = {Applied Intelligence},
month = apr,
pages = {507–525},
numpages = {19},
keywords = {Data clustering, Generalized inverted Dirichlet, Handwritten digits, Inverted beta, Mixture models, Model selection, Variational Bayesian inference, Visual scenes}
}

@article{10.1007/s11222-015-9569-2,
author = {Perthame, \'{E}meline and Friguet, Chlo\'{e} and Causeur, David},
title = {Stability of feature selection in classification issues for high-dimensional correlated data},
year = {2016},
issue_date = {July      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0960-3174},
url = {https://doi.org/10.1007/s11222-015-9569-2},
doi = {10.1007/s11222-015-9569-2},
abstract = {Handling dependence or not in feature selection is still an open question in supervised classification issues where the number of covariates exceeds the number of observations. Some recent papers surprisingly show the superiority of naive Bayes approaches based on an obviously erroneous assumption of independence, whereas others recommend to infer on the dependence structure in order to decorrelate the selection statistics. In the classical linear discriminant analysis (LDA) framework, the present paper first highlights the impact of dependence in terms of instability of feature selection. A second objective is to revisit the above issue using a flexible factor modeling for the covariance. This framework introduces latent components of dependence, conditionally on which a new Bayes consistency is defined. A procedure is then proposed for the joint estimation of the expectation and variance parameters of the model. The present method is compared to recent regularized diagonal discriminant analysis approaches, assuming independence among features, and regularized LDA procedures, both in terms of classification performance and stability of feature selection. The proposed method is implemented in the R package FADA, freely available from the R repository CRAN.},
journal = {Statistics and Computing},
month = jul,
pages = {783–796},
numpages = {14},
keywords = {Classification, Discriminant Analysis, High dimension, Stability, Variable selection}
}

@article{10.1016/j.eswa.2011.04.089,
author = {S\'{a}nchez-Maro\~{n}o, Noelia and Alonso-Betanzos, Amparo},
title = {Combining functional networks and sensitivity analysis as wrapper method for feature selection},
year = {2011},
issue_date = {Sep 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {10},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.04.089},
doi = {10.1016/j.eswa.2011.04.089},
journal = {Expert Syst. Appl.},
month = sep,
pages = {12930–12938},
numpages = {9},
keywords = {Feature selection, Wrapper methods, Machine learning, Neural networks}
}

@article{10.1016/j.compbiomed.2016.02.012,
author = {Ding, Hui and Liang, Zhi-Yong and Guo, Feng-Biao and Huang, Jian and Chen, Wei and Lin, Hao},
title = {Predicting bacteriophage proteins located in host cell with feature selection technique},
year = {2016},
issue_date = {April 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {71},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2016.02.012},
doi = {10.1016/j.compbiomed.2016.02.012},
abstract = {A bacteriophage is a virus that can infect a bacterium. The fate of an infected bacterium is determined by the bacteriophage proteins located in the host cell. Thus, reliably identifying bacteriophage proteins located in the host cell is extremely important to understand their functions and discover potential anti-bacterial drugs. Thus, in this paper, a computational method was developed to recognize bacteriophage proteins located in host cells based only on their amino acid sequences. The analysis of variance (ANOVA) combined with incremental feature selection (IFS) was proposed to optimize the feature set. Using a jackknife cross-validation, our method can discriminate between bacteriophage proteins located in a host cell and the bacteriophage proteins not located in a host cell with a maximum overall accuracy of 84.2%, and can further classify bacteriophage proteins located in host cell cytoplasm and in host cell membranes with a maximum overall accuracy of 92.4%. To enhance the value of the practical applications of the method, we built a web server called PHPred ({http://lin.uestc.edu.cn/server/PHPred}). We believe that the PHPred will become a powerful tool to study bacteriophage proteins located in host cells and to guide related drug discovery. Display Omitted Novel analytical method is developed to predict the phage proteins located in host cell.A significant feature selection technique is proposed and used to optimize features of proteinsA powerful predictor is constructed to identify phage proteins distribution in host cell.},
journal = {Comput. Biol. Med.},
month = apr,
pages = {156–161},
numpages = {6},
keywords = {Analysis of variance, Bacteriophage proteins, Feature analysis, g-gap dipeptide}
}

@inproceedings{10.5555/3045390.3045411,
author = {Zhang, Yue and Ray, Soumya and Guo, Weihong},
title = {On the consistency of feature selection with lasso for non-linear targets},
year = {2016},
publisher = {JMLR.org},
abstract = {An important question in feature selection is whether a selection strategy recovers the "true" set of features, given enough data. We study this question in the context of the popular Least Absolute Shrinkage and Selection Operator (Lasso) feature selection strategy. In particular, we consider the scenario when the model is misspecified so that the learned model is linear while the underlying real target is nonlinear. Surprisingly, we prove that under certain conditions, Lasso is still able to recover the correct features in this case. We also carry out numerical studies to empirically verify the theoretical results and explore the necessity of the conditions under which the proof holds.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {183–191},
numpages = {9},
location = {New York, NY, USA},
series = {ICML'16}
}

@article{10.1016/j.eswa.2013.09.004,
author = {Oreski, Stjepan and Oreski, Goran},
title = {Genetic algorithm-based heuristic for feature selection in credit risk assessment},
year = {2014},
issue_date = {March, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.09.004},
doi = {10.1016/j.eswa.2013.09.004},
abstract = {In this paper, an advanced novel heuristic algorithm is presented, the hybrid genetic algorithm with neural networks (HGA-NN), which is used to identify an optimum feature subset and to increase the classification accuracy and scalability in credit risk assessment. This algorithm is based on the following basic hypothesis: the high-dimensional input feature space can be preliminarily restricted to only the important features. In this preliminary restriction, fast algorithms for feature ranking and earlier experience are used. Additionally, enhancements are made in the creation of the initial population, as well as by introducing an incremental stage in the genetic algorithm. The performances of the proposed HGA-NN classifier are evaluated using a real-world credit dataset that is collected at a Croatian bank, and the findings are further validated on another real-world credit dataset that is selected in a UCI database. The classification accuracy is compared with that presented in the literature. Experimental results that were achieved using the proposed novel HGA-NN classifier are promising for feature selection and classification in retail credit risk assessment and indicate that the HGA-NN classifier is a promising addition to existing data mining techniques.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {2052–2064},
numpages = {13},
keywords = {Artificial intelligence, Classification, Credit risk assessment, Genetic algorithms, Incremental feature selection, Neural network}
}

@article{10.1155/2018/6595792,
author = {Shi, Lei and Wan, Youchuan and Gao, Xianjun and Wang, Mingwei and Gutierrez, Pedro Antonio},
title = {Feature Selection for Object-Based Classification of High-Resolution Remote Sensing Images Based on the Combination of a Genetic Algorithm and Tabu Search},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-5265},
url = {https://doi.org/10.1155/2018/6595792},
doi = {10.1155/2018/6595792},
abstract = {In object-based image analysis of high-resolution images, the number of features can reach hundreds, so it is necessary to perform feature reduction prior to classification. In this paper, a feature selection method based on the combination of a genetic algorithm (GA) and tabu search (TS) is presented. The proposed GATS method aims to reduce the premature convergence of the GA by the use of TS. A prematurity index is first defined to judge the convergence situation during the search. When premature convergence does take place, an improved mutation operator is executed, in which TS is performed on individuals with higher fitness values. As for the other individuals with lower fitness values, mutation with a higher probability is carried out. Experiments using the proposed GATS feature selection method and three other methods, a standard GA, the multistart TS method, and ReliefF, were conducted on WorldView-2 and QuickBird images. The experimental results showed that the proposed method outperforms the other methods in terms of the final classification accuracy.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {13}
}

@inproceedings{10.5555/1786574.1786680,
author = {Ren, Jiangtao and Qiu, Zhengyuan and Fan, Wei and Cheng, Hong and Yu, Philip S.},
title = {Forward semi-supervised feature selection},
year = {2008},
isbn = {3540681248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Traditionally, feature selection methods work directly on labeled examples. However, the availability of labeled examples cannot be taken for granted for many real world applications, such as medical diagnosis, forensic science, fraud detection, etc, where labeled examples are hard to find. This practical problem calls the need for "semi-supervised feature selection" to choose the optimal set of features given both labeled and unlabeled examples that return the most accurate classifier for a learning algorithm. In this paper, we introduce a "wrapper-type" forward semi-supervised feature selection framework. In essence, it uses unlabeled examples to extend the initial labeled training set. Extensive experiments on publicly available datasets shows that our proposed framework, generally, outperforms both traditional supervised and state of-the-art "filter-type" semi-supervised feature selection algorithms [5] by 1% to 10% in accuracy.},
booktitle = {Proceedings of the 12th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {970–976},
numpages = {7},
keywords = {feature selection, semi-supervised learning},
location = {Osaka, Japan},
series = {PAKDD'08}
}

@article{10.1016/j.jksuci.2017.08.005,
author = {Dash, Rasmita},
title = {A two stage grading approach for feature selection and classification of microarray data using Pareto based feature ranking techniques: A case study},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {2},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2017.08.005},
doi = {10.1016/j.jksuci.2017.08.005},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = feb,
pages = {232–247},
numpages = {16},
keywords = {Feature ranking technique, Statistical analysis, Pareto front, Multi-objective optimization, Classification technique, Microarray database}
}

@article{10.1016/j.cmpb.2019.105047,
author = {Rachmani, Enny and Hsu, Chien-Yeh and Nurjanah, Nurjanah and Chang, Peter Wushou and Shidik, Guruh Fajar and Noersasongko, Edi and Jumanto, Jumanto and Fuad, Anis and Ningrum, Dina Nur Anggraini and Kurniadi, Arif and Lin, Ming-Chin},
title = {Developing an Indonesia's health literacy short-form survey questionnaire (HLS-EU-SQ10-IDN) using the feature selection and genetic algorithm},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.105047},
doi = {10.1016/j.cmpb.2019.105047},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
numpages = {10},
keywords = {Data mining, Multifactor dimensional reduction, HLS-EU-Q47, Health Literacy, Design Questionnaire, Indonesia}
}

@article{10.1016/j.ins.2014.03.093,
author = {Han, Yahong and Chen, Jingjing and Cao, Xiaochun and Xu, Congfu and Shen, Haoquan},
title = {Feature selection with spatial path coding for multimedia analysis},
year = {2014},
issue_date = {October, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {281},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2014.03.093},
doi = {10.1016/j.ins.2014.03.093},
abstract = {The selection of a subset of discriminative features for semantic recognition is crucial to making multimedia analysis more interpretable. This paper proposes a model of spatial path coding (SPC) that uses a supervised technique to select sparse features. SPC is a regularized penalty that encodes the spatial correlations of features obtained by the spatial pyramid model. In SPC, each feature dimension is considered as a vertex in a direct acyclic graph (DAG), and the spatial correlations among features are considered as directed edges associated with predefined weights. Thus, the process of supervised feature selection can be directly formulated to solve a path selection problem with minimum cost. Experiments are conducted to evaluate the performance of supervised feature selection with SPC for the tasks of scene classification and action recognition using four benchmark datasets. The results show that SPC can be used to automatically select a subgraph of the DAG with a small number of discriminative features for a certain category. In addition, the method proposed in this paper shows better performance in terms of classification and recognition accuracy as compared with state-of-the-art algorithms.},
journal = {Inf. Sci.},
month = oct,
pages = {523–535},
numpages = {13},
keywords = {Multimedia analysis, Spatial path coding, Structured sparsity}
}

@article{10.1016/j.ins.2019.04.046,
author = {Sosa-Cabrera, Gustavo and Garc\'{\i}a-Torres, Miguel and G\'{o}mez-Guerrero, Santiago and Schaerer, Christian E. and Divina, Federico},
title = {A multivariate approach to the symmetrical uncertainty measure: Application to feature selection problem},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {494},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.04.046},
doi = {10.1016/j.ins.2019.04.046},
journal = {Inf. Sci.},
month = aug,
pages = {1–20},
numpages = {20},
keywords = {Multivariate symmetrical uncertainty, Mutual information, Categorical feature correlation, Entropy, Multivariate sample size, Total representativeness, Feature selection}
}

@article{10.1016/j.knosys.2016.10.022,
author = {Wei, Zexian and Wang, Yanxue and He, Shuilong and Bao, Jiading},
title = {A novel intelligent method for bearing fault diagnosis based on affinity propagation clustering and adaptive feature selection},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {116},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.10.022},
doi = {10.1016/j.knosys.2016.10.022},
abstract = {Bearings faults are one of the main causes of breakdown of rotating machines. Thus detection and diagnosis of mechanical faults in bearings is very crucial for the reliable operation. A novel intelligent fault diagnosis method for roller bearings based on affinity propagation (AP) clustering algorithm and adaptive feature selection technique is proposed to better equip with a non-expert to carry out diagnosis operations. Ensemble empirical mode decomposition (EEMD) and wavelet packet transform (WPT) are utilized to accurately extract the fault characteristic information buried in the vibration signals. Moreover, in order to improve the efficiency of clustering algorithm and avoid the curse of dimensionality, a new adaptive features selection technique is developed in this work, whose effectiveness is verified in comparison with other methods. The proposed intelligent method is then applied to the bearing fault diagnosis. Results demonstrate that the proposed method is able to reliably and accurately identify different fault categories and severities of bearings.},
journal = {Know.-Based Syst.},
month = jan,
pages = {1–12},
numpages = {12},
keywords = {Adaptive feature selection, Affinity propagation, Intelligent fault diagnosis, Rolling element bearing}
}

@article{10.1016/j.neucom.2012.01.005,
author = {Liu, Quanjin and Zhao, Zhimin and Li, Ying-Xin and Li, Yuanyuan},
title = {Feature selection based on sensitivity analysis of fuzzy ISODATA},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.01.005},
doi = {10.1016/j.neucom.2012.01.005},
abstract = {A feature selection method based on sensitivity analysis and the fuzzy Interactive Self-Organizing Data Algorithm (ISODATA) is proposed for selecting features from high dimensional gene expression data sets. First, feature sensitivities for discriminating classes are calculated on the basis of the fuzzy ISODATA method. Then, candidate feature subsets are generated according to feature sensitivities with the recursive feature elimination procedure. Finally, the obtained optimal feature subsets are evaluated using both supervised and unsupervised methods to validate their abilities for separating different categories. The proposed method is applied to five microarray datasets, and the experimental results indicate its effectiveness.},
journal = {Neurocomput.},
month = may,
pages = {29–37},
numpages = {9},
keywords = {Classification, Clustering, Feature selection, Fuzzy ISODATA, Microarray, Sensitivity analysis}
}

@inproceedings{10.1145/2393216.2393219,
author = {Adeli, Ali and Sinaee, Mehrnoosh and Zomorodian, Javad and Neshat, Mehdi},
title = {Harmony-based feature selection to improve the nearest neighbor classification},
year = {2012},
isbn = {9781450313100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393216.2393219},
doi = {10.1145/2393216.2393219},
abstract = {A new approach for feature selection is presented in this paper. The proposed approach uses the Harmony Search with a novel fitness function to eliminate noisy and irrelevant features. Harmony vectors contain real weights which refer to feature space. The best and significant features are selected according to a threshold. Fitness function of Harmony Search is based on the Area Under the receiver operating characteristics Curve (AUC). All of the selected features are employed to improve the classification of the k Nearest Neighbor (k-NN) classifier. Experimental results claim that the proposed method is able to improve the classification performance of k-NN algorithm in comparison with the other important methods in realm of feature selection such as BAHSIC, FSS, BSS and MFS.},
booktitle = {Proceedings of the Second International Conference on Computational Science, Engineering and Information Technology},
pages = {12–18},
numpages = {7},
keywords = {k-NN, AUC, feature selection, harmony search, noisy feature elimination},
location = {Coimbatore UNK, India},
series = {CCSEIT '12}
}

@article{10.1016/j.eswa.2012.05.014,
author = {Tsimpiris, Alkiviadis and Vlachos, Ioannis and Kugiumtzis, Dimitris},
title = {Nearest neighbor estimate of conditional mutual information in feature selection},
year = {2012},
issue_date = {November, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {16},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.05.014},
doi = {10.1016/j.eswa.2012.05.014},
abstract = {Mutual information (MI) is used in feature selection to evaluate two key-properties of optimal features, the relevance of a feature to the class variable and the redundancy of similar features. Conditional mutual information (CMI), i.e., MI of the candidate feature to the class variable conditioning on the features already selected, is a natural extension of MI but not so far applied due to estimation complications for high dimensional distributions. We propose the nearest neighbor estimate of CMI, appropriate for high-dimensional variables, and build an iterative scheme for sequential feature selection with a termination criterion, called CMINN. We show that CMINN is equivalent to feature selection MI filters, such as mRMR and MaxiMin, in the presence of solely single feature effects, and more appropriate for combined feature effects. We compare CMINN to mRMR and MaxiMin on simulated datasets involving combined effects and confirm the superiority of CMINN in selecting the correct features (indicated also by the termination criterion) and giving best classification accuracy. The application to ten benchmark databases shows that CMINN obtains the same or higher classification accuracy compared to mRMR and MaxiMin at a smaller cardinality of the selected feature subset.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {12697–12708},
numpages = {12},
keywords = {Classification, Conditional mutual information, Feature selection, MaxiMin, Nearest neighbor estimate, mRMR}
}

@inproceedings{10.1007/11552499_86,
author = {Vilari\~{n}o, Fernando and Spyridonos, Panagiota and Vitri\`{a}, Jordi and Radeva, Petia},
title = {Experiments with SVM and stratified sampling with an imbalanced problem: detection of intestinal contractions},
year = {2005},
isbn = {3540288333},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11552499_86},
doi = {10.1007/11552499_86},
abstract = {In this paper we show some preliminary results of our research in the fieldwork of classification of imbalanced datasets with SVM and stratified sampling. Our main goal is to deal with the clinical problem of automatic intestinal contractions detection in endoscopic video images. The prevalence of contractions is very low, and this yields to highly skewed training sets. Stratified sampling together with SVM have been reported in the literature to behave well in this kind of problems. We applied both the SMOTE algorithm developed by Chawla et al. and under-sampling, in a cascade system implementation to deal with the skewed training sets in the final SVM classifier. We show comparative results for both sampling techniques using precision-recall curves, which appear to be useful tools for performance testing.},
booktitle = {Proceedings of the Third International Conference on Pattern Recognition and Image Analysis - Volume Part II},
pages = {783–791},
numpages = {9},
location = {Bath, UK},
series = {ICAPR'05}
}

@article{10.1016/j.asoc.2021.107533,
author = {Macedo, Mariana and Santana, Maira and dos Santos, Wellington P. and Menezes, Ronaldo and Bastos-Filho, Carmelo},
title = {Breast cancer diagnosis using thermal image analysis: A data-driven approach based on swarm intelligence and supervised learning for optimized feature selection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {109},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107533},
doi = {10.1016/j.asoc.2021.107533},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {21},
keywords = {Breast cancer image diagnosis, Swarm intelligence, Fish school search, Feature extraction, Feature selection, Convolutional Neural Networks}
}

@article{10.1109/TCBB.2016.2591539,
author = {Moskovitch, Robert and Choi, Hyunmi and Hripcsak, George and Tatonetti, Nicholas},
title = {Prognosis of Clinical Outcomes with Temporal Patterns and Experiences with One Class Feature Selection},
year = {2017},
issue_date = {May 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2591539},
doi = {10.1109/TCBB.2016.2591539},
abstract = {Accurate prognosis of outcome events, such as clinical procedures or disease diagnosis, is central in medicine. The emergence of longitudinal clinical data, like the Electronic Health Records EHR, represents an opportunity to develop automated methods for predicting patient outcomes. However, these data are highly dimensional and very sparse, complicating the application of predictive modeling techniques. Further, their temporal nature is not fully exploited by current methods, and temporal abstraction was recently used which results in symbolic time intervals representation. We present Maitreya, a framework for the prediction of outcome events that leverages these symbolic time intervals. Using Maitreya, learn predictive models based on the temporal patterns in the clinical records that are prognostic markers and use these markers to train predictive models for eight clinical procedures. In order to decrease the number of patterns that are used as features, we propose the use of three one class feature selection methods. We evaluate the performance of Maitreya under several parameter settings, including the one-class feature selection, and compare our results to that of atemporal approaches. In general, we found that the use of temporal patterns outperformed the atemporal methods, when representing the number of pattern occurrences.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {555–563},
numpages = {9}
}

@article{10.1504/IJBIC.2018.091234,
title = {Feature selection based on binary particle swarm optimisation and neural networks for pathological voice detection},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {2},
issn = {1758-0366},
url = {https://doi.org/10.1504/IJBIC.2018.091234},
doi = {10.1504/IJBIC.2018.091234},
abstract = {In this work, 52 Haralick texture features, extracted from two-dimensional wavelet coefficients of speech signals from recurrence plots RPs pathologies are used for pathological voice discrimination. Here, three pathologies are considered for analysis: vocal fold paralysis, edema and nodules. For feature selection, a binary particle swarm optimisation PSO algorithm using multilayer perceptron MLP neural network with cross validation is employed. The adopted fitness function is based on the maxima average accuracy rate. Statistical tests for individual measures were made and their results show statistical significance for several employed measures. The measures were combined and the more relevant ones based on the highest accuracy were selected by the PSO. The comparison with and without PSO by applying the statistical test of mean difference showed that the PSO use increased the accuracy rates. Furthermore, the PSO use reduced the amount of features for almost half of all initially used.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {91–101},
numpages = {11}
}

@article{10.1109/TCBB.2016.2515582,
author = {Tang, Jian and Zhou, Shuigeng},
title = {A New Approach for Feature Selection from Microarray Data Based on Mutual Information},
year = {2016},
issue_date = {November 2016},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {13},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2515582},
doi = {10.1109/TCBB.2016.2515582},
abstract = {Mutual information MI is a powerful concept for correlation-centric applications. It has been used for feature selection from microarray gene expression data in many works. One of the merits of MI is that, unlike many other heuristic methods, it is based on a mature theoretic foundation. When applied to microarray data, however, it faces some challenges. First, due to the large number of features i.e., genes present in microarray data, the true distributions for the expression values of some genes may be distorted by noise. Second, evaluating inter-group mutual information requires estimating multi-variate distributions, which is quite difficult if not impossible. To address these problems, in this paper, we propose a new MI-based feature selection approach for microarray data. Our approach relies on two strategies: one is relevance boosting, which requires a desirable feature to show substantially additional relevance with class labeling beyond the already selected features, the other is  feature interaction enhancing, which probabilistically compensates for feature interaction missing from simple aggregation-based evaluation. We justify our approach from both theoretical perspective and experimental results. We use a synthetic dataset to show the statistical significance of the proposed strategies, and real-life datasets to show the improved performance of our approach over the existing methods.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = nov,
pages = {1004–1015},
numpages = {12}
}

@article{10.1016/j.patcog.2009.09.003,
author = {Nguyen, Minh Hoai and de la Torre, Fernando},
title = {Optimal feature selection for support vector machines},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {3},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.09.003},
doi = {10.1016/j.patcog.2009.09.003},
abstract = {Selecting relevant features for support vector machine (SVM) classifiers is important for a variety of reasons such as generalization performance, computational efficiency, and feature interpretability. Traditional SVM approaches to feature selection typically extract features and learn SVM parameters independently. Independently performing these two steps might result in a loss of information related to the classification process. This paper proposes a convex energy-based framework to jointly perform feature selection and SVM parameter learning for linear and non-linear kernels. Experiments on various databases show significant reduction of features used while maintaining classification performance.},
journal = {Pattern Recogn.},
month = mar,
pages = {584–591},
numpages = {8},
keywords = {Feature extraction, Feature selection, Support vector machine}
}

@article{10.1016/j.eswa.2010.08.062,
author = {Wu, Yi-Leh and Tang, Cheng-Yuan and Hor, Maw-Kae and Wu, Pei-Fen},
title = {Feature selection using genetic algorithm and cluster validation},
year = {2011},
issue_date = {March, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.08.062},
doi = {10.1016/j.eswa.2010.08.062},
abstract = {Feature selection plays an important role in image retrieval systems. The better selection of features usually results in higher retrieval accuracy. This work tries to select the best feature set from a total of 78 low level image features, including regional, color, and textual features, using the genetic algorithms (GA). However, the GA is known to be slow to converge. In this work we propose two directions to improve the convergence time of the GA. First we employ the Taguchi method to reduce the number of necessary offspring to be tested in every generation in the GA. Second we propose to use an alternative measure, the Hubert's @C statistics, to evaluate the fitness of each offspring instead of evaluating the retrieval accuracy directly. The experiment results show that the proposed techniques improve the feature selection results by using the GA in both time and accuracy.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {2727–2732},
numpages = {6},
keywords = {Feature selection, Genetic algorithms, Hubert's Γ statistics, Image retrieval, Taguchi method}
}

@inproceedings{10.5555/1883700.1883708,
author = {Koprinska, Irena},
title = {Feature selection for brain-computer interfaces},
year = {2009},
isbn = {3642146392},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we empirically evaluate feature selection methods for classification of Brain-Computer Interface (BCI) data. We selected five state-of the-art methods, suitable for the noisy, correlated and highly dimensional BCI data, namely: information gain ranking, correlation-based feature selection, ReliefF, consistency-based feature selection and 1R ranking. We tested them with ten classification algorithms, representing different learning paradigms, on a benchmark BCI competition dataset. The results show that all feature selectors significantly reduced the number of features and also improved accuracy when used with suitable classification algorithms. The top three feature selectors in terms of classification accuracy were correlation-based feature selection, information gain and 1R ranking, with correlation based feature selection choosing the smallest number of features.},
booktitle = {Proceedings of the 13th Pacific-Asia International Conference on Knowledge Discovery and Data Mining: New Frontiers in Applied Data Mining},
pages = {106–117},
numpages = {12},
keywords = {1R ranking, RelifF, brain-computer interfaces, classification of EEG data, consistency-based feature selection, correlation-based feature selection, information gain ranking},
location = {Bangkok, Thailand},
series = {PAKDD'09}
}

@article{10.1016/j.cor.2005.01.021,
author = {Yang, Jaekyung and Olafsson, Sigurdur},
title = {Optimization-based feature selection with adaptive instance sampling},
year = {2006},
issue_date = {November 2006},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {33},
number = {11},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2005.01.021},
doi = {10.1016/j.cor.2005.01.021},
abstract = {Preprocessing the data to filter out redundant and irrelevant features is one of the most important steps in the data mining process. Careful feature selection may improve both the computational time of inducing subsequent models and the quality of those models. Using fewer features often leads to simpler and easier to interpret models, and selecting important feature can lead to important insights into the application. The feature selection problem is inherently a combinatorial optimization problem. This paper builds on a metaheuristic called the nested partitions method that has been shown to be particularly effective for the feature selection problem. Specifically, we focus on the scalability of the method and show that its performance is vastly improved by incorporating random sampling of instances. Furthermore, we develop an adaptive variant of the algorithm that dynamically determines the required sample rate. The adaptive algorithm is shown to perform very well when applied to a set of standard test problems.},
journal = {Comput. Oper. Res.},
month = nov,
pages = {3088–3106},
numpages = {19},
keywords = {combinatorial optimization, data mining, feature selection, metaheuristics}
}

@article{10.1016/j.cose.2019.02.008,
author = {Gottwalt, Florian and Chang, Elizabeth and Dillon, Tharam},
title = {CorrCorr: A feature selection method for multivariate correlation network anomaly detection techniques},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {83},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2019.02.008},
doi = {10.1016/j.cose.2019.02.008},
journal = {Comput. Secur.},
month = jun,
pages = {234–245},
numpages = {12},
keywords = {Feature selection, Multivariate correlation, Correlation anomaly detection, Intrusion detection, Network anomaly detection}
}

@inproceedings{10.1007/978-3-642-40261-6_45,
author = {Rodrigues, Douglas and Pereira, Luis A. and Papa, Joao P. and Ramos, Caio C. and Souza, Andre N. and Papa, Luciene P.},
title = {Optimizing Feature Selection through Binary Charged System Search},
year = {2013},
isbn = {9783642402609},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40261-6_45},
doi = {10.1007/978-3-642-40261-6_45},
abstract = {Feature selection aims to find the most important information from a given set of features. As this task can be seen as an optimization problem, the combinatorial growth of the possible solutions may be inviable for a exhaustive search. In this paper we propose a new nature-inspired feature selection technique based on the Charged System Search CSS, which has never been applied to this context so far. The wrapper approach combines the power of exploration of CSS together with the speed of the Optimum-Path Forest classifier to find the set of features that maximizes the accuracy in a validating set. Experiments conducted in four public datasets have demonstrated the validity of the proposed approach can outperform some well-known swarm-based techniques.},
booktitle = {Proceedings, Part I, of the 15th International Conference on Computer Analysis of Images and Patterns - Volume 8047},
pages = {377–384},
numpages = {8},
keywords = {Charged System Search, Evolutionary Optimization, Feature Felection},
location = {York, UK},
series = {CAIP 2013}
}

@article{10.1007/s10489-018-1261-8,
author = {Sayed, Gehad Ismail and Tharwat, Alaa and Hassanien, Aboul Ella},
title = {Chaotic dragonfly algorithm: an improved metaheuristic algorithm for feature selection},
year = {2019},
issue_date = {January   2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1261-8},
doi = {10.1007/s10489-018-1261-8},
abstract = {Selecting the most discriminative features is a challenging problem in many applications. Bio-inspired optimization algorithms have been widely applied to solve many optimization problems including the feature selection problem. In this paper, the most discriminating features were selected by a new Chaotic Dragonfly Algorithm (CDA) where chaotic maps embedded with searching iterations of the Dragonfly Algorithm (DA). Ten chaotic maps were employed to adjust the main parameters of dragonflies' movements through the optimization process to accelerate the convergence rate and improve the efficiency of DA. The proposed algorithm is employed for selecting features from the dataset that were extracted from the Drug bank database, which contained 6712 drugs. In this paper, 553 drugs that were bio-transformed into liver are used. This data have four toxic effects, namely, irritant, mutagenic, reproductive, and tumorigenic effect, where each drug is represented by 31 chemical descriptors. The proposed model is mainly comprised of three phases; data pre-processing, features selection, and the classification phase. In the data pre-processing phase, Synthetic Minority Over-sampling Technique (SMOTE) was used to solve the problem of the imbalanced dataset. At the features selection phase, the most discriminating features were selected using CDA. Finally, the selected features from CDA were used to feed Support Vector Machine (SVM) classifier at the classification phase. Experimental results proved the capability of CDA to find the optimal feature subset, which maximizing the classification performance and minimizing the number of selected features compared with DA and the other meta-heuristic optimization algorithms. Moreover, the experiments showed that Gauss chaotic map was the appropriate map to significantly boost the performance of DA. Additionally, the high obtained value of accuracy (81.82---96.08%), recall (80.84---96.11%), precision (81.45---96.08%) and F-Score (81.14---96.1%) for all toxic effects proved the robustness of the proposed model.},
journal = {Applied Intelligence},
month = jan,
pages = {188–205},
numpages = {18},
keywords = {Chaos theory, Dragonfly algorithm, Feature selection, Optimization algorithm, Toxic effects}
}

@inproceedings{10.5555/3042573.3042699,
author = {Arnosti, Nicholas A. and Danyluk, Andrea Pohoreckyj},
title = {Feature selection via probabilistic outputs},
year = {2012},
isbn = {9781450312851},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by Shen et al. (2008) and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions.},
booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
pages = {971–978},
numpages = {8},
location = {Edinburgh, Scotland},
series = {ICML'12}
}

@inproceedings{10.5555/3042817.3042856,
author = {Kolar, Mladen and Liu, Han},
title = {Feature selection in high-dimensional classification},
year = {2013},
publisher = {JMLR.org},
abstract = {High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–329–I–337},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{10.1109/TCBB.2017.2715016,
author = {He, Xinyu and Li, Lishuang and Liu, Yang and Yu, Xiaoming and Meng, Jun},
title = {A Two-Stage Biomedical Event Trigger Detection Method Integrating Feature Selection and Word Embeddings},
year = {2018},
issue_date = {July 2018},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {15},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2017.2715016},
doi = {10.1109/TCBB.2017.2715016},
abstract = {Extracting biomedical events from biomedical literature plays an important role in the field of biomedical text mining, and the trigger detection is a key step in biomedical event extraction. We propose a two-stage method for trigger detection, which divides trigger detection into recognition stage and classification stage, and different features are selected in each stage. In the first stage, we select the features which are more suitable for recognition, and in the second stage, the features that are more helpful to classification are adopted. Furthermore, we integrate word embeddings to represent words semantically and syntactically. On the multi-level event extraction MLEE corpus test dataset, our method achieves an F-score of 79.75 percent, which outperforms the state-of-the-art systems.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1325–1332},
numpages = {8}
}

@inproceedings{10.5555/2045625.2045690,
author = {Liu, Huawen and Li, Minshuo and Zhao, Jianmin and Mo, Yuchang},
title = {An effective feature selection method using dynamic information criterion},
year = {2011},
isbn = {9783642238802},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {With rapid development of information technology, dimensionality of data in many applications is getting higher and higher. However, many features in the high-dimensional data are redundant. Their presence may pose a great number of challenges to traditional learning algorithms. Thus, it is necessary to develop an effective technique to remove irrelevant features from data. Currently, many endeavors have been attempted in this field. In this paper, we propose a new feature selection method by using conditional mutual information estimated dynamically. Its advantage is that it can exactly represent the correlation between features along with the selection procedure. Our performance evaluations on eight benchmark datasets show that our proposed method achieves comparable performance to other well-established feature selection algorithms in most cases.},
booktitle = {Proceedings of the Third International Conference on Artificial Intelligence and Computational Intelligence - Volume Part I},
pages = {450–455},
numpages = {6},
keywords = {classification, data mining, feature selection, learning algorithm, mutual information},
location = {Taiyuan, China},
series = {AICI'11}
}

@article{10.1155/2021/1670593,
author = {Xiao, Wen and Ji, Ping and Hu, Juan and Wang, Pengwei},
title = {RnkHEU: A Hybrid Feature Selection Method for Predicting Students’ Performance},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/1670593},
doi = {10.1155/2021/1670593},
abstract = {Predicting students’ performance is one of the most concerned issues in education data mining (EDM), which has received more and more attentions. Feature selection is the key step to build prediction model of students’ performance, which can improve the accuracy of prediction and help to identify factors that have significant impact on students’ performance. In this paper, a hybrid feature selection method named rank and heuristic (RnkHEU) was proposed. This novel feature selection method generates the set of candidate features by scoring and ranking firstly and then uses heuristic method to generate the final results. The experimental results show that the four major evaluation criteria have similar performance in predicting students’ performance, and the heuristic search strategy can significantly improve the accuracy of prediction compared with forward search method. Because the proposed RnkHEU integrates ranking-based forward and heuristic search, it can further improve the accuracy of predicting students’ performance with commonly used classifiers about 10% and improve the precision of predicting students’ academic failure by up to 45%.},
journal = {Sci. Program.},
month = jan,
numpages = {16}
}

@article{10.1155/2016/6184823,
author = {Gao, Jia and Wang, Wei and Zhang, Ji},
title = {Explore interregional EEG correlations changed by sport training using feature selection},
year = {2016},
issue_date = {January 2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-5265},
url = {https://doi.org/10.1155/2016/6184823},
doi = {10.1155/2016/6184823},
abstract = {This paper investigated the interregional correlation changed by sport training through electroencephalography (EEG) signals using the techniques of classification and feature selection. The EEG data are obtained from students with long-time professional sport training and normal students without sport training as baseline. Every channel of the 19-channel EEG signals is considered as a node in the brain network and Pearson Correlation Coefficients are calculated between every two nodes as the new features of EEG signals. Then, the Partial Least Square (PLS) is used to select the top 10 most varied features and Pearson Correlation Coefficients of selected features are compared to show the difference of two groups. Result shows that the classification accuracy of two groups is improved from 88.13% by the method using measurement of EEG overall energy to 97.19% by the method using EEG correlation measurement. Furthermore, the features selected reveal that the most important interregional EEG correlation changed by training is the correlation between left inferior frontal and left middle temporal with a decreased value.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {30},
numpages = {1}
}

@inproceedings{10.1145/1401890.1401903,
author = {Boutsidis, Christos and Mahoney, Michael W. and Drineas, Petros},
title = {Unsupervised feature selection for principal components analysis},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401903},
doi = {10.1145/1401890.1401903},
abstract = {Principal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and has been widely applied on datasets in all scientific domains. We consider, both theoretically and empirically, the topic of unsupervised feature selection for PCA, by leveraging algorithms for the so-called Column Subset Selection Problem (CSSP). In words, the CSSP seeks the "best" subset of exactly k columns from an m x n data matrix A, and has been extensively studied in the Numerical Linear Algebra community. We present a novel two-stage algorithm for the CSSP. From a theoretical perspective, for small to moderate values of k, this algorithm significantly improves upon the best previously-existing results [24, 12] for the CSSP. From an empirical perspective, we evaluate this algorithm as an unsupervised feature selection strategy in three application domains of modern statistical data analysis: finance, document-term data, and genetics. We pay particular attention to how this algorithm may be used to select representative or landmark features from an object-feature matrix in an unsupervised manner. In all three application domains, we are able to identify k landmark features, i.e., columns of the data matrix, that capture nearly the same amount of information as does the subspace that is spanned by the top k "eigenfeatures."},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {61–69},
numpages = {9},
keywords = {PCA, random sampling, subset selection},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}

@inproceedings{10.5555/2075619.2075635,
author = {Zakharov, Roman and Dupont, Pierre},
title = {Ensemble logistic regression for feature selection},
year = {2011},
isbn = {9783642248542},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper describes a novel feature selection algorithm embedded into logistic regression. It specifically addresses high dimensional data with few observations, which are commonly found in the biomedical domain such as microarray data. The overall objective is to optimize the predictive performance of a classifier while favoring also sparse and stable models.Feature relevance is first estimated according to a simple t-test ranking. This initial feature relevance is treated as a feature sampling probability and a multivariate logistic regression is iteratively reestimated on subsets of randomly and non-uniformly sampled features. At each iteration, the feature sampling probability is adapted according to the predictive performance and the weights of the logistic regression. Globally, the proposed selection method can be seen as an ensemble of logistic regression models voting jointly for the final relevance of features.Practical experiments reported on several microarray datasets show that the proposed method offers a comparable or better stability and significantly better predictive performances than logistic regression regularized with Elastic Net. It also outperforms a selection based on Random Forests, another popular embedded feature selection from an ensemble of classifiers.},
booktitle = {Proceedings of the 6th IAPR International Conference on Pattern Recognition in Bioinformatics},
pages = {133–144},
numpages = {12},
keywords = {logistic regression, microarray data classification, stability of gene selection},
location = {Delft, The Netherlands},
series = {PRIB'11}
}

@inproceedings{10.1109/ICMLA.2011.74,
author = {Shanab, Ahmad Abu and Khoshgoftaar, Taghi M. and Wald, Randall},
title = {Impact of Noise and Data Sampling on Stability of Feature Selection},
year = {2011},
isbn = {9780769546070},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2011.74},
doi = {10.1109/ICMLA.2011.74},
abstract = {High dimensionality is one of the major problems in data mining, occurring when there is a large abundance of attributes. One common technique used to alleviate high dimensionality is feature selection, the process of selecting the most relevant attributes and removing irrelevant and redundant ones. Much research has been done towards evaluating the performance of classifiers before and after feature selection, but little work has been done examining how sensitive the selected feature subsets are to changes (additions/deletions) in the dataset. In this study we evaluate the robustness of six commonly used feature selection techniques, investigating the impact of data sampling and class noise on the stability of feature selection. All experiments are carried out with six commonly used feature rankers on four groups of datasets from the biology domain. We employ three sampling techniques, and generate artificial class noise to better simulate real-world datasets. The results demonstrate that although no ranker consistently outperforms the others, Gain Ratio shows the least stability on average. Additional tests using our feature rankers for building classification models also show that a feature ranker's stability is not an indicator of its performance in classification.},
booktitle = {Proceedings of the 2011 10th International Conference on Machine Learning and Applications  and  Workshops - Volume 01},
pages = {172–177},
numpages = {6},
keywords = {bioinformatics, class imbalance, classification, feature selection, noise injection, stability},
series = {ICMLA '11}
}

@article{10.1155/2020/3180628,
author = {Zhang, Zhe and Wang, Cheng and Gao, Yueer and Chen, Jianwei and Zhang, Yiwen and Huang, Chenxi},
title = {Short-Term Passenger Flow Forecast of Rail Transit Station Based on MIC Feature Selection and ST-LightGBM considering Transfer Passenger Flow},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1058-9244},
url = {https://doi.org/10.1155/2020/3180628},
doi = {10.1155/2020/3180628},
abstract = {To solve the problems of current short-term forecasting methods for metro passenger flow, such as unclear influencing factors, low accuracy, and high time-space complexity, a method for metro passenger flow based on ST-LightGBM after considering transfer passenger flow is proposed. Firstly, using historical data as the training set to transform the problem into a data-driven multi-input single-output regression prediction problem, the problem of the short-term prediction of metro passenger flow is formalized and the difficulties of the problem are identified. Secondly, we extract the candidate temporal and spatial features that may affect passenger flow at a metro station from passenger travel data based on the spatial transfer and spatial similarity of passenger flow. Thirdly, we use a maximal information coefficient (MIC) feature selection algorithm to select the significant impact features as the input. Finally, a short-term forecasting model for metro passenger flow based on the light gradient boosting machine (LightGBM) model is established. Taking transfer passenger flow into account, this method has a low space-time cost and high accuracy. The experimental results on the dataset of Lianban metro station in Xiamen city show that the proposed method obtains higher prediction accuracy than SARIMA, SVR, and BP network.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@inproceedings{10.5555/3042817.3042875,
author = {Nguyen, Trung Thanh and Li, Zhuoru and Silander, Tomi and Leong, Tze-Yun},
title = {Online feature selection for model-based reinforcement learning},
year = {2013},
publisher = {JMLR.org},
abstract = {We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {I–498–I–506},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{10.1109/ICMLA.2013.156,
author = {Khoshgoftaar, Taghi M. and Dittman, David J. and Wald, Randall and Napolitano, Amri},
title = {Contrasting Undersampled Boosting with Internal and External Feature Selection for Patient Response Datasets},
year = {2013},
isbn = {9780769551449},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2013.156},
doi = {10.1109/ICMLA.2013.156},
abstract = {Class imbalance (where one class has many more instances than the other class(es)) and high dimensionality (large number of features per instance) are two prevalent problems that are frequently present in patient response datasets. In addition to these problems, these datasets are notoriously difficult to build effective models from. This paper introduces a new hybrid boosting algorithm named SelectRUSBoost which combines data sampling and feature selection with every iteration of boosting. We test SelectRUSBoost along with RUSBoost combined with external feature selection on a set of five patient response datasets. In addition to the datasets we also utilize two classifiers, three filter-based feature selection techniques, and four feature subset sizes. Our results show that SelectRUSBoost will, with few exceptions, outperform RUSBoost combined with external feature selection. Also, the feature selection technique information gain outperformed the other techniques for all combinations of boosting approach, classifier, and feature subset size, and in addition for this feature selection technique SelectRUSBoost always (without exception) outperformed RUSBoost combined with external selection. Statistical analysis confirmed that SelectRUSBoost gives better performance than RUSBoost combined with external selection. This is the first work which utilizes SelectRUSBoost in a bioinformatics study.},
booktitle = {Proceedings of the 2013 12th International Conference on Machine Learning and Applications - Volume 02},
pages = {404–410},
numpages = {7},
keywords = {Bioinformatics, Boosting, Class Imbalance, Feature Selection, High Dimensionality, Patient Response},
series = {ICMLA '13}
}

@article{10.1016/j.adhoc.2016.06.011,
author = {Panousopoulou, Athanasia and Azkune, Mikel and Tsakalides, Panagiotis},
title = {Feature selection for performance characterization in multi-hop wireless sensor networks},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2016.06.011},
doi = {10.1016/j.adhoc.2016.06.011},
abstract = {Current trends in Wireless Sensor Networks are faced with the challenge of shifting from testbeds in controlled environments to real-life deployments, characterized by unattended and long-term operation. The network performance in such settings depends on various factors, ranging from the operational space, the behavior of the protocol stack, the intra-network dynamics, and the status of each individual node. As such, characterizing the network's high-level performance based exclusively on link-quality estimation, can yield episodic snapshots on the performance of specific, point-to-point links. The objective of this work is to provide an integrated framework for the unsupervised selection of the dominant features that have crucial impact on the performance of end-to-end links, established over a multi-hop topology. Our focus is on compressing the original feature vector of network parameters, by eliminating redundant network attributes with predictable behavior. The proposed approach is implemented alongside different cases of protocol stacks and evaluated on data collected from real-life deployments in rural and industrial environments. Discussions on the efficacy of the proposed scheme, and the dominant network characteristics per deployment are offered.},
journal = {Ad Hoc Netw.},
month = oct,
pages = {70–89},
numpages = {20},
keywords = {Network measurement and analysis, Testbeds and experimental evaluation, Unsupervised learning, Wireless sensor networks}
}

@inproceedings{10.1007/978-3-319-03756-1_19,
author = {Selvaraj, Gunasundari and S., Janakiraman},
title = {Improved Feature Selection Based on Particle Swarm Optimization for Liver Disease Diagnosis},
year = {2013},
isbn = {9783319037554},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03756-1_19},
doi = {10.1007/978-3-319-03756-1_19},
abstract = {Dimensionality reduction of a feature set is a usual pre-processing step used for image classification to improve their accuracy. In this paper an automatic Computer Aided Diagnostic system (CAD) is proposed for detection of liver diseases like hepatoma and hemangioma from abdominal Computed Tomography (CT) images using an evolutionary approach for feature selection. The liver is segmented using adaptive thresholding. Histogram analyzer is used to fix the threshold and morphological operation is used for post processing. Rules are applied to remove the obstacles. Fuzzy c-Mean (FCM) clustering is used to extract the lesion from the segmented liver. Auto covariance features are extracted from the segmented lesion. The Binary Particle Swarm Optimization (BPSO) is applied to get the best reduced feature set. The textual information obtained after feature reduction was used to train Probabilistic Neural Network (PNN). The results obtained from different transfer functions are analyzed and compared.},
booktitle = {Proceedings of the 4th International Conference on Swarm, Evolutionary, and Memetic Computing - Volume 8298},
pages = {214–225},
numpages = {12},
keywords = {BPSO, Covariance Matrix, FCM, Feature Selection, Liver CAD System, Morphological Operation, PNN},
location = {Chennai, India},
series = {SEMCCO 2013}
}

@article{10.1016/j.knosys.2016.12.018,
author = {Ravi, Kumar and Ravi, Vadlamani},
title = {A novel automatic satire and irony detection using ensembled feature selection and data mining},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {120},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.12.018},
doi = {10.1016/j.knosys.2016.12.018},
abstract = {Figurative language detection has always been a difficult task for human beings while being a more difficult proposition, even if automated using text and data mining. The available computational approaches are also quite limited in their capabilities and scope. In this regard, we propose an ensembled text feature selection method followed by a new framework in the paradigm of text and data mining to automatically detect satire, sarcasm, and irony found in news and customer reviews. The effectiveness of the proposed approach was demonstrated on three datasets including two satiric and one ironic dataset. The proposed methodology performed well on one satiric dataset and yielded promising results on the remaining two datasets. Moreover, we found out some interesting common characteristics of satire and irony like affective process (negative emotion), personal concern (leisure), biological process (body and sexual), perception (see), informal language (swear), social process (male), cognitive process (certain), and psycholinguistic (concreteness and imageability), which were extracted from three corpora. Of particular significance is the comparison of our approach with human annotators' evaluations, which served as a baseline in these tasks.},
journal = {Know.-Based Syst.},
month = mar,
pages = {15–33},
numpages = {19},
keywords = {Customer reviews, Ensembled feature subset selection, Irony detection, LIWC, Satire detection, Satiric news, Sentiment analysis, TAALES}
}

@article{10.1007/s00138-012-0461-1,
author = {Marques, Joselene and Igel, Christian and Lillholm, Martin and Dam, Erik B.},
title = {Linear feature selection in texture analysis - A PLS based method},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {7},
issn = {0932-8092},
url = {https://doi.org/10.1007/s00138-012-0461-1},
doi = {10.1007/s00138-012-0461-1},
abstract = {We present a texture analysis methodology that combined uncommitted machine-learning techniques and partial least square (PLS) in a fully automatic framework. Our approach introduces a robust PLS-based dimensionality reduction (DR) step to specifically address outliers and high-dimensional feature sets. The texture analysis framework was applied to diagnosis of knee osteoarthritis (OA). To classify between healthy subjects and OA patients, a generic bank of texture features was extracted from magnetic resonance images of tibial knee bone. The features were used as input to the DR algorithm, which first applied a PLS regression to rank the features and then defined the best number of features to retain in the model by an iterative learning phase. The outliers in the dataset, that could inflate the number of selected features, were eliminated by a pre-processing step. To cope with the limited number of samples, the data were evaluated using Monte Carlo cross validation (CV). The developed DR method demonstrated consistency in selecting a relatively homogeneous set of features across the CV iterations. Per each CV group, a median of 19 % of the original features was selected and considering all CV groups, the methods selected 36 % of the original features available. The diagnosis evaluation reached a generalization area-under-the-ROC curve of 0.92, which was higher than established cartilage-based markers known to relate to OA diagnosis.},
journal = {Mach. Vision Appl.},
month = oct,
pages = {1435–1444},
numpages = {10},
keywords = {Classification, Feature extraction, Feature selection, Machine learning, Partial least squares, Texture analysis}
}

@article{10.1007/s10796-013-9430-0,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri and Wald, Randall},
title = {A comparative study of iterative and non-iterative feature selection techniques for software defect prediction},
year = {2014},
issue_date = {November  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {5},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-013-9430-0},
doi = {10.1007/s10796-013-9430-0},
abstract = {Two important problems which can affect the performance of classification models are high-dimensionality (an overabundance of independent features in the dataset) and imbalanced data (a skewed class distribution which creates at least one class with many fewer instances than other classes). To resolve these problems concurrently, we propose an iterative feature selection approach, which repeated applies data sampling (in order to address class imbalance) followed by feature selection (in order to address high-dimensionality), and finally we perform an aggregation step which combines the ranked feature lists from the separate iterations of sampling. This approach is designed to find a ranked feature list which is particularly effective on the more balanced dataset resulting from sampling while minimizing the risk of losing data through the sampling step and missing important features. To demonstrate this technique, we employ 18 different feature selection algorithms and Random Undersampling with two post-sampling class distributions. We also investigate the use of sampling and feature selection without the iterative step (e.g., using the ranked list from a single iteration, rather than combining the lists from multiple iterations), and compare these results from the version which uses iteration. Our study is carried out using three groups of datasets with different levels of class balance, all of which were collected from a real-world software system. All of our experiments use four different learners and one feature subset size. We find that our proposed iterative feature selection approach outperforms the non-iterative approach.},
journal = {Information Systems Frontiers},
month = nov,
pages = {801–822},
numpages = {22},
keywords = {Class imbalance, Date sampling, High dimensionality, Iterative feature selection, Software defect prediction}
}

@article{10.1016/j.patcog.2012.04.015,
author = {Boubezoul, Abderrahmane and Paris, S\'{e}Bastien},
title = {Application of global optimization methods to model and feature selection},
year = {2012},
issue_date = {October, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {45},
number = {10},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2012.04.015},
doi = {10.1016/j.patcog.2012.04.015},
abstract = {Many data mining applications involve the task of building a model for predictive classification. The goal of this model is to classify data instances into classes or categories of the same type. The use of variables not related to the classes can reduce the accuracy and reliability of classification or prediction model. Superfluous variables can also increase the costs of building a model particularly on large datasets. The feature selection and hyper-parameters optimization problem can be solved by either an exhaustive search over all parameter values or an optimization procedure that explores only a finite subset of the possible values. The objective of this research is to simultaneously optimize the hyper-parameters and feature subset without degrading the generalization performances of the induction algorithm. We present a global optimization approach based on the use of Cross-Entropy Method to solve this kind of problem.},
journal = {Pattern Recogn.},
month = oct,
pages = {3676–3686},
numpages = {11},
keywords = {Cross-Entropy Method, Feature selection, Hyper-parameters optimization, Particle swarm optimization, Support vector machines}
}

@article{10.1016/j.knosys.2015.04.015,
author = {Ekbal, Asif and Saha, Sriparna},
title = {Joint model for feature selection and parameter optimization coupled with classifier ensemble in chemical mention recognition},
year = {2015},
issue_date = {September 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.04.015},
doi = {10.1016/j.knosys.2015.04.015},
abstract = {Mention recognition in chemical texts plays an important role in a wide-spread range of application areas. Feature selection and parameter optimization are the two important issues in machine learning. While the former improves the quality of a classifier by removing the redundant and irrelevant features, the later concerns finding the most suitable parameter values, which have significant impact on the overall classification performance. In this paper we formulate a joint model that performs feature selection and parameter optimization simultaneously, and propose two approaches based on the concepts of single and multiobjective optimization techniques. Classifier ensemble techniques are also employed to improve the performance further. We identify and implement variety of features that are mostly domain-independent. Experiments are performed with various configurations on the benchmark patent and Medline datasets. Evaluation shows encouraging performance in all the settings.},
journal = {Know.-Based Syst.},
month = sep,
pages = {37–51},
numpages = {15},
keywords = {Conditional random field (CRF), Feature selection, Multiobjective optimization (MOO), Parameter optimization, Single objective optimization (SOO), Support vector machine (SVM)}
}

@inproceedings{10.1007/978-3-642-30220-6_36,
author = {Gupta, Akshansh and Agrawal, R. K.},
title = {Relevant feature selection from EEG signal for mental task classification},
year = {2012},
isbn = {9783642302190},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30220-6_36},
doi = {10.1007/978-3-642-30220-6_36},
abstract = {In last few years, the research community has shown interest in the development of Brain Computer Interface which may assists physically challenged people to communicate with the help of brain signal. The two important components of such BCI system are to determine appropriate features and classification method to achieve better performance. In literature, Empirical Mode Decomposition is suggested for feature extraction from EEG which is suitable for the analysis of non-linear and non-stationary time series. However, the features obtained from EEG may contain irrelevant and redundant features which make them inefficient for machine learning. Relevant features not only decrease the processing time to train a classifier but also provide better generalization. Hence, relevant features which provide maximum classification accuracy are selected using ratio of scatter matrices, Chernoff distance measure and linear regression. The performance of different mental task using different measures used for feature selection is compared and evaluated in terms of classification accuracy. Experimental results show that there is significant improvement in classification accuracy with features selected using all feature selection methods and in particular with ratio of scatter matrices.},
booktitle = {Proceedings of the 16th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining - Volume Part II},
pages = {431–442},
numpages = {12},
keywords = {brain computer interface, chernoff distance measure, empirical mode decomposition, feature selection, linear regression, scatter matrices},
location = {Kuala Lumpur, Malaysia},
series = {PAKDD'12}
}

@article{10.1007/s00521-017-2837-7,
author = {Sindhu, R. and Ngadiran, Ruzelita and Yacob, Yasmin Mohd and Zahri, Nik Adilah Hanin and Hariharan, M.},
title = {RETRACTED ARTICLE: Sine–cosine algorithm for feature selection with elitism strategy and new updating mechanism},
year = {2017},
issue_date = {Oct 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-2837-7},
doi = {10.1007/s00521-017-2837-7},
abstract = {Pattern recognition is the task of choosing the pertinent and descriptive features that best describes the target concept during feature selection (FS). Choosing such descriptive features becomes a daunting task in large-volume datasets which have high dimensionality. In such cases, selecting the discriminative features with better classification accuracy is tedious. To overcome this issue, in recent times, many search heuristics have been used to select the best features from these large-volume datasets. In this work, a sine–cosine algorithm (SCA) with Elitism strategy and new best solution update mechanism is proposed to select best features/attributes to improve the classification accuracy. Improved version of SCA is named as improved sine–cosine algorithm (ISCA). Wrapper-based FS approach is used. ELM with radial basis function kernel is used as the learning algorithm. For experimentation, ISCA is tested with ten benchmark datasets. Experimental results have proved the efficiency of ISCA in achieving better classification performance along with less number of features. Both computational and time complexity has been handled by this algorithm in an expedite manner. The potency of this algorithm is proved by comparing its results with three well-known meta-heuristics such as GA, PSO and basic SCA. Finally, it can be seen that pattern classification using ISCA has been commendable in achieving better classification performance.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {2947–2958},
numpages = {12},
keywords = {Pattern classification, Wrapper-based FS, Improved sine–cosine algorithm, Feature optimization}
}

@article{10.5555/1368376.1368381,
author = {Li, Yun and Lu, Bao-Liang and Wu, Zhong-Fu},
title = {Hierarchical fuzzy filter method for unsupervised feature selection},
year = {2007},
issue_date = {April 2007},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {2},
issn = {1064-1246},
abstract = {The problem of feature selection has long been an active research topic within statistics and pattern recognition. So far, most methods of feature selection focus on supervised data where class information is available. For unsupervised data, the related methods of feature selection are few. The presented article demonstrates a way of unsupervised feature selection, which is a two-level filter model removing the redundant and irrelevant features, respectively. The redundant features are eliminated using any clustering algorithm, and a new method is proposed to remove the irrelevant features: first rank the features according to their relevance to cluster and then a subset of relevant features is selected using the Fuzzy Feature Evaluation Index (FFEI) with some changes and extensions. The experimental results have shown the effectiveness of the proposed method for high-dimensional data. Our major contributions are: (1) to present a new hierarchical filter method for unsupervised feature selection; (2) to propose a new algorithm for removing the irrelevant features; (3) to extend the FFEI, and present a method for calculating the approximate weight of feature in FFEI, which improves the efficiency and robustness of the method.},
journal = {J. Intell. Fuzzy Syst.},
month = apr,
pages = {157–169},
numpages = {13},
keywords = {Unsupervised feature selection, filter method, fuzzy set, ranking index}
}

@article{10.1007/s10115-011-0467-4,
author = {Bouguila, Nizar and Ziou, Djemel},
title = {A countably infinite mixture model for clustering and feature selection},
year = {2012},
issue_date = {November  2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-011-0467-4},
doi = {10.1007/s10115-011-0467-4},
abstract = {Mixture modeling is one of the most useful tools in machine learning and data mining applications. An important challenge when applying finite mixture models is the selection of the number of clusters which best describes the data. Recent developments have shown that this problem can be handled by the application of non-parametric Bayesian techniques to mixture modeling. Another important crucial preprocessing step to mixture learning is the selection of the most relevant features. The main approach in this paper, to tackle these problems, consists on storing the knowledge in a generalized Dirichlet mixture model by applying non-parametric Bayesian estimation and inference techniques. Specifically, we extend finite generalized Dirichlet mixture models to the infinite case in which the number of components and relevant features do not need to be known a priori. This extension provides a natural representation of uncertainty regarding the challenging problem of model selection. We propose a Markov Chain Monte Carlo algorithm to learn the resulted infinite mixture. Through applications involving text and image categorization, we show that infinite mixture models offer a more powerful and robust performance than classic finite mixtures for both clustering and feature selection.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {351–370},
numpages = {20},
keywords = {Categorization, Clustering, Dirichlet process, Feature selection, Generalized Dirichlet, MCMC, Mixture models, Non-parametric Bayesian methods}
}

@inproceedings{10.5555/1888258.1888299,
author = {Hern\'{a}ndez-Lobato, Daniel and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Helleputte, Thibault and Dupont, Pierre},
title = {Expectation propagation for Bayesian multi-task feature selection},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we propose a Bayesian model for multitask feature selection. This model is based on a generalized spike and slab sparse prior distribution that enforces the selection of a common subset of features across several tasks. Since exact Bayesian inference in this model is intractable, approximate inference is performed through expectation propagation (EP). EP approximates the posterior distribution of the model using a parametric probability distribution. This posterior approximation is particularly useful to identify relevant features for prediction. We focus on problems for which the number of features d is significantly larger than the number of instances for each task. We propose an efficient parametrization of the EP algorithm that offers a computational complexity linear in d. Experiments on several multitask datasets show that the proposed model outperforms baseline approaches for single-task learning or data pooling across all tasks, as well as two state-of-the-art multi-task learning approaches. Additional experiments confirm the stability of the proposed feature selection with respect to various sub-samplings of the training data.},
booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
pages = {522–537},
numpages = {16},
keywords = {approximate bayesian inference, expectation propagation, feature selection, multi-task learning},
location = {Barcelona, Spain},
series = {ECML PKDD'10}
}

@article{10.1504/IJDMB.2016.080041,
author = {Lee, Sangbum and Oh, Sejong},
title = {Development of a library with feature selection algorithm based on microarray gene expression dataset for biomarker identification},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {2},
issn = {1748-5673},
url = {https://doi.org/10.1504/IJDMB.2016.080041},
doi = {10.1504/IJDMB.2016.080041},
abstract = {Gene expression data is used to find significant genes related to specific disease, such as lung cancer. These significant genes can be used as biomarkers to diagnose disease, and data mining techniques are useful in finding such biomarkers. Feature selection and classification schemes are extensively used for this purpose. Researchers should test various combinations of data mining schemes to find the best biomarker since there is no ultimate scheme for every case of datasets. Thus, the process is tedious and requires effort. In this study, we propose a software library that finds biomarker genes based on microarray datasets. The proposed library contains procedural steps to identify and test biomarker genes and is implemented as an R library for general use. This library with feature selection algorithm, helps to save time and effort in analysing and combining codes to test their research ideas.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {93–110},
numpages = {18},
keywords = {R library, bioinformatics, biomarker genes, biomarker identification, biomarkers, classification, data mining, dimensionality, feature evaluation, feature selection, gene expression dataset, microarray datasets, software library}
}

@article{10.1016/j.compbiomed.2013.01.014,
author = {Garde, Ainara and Voss, Andreas and Caminal, Pere and Benito, Salvador and Giraldo, Beatriz F.},
title = {SVM-based feature selection to optimize sensitivity-specificity balance applied to weaning},
year = {2013},
issue_date = {June, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {43},
number = {5},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2013.01.014},
doi = {10.1016/j.compbiomed.2013.01.014},
abstract = {Classification algorithms with unbalanced datasets tend to produce high predictive accuracy over the majority class, but poor predictive accuracy over the minority class. This problem is very common in biomedical data mining. This paper introduces a Support Vector Machine (SVM)-based optimized feature selection method, to select the most relevant features and maintain an accurate and well-balanced sensitivity-specificity result between unbalanced groups. A new metric called the balance index (B) is defined to implement this optimization. The balance index measures the difference between the misclassified data within each class. The proposed optimized feature selection is applied to the classification of patients' weaning trials from mechanical ventilation: patients with successful trials who were able to maintain spontaneous breathing after 48h and patients who failed to maintain spontaneous breathing and were reconnected to mechanical ventilation after 30min. Patients are characterized through cardiac and respiratory signals, applying joint symbolic dynamic (JSD) analysis to cardiac interbeat and breath durations. First, the most suitable parameters (C"+,C"-,@s) are selected to define the appropriate SVM. Then, the feature selection process is carried out with this SVM, to maintain B lower than 40%. The best result is obtained using 6 features with an accuracy of 80%, a B of 18.64%, a sensitivity of 74.36% and a specificity of 82.42%.},
journal = {Comput. Biol. Med.},
month = jun,
pages = {533–540},
numpages = {8},
keywords = {Balance index, Cardiorespiratory interaction, Feature selection, Joint symbolic dynamics, Sensitivity-specificity balance, Support vector machines, Weaning procedure}
}

@article{10.1016/j.eswa.2012.09.009,
author = {Savio, Alexandre and Gra\~{n}A, Manuel},
title = {Deformation based feature selection for Computer Aided Diagnosis of Alzheimer's Disease},
year = {2013},
issue_date = {April, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.09.009},
doi = {10.1016/j.eswa.2012.09.009},
abstract = {Deformation-based Morphometry (DBM) allows detection of significant morphological differences of brain anatomy, such as those related to brain atrophy in Alzheimer's Disease (AD). DBM process is as follows: First, performs the non-linear registration of a subject's structural MRI volume to a reference template. Second, computes scalar measures of the registration's deformation field. Third, performs across volume statistical group analysis of these scalar measures to detect effects. In this paper we use the scalar deformation measures for Computer Aided Diagnosis (CAD) systems for AD. Specifically this paper deals with feature extraction methods over five such scalar measures. We evaluate three supervised feature selection methods based on voxel site significance measures given by Pearson correlation, Bhattacharyya distance and Welch's t-test, respectively. The CAD system discriminating between healthy control subjects (HC) and AD patients consists of a Support Vector Machine (SVM) classifier trained on the DBM selected features. The paper reports experimental results on structural MRI data from the cross-sectional OASIS database. Average 10-fold cross-validation classification results are comparable or improve the state-of-the-art results of other approaches performing CAD from structural MRI data. Localization in the brain of the most discriminant deformation voxel sites is in agreement with findings reported in the literature.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {1619–1628},
numpages = {10},
keywords = {Alzheimer's disease, Computed Aided Diagnosis, Feature selection, Magnetic Resonance Imaging, Suppor Vector Machines}
}

@article{10.1016/j.artint.2004.05.009,
author = {Liu, Huan and Motoda, Hiroshi and Yu, Lei},
title = {A selective sampling approach to active feature selection},
year = {2004},
issue_date = {November 2004},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {159},
number = {1–2},
issn = {0004-3702},
url = {https://doi.org/10.1016/j.artint.2004.05.009},
doi = {10.1016/j.artint.2004.05.009},
abstract = {Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. Traditional feature selection methods resort to random sampling in dealing with data sets with a huge number of instances. In this paper, we introduce the concept of active feature selection, and investigate a selective sampling approach to active feature selection in a filter model setting. We present a formalism of selective sampling based on data variance, and apply it to a widely used feature selection algorithm Relief. Further, we show how it realizes active feature selection and reduces the required number of training instances to achieve time savings without performance deterioration. We design objective evaluation measures of performance, conduct extensive experiments using both synthetic and benchmark data sets, and observe consistent and significant improvement. We suggest some further work based on our study and experiments.},
journal = {Artif. Intell.},
month = nov,
pages = {49–74},
numpages = {26},
keywords = {dimensionality reduction, feature selection and ranking, learning, sampling}
}

@article{10.1016/j.dss.2012.05.023,
author = {Duric, Adnan and Song, Fei},
title = {Feature selection for sentiment analysis based on content and syntax models},
year = {2012},
issue_date = {November, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {4},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2012.05.023},
doi = {10.1016/j.dss.2012.05.023},
abstract = {Recent solutions for sentiment analysis have relied on feature selection methods ranging from lexicon-based approaches where the set of features are generated by humans, to approaches that use general statistical measures where features are selected solely on empirical evidence. The advantage of statistical approaches is that they are fully automatic, however, they often fail to separate features that carry sentiment from those that do not. In this paper we propose a set of new feature selection schemes that use a Content and Syntax model to automatically learn a set of features in a review document by separating the entities that are being reviewed from the subjective expressions that describe those entities in terms of polarities. By focusing only on the subjective expressions and ignoring the entities, we can choose more salient features for document-level sentiment analysis. The results obtained from using these features in a maximum entropy classifier are competitive with the state-of-the-art machine learning approaches.},
journal = {Decis. Support Syst.},
month = nov,
pages = {704–711},
numpages = {8},
keywords = {Content and Syntax models, Feature selection, Maximum entropy modeling, Sentiment analysis, Text classification, Topic modeling}
}

@inproceedings{10.5555/2034161.2034190,
author = {Uguroglu, Selen and Carbonell, Jaime},
title = {Feature selection for transfer learning},
year = {2011},
isbn = {9783642238079},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Common assumption in most machine learning algorithms is that, labeled (source) data and unlabeled (target) data are sampled from the same distribution. However, many real world tasks violate this assumption: in temporal domains, feature distributions may vary over time, clinical studies may have sampling bias, or sometimes sufficient labeled data for the domain of interest does not exist, and labeled data from a related domain must be utilized. In such settings, knowing in which dimensions source and target data vary is extremely important to reduce the distance between domains and accurately transfer knowledge. In this paper, we present a novel method to identify variant and invariant features between two datasets. Our contribution is two fold: First, we present a novel transfer learning approach for domain adaptation, and second, we formalize the problem of finding differently distributed features as a convex optimization problem. Experimental studies on synthetic and benchmark real world datasets show that our approach outperform other transfer learning approaches, and it aids the prediction accuracy significantly.},
booktitle = {Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part III},
pages = {430–442},
numpages = {13},
location = {Athens, Greece},
series = {ECML PKDD'11}
}

@phdthesis{10.5555/AAI28169429,
author = {Ahmed, Murat \"{O}mer},
advisor = {Guenther, Walther, and L., Lai, T. and Robert, Tibshirani,},
title = {Topics in Unsupervised Learning: Feature Selection and Multi-Modality},
year = {2010},
isbn = {9798672117676},
publisher = {Stanford University},
address = {Stanford, CA, USA},
abstract = {Often in the unsupervised setting one clusters data attempting to learn the unobserved latent class variable. Proper inference requires determining both the correct number of clusters and the subset of features dependent on the class variable. In the supervised setting one has prediction error to guide the decision making process. An analog for unsupervised data is prediction strength, Tibshirani and Walther (2005), whereby one attempts to estimate this error by measuring cluster stability. Originally proposed as a method for determining the number of clusters, we will show that prediction strength can also be used for feature selection. Additionally, one can compute the likelihood a feature depends on the latent variable when feature selection is posed as a model selection problem. As the dimensionality of the problem gets large sampling models must be approached with care, motivating a survey of various sampling methods. The second part of the thesis considers low-dimensional projections of the data via principal curves, Hastie and Stuetzle (1989), as a vehicle for determining the number of clusters. In the low-dimensional setting (often a single dimension) multi-modality investigation is simplified resulting in flexible estimation of the actual number of clusters.},
note = {AAI28169429}
}

@article{10.1016/j.cviu.2012.09.003,
author = {Pan, Hong and Zhu, Yaping and Xia, Liangzheng},
title = {Efficient and accurate face detection using heterogeneous feature descriptors and feature selection},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {1},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2012.09.003},
doi = {10.1016/j.cviu.2012.09.003},
abstract = {The performance of an efficient and accurate face detection system depends on several issues: (1) distinctive representation for face patterns; (2) effective algorithm for feature selection and classifier learning; (3) suitable framework for rapid background removal. To address the first issue, we propose to represent face patterns with a set of heterogeneous and complementary feature descriptors including the Generalized Haar-like (GH) descriptor, Multi-Block Local Binary Patterns (MB-LBP) descriptor and Speeded-Up Robust Features (SURF) descriptor. To address the second issue, Particle Swarm Optimization (PSO) algorithm is incorporated into the Adaboost framework, replacing the exhaustive search used in original Adaboost for efficient feature selection. The utilization of heterogeneous feature descriptors enriches the diversity of feature types for Adaboost learning algorithm. As a result, classification performance of the boosted ensemble classifier also improves significantly. A three-stage hierarchical classifier structure is proposed to tackle the last issue. In particular, a new stage is added to detect candidate face regions more quickly by using a large size window with a large moving step. Nonlinear support vector machine (SVM) classifiers are used instead of decision stump classifiers in the last stage to remove those remaining complex non-face patterns that cannot be rejected in the previous two stages. Combining the abovementioned effective modules, we derive the proposed Hetero-PSO-Adaboost-SVM face detector that achieves superior detection accuracy while maintaining a low training and detection complexity. Extensive experiments demonstrate the robustness and efficiency of our system by comparing it with several popular state-of-the-art algorithms on our own test set as well as the widely used CMU+MIT frontal and CMU profile face dataset.},
journal = {Comput. Vis. Image Underst.},
month = jan,
pages = {12–28},
numpages = {17},
keywords = {Adaboost, Cascade classifier, Face detection, Feature selection, PSO}
}

@article{10.1007/s11045-016-0404-5,
author = {Chang, Chuan-Yu and Chang, Chuan-Wang and Kathiravan, S. and Lin, Chen and Chen, Szu-Ta},
title = {DAG-SVM based infant cry classification system using sequential forward floating feature selection},
year = {2017},
issue_date = {July      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {3},
issn = {0923-6082},
url = {https://doi.org/10.1007/s11045-016-0404-5},
doi = {10.1007/s11045-016-0404-5},
abstract = {Crying is the infant's first verbal communication. Before learning how to express the emotions or physiological/psychological requirements with language, infants usually express how they feel through crying. Crying is a response to a stimulus such as hunger, pain, or discomfort. However, it is sometimes difficult to figure out why an infant is crying. This can certainly be frustrating or even frightening for a caretaker, and so we in this paper have proposed an infant cry classification system to categorize the types of infant crying to help parents and nursing staffs attending to the needs of the infants. Currently, three kinds of distinct infant cries have been identified: hunger; pain; and feeling sleepy. Fifteen features are extracted from each crying frame and the sequential forward floating selection is then adopted to pick out high discriminative features. The directed acyclic graph support vector machine is finally used to classify infant crying. Experimental results have revealed the good performance of the proposed system and the classification accuracy is up to 92.17 %.},
journal = {Multidimensional Syst. Signal Process.},
month = jul,
pages = {961–976},
numpages = {16},
keywords = {Infant cry, Patterns of crying, Support vector machine}
}

@article{10.1109/TCBB.2016.2631164,
author = {Gangeh, Mehrdad J. and Zarkoob, Hadi and Ghodsi, Ali},
title = {Fast and Scalable Feature Selection for Gene Expression Data Using Hilbert-Schmidt Independence Criterion},
year = {2017},
issue_date = {January 2017},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {14},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2631164},
doi = {10.1109/TCBB.2016.2631164},
abstract = {Goal: In computational biology, selecting a small subset of informative genes from microarray data continues to be a challenge due to the presence of thousands of genes. This paper aims at quantifying the dependence between gene expression data and the response variables and to identifying a subset of the most informative genes using a fast and scalable multivariate algorithm. Methods: A novel algorithm for feature selection from gene expression data was developed. The algorithm was based on the Hilbert-Schmidt independence criterion HSIC, and was partly motivated by singular value decomposition SVD. Results: The algorithm is computationally fast and scalable to large datasets. Moreover, it can be applied to problems with any type of response variables including, biclass, multiclass, and continuous response variables. The performance of the proposed algorithm in terms of accuracy, stability of the selected genes, speed, and scalability was evaluated using both synthetic and real-world datasets. The simulation results demonstrated that the proposed algorithm effectively and efficiently extracted stable genes with high predictive capability, in particular for datasets with multiclass response variables. Conclusion/Significance: The proposed method does not require the whole microarray dataset to be stored in memory, and thus can easily be scaled to large datasets. This capability is an important attribute in big data analytics, where data can be large and massively distributed.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {167–181},
numpages = {15}
}

@article{10.1016/j.patrec.2012.01.020,
author = {Xie, Yuan and Qu, Yanyun and Li, Cuihua and Zhang, Wensheng},
title = {Online multiple instance gradient feature selection for robust visual tracking},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {9},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2012.01.020},
doi = {10.1016/j.patrec.2012.01.020},
abstract = {In this paper, we focus on learning an adaptive appearance model robustly and effectively for object tracking. There are two important factors to affect object tracking, the one is how to represent the object using a discriminative appearance model, the other is how to update appearance model in an appropriate manner. In this paper, following the state-of-the-art tracking techniques which treat object tracking as a binary classification problem, we firstly employ a new gradient-based Histogram of Oriented Gradient (HOG) feature selection mechanism under Multiple Instance Learning (MIL) framework for constructing target appearance model, and then propose a novel optimization scheme to update such appearance model robustly. This is an unified framework that not only provides an efficient way of selecting the discriminative feature set which forms a powerful appearance model, but also updates appearance model in online MIL Boost manner which could achieve robust tracking overcoming the drifting problem. Experiments on several challenging video sequences demonstrate the effectiveness and robustness of our proposal.},
journal = {Pattern Recogn. Lett.},
month = jul,
pages = {1075–1082},
numpages = {8},
keywords = {Gradient-based feature selection, HOG, Multiple Instance Learning, Online object tracking}
}

@article{10.1504/IJDMB.2015.070852,
author = {Yao, Dengju and Yang, Jing and Zhan, Xiaojuan and Zhan, Xiaorong and Xie, Zhiqiang},
title = {A novel random forests-based feature selection method for microarray expression data analysis},
year = {2015},
issue_date = {July 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {1},
issn = {1748-5673},
url = {https://doi.org/10.1504/IJDMB.2015.070852},
doi = {10.1504/IJDMB.2015.070852},
abstract = {High-dimensional data and a large number of redundancy features in bioinformatics research have created an urgent need for feature selection. In this paper, a novel random forests-based feature selection method is proposed that adopts the idea of stratifying feature space and combines generalised sequence backward searching and generalised sequence forward searching strategies. A random forest variable importance score is used to rank features, and different classifiers are used as a feature subset evaluating function. The proposed method is examined on five microarray expression datasets, including leukaemia, prostate, breast, nervous and DLBCL, and the average accuracies of the SVM classifier in these datasets are 100%, 95.24%, 85%, 91.67%, and 91.67%, respectively. The results show that the proposed method could not only improve the classification accuracy but also greatly reduce the computation time of the feature selection process.},
journal = {Int. J. Data Min. Bioinformatics},
month = jul,
pages = {84–101},
numpages = {18}
}

@inproceedings{10.5555/2050461.2050472,
author = {Janusz, Andrzej and Stawicki, Sebastian},
title = {Applications of approximate reducts to the feature selection problem},
year = {2011},
isbn = {9783642244247},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we overview two feature rankings methods that utilize basic notions from the rough set theory, such as the idea of the decision reducts. We also propose a new algorithm, called Rough Attribute Ranker. In our approach, the usefulness of features is measured by their impact on quality of the reducts that contain them. We experimentally compare the reduct-based methods with several classic attribute rankers using synthetic, as well as real-life high dimensional datasets.},
booktitle = {Proceedings of the 6th International Conference on Rough Sets and Knowledge Technology},
pages = {45–50},
numpages = {6},
keywords = {approximate reducts, attribute filtering, feature selection},
location = {Banff, Canada},
series = {RSKT'11}
}

@article{10.1155/2011/643816,
author = {Doquire, G. and De Lannoy, G. and Fran\c{c}ois, D. and Verleysen, M.},
title = {Feature selection for interpatient supervised heart beat classification},
year = {2011},
issue_date = {January 2011},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2011},
issn = {1687-5265},
url = {https://doi.org/10.1155/2011/643816},
doi = {10.1155/2011/643816},
abstract = {Supervised and interpatient classification of heart beats is primordial in many applications requiring long-term monitoring of the cardiac function. Several classification models able to cope with the strong class unbalance and a large variety of feature sets have been proposed for this task. In practice, over 200 features are often considered, and the features retained in the final model are either chosen using domain knowledge or an exhaustive search in the feature sets without evaluating the relevance of each individual feature included in the classifier. As a consequence, the results obtained by these models can be suboptimal and difficult to interpret. In this work, feature selection techniques are considered to extract optimal feature subsets for state-of-the-art ECG classification models. The performances are evaluated on real ambulatory recordings and compared to previously reported feature choices using the same models. Results indicate that a small number of individual features actually serve the classification and that better performances can be achieved by removing useless features.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {1},
numpages = {9}
}

@inproceedings{10.1145/1835804.1835849,
author = {Yang, Jian-Bo and Ong, Chong-Jin},
title = {Feature selection for support vector regression using probabilistic prediction},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835849},
doi = {10.1145/1835804.1835849},
abstract = {This paper presents a novel wrapper-based feature selection method for Support Vector Regression (SVR) using its probabilistic predictions. The method computes the importance of a feature by aggregating the difference, over the feature space, of the conditional density functions of the SVR prediction with and without the feature. As the exact computation of this importance measure is expensive, two approximations are proposed. The effectiveness of the measure using these approximations, in comparison to several other existing feature selection methods for SVR, is evaluated on both artificial and real-world problems. The result of the experiment shows that the proposed method generally performs better, and at least as well as the existing methods, with notable advantage when the data set is sparse.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {343–352},
numpages = {10},
keywords = {feature ranking, feature selection, probabilistic predictions, random permutation, support vector regression},
location = {Washington, DC, USA},
series = {KDD '10}
}

@inproceedings{10.5555/1770904.1771031,
author = {K\v{r}\'{\i}\v{z}ek, Pavel and Kittler, Josef and Hlav\'{a}\v{c}, V\'{a}clav},
title = {Improving stability of feature selection methods},
year = {2007},
isbn = {9783540742715},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An improper design of feature selection methods can often lead to incorrect conclusions. Moreover, it is not generally realised that functional values of the criterion guiding the search for the best feature set are random variables with some probability distribution. This contribution examines the influence of several estimation techniques on the consistency of the final result. We propose an entropy based measure which can assess the stability of feature selection methods with respect to perturbations in the data. Results show that filters achieve a better stability and performance if more samples are employed for the estimation, i.e., using leave-one-out cross-validation, for instance. However, the best results for wrappers are acquired with the 50/50 holdout validation.},
booktitle = {Proceedings of the 12th International Conference on Computer Analysis of Images and Patterns},
pages = {929–936},
numpages = {8},
keywords = {entropy, feature selection, stability},
location = {Vienna, Austria},
series = {CAIP'07}
}

@article{10.1016/j.asoc.2018.02.051,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio},
title = {Dealing with high-dimensional class-imbalanced datasets: Embedded feature selection for SVM classification},
year = {2018},
issue_date = {Jun 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {67},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.02.051},
doi = {10.1016/j.asoc.2018.02.051},
journal = {Appl. Soft Comput.},
month = jun,
pages = {94–105},
numpages = {12},
keywords = {Feature selection, Support Vector Data Description, Cost-sensitive learning, Embedded approaches, Imbalanced data classification}
}

@article{10.1016/j.neucom.2015.05.089,
author = {Liu, Zhen and Wang, Ruoyu and Tao, Ming and Cai, Xianfa},
title = {A class-oriented feature selection approach for multi-class imbalanced network traffic datasets based on local and global metrics fusion},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {168},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.05.089},
doi = {10.1016/j.neucom.2015.05.089},
abstract = {Feature selection is often used as a pre-processing step for machine learning based network traffic classification. Many feature selection techniques have been developed to find an optimal subset of relevant features and to improve overall classification accuracy. But such techniques ignore the class imbalance problem encountered in network traffic classification. The selected feature subset may bias towards the traffic class that occupies the majority of traffic flows on the Internet. To address this issue, this paper proposes a new approach, called class-oriented feature selection (COFS), to identify a relevant feature subset for every class. It combines the proposed local metric and the existing global metric to yield a potentially optimal feature subset for each class, and then removes the redundant features in each feature subset based on the weighted symmetric uncertainty. Additionally, to enhance the generalization on network traffic data, an ensemble learning based scheme is presented with COFS to overcome the negative impacts of the data drift on a traffic classifier. Experiments on real-world network traffic data show that COFS outperforms existing feature selection techniques in most cases. Moreover, our approach achieves &gt;96% flow accuracy and &gt;93% byte accuracy on average.},
journal = {Neurocomput.},
month = nov,
pages = {365–381},
numpages = {17},
keywords = {Data drift, Feature selection, Local metrics, Multi-class imbalance, Network traffic}
}

@article{10.1016/j.eswa.2012.04.050,
author = {Bozkurt G\"{o}Nen, G\"{u}Lef\c{s}An and G\"{o}Nen, Mehmet and G\"{u}Rgen, Fikret},
title = {Probabilistic and discriminative group-wise feature selection methods for credit risk analysis},
year = {2012},
issue_date = {October, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {14},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.04.050},
doi = {10.1016/j.eswa.2012.04.050},
abstract = {Many financial organizations such as banks and retailers use computational credit risk analysis (CRA) tools heavily due to recent financial crises and more strict regulations. This strategy enables them to manage their financial and operational risks within the pool of financial institutes. Machine learning algorithms especially binary classifiers are very popular for that purpose. In real-life applications such as CRA, feature selection algorithms are used to decrease data acquisition cost and to increase interpretability of the decision process. Using feature selection methods directly on CRA data sets may not help due to categorical variables such as marital status. Such features are usually are converted into binary features using 1-of-k encoding and eliminating a subset of features from a group does not help in terms of data collection cost or interpretability. In this study, we propose to use the probit classifier with a proper prior structure and multiple kernel learning with a proper kernel construction procedure to perform group-wise feature selection (i.e., eliminating a group of features together if they are not helpful). Experiments on two standard CRA data sets show the validity and effectiveness of the proposed binary classification algorithm variants.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {11709–11717},
numpages = {9},
keywords = {Credit risk analysis, Feature selection, Multiple kernel learning, Probit classifier, Sparsity}
}

@inproceedings{10.1007/978-3-642-23808-6_28,
author = {Uguroglu, Selen and Carbonell, Jaime},
title = {Feature selection for transfer learning},
year = {2011},
isbn = {9783642238079},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-23808-6_28},
doi = {10.1007/978-3-642-23808-6_28},
abstract = {Common assumption in most machine learning algorithms is that, labeled (source) data and unlabeled (target) data are sampled from the same distribution. However, many real world tasks violate this assumption: in temporal domains, feature distributions may vary over time, clinical studies may have sampling bias, or sometimes sufficient labeled data for the domain of interest does not exist, and labeled data from a related domain must be utilized. In such settings, knowing in which dimensions source and target data vary is extremely important to reduce the distance between domains and accurately transfer knowledge. In this paper, we present a novel method to identify variant and invariant features between two datasets. Our contribution is two fold: First, we present a novel transfer learning approach for domain adaptation, and second, we formalize the problem of finding differently distributed features as a convex optimization problem. Experimental studies on synthetic and benchmark real world datasets show that our approach outperform other transfer learning approaches, and it aids the prediction accuracy significantly.},
booktitle = {Proceedings of the 2011th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part III},
pages = {430–442},
numpages = {13},
location = {Athens, Greece},
series = {ECMLPKDD'11}
}

@article{10.1016/j.neucom.2012.08.023,
author = {Yao, Zhijun and Liu, Wenyu},
title = {Extracting robust distribution using adaptive Gaussian Mixture Model and online feature selection},
year = {2013},
issue_date = {February, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.08.023},
doi = {10.1016/j.neucom.2012.08.023},
abstract = {This paper presents a novel method to extract robust distribution using adaptive Gaussian Mixture Model (GMM) and online feature selection to improve the tracking performance. Traditional histogram-based tracking algorithms use all histogram bins to track the object, and some recent work attempt selecting important bins by directly comparing the corresponding probability value between the foreground and background histogram distribution and give higher weight to those important bins. However, the selected bins may not be discriminative enough. In this paper, we use an adaptive GMM to model histogram and select good Gaussian components corresponding to the discriminative bins and the stable part of the foreground, which form the robust distribution. Given a set of seed, for each feature, the foreground and surrounding background are modeled by an adaptive GMM, respectively. Then, we use the Gaussian Component Separability measure to find good components, to extract the robust distribution of the foreground and to evaluate the separability of each feature. Finally, after ranking features based on their separability, the K most discriminative features are used to generate a weight image and the CAMSHIFT algorithm is employed to locate the object. Experiments show that the proposed method can extract the robust distribution and improve the tracking performance.},
journal = {Neurocomput.},
month = feb,
pages = {258–274},
numpages = {17},
keywords = {Adaptive, Feature selection, Gaussian Mixture Model, Object tracking}
}

@article{10.3233/HIS-130182,
author = {Dutta, Dipankar and Dutta, Paramartha and Sil, Jaya},
title = {Simultaneous feature selection and clustering with mixed features by multi objective genetic algorithm},
year = {2014},
issue_date = {January 2014},
publisher = {IOS Press},
address = {NLD},
volume = {11},
number = {1},
issn = {1448-5869},
url = {https://doi.org/10.3233/HIS-130182},
doi = {10.3233/HIS-130182},
abstract = {In this paper, we propose a novel evolutionary clustering algorithm for mixed type data numerical and categorical. It is doing clustering and feature selection simultaneously. Feature subset selection improves quality of clustering. It also improves understandability and scalability. It unfastens attraction on numerical or categorical dataset only. K-prototype KP is a well-known partitional clustering algorithm for mixed type data. However, this type of algorithm is sensitive to initialization and may converge to local optima. It is optimizing a single measure only i.e. minimizations of intra cluster distance. We have considered clustering as a multi objective optimization problem MOOP. Minimization of intra cluster distance and maximization of inter cluster distance are two objectives of optimization. Multi objective genetic algorithm MOGA is a well-known algorithm which can be applicable for MOOP to find out near global optimal solution. So in this paper we have developed a hybridized genetic clustering algorithm by combining the global search ability of MOGA and local search ability of KP. Experiments on real-life benchmark datasets from UCI machine learning repository prove the superiority of the proposed algorithm.},
journal = {Int. J. Hybrid Intell. Syst.},
month = jan,
pages = {41–54},
numpages = {14},
keywords = {Clustering, Feature Selection, Hybridization, K-Prototype Kp, Mixed Type Data, Multi Objective Genetic Algorithm Moga}
}

@article{10.1016/j.fss.2008.03.015,
author = {Pizzi, Nick J. and Pedrycz, Witold},
title = {Effective classification using feature selection and fuzzy integration},
year = {2008},
issue_date = {November, 2008},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {159},
number = {21},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2008.03.015},
doi = {10.1016/j.fss.2008.03.015},
abstract = {Many classification problems involve features whose specificity demand some form of feature space transformation (preprocessing) coupled with post-processing consensus analysis in order to accomplish a successful discrimination between different classes. In this study, we present a new methodology, which systematically addresses these design classification issues. At the preprocessing phase we offer a new approach of stochastic feature selection. This type of feature selection, collates quadratically transformed feature subsets for presentation to a collection of respective classifiers. In the sequel, independent classification outcomes are aggregated through fuzzy integration. The motivation behind the proposed methodology is twofold. Often, only a subset of features possesses discriminatory power while the remainder has a tendency to confound the effectiveness of the underlying classifier. Quite commonly, classification based on some consensus of classification outcomes coming from a set of classifiers operating upon different feature subsets becomes more accurate than the classification results produced by any individual classifier. To illustrate this design methodology, we discuss a classification problem coming from software engineering. Here we are concerned with a dataset comprosed of features describing a collection of qualitative attributes of a software system. The experiments demonstrate that the aggregated classification results using fuzzy integration are superior to the predictions from the respective best single classifiers.},
journal = {Fuzzy Sets Syst.},
month = nov,
pages = {2859–2872},
numpages = {14},
keywords = {Feature subsets, Fuzzy integration, Fuzzy measure, Quantitative software engineering, Stochastic feature selection}
}

@article{10.1016/j.neucom.2009.08.012,
author = {Liu, Huawen and Liu, Lei and Zhang, Huijie},
title = {Boosting feature selection using information metric for classification},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {1–3},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2009.08.012},
doi = {10.1016/j.neucom.2009.08.012},
abstract = {Feature selection plays an important role in pattern classification. Its purpose is to remove redundant features from data set as many as possible. The presence of useless features may not only deteriorate the performance of learning algorithms, but also obscure important information (e.g., intrinsic structure) behind data. Along with new and emerging techniques, data sets in many domains are becoming larger and larger and many irrelevant features are often prevailing in these data sets. This, however, poses great challenges to traditional learning algorithms, such as low efficiency and over-fitting. Thus, it becomes apparent that an efficient technique is needed to eliminate redundant or irrelevant features from the data sets. Currently, many endeavors to cope with this problem have been attempted and various outstanding feature selection methods have been proposed. Unlike other selection methods, in this paper we propose a general scheme of boosting feature selection method using information metric. The primary characteristic of our method is that it exploits weight of data to select salient features. Furthermore, the weight of data will be dynamically changed after each candidate feature has been selected. Thus, the information criteria used in feature selector can exactly represent the relevant degree between features and the class labels. As a result, the selected feature subset has maximal relevance to the class labels. Simulation studies carried out on UCI data sets show that the classification performance achieved by our proposed method is better than those of other selection methods in most cases.},
journal = {Neurocomput.},
month = dec,
pages = {295–303},
numpages = {9},
keywords = {Boosting, Classification, Feature selection, Filter model, Information metric}
}

@article{10.1007/s00521-016-2236-5,
author = {Udhaya Kumar, S. and Hannah Inbarani, H.},
title = {PSO-based feature selection and neighborhood rough set-based classification for BCI multiclass motor imagery task},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-016-2236-5},
doi = {10.1007/s00521-016-2236-5},
abstract = {In recent years, most of the researchers are developing brain---computer interface (BCI) applications for the physically disabled to be able to interconnect with peripheral devices based on brain activities. Electroencephalogram (EEG) is a very powerful tool for investigating patient's health and different physiological activities of the brain. A significant challenge in this BCI application is the accurate and reliable recognition of motor imagery (MI) task. A brain---computer interface based on MI interprets the patient's brain activities into a control signal through classifying EEG patterns of various motor imagination tasks. The appropriate features are essential to achieving higher classification accuracy of EEG motor imagery task. For EEG signal feature extraction, wavelet transform is suitable for analysis of nonlinear time series signals. Nevertheless, the dimension of the extracted feature is huge and it may reduce the performance of classification method. Dimensionality reduction and classification play an important role in BCI motor imagery research. In this study, hybridization of particle swarm optimization (PSO)-based rough set feature selection technique is proposed for achieving a minimal set of relevant features from extracted features. The selected features are applied to the proposed novel neighborhood rough set classifier (NRSC) method for classification of multiclass motor imagery. The experimental results are delivered for nine subjects of the BCI Competition 2008 Dataset IIa to show the greater performance of the proposed algorithm. The outcome of proposed algorithms produces a higher mean kappa of 0.743 compared to 0.70 from sequential updating semi-supervised spectral regression kernel discriminant analysis. Experimental results show that the strength of the proposed PSO-rough set and NRSC algorithms outperforms the champion of the BCI Competition IV Dataset IIa and other existing research using this dataset.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {3239–3258},
numpages = {20},
keywords = {Brain---computer interface, Electroencephalogram, Motor imagery, Neighborhood rough set, Rough set}
}

@inproceedings{10.1145/1401890.1401986,
author = {Yu, Lei and Ding, Chris and Loscalzo, Steven},
title = {Stable feature selection via dense feature groups},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401986},
doi = {10.1145/1401890.1401986},
abstract = {Many feature selection algorithms have been proposed in the past focusing on improving classification accuracy. In this work, we point out the importance of stable feature selection for knowledge discovery from high-dimensional data, and identify two causes of instability of feature selection algorithms: selection of a minimum subset without redundant features and small sample size. We propose a general framework for stable feature selection which emphasizes both good generalization and stability of feature selection results. The framework identifies dense feature groups based on kernel density estimation and treats features in each dense group as a coherent entity for feature selection. An efficient algorithm DRAGS (Dense Relevant Attribute Group Selector) is developed under this framework. We also introduce a general measure for assessing the stability of feature selection algorithms. Our empirical study based on microarray data verifies that dense feature groups remain stable under random sample hold out, and the DRAGS algorithm is effective in identifying a set of feature groups which exhibit both high classification accuracy and stability.},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {803–811},
numpages = {9},
keywords = {classification, feature selection, high-dimensional data, kernel density estimation, stability},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}

@inproceedings{10.1109/ICTAI.2011.172,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Impact of Data Sampling on Stability of Feature Selection for Software Measurement Data},
year = {2011},
isbn = {9780769545967},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2011.172},
doi = {10.1109/ICTAI.2011.172},
abstract = {Software defect prediction can be considered a binary classification problem. Generally, practitioners utilize historical software data, including metric and fault data collected during the software development process, to build a classification model and then employ this model to predict new program modules as either fault-prone (fp) or not-fault-prone (nfp). Limited project resources can then be allocated according to the prediction results by (for example) assigning more reviews and testing to the modules predicted to be potentially defective. Two challenges often come with the modeling process: (1) high-dimensionality of software measurement data and (2) skewed or imbalanced distributions between the two types of modules (fp and nfp) in those datasets. To overcome these problems, extensive studies have been dedicated towards improving the quality of training data. The commonly used techniques are feature selection and data sampling. Usually, researchers focus on evaluating classification performance after the training data is modified. The present study assesses a feature selection technique from a different perspective. We are more interested in studying the stability of a feature selection method, especially in understanding the impact of data sampling techniques on the stability of feature selection when using the sampled data. Some interesting findings are found based on two case studies performed on datasets from two real-world software projects.},
booktitle = {Proceedings of the 2011  IEEE 23rd International Conference on Tools with Artificial Intelligence},
pages = {1004–1011},
numpages = {8},
keywords = {data sampling, defect prediction, feature selection, software metrics, stability},
series = {ICTAI '11}
}

@article{10.1016/j.patcog.2009.12.011,
author = {Sun, Dan and Zhang, Daoqiang},
title = {Bagging Constraint Score for feature selection with pairwise constraints},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.011},
doi = {10.1016/j.patcog.2009.12.011},
abstract = {Constraint Score is a recently proposed method for feature selection by using pairwise constraints which specify whether a pair of instances belongs to the same class or not. It has been shown that the Constraint Score, with only a small amount of pairwise constraints, achieves comparable performance to those fully supervised feature selection methods such as Fisher Score. However, one major disadvantage of the Constraint Score is that its performance is dependent on a good selection on the composition and cardinality of constraint set, which is very challenging in practice. In this work, we address the problem by importing Bagging into Constraint Score and a new method called Bagging Constraint Score (BCS) is proposed. Instead of seeking one appropriate constraint set for single Constraint Score, in BCS we perform multiple Constraint Score, each of which uses a bootstrapped subset of original given constraint set. Diversity analysis on individuals of ensemble shows that resampling pairwise constraints is helpful for simultaneously improving accuracy and diversity of individuals. We conduct extensive experiments on a series of high-dimensional datasets from UCI repository and gene databases, and the experimental results validate the effectiveness of the proposed method.},
journal = {Pattern Recogn.},
month = jun,
pages = {2106–2118},
numpages = {13},
keywords = {Bagging, Constraint Score, Ensemble learning, Feature selection, Pairwise constraints}
}

@article{10.1016/j.jbi.2009.07.008,
author = {Peng, Yonghong and Wu, Zhiqing and Jiang, Jianmin},
title = {A novel feature selection approach for biomedical data classification},
year = {2010},
issue_date = {February, 2010},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {43},
number = {1},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2009.07.008},
doi = {10.1016/j.jbi.2009.07.008},
abstract = {This paper presents a novel feature selection approach to deal with issues of high dimensionality in biomedical data classification. Extensive research has been performed in the field of pattern recognition and machine learning. Dozens of feature selection methods have been developed in the literature, which can be classified into three main categories: filter, wrapper and hybrid approaches. Filter methods apply an independent test without involving any learning algorithm, while wrapper methods require a predetermined learning algorithm for feature subset evaluation. Filter and wrapper methods have their, respectively, drawbacks and are complementary to each other in that filter approaches have low computational cost with insufficient reliability in classification while wrapper methods tend to have superior classification accuracy but require great computational power. The approach proposed in this paper integrates filter and wrapper methods into a sequential search procedure with the aim to improve the classification performance of the features selected. The proposed approach is featured by (1) adding a pre-selection step to improve the effectiveness in searching the feature subsets with improved classification performances and (2) using Receiver Operating Characteristics (ROC) curves to characterize the performance of individual features and feature subsets in the classification. Compared with the conventional Sequential Forward Floating Search (SFFS), which has been considered as one of the best feature selection methods in the literature, experimental results demonstrate that (i) the proposed approach is able to select feature subsets with better classification performance than the SFFS method and (ii) the integrated feature pre-selection mechanism, by means of a new selection criterion and filter method, helps to solve the over-fitting problems and reduces the chances of getting a local optimal solution.},
journal = {J. of Biomedical Informatics},
month = feb,
pages = {15–23},
numpages = {9},
keywords = {Biomedical data classification, Feature selection, ROC, SFFS}
}

@article{10.1109/TCBB.2011.30,
author = {Peters, Tim and Bulger, David W. and Loi, To-ha and Yang, Jean Yee Hwa and Ma, David},
title = {Two-Step Cross-Entropy Feature Selection for Microarrays—Power Through Complementarity},
year = {2011},
issue_date = {July 2011},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {8},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2011.30},
doi = {10.1109/TCBB.2011.30},
abstract = {Current feature selection methods for supervised classification of tissue samples from microarray data generally fail to exploit complementary discriminatory power that can be found in sets of features [CHECK END OF SENTENCE]. Using a feature selection method with the computational architecture of the cross-entropy method [CHECK END OF SENTENCE], including an additional preliminary step ensuring a lower bound on the number of times any feature is considered, we show when testing on a human lymph node data set that there are a significant number of genes that perform well when their complementary power is assessed, but "pass under the radar” of popular feature selection methods that only assess genes individually on a given classification tool. We also show that this phenomenon becomes more apparent as diagnostic specificity of the tissue samples analysed increases.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1148–1151},
numpages = {4},
keywords = {Feature selection, data mining, genetic interdependence, lymphoma., microarray}
}

@article{10.1155/2015/781207,
author = {Martinez-Leon, Juan-Antonio and Cano-Izquierdo, Jose-Manuel and Ibarrola, Julio},
title = {Feature selection applying statistical and neurofuzzy methods to EEG-Based BCI},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-5265},
url = {https://doi.org/10.1155/2015/781207},
doi = {10.1155/2015/781207},
abstract = {This paper presents an investigation aimed at drastically reducing the processing burden required by motor imagery brain-computer interface (BCI) systems based on electroencephalography (EEG). In this research, the focus has moved from the channel to the feature paradigm, and a 96% reduction of the number of features required in the process has been achieved maintaining and even improving the classification success rate. This way, it is possible to build cheaper, quicker, and more portable BCI systems. The data set used was provided within the framework of BCI Competition III, which allows it to compare the presented results with the classification accuracy achieved in the contest. Furthermore, a new three-step methodology has been developed which includes a feature discriminant character calculation stage; a score, order, and selection phase; and a final feature selection step. For the first stage, both statistics method and fuzzy criteria are used. The fuzzy criteria are based on the S-dFasArt classification algorithm which has shown excellent performance in previous papers undertaking the BCI multiclass motor imagery problem. The score, order, and selection stage is used to sort the features according to their discriminant nature. Finally, both order selection and Group Method Data Handling (GMDH) approaches are used to choose the most discriminant ones.},
journal = {Intell. Neuroscience},
month = jan,
articleno = {54},
numpages = {1}
}

@inproceedings{10.1145/3093293.3093299,
author = {Bai, J. and Fan, Z. C. and Zhang, L. P. and Xu, X. Y. and Zhang, Z. L.},
title = {Classification of Methicillin-Resistant and Methicillin-Susceptible Staphylococcus Aureus Using an Improved Genetic Algorithm for Feature Selection Based on Mass Spectra},
year = {2017},
isbn = {9781450348799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3093293.3093299},
doi = {10.1145/3093293.3093299},
abstract = {Methicillin-resistant staphylococcus aureus (MRSA) cause nosocomial and communal infections seriously. Rapid and accurate identification of MRSA is vital for prevention of human morbidity. Matrix-assisted laser desorption ionization time-of-flight mass spectrometry (MALDI-TOF-MS) has been widely used for identification and typing of micro-organisms. To identify MRSA based on the mass spectra of clinical S.aureus, we propose a genetic algorithm with a t-test based population seeding for wrapper feature selection, in which the t-test statistics are used as the prior information for initial population. The results of some compared experiments show that the proposed method improves the average sensitivity from 0.55 to 0.71, and the balanced accuracy is a larger value on contrast group, whose average value is 0.72. As the result, the proposed GA with prior information can identify MRSA effectively.},
booktitle = {Proceedings of the 9th International Conference on Bioinformatics and Biomedical Technology},
pages = {57–63},
numpages = {7},
keywords = {Genetic Algorithm, MALDI-TOF-MS, Methicillin-resistant, Staphylococcus aureus, Support Vector Machine, Wrapper Feature Selection},
location = {Lisbon, Portugal},
series = {ICBBT '17}
}

@article{10.1016/j.neucom.2008.12.035,
author = {G\'{o}mez-Verdejo, Vanessa and Verleysen, Michel and Fleury, J\'{e}r\^{o}me},
title = {Information-theoretic feature selection for functional data classification},
year = {2009},
issue_date = {October, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {72},
number = {16–18},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2008.12.035},
doi = {10.1016/j.neucom.2008.12.035},
abstract = {The classification of functional or high-dimensional data requires to select a reduced subset of features among the initial set, both to help fighting the curse of dimensionality and to help interpreting the problem and the model. The mutual information criterion may be used in that context, but it suffers from the difficulty of its estimation through a finite set of samples. Efficient estimators are not designed specifically to be applied in a classification context, and thus suffer from further drawbacks and difficulties. This paper presents an estimator of mutual information that is specifically designed for classification tasks, including multi-class ones. It is combined to a recently published stopping criterion in a traditional forward feature selection procedure. Experiments on both traditional benchmarks and on an industrial functional classification problem show the added value of this estimator.},
journal = {Neurocomput.},
month = oct,
pages = {3580–3589},
numpages = {10},
keywords = {Classification, Feature selection, Functional data, Mutual information}
}

@inproceedings{10.1007/978-3-642-03348-3_56,
author = {Liu, Yuhai and Ma, Lintao and Yang, Ning and He, Ying},
title = {Asymmetric Feature Selection for BGP Abnormal Events Detection},
year = {2009},
isbn = {9783642033476},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-03348-3_56},
doi = {10.1007/978-3-642-03348-3_56},
abstract = {Border Gateway Protocol (BGP), which is the defacto standard inter-domain routing protocol in the Internet today, has severe problems, such as worm viruses, denial of service (DoS) attacks, etc. To ensure the stability and security of the inter-domain routing system in the autonomy system, it is critical to accurately and quickly detect abnormal BGP events. In this paper, a novel feature selection algorithm based on the asymmetric entropy named FSAMI is proposed to evaluate the characteristics of describing the BGP abnormal events, which is independent on the machine learning methods. Meanwhile the under-sampling, neural network (NN) and feature selection are introduced to predict BGP abnormal activities to treat the imbalance problem. Numerical experimental results on RIPE archive data set show that the FSAMI method improves the g_means values of abnormal events detection and helps to improve the prediction ability.},
booktitle = {Proceedings of the 5th International Conference on Advanced Data Mining and Applications},
pages = {553–560},
numpages = {8},
keywords = {Asymmetric Mutual Information, BGP, Feature Selection, Neural Networks},
location = {Beijing, China},
series = {ADMA '09}
}

@article{10.1016/j.patcog.2008.08.029,
author = {Mac Parthal\'{a}in, Neil and Shen, Qiang},
title = {Exploring the boundary region of tolerance rough sets for feature selection},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.08.029},
doi = {10.1016/j.patcog.2008.08.029},
abstract = {Of all of the challenges which face the effective application of computational intelligence technologies for pattern recognition, dataset dimensionality is undoubtedly one of the primary impediments. In order for pattern classifiers to be efficient, a dimensionality reduction stage is usually performed prior to classification. Much use has been made of rough set theory for this purpose as it is completely data-driven and no other information is required; most other methods require some additional knowledge. However, traditional rough set-based methods in the literature are restricted to the requirement that all data must be discrete. It is therefore not possible to consider real-valued or noisy data. This is usually addressed by employing a discretisation method, which can result in information loss. This paper proposes a new approach based on the tolerance rough set model, which has the ability to deal with real-valued data whilst simultaneously retaining dataset semantics. More significantly, this paper describes the underlying mechanism for this new approach to utilise the information contained within the boundary region or region of uncertainty. The use of this information can result in the discovery of more compact feature subsets and improved classification accuracy. These results are supported by an experimental evaluation which compares the proposed approach with a number of existing feature selection techniques.},
journal = {Pattern Recogn.},
month = may,
pages = {655–667},
numpages = {13},
keywords = {Attribute reduction, Classification, Feature selection, Rough sets}
}

@inproceedings{10.5555/2009324.2009331,
author = {Wang, Hongyan and Ma, Jinwen},
title = {Simultaneous model selection and feature selection via BYY harmony learning},
year = {2011},
isbn = {9783642210891},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Model selection for Gaussian mixture learning on a given dataset is an important but difficulty task and also depends on the feature or variable selection in practical applications. In this paper, we propose a new kind of learning algorithm for Gaussian mixtures with simultaneous model selection and variable selection (MSFS) based on the BYY harmony learning framework. It is demonstrated by simulation experiments that the proposed MSFS algorithm is able to solve the model selection and feature selection problems of Gaussian mixture learning on a given dataset simultaneously.},
booktitle = {Proceedings of the 8th International Conference on Advances in Neural Networks - Volume Part II},
pages = {47–56},
numpages = {10},
keywords = {Baysian Ying-Yang (BYY) Harmony learning, Gaussian mixtures, clustering analysis, feature selection, model selection},
location = {Guilin, China},
series = {ISNN'11}
}

@inproceedings{10.1145/2108616.2108704,
author = {Zhang, Zhen-Xing and Lee, Sang-Hong and Lim, Joon S.},
title = {Comparison of feature selection methods in ECG signal classification},
year = {2010},
isbn = {9781605588933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2108616.2108704},
doi = {10.1145/2108616.2108704},
abstract = {In past years, the Principal Component Analysis (PCA) has been applied to select features for classification applications. This paper presents a performance comparison between PCA and Non-overlap Area Distribution Measurement (NADM), which is based on a neural fuzzy network. This paper performs an experiment on Normal Sinus Rhythm (NSR) and Ventricular Tachycardia/Fibrillation (VT/VF) classification with the two feature selection methods. The performance result is 89.34% while the number of initial features is projected from six to four by the PCA method. The performance result is 91.02% while the number of initial features is decreased from six to two by NADM. The results clearly show that NADM outperforms PCA by 1.68% with fewer features.},
booktitle = {Proceedings of the 4th International Conference on Uniquitous Information Management and Communication},
articleno = {73},
numpages = {5},
keywords = {electrocardiogram, feature selection, principal component analysis},
location = {Suwon, Republic of Korea},
series = {ICUIMC '10}
}

@inproceedings{10.1109/ICMLA.2013.85,
author = {Wald, Randall and Khoshgoftaar, Taghi and Shanab, Ahmad Abu and Napolitano, Amri},
title = {Comparative Analysis on the Stability of Feature Selection Techniques Using Three Frameworks on Biological Datasets},
year = {2013},
isbn = {9780769551449},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2013.85},
doi = {10.1109/ICMLA.2013.85},
abstract = {Feature (gene) selection is a common preprocessing technique used to counter the problem of high dimensionality(too many independent features) found in many bioinformaticsdatasets, addressing this problem by creating a smaller feature subset including only the most important features. Although feature selection techniques are often evaluated based on how they can help improve classification performance, it is also important to find stable feature selection techniques which will give consistent results even in the face of dataset perturbations(such as class noise or sampling used to alleviate the problem of imbalanced data). This is especially important in bioinformatics, where the prime concern may be gene discovery rather than classification. In this study we use three frameworks to evaluate the stability of gene selection techniques: "sampledcleanvs. sampled-clean, " "sampled-noisy vs. sampled-noisy, " and" sampled-clean vs. sampled-noisy." All frameworks involve pairwisecomparisons among the results from the perturbed datasets(due to sampling or class noise injection followed by sampling). They differ in terms of whether they observe how sampling can create variation within the feature subsets (sampled-clean vs. sampled-clean), how noisy datasets (which were then sampled)can create a wide spread of selected features (sampled-noisyvs. sampled-noisy), or how features selected on clean and noisy datasets differ, after both datasets have been sampled (sampledcleanvs. sampled-noisy). Along with these three frameworks, our comparison of seven feature ranking techniques uses four cancer gene datasets, applies three sampling techniques, and generates artificial class noise to better simulate real-world datasets. The results from the frameworks are generally similar, with Signal-To-Noise and ReliefF showing the best stability and Gain Ratio showing the worst across all three frameworks, although Relief-W is notable for showing moderate to above-average stability when the clean datasets are used, but giving the second worst performance when noise was present.},
booktitle = {Proceedings of the 2013 12th International Conference on Machine Learning and Applications - Volume 01},
pages = {418–423},
numpages = {6},
keywords = {Feature Selection, Imbalanced Data, Noise Injection, Stability},
series = {ICMLA '13}
}

@article{10.1016/j.knosys.2012.07.018,
author = {Lin, Hung-Yi},
title = {Feature selection based on cluster and variability analyses for ordinal multi-class classification problems},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {37},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2012.07.018},
doi = {10.1016/j.knosys.2012.07.018},
abstract = {Feature selection is an essential problem for pattern classification systems. This paper studies how to provide systems with the most characterizing features for ordinal multi-class classification task. The integration of cluster analyses and variability analyses advances a novel feature selection scheme with efficiency. The Huang-index method using fuzzy c-means is employed to enhance cluster validity and optimizes a consistent number of clusters among the features. A new entropy-based feature evaluation method is formulated for the authentication of relevant features. Then, multivariate statistical analyses are utilized to solve the redundancy between relevant features. Experimental results show that our new feature selection scheme sifts successfully a compact subset of characterizing features for classification problems with multiple classes.},
journal = {Know.-Based Syst.},
month = jan,
pages = {94–104},
numpages = {11},
keywords = {Cluster validity, Feature selection, Huang-index, Multi-class classification, Multivariate analysis}
}

@article{10.1162/neco.2007.19.7.1939,
author = {Cohen, Shay and Dror, Gideon and Ruppin, Eytan},
title = {Feature Selection via Coalitional Game Theory},
year = {2007},
issue_date = {July 2007},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {19},
number = {7},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2007.19.7.1939},
doi = {10.1162/neco.2007.19.7.1939},
abstract = {We present and study the contribution-selection algorithm (CSA), a novel algorithm for feature selection. The algorithm is based on the multiperturbation shapley analysis (MSA), a framework that relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. It can optimize various performance measures over unseen data such as accuracy, balanced error rate, and area under receiver-operator-characteristic curve. Empirical comparison with several other existing feature selection methods shows that the backward elimination variant of CSA leads to the most accurate classification results on an array of data sets.},
journal = {Neural Comput.},
month = jul,
pages = {1939–1961},
numpages = {23}
}

@inproceedings{10.1145/2350716.2350726,
author = {Hung, Le Viet and Anh, Nguyen Thi Kim and Dang, Nguyen Hai},
title = {Improving Vietnamese web page clustering by combining neighbors' content and using iterative feature selection},
year = {2012},
isbn = {9781450312325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2350716.2350726},
doi = {10.1145/2350716.2350726},
abstract = {Web page clustering is a fundamental technique to offer a solution for data management, information locating and its interpretation of Web data and to facilitate users for navigation, discrimination and understanding. Most existing clustering algorithms can't adapt well to Web page clustering directly in terms of efficiency and effectiveness due to the problems of high dimensionality and data sparseness. Furthermore, the uncontrolled nature of web content presents additional challenges to web page clustering, whereas the interconnected characteristic of hypertext can provide useful information for the process. To address this problem, we propose a new Web page clustering method with combining neighbors' content to overcome data sparseness and using Iterative Feature Selection to remove noisy and redundant features and to improve the performance of clustering algorithm. Experimental results show that the proposed method significantly improves the performance of the Vietnamese web page clustering with a relatively small number of good descriptive features for web pages.},
booktitle = {Proceedings of the 3rd Symposium on Information and Communication Technology},
pages = {47–54},
numpages = {8},
keywords = {Iterative Feature Selection, feature selection, web mining, web page clustering},
location = {Ha Long, Vietnam},
series = {SoICT '12}
}

@article{10.1016/j.eswa.2010.09.153,
author = {Ogura, Hiroshi and Amano, Hiromi and Kondo, Masato},
title = {Comparison of metrics for feature selection in imbalanced text classification},
year = {2011},
issue_date = {May, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.153},
doi = {10.1016/j.eswa.2010.09.153},
abstract = {Research highlights Feature selections using Type-I metrics ( P 2 and Gini index) achieve the comparable classification performances with those of the combination framework using Type-III metrics (signed 2 and signed information gain). The performances with Type-II metrics ( 2 and information gain) are significantly degraded with increasing the degree of class imbalance. Type-III metrics produced the best performance; however, the optimization with these metrics is not easy in real applications. Type-I metrics serve as more simplified alternative methods for the combination framework. The classification performances using Type-I and Type-II metrics have positive correlations with the number of negative features. AbstractClass imbalance problems are often encountered in real applications of automatic text classifications especially at the so-called "one-against-all" settings and thus handling the problem with satisfactory performance is substantially important. In this paper, we focus our attention on a feature selection scheme for solving this problem and explore the abilities and characteristics of various metrics for feature selection. We examine three different types of metrics; Type-I: P 2 and Gini index, Type-II: 2 and information gain and Type-III: signed 2 and signed information gain. Type-I and Type-II metrics implicitly combine positive and negative features which indicate the membership and nonmembership of positive class, respectively. Type-III metrics were utilized in the combination framework in which the positive and negative features are explicitly combined and the degree of combination is optimized to improve the performance at imbalanced situations. Our experimental results show that feature selections using Type-I metrics on imbalanced data set achieve the comparable classification performances with those of the combination framework using Type-III metrics and proved to be much more superior to those of Type-II metrics. This result indicates that Type-I metrics serve as more simplified alternative methods for the combination framework. The characteristic behaviors and the performance of each of the used metrics are also investigated closely in terms of the distribution and quality of selected features.},
journal = {Expert Syst. Appl.},
month = may,
pages = {4978–4989},
numpages = {12},
keywords = {Combination framework, Feature selection, Imbalanced data, Poisson distribution, Text classification, k-NN classifier}
}

@article{10.1016/j.eswa.2008.07.026,
author = {Wang, Chia-Ming and Huang, Yin-Fu},
title = {Evolutionary-based feature selection approaches with new criteria for data mining: A case study of credit approval data},
year = {2009},
issue_date = {April, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.07.026},
doi = {10.1016/j.eswa.2008.07.026},
abstract = {In this paper, the feature selection problem was formulated as a multi-objective optimization problem, and new criteria were proposed to fulfill the goal. Foremost, data were pre-processed with missing value replacement scheme, re-sampling procedure, data type transformation procedure, and min-max normalization procedure. After that a wide variety of classifiers and feature selection methods were conducted and evaluated. Finally, the paper presented comprehensive experiments to show the relative performance of the classification tasks. The experimental results revealed the success of proposed methods in credit approval data. In addition, the numeric results also provide guides in selection of feature selection methods and classifiers in the knowledge discovery process.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {5900–5908},
numpages = {9},
keywords = {Data Mining, Evolutionary algorithm, Feature selection, Multi-objective optimization}
}

@inproceedings{10.1007/978-3-642-40319-4_32,
author = {Lamirel, Jean-Charles and Cuxac, Pascal and Chivukula, Aneesh Sreevallabh and Hajlaoui, Kafil},
title = {A New Feature Selection and Feature Contrasting Approach Based on Quality Metric: Application to Efficient Classification of Complex Textual Data},
year = {2013},
isbn = {9783642403187},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-40319-4_32},
doi = {10.1007/978-3-642-40319-4_32},
abstract = {Feature maximization is a cluster quality metric which favors clusters with maximum feature representation as regard to their associated data. In this paper we go one step further showing that a straightforward adaptation of such metric can provide a highly efficient feature selection and feature contrasting model in the context of supervised classification. We more especially show that this technique can enhance the performance of classification methods whilst very significantly outperforming (+80%) the state-of-the art feature selection techniques in the case of the classification of unbalanced, highly multidimensional and noisy textual data, with a high degree of similarity between the classes.},
booktitle = {Revised Selected Papers of PAKDD 2013 International Workshops on Trends and Applications in Knowledge Discovery and Data Mining - Volume 7867},
pages = {367–378},
numpages = {12},
keywords = {clustering quality index, feature maximization, feature selection, supervised learning, text, unbalanced data}
}

@inproceedings{10.5555/3016387.3016513,
author = {Arai, Hiromasa and Xu, Ke and Maung, Crystal and Schweitzer, Haim},
title = {Weighted A* algorithms for unsupervised feature selection with provable bounds on suboptimality},
year = {2016},
publisher = {AAAI Press},
abstract = {Identifying a small number of features that can represent the data is believed to be NP-hard. Previous approaches exploit algebraic structure and use randomization. We propose an algorithm based on ideas similar to the Weighted A* algorithm in heuristic search. Our experiments show this new algorithm to be more accurate than the current state of the art.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4194–4195},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{10.1016/j.compbiomed.2011.10.004,
author = {Huang, Hu and Xie, Hong-Bo and Guo, Jing-Yi and Chen, Hui-Juan},
title = {Ant colony optimization-based feature selection method for surface electromyography signals classification},
year = {2012},
issue_date = {January, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {1},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2011.10.004},
doi = {10.1016/j.compbiomed.2011.10.004},
abstract = {This paper presented a new ant colony optimization (ACO) feature selection method to classify hand motion surface electromyography (sEMG) signals. The multiple channels of sEMG recordings make the dimensionality of sEMG feature grow dramatically. It is known that the informative feature subset with small size is a precondition for the accurate and computationally efficient classification strategy. Therefore, this study proposed an ACO based feature selection scheme using the heuristic information measured by the minimum redundancy maximum relevance criterion (ACO-mRMR). The experiments were conducted on ten subjects with eight upper limb motions. Two feature sets, i.e., time domain features combined with autoregressive model coefficients (TDAR) and wavelet transform (WT) features, were extracted from the recorded sEMG signals. The average classification accuracies of using ACO reduced TDAR and WT features were 95.45+/-2.2% and 96.08+/-3.3%, respectively. The principal component analysis (PCA) was also conducted on the same data sets for comparison. The average classification accuracies of using PCA reduced TDAR and WT features were 91.51+/-4.9% and 89.87+/-4.4%, respectively. The results demonstrated that the proposed ACO-mRMR based feature selection method can achieve considerably high classification rates in sEMG motion classification task and be applicable to other biomedical signals pattern analysis.},
journal = {Comput. Biol. Med.},
month = jan,
pages = {30–38},
numpages = {9},
keywords = {Ant colony optimization, Feature selection, Minimum redundancy maximum relevance, Pattern classification, Surface electromyography}
}

@article{10.1007/s11042-013-1548-z,
author = {Fan, Wentao and Bouguila, Nizar},
title = {Face detection and facial expression recognition using simultaneous clustering and feature selection via an expectation propagation statistical learning framework},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-013-1548-z},
doi = {10.1007/s11042-013-1548-z},
abstract = {In this paper, we focus on developing a novel framework which can be effectively used for both face detection (i.e. discriminate faces from non-face patterns) and facial expression recognition. The proposed statistical framework is based on a Dirichlet process mixture of generalized Dirichlet (GD) distributions used to model local binary pattern (LBP) features. Our method is built on nonparametric Bayesian analysis where the determination of the number of clusters is sidestepped by assuming an infinite number of mixture components. An unsupervised feature selection scheme is also integrated with the proposed nonparametric framework to improve modeling performance and generalization capabilities. By learning the proposed model using an expectation propagation (EP) inference approach, all the involved model parameters and feature saliencies can be evaluated simultaneously in a single optimization framework. Furthermore, the proposed framework is extended by adopting a localized feature selection scheme which has shown, according to our results, superior performance, to determine the most important facial features, as compared to the global one. The effectiveness and utility of the proposed method is illustrated through extensive empirical results using both synthetic data and two challenging applications involving face detection, and facial expression recognition.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {4303–4327},
numpages = {25},
keywords = {Dirichlet process, Expectation propagation, Face detection, Facial expression recognition, Feature selection, Generalized Dirichlet mixture}
}

@article{10.1504/IJBIC.2017.086700,
title = {Genetic algorithm-based feature selection for classification of land cover changes using combined LANDSAT and ENVISAT images},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {3},
issn = {1758-0366},
url = {https://doi.org/10.1504/IJBIC.2017.086700},
doi = {10.1504/IJBIC.2017.086700},
abstract = {Recent monsoon failures and reduced rain falls urge the environmental and ecology researchers to concentrate on the land cover changes. Significant and efficient way to monitor the land cover changes is satellite image classification. This work describes the combination of remotely sensed data, LANDSAT and ENVISAT images, to improve the classification accuracy. Instead of predictor space, embedding space is considered in the proposed KNNES and SVMES methods and applied for the classification of combined LANDSAT and ENVISAT datasets. Genetic algorithm-based GA feature selection is adopted to enhance the proposed classification methods. Classification of land cover changes of the study area are identified as used land, unused land, forest and vegetation. Proposed methods are evaluated by an accuracy analysis which follows good practice recommendations. Accuracy is quantified by reporting standard errors, i.e., producer accuracy, user accuracy, omission error and commission error. Performance of the proposed SVM and KNN-based methods using GA-based feature selection for combined dataset is improved significantly and provide overall accuracy 80% and 76% respectively.},
journal = {Int. J. Bio-Inspired Comput.},
month = jan,
pages = {172–187},
numpages = {16}
}

@article{10.1016/j.compeleceng.2015.03.018,
author = {Fan, Wentao and Sallay, Hassen and Bouguila, Nizar and Bourouis, Sami},
title = {A hierarchical Dirichlet process mixture of generalized Dirichlet distributions for feature selection},
year = {2015},
issue_date = {April 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {43},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2015.03.018},
doi = {10.1016/j.compeleceng.2015.03.018},
abstract = {Display Omitted A statistical framework based on hierarchical Dirichlet processes and generalized Dirichlet distribution is developed.The framework simultaneously performs model parameters estimations as well as model complexity determination.The learning of the model is done via variational Bayes inference.The efficiency of the proposed algorithm is validated via challenging applications. This paper addresses the problem of identifying meaningful patterns and trends in data via clustering (i.e. automatically dividing a data set into meaningful homogenous sub-groups such that the data within the same sub-group are very similar, and data in different sub-groups are very different). The clustering framework that we propose is based on the generalized Dirichlet distribution, which is widely accepted as a flexible modeling approach, and a hierarchical Dirichlet process mixture prior. A main advantage of the adopted hierarchical Dirichlet process is that it provides a principled elegant nonparametric Bayesian approach to model selection by supposing that the number of mixture components can go to infinity. In addition to capturing the structure of the data, the combination of hierarchical Dirichlet process and generalized Dirichlet distribution is shown to offer a natural efficient solution to the feature selection problem when dealing with high-dimensional data. We develop two variational learning approaches (i.e. batch and incremental) for learning the parameters of the proposed model. The batch algorithm examines the entire data set at once while the incremental one learns the model one step at a time (i.e. update the model's parameters each time new data are introduced). The utility of the proposed approach is demonstrated on real applications namely face detection, facial expression recognition, human gesture recognition, and off-line writer identification. The obtained results show clearly the merits of our statistical framework.},
journal = {Comput. Electr. Eng.},
month = apr,
pages = {48–65},
numpages = {18},
keywords = {Clustering, Face detection, Facial expression recognition, Hierarchical Dirichlet process, Human gesture recognition, Variational learning}
}

@article{10.3103/S000510551401004X,
author = {Yashkov, I. B.},
title = {Feature selection using decision trees in the problem of JSM classification},
year = {2014},
issue_date = {January   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {48},
number = {1},
issn = {0005-1055},
url = {https://doi.org/10.3103/S000510551401004X},
doi = {10.3103/S000510551401004X},
abstract = {A method of feature selection is considered for training a classification algorithm based on the JSM method. The method is based on the use of decision trees. It includes construction of a maximal tree using the C4.5 algorithm and the preparation of a series of truncated trees based on the criterion of minimal cost complexity. The results and parameters of the JSM classifier with different numbers of selected features are compared.},
journal = {Autom. Doc. Math. Linguist.},
month = jan,
pages = {6–11},
numpages = {6},
keywords = {JSM method, classification, decision trees, entropy, machine learning, reduction of dimension, selection of parameters}
}

@article{10.1007/s10590-014-9164-x,
author = {Shah, Kashif and Cohn, Trevor and Specia, Lucia},
title = {A Bayesian non-linear method for feature selection in machine translation quality estimation},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0922-6567},
url = {https://doi.org/10.1007/s10590-014-9164-x},
doi = {10.1007/s10590-014-9164-x},
abstract = {We perform a systematic analysis of the effectiveness of features for the problem of predicting the quality of machine translation (MT) at the sentence level. Starting from a comprehensive feature set, we apply a technique based on Gaussian processes, a Bayesian non-linear learning method, to automatically identify features leading to accurate model performance. We consider application to several datasets across different language pairs and text domains, with translations produced by various MT systems and scored for quality according to different evaluation criteria. We show that selecting features with this technique leads to significantly better performance in most datasets, as compared to using the complete feature sets or a state-of-the-art feature selection approach. In addition, we identify a small set of features which seem to perform well across most datasets.},
journal = {Machine Translation},
month = jun,
pages = {101–125},
numpages = {25},
keywords = {Gaussian Processes, Machine translation, Quality estimation}
}

@article{10.5555/1662565.1662568,
author = {Jensen, Richard and Shen, Qiang},
title = {Feature selection for aiding glass forensic evidence analysis},
year = {2009},
issue_date = {October 2009},
publisher = {IOS Press},
address = {NLD},
volume = {13},
number = {5},
issn = {1088-467X},
abstract = {The evaluation of glass evidence in forensic science is an important issue. Traditionally, this has depended on the comparison of the physical and chemical attributes of an unknown fragment with a control fragment. A high degree of discrimination between glass fragments is now achievable due to advances in analytical capabilities. A random effects model using two levels of hierarchical nesting is applied to the calculation of a likelihood ratio (LR) as a solution to the problem of comparison between two sets of replicated continuous observations where it is unknown whether the sets of measurements shared a common origin. Replicate measurements from a population of such measurements allow the calculation of both within-group and between-group variances. Univariate normal kernel estimation procedures have been used for this, where the between-group distribution is considered to be non-normal. However, the choice of variable for use in LR estimation is critical to the quality of LR produced. This paper investigates the use of feature selection for the purpose of selecting the variable for estimation without the need for expert knowledge. Results are recorded for several selectors using normal, exponential, adaptive and biweight kernel estimation techniques. Misclassification rates for the LR estimators are used to measure performance. The experiments performed reveal the capability of the proposed approach for this task.},
journal = {Intell. Data Anal.},
month = oct,
pages = {703–723},
numpages = {21},
keywords = {Feature selection, forensic evidence, fuzzy-rough sets, glass analysis, two-level model}
}

@article{10.1007/s00500-020-05222-x,
author = {Shetty, Rashmi P. and Sathyabhama, A. and Pai, P. Srinivasa},
title = {An efficient online sequential extreme learning machine model based on feature selection and parameter optimization using cuckoo search algorithm for multi-step wind speed forecasting},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05222-x},
doi = {10.1007/s00500-020-05222-x},
abstract = {Accurate wind speed forecasting (WSF) has become increasingly important to overcome the adverse effects of stochastic nature of the wind on wind power generation. This paper proposes a multi-step hybrid online WSF model by combining online sequential extreme learning machine (OSELM), optimized variational mode decomposition (OVMD) and cuckoo search optimization algorithm (CSO). OVMD decomposes the wind speed series into subseries, and CSO selects the input features for each subseries. Multi-step forecasting for each subseries is performed using OSELM model optimized by CSO. Finally, the forecasting results are obtained by the aggregate calculations. The proposed model has been examined by using 10-min average wind speed data collected in monsoon and winter seasons from a supervisory control and data acquisition system of a 1.5 MW wind turbine situated in central dry zone of Karnataka, India. The results reveal that the model proposed captures the nonlinear characteristics of the wind speed in a better manner in comparison with the batch learning approach, giving accurate wind speed forecasts. This can help wind farms to estimate the wind power in a location efficiently.},
journal = {Soft Comput.},
month = jan,
pages = {1277–1295},
numpages = {19},
keywords = {Multi-step wind speed forecasting, Online sequential extreme learning machine, Optimized variational mode decomposition, Cuckoo search optimization}
}

@article{10.1016/j.patrec.2006.03.012,
author = {Lee, Gobert N. and Bottema, Murk J.},
title = {Significance of classification scores subsequent to feature selection},
year = {2006},
issue_date = {15 October 2006},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {27},
number = {14},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2006.03.012},
doi = {10.1016/j.patrec.2006.03.012},
abstract = {Feature selection prior to classification is shown to inflate performance. Empirical distributions are used to estimate statistical significance of classification scores. In an example study, eleven high classification scores are obtained but only three are found to be significant at p=0.05.},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {1702–1709},
numpages = {8},
keywords = {Classification, Computer-aided diagnosis, Feature selection, Multiple comparisons, Statistical significance}
}

@article{10.1016/j.imavis.2013.09.005,
author = {Chen, Yanzhi and Dick, Anthony and Li, Xi and Van Den Hengel, Anton},
title = {Spatially aware feature selection and weighting for object retrieval},
year = {2013},
issue_date = {December, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {31},
number = {12},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2013.09.005},
doi = {10.1016/j.imavis.2013.09.005},
abstract = {Many recent image retrieval methods are based on the ''bag-of-words'' (BoW) model with some additional spatial consistency checking. This paper proposes a more accurate similarity measurement that takes into account spatial layout of visual words in an offline manner. The similarity measurement is embedded in the standard pipeline of the BoW model, and improves two features of the model: i) latent visual words are added to a query based on spatial co-occurrence, to improve query recall; and ii) weights of reliable visual words are increased to improve the precision. The combination of these methods leads to a more accurate measurement of image similarity. This is similar in concept to the combination of query expansion and spatial verification, but does not require query time processing, which is too expensive to apply to full list of ranked results. Experimental results demonstrate the effectiveness of our proposed method on three public datasets.},
journal = {Image Vision Comput.},
month = dec,
pages = {935–948},
numpages = {14},
keywords = {Bag-of-words, Object retrieval, Spatial expansion, Visual word re-weighting}
}

@article{10.5555/3192212.3192214,
title = {Evaluation of various feature sets and feature selection towards automatic recognition of bird species},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {56},
number = {3},
issn = {0952-8091},
abstract = {It is necessary to develop efficient methods for monitoring and recognising bird species that will help in evaluating the biodiversity of a region. In this paper we present techniques for automatic recognition of bird species based on audio recordings of their sounds. In this work, various audio features like descriptive features, wavelet packet decomposition-based features and perceptual features like Mel-frequency cepstral coefficients, perceptual linear prediction, and human factor cepstral coefficients are evaluated. Combination of these feature sets has also been evaluated. Classification of ten bird species is carried out using Gaussian Mixture Modelling GMM and Support Vector Machines SVMs. When a number of features are extracted, the feature vector may contain redundancy. Redundant features may either degrade the performance of the system or add no value to the system. For feature subset selection, this work implements a technique based on singular value decomposition and QR decomposition using column pivoting.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {172–184},
numpages = {13}
}

@inproceedings{10.1109/UKSim.2012.116,
author = {Qazi, Nadeem and Raza, Kamran},
title = {Effect of Feature Selection, SMOTE and under Sampling on Class Imbalance Classification},
year = {2012},
isbn = {9780769546827},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/UKSim.2012.116},
doi = {10.1109/UKSim.2012.116},
abstract = {Accurate identification of network intrusions is one of the biggest challenges of Network Intrusion Detection (NID) systems. In recent years Machine learning classification techniques have been used to precisely identify network intrusion. However, the multi class distribution in network intrusion detection system has found to be highly skewed, leading to classification accuracy problem due to class imbalance data set. The work presented in this paper not only explores the role of the attribute selection in improving classification accuracy but also investigates the problem of class imbalance using the Synthetic Minority Over-sampling (SMOTE) and under sampling of major classes. The classification performance is then evaluated over several types of classifiers. The outcome of this work is that for the class imbalance data set the under-sampling technique is more effective than SMOTE in detecting minor classes. It has also found during this research work that the decision tree algorithms (JRIP) and Na\"{\i}ve Bayes are more accurate classifiers as compared to the Radial basis neural network and support vector machine. However no single algorithm can be used for the classification of multiclass and it is proposed in this research work that combination of classifier consisting of Na\"{\i}ve Bayes and JRIP could be used for the classification of minor classes in an imbalance class data set of intrusion detection system},
booktitle = {Proceedings of the 2012 UKSim 14th International Conference on Modelling and Simulation},
pages = {145–150},
numpages = {6},
keywords = {Class imbalance, Feature Selection, Network intrusion, Support Vector Machines},
series = {UKSIM '12}
}

@inproceedings{10.1109/ICMLA.2009.18,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan},
title = {Feature Selection with Imbalanced Data for Software Defect Prediction},
year = {2009},
isbn = {9780769539263},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2009.18},
doi = {10.1109/ICMLA.2009.18},
abstract = {In this paper, we study the learning impact of data sampling followed by attribute selection on the classification models built with binary class imbalanced data within the scenario of software quality engineering. We use a wrapper-based attribute ranking technique to select a subset of attributes, and the random undersampling technique (RUS) on the majority class to alleviate the negative effects of imbalanced data on the prediction models. The datasets used in the empirical study were collected from numerous software projects. Five data preprocessing scenarios were explored in these experiments, including: (1) training on the original, unaltered fit dataset, (2) training on a sampled version of the fit dataset, (3) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on the unsampled fit dataset, (4) training on an unsampled version of the fit dataset using only the attributes chosen by feature selection based on a sampled version of the fit dataset, and (5) training on a sampled version of the fit dataset using only the attributes chosen by feature selection based on the sampled version of the fit dataset. We compared the performances of the classification models constructed over these five different scenarios. The results demonstrate that the classification models constructed on the sampled fit data with or without feature selection (case 2 and case 5) significantly outperformed the classification models built with the other cases (unsampled fit data). Moreover, the two scenarios using sampled data (case 2 and case 5) showed very similar performances, but the subset of attributes (case 5) is only around 15% or 30% of the complete set of attributes (case 2).},
booktitle = {Proceedings of the 2009 International Conference on Machine Learning and Applications},
pages = {235–240},
numpages = {6},
keywords = {feature selection, imbalanced data, software defect prediction, wrapper-based attribute ranking},
series = {ICMLA '09}
}

@article{10.1016/j.dss.2008.07.008,
author = {Chen, Ye and Liginlal, Divakaran},
title = {A maximum entropy approach to feature selection in knowledge-based authentication},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2008.07.008},
doi = {10.1016/j.dss.2008.07.008},
abstract = {Feature selection is critical to knowledge-based authentication. In this paper, we adopt a wrapper method in which the learning machine is a generative probabilistic model, and the objective is to maximize the Kullback-Leibler divergence between the true empirical distribution defined by the legitimate knowledge and the approximating distribution representing an attacking strategy, both in the same feature space. The closed-form solutions to this optimization problem lead to three adaptive algorithms, unified under the principle of maximum entropy. Our experimental results show that the proposed adaptive methods are superior to the commonly used random selection method.},
journal = {Decis. Support Syst.},
month = dec,
pages = {388–398},
numpages = {11},
keywords = {Feature selection, Knowledge-based authentication, Maximum entropy, Metrics, Probabilistic model, Security}
}

@article{10.1016/j.patrec.2010.01.030,
author = {Tahir, Muhammad Atif and Smith, Jim},
title = {Creating diverse nearest-neighbour ensembles using simultaneous metaheuristic feature selection},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {11},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2010.01.030},
doi = {10.1016/j.patrec.2010.01.030},
abstract = {The nearest-neighbour (1NN) classifier has long been used in pattern recognition, exploratory data analysis, and data mining problems. A vital consideration in obtaining good results with this technique is the choice of distance function, and correspondingly which features to consider when computing distances between samples. In recent years there has been an increasing interest in creating ensembles of classifiers in order to improve classification accuracy. This paper proposes a new ensemble technique which combines multiple 1NN classifiers, each using a different distance function, and potentially a different set of features (feature vector). These feature vectors are determined for each distance metric simultaneously using Tabu Search to minimise the ensemble error rate. We show that this approach implicitly selects for a diverse set of classifiers, and by doing so achieves greater performance improvements than can be achieved by treating the classifiers independently, or using a single feature set. Naturally, optimising the level of ensembles necessitates a much larger solution space, to make this approach tractable, we show how Tabu Search at the ensemble level can be hybridised with local search at the level of individual classifiers. The proposed ensemble classifier with different distance metrics and different feature vectors is evaluated using various benchmark datasets from UCI Machine Learning Repository and a real-world machine-vision application. Results have indicated a significant increase in the performance when compared with various well-known classifiers. Furthermore, the proposed ensemble method is also compared with ensemble classifier using different distance metrics but with same feature vector (with or without feature selection (FS)).},
journal = {Pattern Recogn. Lett.},
month = aug,
pages = {1470–1480},
numpages = {11},
keywords = {1NN classifier, Ensemble classifiers, Feature selection, Tabu Search}
}

@article{10.1016/j.eswa.2009.10.037,
author = {Yeh, Yun-Chi and Wang, Wen-June and Chiou, Che Wun},
title = {Feature selection algorithm for ECG signals using Range-Overlaps Method},
year = {2010},
issue_date = {April, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.10.037},
doi = {10.1016/j.eswa.2009.10.037},
abstract = {This study proposes a simple and reliable feature selection algorithm for ECG signals, termed the Range-Overlaps Method. The proposed method has the advantages of good detection results, no complex mathematic computations, fast and low memory space and low time complexity. Both cluster analysis and fuzzy logic methods are applied to evaluate the performance of the proposed method. Experimental results show that the total classification accuracy is above 93%. Thus, the proposed algorithm provides an efficient, simple and fast method for feature selection on ECG signals.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {3499–3512},
numpages = {14},
keywords = {Cluster analysis, ECG signal, Feature selection, Fuzzy logic methods, MIT-BIH arrhythmia database}
}

@article{10.1016/j.cviu.2010.11.007,
author = {Chen, Cheng and Yang, Yi and Nie, Feiping and Odobez, Jean-Marc},
title = {3D human pose recovery from image by efficient visual feature selection},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {115},
number = {3},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2010.11.007},
doi = {10.1016/j.cviu.2010.11.007},
abstract = {In this paper we propose a new examplar-based approach to recover 3D human poses from monocular images. Given the visual feature of each frame, pose retrieval is first conducted in the examplar database to find relevant pose candidates. Then, dynamic programming is applied on the pose candidates to recover a continuous pose sequence. We make two contributions within this framework. First, we propose to use an efficient feature selection algorithm to select effective visual feature components. The task is formulated as a trace-ratio criterion which measures the score of the selected feature component subset, and the criterion is efficiently optimized to achieve the global optimum. The selected components are used instead of the original full feature set to improve the accuracy and efficiency of pose recovery. As second contribution, we propose to use sparse representation to retrieve the pose candidates, where the measured visual feature is expressed as a sparse linear combination of the examplars in the database. Sparse representation ensures that semantically similar poses have larger probability to be retrieved. The effectiveness of our approach is validated quantitatively through extensive evaluations on both synthetic and real data, and qualitatively by inspecting the results of the real time system we have implemented.},
journal = {Comput. Vis. Image Underst.},
month = mar,
pages = {290–299},
numpages = {10},
keywords = {Feature selection, Motion understanding, Pose recovery, Sparse representation}
}

@article{10.1016/j.eswa.2014.09.048,
author = {Lee, Jong-Hyun and Rahimipour Anaraki, Javad and Ahn, Chang Wook and An, Jinung},
title = {Efficient classification system based on Fuzzy-Rough Feature Selection and Multitree Genetic Programming for intension pattern recognition using brain signal},
year = {2015},
issue_date = {February 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.09.048},
doi = {10.1016/j.eswa.2014.09.048},
abstract = {We design a classification system for brain signal based on evolutionary algorithm.The proposed methods base on fuzzy rough theory and Multitree Genetic Programming.We examine the intension pattern of brain signals with fNIRS.The proposed FRFS reduced the data volume and extracted the informative features.The proposed GP classified with higher accuracy than conventional methods. Recently, many researchers have studied in engineering approach to brain activity pattern of conceptual activities of the brain. In this paper we proposed a intension recognition framework (i.e. classification system) for high accuracy which is based on Fuzzy-Rough Feature Selection and Multitree Genetic Programming. The enormous brain signal data measured by fNIRS are reduced by proposed feature selection and extracted the informative features. Also, proposed Multitree Genetic Programming use the remain data to construct the intension recognition model effectively. The performance of proposed classification system is demonstrated and compared with existing classifiers and unreduced dataset. Experimental results show that classification accuracy increases while number of features decreases in proposed system.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1644–1651},
numpages = {8},
keywords = {Brain signal, Feature selection, Fuzzy-rough sets, Intension recognition, Multitree GP}
}

@inproceedings{10.5555/2027016.2027035,
author = {Moreno, Plinio and Ribeiro, Pedro and Santos-Victor, Jos\'{e}},
title = {Feature selection for tracker-less human activity recognition},
year = {2011},
isbn = {9783642215926},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We address the empirical feature selection for tracker-less recognition of human actions. We rely on the appearance plus motion model over several video frames to model the human movements. We use the L2Boost algorithm, a versatile boosting algorithm which simplifies the gradient search. We study the following options in the feature computation and learning: (i) full model vs. component-wise model, (ii) sampling strategy of the histogram cells and (iii) number of previous frames to include, amongst others. We select the features' parameters that provide the best compromise between performance and computational efficiency and apply the features in a challenging problem, the tracker-less and detection-less human activity recognition.},
booktitle = {Proceedings of the 8th International Conference on Image Analysis and Recognition - Volume Part I},
pages = {152–160},
numpages = {9},
location = {Burnaby, BC, Canada},
series = {ICIAR'11}
}

@inproceedings{10.5555/1802514.1802567,
author = {Dash, Manoranjan and Gopalkrishnan, Vivekanand},
title = {Distance based feature selection for clustering microarray data},
year = {2008},
isbn = {3540785671},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In microarray data, clustering is the fundamental task for separating genes into biologically functional groups or for classifying tissues and phenotypes. Recently, with innovative gene expression microarray data technologies, thousands of expression levels of genes (features) can be measured simultaneously in a single experiment. The large number of genes with a lot of noise causes high complexity for cluster analysis. This challenge has raised the demand for feature selection - an effective dimensionality reduction technique that removes noisy features. In this paper we propose a novel filter method for feature selection. The suggested method, called ClosestFS, is based on a distance measure. For each feature, the distance is evaluated by computing its impact on the histogram for the whole data. Our experimental results show that the quality of clustering results (evaluated by several widely used measures) of K-means algorithm using ClosestFS as the pre-processing step is significantly better than that of the pure K-means.},
booktitle = {Proceedings of the 13th International Conference on Database Systems for Advanced Applications},
pages = {512–519},
numpages = {8},
keywords = {clustering, distance function, feature selection, microarray data},
location = {New Delhi, India},
series = {DASFAA'08}
}

@inproceedings{10.5555/2318776.2318798,
author = {Zhang, Mi and Sawchuk, Alexander A.},
title = {A feature selection-based framework for human activity recognition using wearable multimodal sensors},
year = {2011},
isbn = {9781936968299},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
abstract = {Human activity recognition is important for many applications. This paper describes a human activity recognition framework based on feature selection techniques. The objective is to identify the most important features to recognize human activities. We first design a set of new features (called physical features) based on the physical parameters of human motion to augment the commonly used statistical features. To systematically analyze the impact of the physical features on the performance of the recognition system, a single-layer feature selection framework is developed. Experimental results indicate that physical features are always among the top features selected by different feature selection methods and the recognition accuracy is generally improved to 90%, or 8% better than when only statistical features are used. Moreover, we show that the performance is further improved by 3.8% by extending the single-layer framework to a multi-layer framework which takes advantage of the inherent structure of human activities and performs feature selection and classification in a hierarchical manner.},
booktitle = {Proceedings of the 6th International Conference on Body Area Networks},
pages = {92–98},
numpages = {7},
keywords = {feature design, feature selection, human activity recognition, pattern recognition, wearable multimodal sensors},
location = {Beijing, China},
series = {BodyNets '11}
}

@article{10.5555/2567709.2567724,
author = {Hern\'{a}ndez-Lobato, Daniel and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel and Dupont, Pierre},
title = {Generalized spike-and-slab priors for Bayesian group feature selection using expectation propagation},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {1891–1945},
numpages = {55},
keywords = {approximate inference, expectation propagation, generalized spike-and-slab priors, group feature selection, sequential experimental design, signal reconstruction, sparse linear model}
}

@inproceedings{10.1109/PDP.2014.24,
author = {Rughetti, Diego and Sanzo, Pierangelo Di and Ciciani, Bruno and Quaglia, Francesco},
title = {Dynamic Feature Selection for Machine-Learning Based Concurrency Regulation in STM},
year = {2014},
isbn = {9781479927296},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PDP.2014.24},
doi = {10.1109/PDP.2014.24},
abstract = {In this paper we explore machine-learning approaches for dynamically selecting the well suited amount of concurrent threads in applications relying on Software Transactional Memory (STM). Specifically, we present a solution that dynamically shrinks or enlarges the set of input features to be exploited by the machine-learner. This allows for tuning the concurrency level while also minimizing the overhead for input-features sampling, given that the cardinality of the input-feature set is always tuned to the minimum value that still guarantees reliability of workload characterization. We also present a fully heedged implementation of our proposal within the TinySTM open source framework, and provide the results of an experimental study relying on the STAMP benchmark suite, which show significant reduction of the response time with respect to proposals based on static feature selection.},
booktitle = {Proceedings of the 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing},
pages = {68–75},
numpages = {8},
keywords = {Concurrency, Performance Models, Performance Optimization, Software Transactional Memory},
series = {PDP '14}
}

@article{10.1016/j.cmpb.2011.12.015,
author = {Yu, Sung-Nien and Lee, Ming-Yuan},
title = {Conditional mutual information-based feature selection for congestive heart failure recognition using heart rate variability},
year = {2012},
issue_date = {October, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {108},
number = {1},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2011.12.015},
doi = {10.1016/j.cmpb.2011.12.015},
abstract = {Highlights We propose a feature selector UCMIFS for congestive heart failure (CHF) recognition. UCMIFS is based on the mutual information conditioned by the first-selected feature. The performance of UCMIFS is superior to the other MI-based feature selectors. UCMIFS selects only 15 features to achieve a high recognition rate of 97.59%. Feature selection plays an important role in pattern recognition systems. In this study, we explored the problem of selecting effective heart rate variability (HRV) features for recognizing congestive heart failure (CHF) based on mutual information (MI). The MI-based greedy feature selection approach proposed by Battiti was adopted in the study. The mutual information conditioned by the first-selected feature was used as a criterion for feature selection. The uniform distribution assumption was used to reduce the computational load. And, a logarithmic exponent weighting was added to model the relative importance of the MI with respect to the number of the already-selected features. The CHF recognition system contained a feature extractor that generated four categories, totally 50, features from the input HRV sequences. The proposed feature selector, termed UCMIFS, proceeded to select the most effective features for the succeeding support vector machine (SVM) classifier. Prior to feature selection, the 50 features produced a high accuracy of 96.38%, which confirmed the representativeness of the original feature set. The performance of the UCMIFS selector was demonstrated to be superior to the other MI-based feature selectors including MIFS-U, CMIFS, and mRMR. When compared to the other outstanding selectors published in the literature, the proposed UCMIFS outperformed them with as high as 97.59% accuracy in recognizing CHF using only 15 features. The results demonstrated the advantage of using the recruited features in characterizing HRV sequences for CHF recognition. The UCMIFS selector further improved the efficiency of the recognition system with substantially lowered feature dimensions and elevated recognition rate.},
journal = {Comput. Methods Prog. Biomed.},
month = oct,
pages = {299–309},
numpages = {11},
keywords = {Congestive heart failure, Feature selection, Heart rate variability, Mutual information}
}

@inproceedings{10.5555/1964478.1964495,
author = {Zuluaga, Maria A. and Leyton, Edgar J. F. Delgado and Hoyos, Marcela Hern\'{a}ndez and Orkisz, Maciej},
title = {Feature selection for SVM-based vascular anomaly detection},
year = {2010},
isbn = {9783642184208},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This work explores feature selection to improve the performance in the vascular anomaly detection domain. Starting from a previously defined classification framework based on Support Vector Machines (SVM), we attempt to determine features that improve classification performance and to define guidelines for feature selection. Three different strategies were used in the feature selection stage, while a Density Level Detection-SVM (DLD-SVM) was used to validate the performance of the selected features over testing data. Results show that a careful feature selection results in a good classification performance. DLD-SVM shows a poor performance when using all the features together, owing to the curse of dimensionality.},
booktitle = {Proceedings of the 2010 International MICCAI Conference on Medical Computer Vision: Recognition Techniques and Applications in Medical Imaging},
pages = {141–152},
numpages = {12},
location = {Beijing, China},
series = {MCV'10}
}

@article{10.1016/j.cmpb.2012.02.006,
author = {Maca\v{s}, Martin and Lhotsk\'{a}, Lenka and Bakstein, Eduard and Nov\'{a}k, Daniel and Wild, Ji\v{r}\'{\i} and Sieger, Tom\'{a}\v{s} and Vostatek, Pavel and Jech, Robert},
title = {Wrapper feature selection for small sample size data driven by complete error estimates},
year = {2012},
issue_date = {October, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {108},
number = {1},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2012.02.006},
doi = {10.1016/j.cmpb.2012.02.006},
abstract = {This paper focuses on wrapper-based feature selection for a 1-nearest neighbor classifier. We consider in particular the case of a small sample size with a few hundred instances, which is common in biomedical applications. We propose a technique for calculating the complete bootstrap for a 1-nearest-neighbor classifier (i.e., averaging over all desired test/train partitions of the data). The complete bootstrap and the complete cross-validation error estimate with lower variance are applied as novel selection criteria and are compared with the standard bootstrap and cross-validation in combination with three optimization techniques - sequential forward selection (SFS), binary particle swarm optimization (BPSO) and simplified social impact theory based optimization (SSITO). The experimental comparison based on ten datasets draws the following conclusions: for all three search methods examined here, the complete criteria are a significantly better choice than standard 2-fold cross-validation, 10-fold cross-validation and bootstrap with 50 trials irrespective of the selected output number of iterations. All the complete criterion-based 1NN wrappers with SFS search performed better than the widely-used FILTER and SIMBA methods. We also demonstrate the benefits and properties of our approaches on an important and novel real-world application of automatic detection of the subthalamic nucleus.},
journal = {Comput. Methods Prog. Biomed.},
month = oct,
pages = {138–150},
numpages = {13},
keywords = {Bootstrap, Cross-validation, Feature selection, Nearest neighbor, Small sample size, Subthalamic nucleus detection, Swarm intelligence, Wrappers}
}

@inproceedings{10.5555/951949.952087,
author = {Veeramachaneni, Sriharsha and Avesani, Paolo},
title = {Active Sampling for Feature Selection},
year = {2003},
isbn = {0769519784},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {In knowledge discovery applications, where new featuresare to be added, an acquisition policy can help select thefeatures to be acquired based on their relevance and thecost of extraction. This can be posed as a feature selectionproblem where the feature values are not known in advance.We propose a technique to actively sample the featurevalues with the ultimate goal of choosing between alternativecandidate features with minimum sampling cost.Our heuristic algorithm is based on extracting candidatefeatures in a region of the instance space where the featurevalue is likely to alter our knowledge the most. An experimentalevaluation on a standard database shows that it ispossible outperform a random subsampling policy in termsof the accuracy in feature selection.},
booktitle = {Proceedings of the Third IEEE International Conference on Data Mining},
pages = {665},
series = {ICDM '03}
}

@article{10.1145/3319616,
author = {Wu, Tongshuang and Weld, Daniel S. and Heer, Jeffrey},
title = {Local Decision Pitfalls in Interactive Machine Learning: An Investigation into Feature Selection in Sentiment Analysis},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3319616},
doi = {10.1145/3319616},
abstract = {Tools for Interactive Machine Learning (IML) enable end users to update models in a “rapid, focused, and incremental”—yet local—manner. In this work, we study the question of local decision making in an IML context around feature selection for a sentiment classification task. Specifically, we characterize the utility of interactive feature selection through a combination of human-subjects experiments and computational simulations. We find that, in expectation, interactive modification fails to improve model performance and may hamper generalization due to overfitting. We examine how these trends are affected by the dataset, learning algorithm, and the training set size. Across these factors we observe consistent generalization issues. Our results suggest that rapid iterations with IML systems can be dangerous if they encourage local actions divorced from global context, degrading overall model performance. We conclude by discussing the implications of our feature selection results to the broader area of IML systems and research.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {24},
numpages = {27},
keywords = {Machine learning, performance analysis, text classification}
}

@inproceedings{10.1145/2396761.2396791,
author = {Zhu, Yuanyuan and Yu, Jeffrey Xu and Cheng, Hong and Qin, Lu},
title = {Graph classification: a diversified discriminative feature selection approach},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2396791},
doi = {10.1145/2396761.2396791},
abstract = {A graph models complex structural relationships among objects, and has been prevalently used in a wide range of applications. Building an automated graph classification model becomes very important for predicting unknown graphs or understanding complex structures between different classes. The graph classification framework being widely used consists of two steps, namely, feature selection and classification. The key issue is how to select important subgraph features from a graph database with a large number of graphs including positive graphs and negative graphs. Given the features selected, a generic classification approach can be used to build a classification model. In this paper, we focus on feature selection. We identify two main issues with the most widely used feature selection approach which is based on a discriminative score to select frequent subgraph features, and introduce a new diversified discriminative score to select features that have a higher diversity. We analyze the properties of the newly proposed diversified discriminative score, and conducted extensive performance studies to demonstrate that such a diversified discriminative score makes positive/negative graphs separable and leads to a higher classification accuracy.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {205–214},
numpages = {10},
keywords = {diversity, feature selection, graph classification},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@inproceedings{10.1007/11551188_33,
author = {Redpath, D. B. and Lebart, K.},
title = {Boosting feature selection},
year = {2005},
isbn = {3540287574},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11551188_33},
doi = {10.1007/11551188_33},
abstract = {It is possible to reduce the error rate of a single classifier using a classifier ensemble. However, any gain in performance is undermined by the increased computation of performing classification several times. Here the AdaboostFS algorithm is proposed which builds on two popular areas of ensemble research: Adaboost and Ensemble Feature Selection (EFS). The aim of AdaboostFS is to reduce the number of features used by each base classifer and hence the overall computation required by the ensemble. To do this the algorithm combines a regularised version of Boosting AdaboostReg [1] with a floating feature search for each base classifier.AdaboostFS is compared using four benchmark data sets to AdaboostAll, which uses all features and to AdaboostRSM, which uses a random selection of features. Performance is assessed based on error rate, ensemble error and diversity, and the total number of features used for classification. Results show that AdaboostFS achieves a lower error rate and higher diversity than AdaboostAll, and achieves a lower error rate and comparable diversity to AdaboostRSM. However, over the other methods AdaboostFS produces a significant reduction in the number of features required for classification in each base classifier and the entire ensemble.},
booktitle = {Proceedings of the Third International Conference on Advances in Pattern Recognition - Volume Part I},
pages = {305–314},
numpages = {10},
location = {Bath, UK},
series = {ICAPR'05}
}

@article{10.1007/s11042-009-0340-6,
author = {Hopfgartner, Frank and Urruty, Thierry and Lopez, Pablo Bermejo and Villa, Robert and Jose, Joemon M.},
title = {Simulated evaluation of faceted browsing based on feature selection},
year = {2010},
issue_date = {May       2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-009-0340-6},
doi = {10.1007/s11042-009-0340-6},
abstract = {In this paper we explore the limitations of facet based browsing which uses sub-needs of an information need for querying and organising the search process in video retrieval. The underlying assumption of this approach is that the search effectiveness will be enhanced if such an approach is employed for interactive video retrieval using textual and visual features. We explore the performance bounds of a faceted system by carrying out a simulated user evaluation on TRECVid data sets, and also on the logs of a prior user experiment with the system. We first present a methodology to reduce the dimensionality of features by selecting the most important ones. Then, we discuss the simulated evaluation strategies employed in our evaluation and the effect on the use of both textual and visual features. Facets created by users are simulated by clustering video shots using textual and visual features. The experimental results of our study demonstrate that the faceted browser can potentially improve the search effectiveness.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {631–662},
numpages = {32},
keywords = {Clustering, Feature selection, Log file analysis, Video retrieval}
}

@article{10.1016/j.neucom.2012.12.057,
author = {Tan, Choo Jun and Lim, Chee Peng and Cheah, Yu–N},
title = {A multi-objective evolutionary algorithm-based ensemble optimizer for feature selection and classification with neural network models},
year = {2014},
issue_date = {Feb 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.12.057},
doi = {10.1016/j.neucom.2012.12.057},
journal = {Neurocomput.},
month = feb,
pages = {217–228},
numpages = {12},
keywords = {Multi-objective optimization, Evolutionary algorithm, Feature selection, Neural network classifiers, Ensemble models}
}

@article{10.1016/j.ins.2009.02.014,
author = {Maldonado, Sebasti\'{a}n and Weber, Richard},
title = {A wrapper method for feature selection using Support Vector Machines},
year = {2009},
issue_date = {June, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {13},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2009.02.014},
doi = {10.1016/j.ins.2009.02.014},
abstract = {We introduce a novel wrapper Algorithm for Feature Selection, using Support Vector Machines with kernel functions. Our method is based on a sequential backward selection, using the number of errors in a validation subset as the measure to decide which feature to remove in each iteration. We compare our approach with other algorithms like a filter method or Recursive Feature Elimination SVM to demonstrate its effectiveness and efficiency.},
journal = {Inf. Sci.},
month = jun,
pages = {2208–2217},
numpages = {10},
keywords = {Classification, Feature selection, Mathematical programming, Support Vector Machines, Wrapper methods}
}

@inproceedings{10.1109/WIIAT.2008.336,
author = {Okabe, Masayuki and Yamada, Seiji},
title = {Interactive Spam Filtering with Active Learning and Feature Selection},
year = {2008},
isbn = {9780769534961},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WIIAT.2008.336},
doi = {10.1109/WIIAT.2008.336},
abstract = {This paper proposes an interactive spam filtering method that utilizes active learning and feature selection. Identifying effective features are very important in spam filtering because spam mails include so many meaningless words that are slightly different from each other. Thus identifying effective and ineffective features is promising approach.Although traditional feature selection methods have been done based on some amount of labeled training data, this assumption does not hold in interactive spam filtering. We propose a method to identify effective features through active learning in spam filtering using naive Bayes approach. Experimental results show that our method outperforms traditional methods that operate with no feature selection.},
booktitle = {Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology - Volume 03},
pages = {165–168},
numpages = {4},
keywords = {active learning, spam filtering},
series = {WI-IAT '08}
}

@phdthesis{10.5555/1925278,
author = {Mal-Sarkar, Sanchita},
advisor = {Konangi, Vijay},
title = {Uncertainty management of intelligent feature selection in wireless sensor networks},
year = {2010},
isbn = {9781109714388},
publisher = {Cleveland State University},
address = {USA},
abstract = {Wireless sensor networks (WSN) are envisioned to revolutionize the paradigm of monitoring complex real-world systems at a very high resolution. However, the deployment of a large number of unattended sensor nodes in hostile environments, frequent changes of environment dynamics, and severe resource constraints pose uncertainties and limit the potential use of WSN in complex real-world applications. Although uncertainty management in Artificial Intelligence (AI) is well developed and well investigated, its implications in wireless sensor environments are inadequately addressed. This dissertation addresses uncertainty management issues of spatio-temporal patterns generated from sensor data. It provides a framework for characterizing spatio-temporal pattern in WSN. Using rough set theory and temporal reasoning a novel formalism has been developed to characterize and quantify the uncertainties in predicting spatio-temporal patterns from sensor data. This research also uncovers the trade-off among the uncertainty measures, which can be used to develop a multi-objective optimization model for real-time decision making in sensor data aggregation and sampling.},
note = {AAI3405521}
}

@article{10.1016/j.patcog.2008.03.007,
author = {Hong, Yi and Kwong, Sam and Chang, Yuchou and Ren, Qingsheng},
title = {Unsupervised feature selection using clustering ensembles and population based incremental learning algorithm},
year = {2008},
issue_date = {September, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {9},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.03.007},
doi = {10.1016/j.patcog.2008.03.007},
abstract = {This paper describes a novel feature selection algorithm for unsupervised clustering, that combines the clustering ensembles method and the population based incremental learning algorithm. The main idea of the proposed unsupervised feature selection algorithm is to search for a subset of all features such that the clustering algorithm trained on this feature subset can achieve the most similar clustering solution to the one obtained by an ensemble learning algorithm. In particular, a clustering solution is firstly achieved by a clustering ensembles method, then the population based incremental learning algorithm is adopted to find the feature subset that best fits the obtained clustering solution. One advantage of the proposed unsupervised feature selection algorithm is that it is dimensionality-unbiased. In addition, the proposed unsupervised feature selection algorithm leverages the consensus across multiple clustering solutions. Experimental results on several real data sets demonstrate that the proposed unsupervised feature selection algorithm is often able to obtain a better feature subset when compared with other existing unsupervised feature selection algorithms.},
journal = {Pattern Recogn.},
month = sep,
pages = {2742–2756},
numpages = {15},
keywords = {Clustering ensembles, Dimensionality unbiased, Population based incremental learning algorithm, Unsupervised feature selection}
}

@inproceedings{10.5555/1642293.1642428,
author = {Raghavan, Hema and Madani, Omid and Jones, Rosie},
title = {InterActive feature selection},
year = {2005},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {We study the effects of feature selection and human feedback on features in active learning settings. Our experiments on a variety of text categorization tasks indicate that there is significant potential in improving classifier performance by feature reweighting, beyond that achieved via selective sampling alone (standard active learning) if we have access to an oracle that can point to the important (most predictive) features. Consistent with previous findings, we find that feature selection based on the labeled training set has little effect. But our experiments on human subjects indicate that human feedback on feature relevance can identify a sufficient proportion (65%) of the most relevant features. Furthermore, these experiments show that feature labeling takes much less (about 1/5th) time than document labeling. We propose an algorithm that interleaves labeling features and documents which significantly accelerates active learning.},
booktitle = {Proceedings of the 19th International Joint Conference on Artificial Intelligence},
pages = {841–846},
numpages = {6},
location = {Edinburgh, Scotland},
series = {IJCAI'05}
}

@inproceedings{10.5555/1991289.1991321,
author = {Solorio-Fern\'{a}ndez, Sa\'{u}l and Carrasco-Ochoa, J. Ariel and Mart\'{\i}nez-Trinidad, Jos\'{e} Fco.},
title = {Hybrid feature selection method for supervised classification based on Laplacian score ranking},
year = {2010},
isbn = {3642159915},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we introduce a new hybrid filter-wrapper method for supervised feature selection, based on the Laplacian Score ranking combined with a wrapper strategy. We propose to rank features with the Laplacian Score to reduce the search space, and then we use this order to find the best feature subset. We compare our method against other based on ranking feature selection methods, namely, Information Gain Attribute Ranking, Relief, Correlation-based Feature Selection, and additionally we include in our comparison a Wrapper Subset Evaluation method. Empirical results over ten real-world datasets from the UCI repository show that our hybrid method is competitive and outperforms in most of the cases to the other feature selection methods used in our experiments.},
booktitle = {Proceedings of the 2nd Mexican Conference on Pattern Recognition: Advances in Pattern Recognition},
pages = {260–269},
numpages = {10},
keywords = {feature ranking, laplacian score, supervised feature selection},
location = {Puebla, Mexico},
series = {MCPR'10}
}

@article{10.1016/j.cageo.2015.07.014,
author = {Demyanov, V. and Backhouse, L. and Christie, M.},
title = {Geological feature selection in reservoir modelling and history matching with Multiple Kernel Learning},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {85},
number = {PB},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2015.07.014},
doi = {10.1016/j.cageo.2015.07.014},
abstract = {There is a continuous challenge in identifying and propagating geologically realistic features into reservoir models. Many of the contemporary geostatistical algorithms are limited by various modelling assumptions, like stationarity or Gaussianity. Another related challenge is to ensure the realistic geological features introduced into a geomodel are preserved during the model update in history matching studies, when the model properties are tuned to fit the flow response to production data. The above challenges motivate exploration and application of other statistical approaches to build and calibrate reservoir models, in particular, methods based on statistical learning.The paper proposes a novel data driven approach - Multiple Kernel Learning (MKL) - for modelling porous property distributions in sub-surface reservoirs. Multiple Kernel Learning aims to extract relevant spatial features from spatial patterns and to combine them in a non-linear way. This ability allows to handle multiple geological scenarios, which represent different spatial scales and a range of modelling concepts/assumptions. Multiple Kernel Learning is not restricted by deterministic or statistical modelling assumptions and, therefore, is more flexible for modelling heterogeneity at different scales and integrating data and knowledge.We demonstrate an MKL application to a problem of history matching based on a diverse prior information embedded into a range of possible geological scenarios. MKL was able to select the most influential prior geological scenarios and fuse the selected spatial features into a multi-scale property model. The MKL was applied to Brugge history matching benchmark example by calibrating the parameters of the MKL reservoir model parameters to production data. The history matching results were compared to the ones obtained from other contemporary approaches - EnKF and kernel PCA with stochastic optimisation.},
journal = {Comput. Geosci.},
month = dec,
pages = {16–25},
numpages = {10},
keywords = {Brugge case study, History matching, Kernel learning, Uncertainty quantification}
}

@inbook{10.5555/1880429.1880436,
author = {Mazumdar, Debasis and Mitra, Soma and Mitra, Sushmita},
title = {Evolutionary-rough feature selection for face recognition},
year = {2010},
isbn = {3642144667},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Elastic Bunch Graph Matching is a feature-based face recognition algorithm which has been used to determine facial attributes from an image. However the dimension of the feature vectors, in case of EBGM, is quite high. Feature selection is a useful preprocessing step for reducing dimensionality, removing irrelevant data, improving learning accuracy and enhancing output comprehensibility.In rough set theory reducts are the minimal subsets of attributes that are necessary and sufficient to represent a correct decision about classification. The high complexity of the problem has motivated investigators to apply various approximation techniques like the multi-objective GAs to find near optimal solutions for reducts.We present here an application of the evolutionary-rough feature selection algorithm to the face recognition problem. The input corresponds to biometric features, modeled as Gabor jets at each node of the EBGM. Reducts correspond to feature subsets of reduced cardinality, for efficiently discriminating between the faces. The whole process is optimized using MOGA. The simulation is performed on large number of Caucasian and Indian faces, using the FERET and CDAC databases. The merit of clustering and their optimality is determined using cluster validity indices. Successful retrieval of faces is also performed.},
booktitle = {Transactions on Rough Sets XII},
pages = {117–142},
numpages = {26}
}

@article{10.1016/j.compbiomed.2015.06.021,
author = {Tekin Erguzel, Turker and Tas, Cumhur and Cebi, Merve},
title = {A wrapper-based approach for feature selection and classification of major depressive disorder-bipolar disorders},
year = {2015},
issue_date = {September 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {64},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2015.06.021},
doi = {10.1016/j.compbiomed.2015.06.021},
abstract = {Feature selection (FS) and classification are consecutive artificial intelligence (AI) methods used in data analysis, pattern classification, data mining and medical informatics. Beside promising studies in the application of AI methods to health informatics, working with more informative features is crucial in order to contribute to early diagnosis. Being one of the prevalent psychiatric disorders, depressive episodes of bipolar disorder (BD) is often misdiagnosed as major depressive disorder (MDD), leading to suboptimal therapy and poor outcomes. Therefore discriminating MDD and BD at earlier stages of illness could help to facilitate efficient and specific treatment. In this study, a nature inspired and novel FS algorithm based on standard Ant Colony Optimization (ACO), called improved ACO (IACO), was used to reduce the number of features by removing irrelevant and redundant data. The selected features were then fed into support vector machine (SVM), a powerful mathematical tool for data classification, regression, function estimation and modeling processes, in order to classify MDD and BD subjects. Proposed method used coherence, a promising quantitative electroencephalography (EEG) biomarker, values calculated from alpha, theta and delta frequency bands. The noteworthy performance of novel IACO-SVM approach stated that it is possible to discriminate 46 BD and 55 MDD subjects using 22 of 48 features with 80.19% overall classification accuracy. The performance of IACO algorithm was also compared to the performance of standard ACO, genetic algorithm (GA) and particle swarm optimization (PSO) algorithms in terms of their classification accuracy and number of selected features. In order to provide an almost unbiased estimate of classification error, the validation process was performed using nested cross-validation (CV) procedure. Display Omitted We used the combination of support vector machine (SVM) and improved Ant Colony Optimization (IACO).We used an improved version of standard Ant Colony Optimization.We used coherence as a biomarker.We reduced the feature set from 48 to 22 using IACO.We modeled an SVM model using QEEG coherence values.We increased overall accuracy from 62.37% to 80.19% and AUC from 0.631 to 0.793 while decreasing the computational complexity and the number of features.},
journal = {Comput. Biol. Med.},
month = sep,
pages = {127–137},
numpages = {11},
keywords = {Artificial intelligence, Bipolar disorder, Coherence, Improved Ant Colony Optimization, Major depressive disorder, Support vector machine}
}

@article{10.1016/j.patcog.2013.03.026,
author = {Fan, Wentao and Bouguila, Nizar},
title = {Variational learning of a Dirichlet process of generalized Dirichlet distributions for simultaneous clustering and feature selection},
year = {2013},
issue_date = {October, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {46},
number = {10},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2013.03.026},
doi = {10.1016/j.patcog.2013.03.026},
abstract = {This paper introduces a novel enhancement for unsupervised feature selection based on generalized Dirichlet (GD) mixture models. Our proposal is based on the extension of the finite mixture model previously developed in [1] to the infinite case, via the consideration of Dirichlet process mixtures, which can be viewed actually as a purely nonparametric model since the number of mixture components can increase as data are introduced. The infinite assumption is used to avoid problems related to model selection (i.e. determination of the number of clusters) and allows simultaneous separation of data in to similar clusters and selection of relevant features. Our resulting model is learned within a principled variational Bayesian framework that we have developed. The experimental results reported for both synthetic data and real-world challenging applications involving image categorization, automatic semantic annotation and retrieval show the ability of our approach to provide accurate models by distinguishing between relevant and irrelevant features without over- or under-fitting the data.},
journal = {Pattern Recogn.},
month = oct,
pages = {2754–2769},
numpages = {16},
keywords = {Clustering, Dirichlet process, Feature selection, Generalized Dirichlet, Image auto-annotation, Images categorization, Infinite mixture models}
}

@article{10.1016/j.neucom.2011.06.023,
author = {Peng, Jian-Xun and Ferguson, Stuart and Rafferty, Karen and Kelly, Paul D.},
title = {An efficient feature selection method for mobile devices with application to activity recognition},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {17},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2011.06.023},
doi = {10.1016/j.neucom.2011.06.023},
abstract = {This paper presents a feature selection method for data classification, which combines a model-based variable selection technique and a fast two-stage subset selection algorithm. The relationship between a specified (and complete) set of candidate features and the class label is modeled using a non-linear full regression model which is linear-in-the-parameters. The performance of a sub-model measured by the sum of the squared-errors (SSE) is used to score the informativeness of the subset of features involved in the sub-model. The two-stage subset selection algorithm approaches a solution sub-model with the SSE being locally minimized. The features involved in the solution sub-model are selected as inputs to support vector machines (SVMs) for classification. The memory requirement of this algorithm is independent of the number of training patterns. This property makes this method suitable for applications executed in mobile devices where physical RAM memory is very limited. An application was developed for activity recognition, which implements the proposed feature selection algorithm and an SVM training procedure. Experiments are carried out with the application running on a PDA for human activity recognition using accelerometer data. A comparison with an information gain-based feature selection method demonstrates the effectiveness and efficiency of the proposed algorithm.},
journal = {Neurocomput.},
month = oct,
pages = {3543–3552},
numpages = {10},
keywords = {Activity recognition, Data classification, Feature selection algorithm, Mobile devices}
}

@inproceedings{10.5555/1191831.1192735,
author = {Iswandy, Kuncup and Koenig, Andreas},
title = {Towards Effective Unbiased Automated Feature Selection},
year = {2006},
isbn = {0769526624},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The selection of relevant and non-redundant features or variables from a larger set is an ubiquitous problem in many disciplines. Numerous automated methods have been introduced, however, the important issue of selection stability is still largely uncovered. It can be observed, that small changes in the data can lead to dramatic changes in the selection. This compromises both statistical reliability and recognition rates as well as knowledge extraction. In our work, we pursue an approach employing data sampling techniques, e.g., leave-one-out method, and generate statistics of selection to determine a stability factor and identify stable features. In this paper, we introduce improved selection techniques from first and second order statistics and demonstrate their effectiveness for three benchmark problems of increasing complexity.},
booktitle = {Proceedings of the Sixth International Conference on Hybrid Intelligent Systems},
pages = {29},
series = {HIS '06}
}

@inproceedings{10.1007/11752790_5,
author = {Mladeni\'{c}, Dunja},
title = {Feature selection for dimensionality reduction},
year = {2005},
isbn = {3540341374},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11752790_5},
doi = {10.1007/11752790_5},
abstract = {Dimensionality reduction is a commonly used step in machine learning, especially when dealing with a high dimensional space of features. The original feature space is mapped onto a new, reduced dimensionally space. The dimensionality reduction is usually performed either by selecting a subset of the original dimensions or/and by constructing new dimensions. This paper deals with feature subset selection for dimensionality reduction in machine learning. We provide a brief overview of the feature subset selection techniques that are commonly used in machine learning. Detailed description is provided for feature subset selection as commonly used on text data. For illustration, we show performance of several methods on document categorization of real-world data.},
booktitle = {Proceedings of the 2005 International Conference on Subspace, Latent Structure and Feature Selection},
pages = {84–102},
numpages = {19},
location = {Bohinj, Slovenia},
series = {SLSFS'05}
}

@article{10.1016/j.eswa.2011.08.051,
author = {Alonso-Atienza, Felipe and Rojo-\'{A}lvarez, Jos\'{e} Luis and Rosado-Mu\~{n}oz, Alfredo and Vinagre, Juan J. and Garc\'{\i}a-Alberola, Arcadi and Camps-Valls, Gustavo},
title = {Feature selection using support vector machines and bootstrap methods for ventricular fibrillation detection},
year = {2012},
issue_date = {February, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.08.051},
doi = {10.1016/j.eswa.2011.08.051},
abstract = {Early detection of ventricular fibrillation (VF) is crucial for the success of the defibrillation therapy in automatic devices. A high number of detectors have been proposed based on temporal, spectral, and time-frequency parameters extracted from the surface electrocardiogram (ECG), showing always a limited performance. The combination ECG parameters on different domain (time, frequency, and time-frequency) using machine learning algorithms has been used to improve detection efficiency. However, the potential utilization of a wide number of parameters benefiting machine learning schemes has raised the need of efficient feature selection (FS) procedures. In this study, we propose a novel FS algorithm based on support vector machines (SVM) classifiers and bootstrap resampling (BR) techniques. We define a backward FS procedure that relies on evaluating changes in SVM performance when removing features from the input space. This evaluation is achieved according to a nonparametric statistic based on BR. After simulation studies, we benchmark the performance of our FS algorithm in AHA and MIT-BIH ECG databases. Our results show that the proposed FS algorithm outperforms the recursive feature elimination method in synthetic examples, and that the VF detector performance improves with the reduced feature set.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1956–1967},
numpages = {12},
keywords = {Arrhythmia classification, Bootstrap, Feature selection, Support vector machines, Ventricular fibrillation detection}
}

@inproceedings{10.1007/978-3-319-11656-3_20,
author = {Habibzadeh, Mehdi and Krzy\.{z}ak, Adam and Fevens, Thomas},
title = {Comparative Study of Feature Selection for White Blood Cell Differential Counts in Low Resolution Images},
year = {2014},
isbn = {9783319116556},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-11656-3_20},
doi = {10.1007/978-3-319-11656-3_20},
abstract = {Features that are widely used in digital image analysis and pattern recognition tasks are from three main categories: shape, intensity, and texture invariant features. For computer-aided diagnosis in medical imaging for many specific types of medical problem, the most effective choice of a subset of these features through feature selection is still an open problem. In this work, we consider the problem of white blood cell (leukocyte) recognition into their five primary types: Neutrophils, Lymphocytes, Eosinophils, Monocytes and Basophils using a Support Vector Machine classifier. For features, we use four main intensity histogram calculations, set of 11 invariant moments, the relative area, co-occurrence and run-length matrices, dual tree complex wavelet transform, Haralick and Tamura features. Global sensitivity analysis using Sobol's RS-HDMR which can deal with independent and dependent input variables is used to assess dominate discriminatory power and the reliability of feature models in presence of high dimensional input feature data to build an efficient feature selection. Both the numerical and empirical results of experiments are compared with forward sequential feature selection. Finally, the results obtained from the preliminary analysis of white blood cell classification are presented in confusion matrices and interpreted using Cohen's kappa (  \"{\i} ) with the classification framework being validated with experiments conducted on poor quality white blood cell images.},
booktitle = {Proceedings of the 6th IAPR TC 3 International Workshop on Artificial Neural Networks in Pattern Recognition - Volume 8774},
pages = {216–227},
numpages = {12}
}

@inproceedings{10.5555/1981848.1981852,
author = {Wu, Weijun and Gao, Qigang and Wang, Muhong},
title = {Extended fast feature selection for classification modeling},
year = {2006},
isbn = {9608457475},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {The performance of a classification algorithm in data mining is greatly affected by the quality of data source. Irrelevant and redundant features of data not only increase the cost of mining process, but also degrade the quality of the result in some cases. This issue is particularly important to high-dimensional data, in that many features may either irrelevant or redundant for a selected classification target. Accordingly, feature selection becomes an essential part in data preparation. The feature selection for classification is to identify and remove irrelevant and redundant features, which do not contribute to modeling for a selected target. Among the existing feature selection methods, fast correlation-based filter and correlation-based feature selection are most commonly used approaches. The main concern of the these methods is that they may over simplify the features of a given data set by removing many useful features because of certain inherent limitation in these methods. As a result, the selected feature set may be over-simplified to be useful in practice. In this paper, we analyze the existing issue, and present an extended fast feature selection algorithm to overcome the problem. Experiments are conducted using real data from financial institutions to demonstrate the improvement in terms of quality of selected features. A result comparison between the proposed method and other three major methods is provided.},
booktitle = {Proceedings of the 10th WSEAS International Conference on Computers},
pages = {13–18},
numpages = {6},
keywords = {classification, correlation, feature redundancy, feature selection, filter model, relevant features},
location = {Athens, Greece},
series = {ICCOMP'06}
}

@article{10.1016/j.imavis.2008.10.010,
author = {Levi, Dan and Ullman, Shimon},
title = {Learning to classify by ongoing feature selection},
year = {2010},
issue_date = {April, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {28},
number = {4},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2008.10.010},
doi = {10.1016/j.imavis.2008.10.010},
abstract = {Existing classification algorithms use a set of training examples to select classification features, which are then used for all future applications of the classifier. A major problem with this approach is the selection of a training set: a small set will result in reduced performance, and a large set will require extensive training. In addition, class appearance may change over time requiring an adaptive classification system. In this paper, we propose a solution to these basic problems by developing an on-line feature selection method, which continuously modifies and improves the features used for classification based on the examples provided so far. The method is used for learning a new class, and to continuously improve classification performance as new data becomes available. In ongoing learning, examples are continuously presented to the system, and new features arise from these examples. The method continuously measures the value of the selected features using mutual information, and uses these values to efficiently update the set of selected features when new training information becomes available. The problem is challenging because at each stage the training process uses a small subset of the training data. Surprisingly, with sufficient training data the on-line process reaches the same performance as a scheme that has a complete access to the entire training data.},
journal = {Image Vision Comput.},
month = apr,
pages = {715–723},
numpages = {9},
keywords = {Object recognition, Online learning}
}

@inproceedings{10.1109/ICDMW.2009.35,
author = {Hulse, Jason Van and Khoshgoftaar, Taghi M. and Napolitano, Amri and Wald, Randall},
title = {Feature Selection with High-Dimensional Imbalanced Data},
year = {2009},
isbn = {9780769539027},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICDMW.2009.35},
doi = {10.1109/ICDMW.2009.35},
abstract = {Feature selection is an important topic in data mining, especially for high dimensional datasets. Filtering techniques in particular have received much attention, but detailed comparisons of their performance is lacking. This work considers three filters using classifier performance metrics and six commonly-used filters. All nine filtering techniques are compared and contrasted using five different microarray expression datasets. In addition, given that these datasets exhibit an imbalance between the number of positive and negative examples, the utilization of sampling techniques in the context of feature selection is examined.},
booktitle = {Proceedings of the 2009 IEEE International Conference on Data Mining Workshops},
pages = {507–514},
numpages = {8},
series = {ICDMW '09}
}

@article{10.1016/j.sigpro.2012.07.037,
author = {Elguebaly, Tarek and Bouguila, Nizar},
title = {Simultaneous Bayesian clustering and feature selection using RJMCMC-based learning of finite generalized Dirichlet mixture models},
year = {2013},
issue_date = {June, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
number = {6},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2012.07.037},
doi = {10.1016/j.sigpro.2012.07.037},
abstract = {Selecting relevant features in multidimensional data is important in several pattern analysis and image processing applications. The goal of this paper is to propose a Bayesian approach for identifying clusters of proportional data based on the selection of relevant features. More specifically, we consider the problem of selecting relevant features in unsupervised settings when generalized Dirichlet mixture models are considered to model and cluster proportional data. The learning of the proposed statistical model, to formulate the unsupervised feature selection problem, is carried out using a powerful reversible jump Markov chain Monte Carlo (RJMCMC) technique. Experiments involving the challenging problems of human action videos categorization, pedestrian detection and face recognition indicate that the proposed approach is efficient.},
journal = {Signal Process.},
month = jun,
pages = {1531–1546},
numpages = {16},
keywords = {Bayesian analysis, Clustering, Face recognition, Feature selection, Finite mixture models, Generalized Dirichlet, Gibbs sampling, Metropolis-Hastings, Pedestrian detection, RJMCMC, Video categorization}
}

@inproceedings{10.5555/1760894.1760958,
author = {Liu, Huan and Yu, Lei and Dash, Manoranjan and Motoda, Hiroshi},
title = {Active feature selection using classes},
year = {2003},
isbn = {3540047603},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature selection is frequently used in data pre-processing for data mining. When the training data set is too large, sampling is commonly used to overcome the difficulty. This work investigates the applicability of active sampling in feature selection in a filter model setting. Our objective is to partition data by taking advantage of class information so as to achieve the same or better performance for feature selection with fewer but more relevant instances than random sampling. Two versions of active feature selection that employ class information are proposed and empirically evaluated. In comparison with random sampling, we conduct extensive experiments with benchmark data sets, and analyze reasons why class-based active feature selection works in the way it does. The results will help us deal with large data sets and provide ideas to scale up other feature selection algorithms.},
booktitle = {Proceedings of the 7th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {474–485},
numpages = {12},
location = {Seoul, Korea},
series = {PAKDD '03}
}

@article{10.1016/j.eswa.2009.06.031,
author = {Lutu, Patricia E. N. and Engelbrecht, Andries P.},
title = {A decision rule-based method for feature selection in predictive data mining},
year = {2010},
issue_date = {January, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {1},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.06.031},
doi = {10.1016/j.eswa.2009.06.031},
abstract = {Algorithms for feature selection in predictive data mining for classification problems attempt to select those features that are relevant, and are not redundant for the classification task. A relevant feature is defined as one which is highly correlated with the target function. One problem with the definition of feature relevance is that there is no universally accepted definition of what it means for a feature to be 'highly correlated with the target function or highly correlated with the other features'. A new feature selection algorithm which incorporates domain specific definitions of high, medium and low correlations is proposed in this paper. The proposed algorithm conducts a heuristic search for the most relevant features for the prediction task.},
journal = {Expert Syst. Appl.},
month = jan,
pages = {602–609},
numpages = {8},
keywords = {Feature selection, Feature subset search, Predictive data mining}
}

@article{10.1016/j.asoc.2015.05.030,
author = {Yuwono, Mitchell and Guo, Ying and Wall, Josh and Li, Jiaming and West, Sam and Platt, Glenn and Su, Steven W.},
title = {Unsupervised feature selection using swarm intelligence and consensus clustering for automatic fault detection and diagnosis in Heating Ventilation and Air Conditioning systems},
year = {2015},
issue_date = {September 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {34},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.05.030},
doi = {10.1016/j.asoc.2015.05.030},
abstract = {Graphical abstractDisplay Omitted HighlightsOur algorithm aims to improve the feature quality in general fault diagnosis system.The algorithm filters out redundant features using consensus evolutionary clustering.The algorithm was tested on the ASHRAE-1312-RP experimental fault data.Sensitivity &amp; specificity were &gt;95%, with considerably less false positives up to as low as 1.6%. Various sensory and control signals in a Heating Ventilation and Air Conditioning (HVAC) system are closely interrelated which give rise to severe redundancies between original signals. These redundancies may cripple the generalization capability of an automatic fault detection and diagnosis (AFDD) algorithm. This paper proposes an unsupervised feature selection approach and its application to AFDD in a HVAC system. Using Ensemble Rapid Centroid Estimation (ERCE), the important features are automatically selected from original measurements based on the relative entropy between the low- and high-frequency features. The materials used is the experimental HVAC fault data from the ASHRAE-1312-RP datasets containing a total of 49 days of various types of faults and corresponding severity. The features selected using ERCE (Median normalized mutual information (NMI)=0.019) achieved the least redundancies compared to those selected using manual selection (Median NMI=0.0199) Complete Linkage (Median NMI=0.1305), Evidence Accumulation K-means (Median NMI=0.04) and Weighted Evidence Accumulation K-means (Median NMI=0.048). The effectiveness of the feature selection method is further investigated using two well-established time-sequence classification algorithms: (a) Nonlinear Auto-Regressive Neural Network with eXogenous inputs and distributed time delays (NARX-TDNN); and (b) Hidden Markov Models (HMM); where weighted average sensitivity and specificity of: (a) higher than 99% and 96% for NARX-TDNN; and (b) higher than 98% and 86% for HMM is observed. The proposed feature selection algorithm could potentially be applied to other model-based systems to improve the fault detection performance.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {402–425},
numpages = {24},
keywords = {Consensus clustering, Ensemble Rapid Centroid Estimation (ERCE), Fault detection and diagnosis, Feature selection, Heating Ventilation and Air Conditioning (HVAC) system, Particle Swarm Optimization}
}

@inproceedings{10.5555/3044805.3044906,
author = {Jawanpuria, Pratik and Varma, Manik and Nath, J. Saketha},
title = {On p-norm path following in multiple Kernel learning for non-linear feature selection},
year = {2014},
publisher = {JMLR.org},
abstract = {Our objective is to develop formulations and algorithms for efficiently computing the feature selection path - i.e. the variation in classification accuracy as the fraction of selected features is varied from null to unity. Multiple Kernel Learning subject to lp≤1 regularization (lp-MKL) has been demonstrated to be one of the most effective techniques for non-linear feature selection. However, state-of-the-art lp-MKL algorithms are too computationally expensive to be invoked thousands of times to determine the entire path.We propose a novel conjecture which states that, for certain lp-MKL formulations, the number of features selected in the optimal solution monotonically decreases as p is decreased from an initial value to unity. We prove the conjecture, for a generic family of kernel target alignment based formulations, and show that the feature weights themselves decay (grow) monotonically once they are below (above) a certain threshold at optimality. This allows us to develop a path following algorithm that systematically generates optimal feature sets of decreasing size. The proposed algorithm sets certain feature weights directly to zero for potentially large intervals of p thereby reducing optimization costs while simultaneously providing approximation guarantees.We empirically demonstrate that our formulation can lead to classification accuracies which are as much as 10% higher on benchmark data sets not only as compared to other lp-MKL formulations and uniform kernel baselines but also leading feature selection methods. We further demonstrate that our algorithm reduces training time significantly over other path following algorithms and state-of-the-art lp-MKL optimizers such as SMO-MKL. In particular, we generate the entire feature selection path for data sets with a hundred thousand features in approximately half an hour on standard hardware. Entire path generation for such data set is well beyond the scaling capabilities of other methods.},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–118–II–126},
location = {Beijing, China},
series = {ICML'14}
}

@article{10.1016/j.eswa.2013.09.023,
author = {Rodrigues, Douglas and Pereira, Lu\'{\i}s A. M. and Nakamura, Rodrigo Y. M. and Costa, Kelton A. P. and Yang, Xin-She and Souza, Andr\'{e} N. and Papa, Jo\~{a}o Paulo},
title = {A wrapper approach for feature selection based on Bat Algorithm and Optimum-Path Forest},
year = {2014},
issue_date = {April, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.09.023},
doi = {10.1016/j.eswa.2013.09.023},
abstract = {Besides optimizing classifier predictive performance and addressing the curse of the dimensionality problem, feature selection techniques support a classification model as simple as possible. In this paper, we present a wrapper feature selection approach based on Bat Algorithm (BA) and Optimum-Path Forest (OPF), in which we model the problem of feature selection as an binary-based optimization technique, guided by BA using the OPF accuracy over a validating set as the fitness function to be maximized. Moreover, we present a methodology to better estimate the quality of the reduced feature set. Experiments conducted over six public datasets demonstrated that the proposed approach provides statistically significant more compact sets and, in some cases, it can indeed improve the classification effectiveness.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2250–2258},
numpages = {9},
keywords = {Bat Algorithm, Dimensionality reduction, Optimum-Path Forest, Swarm intelligence}
}

@article{10.1016/j.patcog.2009.12.013,
author = {Mart\'{\i}nez Sotoca, Jos\'{e} and Pla, Filiberto},
title = {Supervised feature selection by clustering using conditional mutual information-based distances},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.013},
doi = {10.1016/j.patcog.2009.12.013},
abstract = {In this paper, a supervised feature selection approach is presented, which is based on metric applied on continuous and discrete data representations. This method builds a dissimilarity space using information theoretic measures, in particular conditional mutual information between features with respect to a relevant variable that represents the class labels. Applying a hierarchical clustering, the algorithm searches for a compression of the information contained in the original set of features. The proposed technique is compared with other state of art methods also based on information measures. Eventually, several experiments are presented to show the effectiveness of the features selected from the point of view of classification accuracy.},
journal = {Pattern Recogn.},
month = jun,
pages = {2068–2081},
numpages = {14},
keywords = {Clustering, Conditional mutual information, Supervised feature selection}
}

@article{10.1016/j.cose.2009.01.001,
author = {Li, Yang and Wang, Jun-Li and Tian, Zhi-Hong and Lu, Tian-Bo and Young, Chen},
title = {Building lightweight intrusion detection system using wrapper-based feature selection mechanisms},
year = {2009},
issue_date = {September, 2009},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {28},
number = {6},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2009.01.001},
doi = {10.1016/j.cose.2009.01.001},
abstract = {Intrusion Detection System (IDS) is an important and necessary component in ensuring network security and protecting network resources and network infrastructures. How to build a lightweight IDS is a hot topic in network security. Moreover, feature selection is a classic research topic in data mining and it has attracted much interest from researchers in many fields such as network security, pattern recognition and data mining. In this paper, we effectively introduced feature selection methods to intrusion detection domain. We propose a wrapper-based feature selection algorithm aiming at building lightweight intrusion detection system by using modified random mutation hill climbing (RMHC) as search strategy to specify a candidate subset for evaluation, as well as using modified linear Support Vector Machines (SVMs) iterative procedure as wrapper approach to obtain the optimum feature subset. We verify the effectiveness and the feasibility of our feature selection algorithm by several experiments on KDD Cup 1999 intrusion detection dataset. The experimental results strongly show that our approach is not only able to speed up the process of selecting important features but also to yield high detection rates. Furthermore, our experimental results indicate that intrusion detection system with feature selection algorithm has better performance than that without feature selection algorithm both in detection performance and computational cost.},
journal = {Comput. Secur.},
month = sep,
pages = {466–475},
numpages = {10},
keywords = {Feature selection, Intrusion detection system, Modified RMHC, Modified linear SVMs, Network security}
}

@article{10.1016/j.eswa.2011.02.181,
author = {Yang, Yang and Liao, Yinxia and Meng, Guang and Lee, Jay},
title = {A hybrid feature selection scheme for unsupervised learning and its application in bearing fault diagnosis},
year = {2011},
issue_date = {September, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {9},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.02.181},
doi = {10.1016/j.eswa.2011.02.181},
abstract = {With the development of the condition-based maintenance techniques and the consequent requirement for good machine learning methods, new challenges arise in unsupervised learning. In the real-world situations, due to the relevant features that could exhibit the real machine condition are often unknown as priori, condition monitoring systems based on unimportant features, e.g. noise, might suffer high false-alarm rates, especially when the characteristics of failures are costly or difficult to learn. Therefore, it is important to select the most representative features for unsupervised learning in fault diagnostics. In this paper, a hybrid feature selection scheme (HFS) for unsupervised learning is proposed to improve the robustness and the accuracy of fault diagnostics. It provides a general framework of the feature selection based on significance evaluation and similarity measurement with respect to the multiple clustering solutions. The effectiveness of the proposed HFS method is demonstrated by a bearing fault diagnostics application and comparison with other features selection methods.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11311–11320},
numpages = {10},
keywords = {Fault diagnostics, Feature selection, Unsupervised learning}
}

@inproceedings{10.1145/1835804.1835901,
author = {Yu, Guan and Huang, Ruizhang and Wang, Zhaojun},
title = {Document clustering via dirichlet process mixture model with feature selection},
year = {2010},
isbn = {9781450300551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1835804.1835901},
doi = {10.1145/1835804.1835901},
abstract = {One essential issue of document clustering is to estimate the appropriate number of clusters for a document collection to which documents should be partitioned. In this paper, we propose a novel approach, namely DPMFS, to address this issue. The proposed approach is designed 1) to group documents into a set of clusters while the number of document clusters is determined by the Dirichlet process mixture model automatically; 2) to identify the discriminative words and separate them from irrelevant noise words via stochastic search variable selection technique. We explore the performance of our proposed approach on both a synthetic dataset and several realistic document datasets. The comparison between our proposed approach and stage-of-the-art document clustering approaches indicates that our approach is robust and effective for document clustering.},
booktitle = {Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {763–772},
numpages = {10},
keywords = {dirichlet process mixture model, document clustering, feature selection},
location = {Washington, DC, USA},
series = {KDD '10}
}

@article{10.1155/2015/140820,
author = {Zheng, Yuhuang},
title = {Human activity recognition based on the hierarchical feature selection and classification framework},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {2090-0147},
url = {https://doi.org/10.1155/2015/140820},
doi = {10.1155/2015/140820},
abstract = {Human activity recognition via triaxial accelerometers can provide valuable information for evaluating functional abilities. In this paper, we present an accelerometer sensor-based approach for human activity recognition. Our proposed recognition method used a hierarchical scheme, where the recognition of ten activity classes was divided into five distinct classification problems. Every classifier used the Least Squares Support Vector Machine (LS-SVM) and Naive Bayes (NB) algorithm to distinguish different activity classes. The activity class was recognized based on the mean, variance, entropy of magnitude, and angle of triaxial accelerometer signal features. Our proposed activity recognition method recognized ten activities with an average accuracy of 95.6% using only a single triaxial accelerometer.},
journal = {JECE},
month = jan,
articleno = {34},
numpages = {1}
}

@inproceedings{10.1145/1390334.1390490,
author = {Peng, Jie and Macdonald, Craig and Ounis, Iadh},
title = {Automatic document prior feature selection for web retrieval},
year = {2008},
isbn = {9781605581644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390334.1390490},
doi = {10.1145/1390334.1390490},
abstract = {Document prior features, such as Pagerank and URL depth, can improve the retrieval effectiveness of Web Information Retrieval (IR) systems. However, not all queries equally benefit from the application of a document prior feature. This paper aims to investigate whether the retrieval performance can be further enhanced by selecting the best document prior feature on a per-query basis. We present a novel method for selecting the best document prior feature on a per-query basis. We evaluate our technique on the TREC .GOV Web test collection and its associated TREC 2003 Web search tasks. Our experiments demonstrate the effectiveness and robustness of our proposed selection method.},
booktitle = {Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {761–762},
numpages = {2},
keywords = {prior feature selection},
location = {Singapore, Singapore},
series = {SIGIR '08}
}

@article{10.1016/j.eswa.2011.01.120,
author = {Chen, Hui-Ling and Yang, Bo and Liu, Jie and Liu, Da-You},
title = {A support vector machine classifier with rough set-based feature selection for breast cancer diagnosis},
year = {2011},
issue_date = {July, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {7},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.01.120},
doi = {10.1016/j.eswa.2011.01.120},
abstract = {Breast cancer is becoming a leading cause of death among women in the whole world, meanwhile, it is confirmed that the early detection and accurate diagnosis of this disease can ensure a long survival of the patients. Expert systems and machine learning techniques are gaining popularity in this field because of the effective classification and high diagnostic capability. In this paper, a rough set (RS) based supporting vector machine classifier (RS_SVM) is proposed for breast cancer diagnosis. In the proposed method (RS_SVM), RS reduction algorithm is employed as a feature selection tool to remove the redundant features and further improve the diagnostic accuracy by SVM. The effectiveness of the RS_SVM is examined on Wisconsin Breast Cancer Dataset (WBCD) using classification accuracy, sensitivity, specificity, confusion matrix and receiver operating characteristic (ROC) curves. Experimental results demonstrate the proposed RS_SVM can not only achieve very high classification accuracy but also detect a combination of five informative features, which can give an important clue to the physicians for breast diagnosis.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {9014–9022},
numpages = {9},
keywords = {Breast cancer diagnosis, Feature selection, Rough set theory, Support vector machines}
}

@inproceedings{10.5555/1885721.1885773,
author = {Yao, Na and Lin, Zongjian and Zhang, Jingxiong},
title = {Feature selection based on mutual information and its application in hyperspectral image classification},
year = {2010},
isbn = {3642152791},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper investigates mutual information-based feature selection for high dimensional hyperspectral imagery, which accounts for both the relevance of features on classes and the redundancy among features. A representative method shortly known as min-redundancy and max-relevance (mRMR) was adopted and compared with a baseline method called Max- Relevance (MR) in experiments with AVIRIS hyperspectral data. Supervised classifications were also carried out to identify classification accuracies obtainable with hyperspectral data of reduced dimensionality through five different classifiers. The results confirm that mRMR is more discrimination- informative than MR in feature selection due to the additional redundancy analysis. Different classifiers with different accuracies manifest that a more impact but more informative subset may exist. However, the intrinsic dimensionality which indicates the optimal performance of a classifier remains an issue for further investigation.},
booktitle = {Proceedings of the 4th International Conference on Knowledge Science, Engineering and Management},
pages = {561–566},
numpages = {6},
keywords = {feature selection, hyperspectral image, mutual information, redundancy, relevance, supervised classification},
location = {Belfast, Northern Ireland, UK},
series = {KSEM'10}
}

@article{10.1007/s10916-009-9301-x,
author = {Huang, Mei-Ling and Hung, Yung-Hsiang and Chen, Wei-Yu},
title = {Neural Network Classifier with Entropy Based Feature Selection on Breast Cancer Diagnosis},
year = {2010},
issue_date = {October   2010},
publisher = {Plenum Press},
address = {USA},
volume = {34},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-009-9301-x},
doi = {10.1007/s10916-009-9301-x},
abstract = {The aim of this research is to combine the feature selection (FS) and optimization algorithms as the optimal tool to improve the learning performance like predictive accuracy of the Wisconsin Breast Cancer Dataset classification. An ensemble of the reduced data patterns based on FS was used to train a neural network (NN) using the Levenberg---Marquardt (LM) and the Particle Swarm Optimization (PSO) algorithms to devise the appropriate NN training weighting parameters, and then construct an effective Neural Network classifier to improve the Wisconsin Breast Cancers' classification accuracy and efficiency. Experimental results show that the accuracy and AROC improved emphatically, and the best performance in accuracy and AROC are 98.83% and 0.9971, respectively.},
journal = {J. Med. Syst.},
month = oct,
pages = {865–873},
numpages = {9},
keywords = {Back-propagation neural network (BPNN), Classification accuracy, Feature selection, Levenberg---Marquardt (LM), Particle Swarm Optimization (PSO)}
}

@article{10.1016/j.patrec.2007.05.011,
author = {Huang, Jinjie and Cai, Yunze and Xu, Xiaoming},
title = {A hybrid genetic algorithm for feature selection wrapper based on mutual information},
year = {2007},
issue_date = {October, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {28},
number = {13},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2007.05.011},
doi = {10.1016/j.patrec.2007.05.011},
abstract = {In this study, a hybrid genetic algorithm is adopted to find a subset of features that are most relevant to the classification task. Two stages of optimization are involved. The outer optimization stage completes the global search for the best subset of features in a wrapper way, in which the mutual information between the predictive labels of a trained classifier and the true classes serves as the fitness function for the genetic algorithm. The inner optimization performs the local search in a filter manner, in which an improved estimation of the conditional mutual information acts as an independent measure for feature ranking taking account of not only the relevance of the candidate feature to the output classes but also the redundancy to the already-selected features. The inner and outer optimizations cooperate with each other and achieve the high global predictive accuracy as well as the high local search efficiency. Experimental results demonstrate both parsimonious feature selection and excellent classification accuracy of the method on a range of benchmark data sets.},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {1825–1844},
numpages = {20},
keywords = {Feature selection, Hybrid genetic algorithm, Machine learning, Mutual information, Pattern classification}
}

@inproceedings{10.5555/1945847.1945920,
author = {Subramanian, Arun and Mehrotra, Kishan G. and Mohan, Chilukuri K. and Varshney, Pramod K. and Damarla, Thyagaraju},
title = {Feature selection and occupancy classification using seismic sensors},
year = {2010},
isbn = {3642130240},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we consider the problem of indoor surveillance and propose a feature selection scheme for occupancy classification in an indoor environment. The classifier aims to determine whether there is exactly one occupant or more than one occupant. Data are obtained from six seismic sensors (geophones) that are deployed in a typical building hallway. Four proposed features exploit amplitude and temporal characteristics of the seismic time series. A neural network classifier achieves performance ranging between 77% to 95% on the test data, depending on the type of construction of the location in the building being monitored.},
booktitle = {Proceedings of the 23rd International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems - Volume Part II},
pages = {605–614},
numpages = {10},
location = {Cordoba, Spain},
series = {IEA/AIE'10}
}

@inproceedings{10.5555/1926680.1926683,
author = {Sohafi-Bonab, Javad and Aghdam, Mehdi Hosseinzadeh},
title = {Feature selection using ant colony optimization for text-independent speaker verification system},
year = {2010},
isbn = {3642164927},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {With the growing trend toward remote security verification procedures for telephone banking, biometric security measures and similar applications, automatic speaker verification (ASV) has received a lot of attention in recent years. The complexity of ASV system and its verification time depends on the number of feature vectors, their dimensionality, the complexity of the speaker models and the number of speakers. In this paper, we concentrate on optimizing dimensionality of feature space by selecting relevant features. It presents another method that is based on ant colony optimization (ACO). The performance of the proposed algorithm is compared to the performance of genetic algorithm on the task of feature selection in TIMIT corpora. The results of experiments indicate that with the optimized feature set, the performance of the ASV system is improved.},
booktitle = {Proceedings of the 5th International Conference on Advances in Computation and Intelligence},
pages = {13–24},
numpages = {12},
keywords = {ant colony optimization, feature selection, gaussian mixture model universal background model, genetic algorithm, speaker verification},
location = {Wuhan, China},
series = {ISICA'10}
}

@phdthesis{10.5555/1023019,
author = {Yang, Jaekyung and Olafsson, Sigurdur},
title = {Scalable optimization-based feature selection with application to recommender systems},
year = {2003},
publisher = {Iowa State University},
address = {USA},
abstract = {Along with development of a variety of data mining techniques, numerous feature selection methods have been introduced to reduce dimensionality. This may improve scalability and make interpreting learning models easier. In this dissertation a new optimization based feature selection method using the nested partition (NP) approach is presented, including both basic analysis of the NP framework and numerical results on various experiment problems. The numerical results show how the optimal structure of the NP makes contributions on a feature selection process. Further, it is addressed how the new intelligent partitioning method obtains very high quality partition efficiently. The feature selection method is implemented as both a filter and a wrapper. In addition, the scalability of the algorithm, which is the most significant issue in mining large databases, is also dealt with according to the instance dimension, the feature dimension, and new features adaptation. However, since the NP naturally handle the feature dimension effectively, the dissertation mostly focuses on scalability with respect to the instance dimension. In this research problem, two systematic approaches to improve scalability of instance dimension are presented, which both utilize random sampling. Through this study, a predicted best solution for the size of instance samples is presented using a two-stage version of the NP that also incorporates statistical selection, and a heuristic solution is as well presented in a new adaptive version of the algorithm. Numerical results report that those two approaches are effective for scalability improvement, and perform better than the generic NP method that uses a static sampling approach. In order to have the NP feature selection method flexible for handling mixed type of features, feature quality evaluators are introduced to determine the order of partitioning with experiment results reporting which one performs better based on a data domain. Finally as a case study, a recommender system that can be effectively used in B2B (business to business) e-business systems is provided using classification, association rules and the new NP-based feature selection method.},
note = {AAI3118270}
}

@article{10.1016/j.asoc.2015.03.036,
author = {Ahila, R. and Sadasivam, V. and Manimala, K.},
title = {An integrated PSO for parameter determination and feature selection of ELM and its application in classification of power system disturbances},
year = {2015},
issue_date = {July 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {32},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.03.036},
doi = {10.1016/j.asoc.2015.03.036},
abstract = {A new learning method for power system disturbances is introduced using extreme learning machine.Simultaneously optimize the feature subset and model selection for the ELM using PSO.Proposed method can improve convergence accuracy and generalization performance of ELM. This paper presents a performance enhancement scheme for the recently developed extreme learning machine (ELM) for classifying power system disturbances using particle swarm optimization (PSO). Learning time is an important factor while designing any computational intelligent algorithms for classifications. ELM is a single hidden layer neural network with good generalization capabilities and extremely fast learning capacity. In ELM, the input weights are chosen randomly and the output weights are calculated analytically. However, ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases. One of the advantages of ELM over other methods is that the parameter that the user must properly adjust is the number of hidden nodes only. But the optimal selection of its parameter can improve its performance. In this paper, a hybrid optimization mechanism is proposed which combines the discrete-valued PSO with the continuous-valued PSO to optimize the input feature subset selection and the number of hidden nodes to enhance the performance of ELM. The experimental results showed the proposed algorithm is faster and more accurate in discriminating power system disturbances.},
journal = {Appl. Soft Comput.},
month = jul,
pages = {23–37},
numpages = {15},
keywords = {Extreme learning machine, Feature selection, Particle swarm optimization, Wavelet transforms}
}

@article{10.1016/j.asoc.2011.03.026,
author = {Yu, Jianbo},
title = {A hybrid feature selection scheme and self-organizing map model for machine health assessment},
year = {2011},
issue_date = {July, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {11},
number = {5},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2011.03.026},
doi = {10.1016/j.asoc.2011.03.026},
abstract = {The sensitivity of various features that are characteristics of machine health may vary considerably under different operation conditions. Thus it is critical to devise a systematic feature selection (FS) scheme that provides a useful and automatic guidance on choosing the most representative features for machine health assessment without human intervention. This paper presents a hybrid feature selection scheme named HFS based on the combination of Gaussian mixture models (GMM) and K-means. The proposed scheme is based on unsupervised learning technique, which does not need too much prior knowledge to improve its utility in real-world applications. The effectiveness of the scheme was evaluated experimentally on bearing test-beds, using degradation prediction and visualization approach where self-organizing map (SOM) model was employed. A novel health assessment indication, i.e., log likelihood probability (LLP) is proposed to provide a comprehensible indication to quantify machine health states. A Bayesian-inference-based failure probability (BIP) approach is proposed to calculate the failure risk probability of machines. This paper focuses on identifying the health states of the key machine component (i.e., bearing) under the assumption that the predictable abnormal patterns are not available. The proposed scheme has shown to provide the good machine health assessment performance with reduced feature inputs. The experimental results indicate its potential applications as an effective tool for machine health assessment.},
journal = {Appl. Soft Comput.},
month = jul,
pages = {4041–4054},
numpages = {14},
keywords = {Bayesian inference, Failure probability, Feature selection, Machine health degradation assessment, Self-organizing map}
}

@article{10.5555/1340786.1343153,
author = {Guo, Baofeng and Damper, R. I. and Gunn, Steve R. and Nelson, J. D. B.},
title = {A fast separability-based feature-selection method for high-dimensional remotely sensed image classification},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0031-3203},
abstract = {Because of the difficulty of obtaining an analytic expression for Bayes error, a wide variety of separability measures has been proposed for feature selection. In this paper, we show that there is a general framework based on the criterion of mutual information (MI) that can provide a realistic solution to the problem of feature selection for high-dimensional data. We give a theoretical argument showing that the MI of multi-dimensional data can be broken down into several one-dimensional components, which makes numerical evaluation much easier and more accurate. It also reveals that selection based on the simple criterion of only retaining features with high associated MI values may be problematic when the features are highly correlated. Although there is a direct way of selecting features by jointly maximising MI, this suffers from combinatorial explosion. Hence, we propose a fast feature-selection scheme based on a 'greedy' optimisation strategy. To confirm the effectiveness of this scheme, simulations are carried out on 16 land-cover classes using the 92AV3C data set collected from the 220-dimensional AVIRIS hyperspectral sensor. We replicate our earlier positive results (which used an essentially heuristic method for MI-based band-selection) but with much reduced computational cost and a much sounder theoretical basis.},
journal = {Pattern Recogn.},
month = may,
pages = {1653–1662},
numpages = {10},
keywords = {Feature selection, Hyperspectral image classification, Mutual information, Remote sensing}
}

@article{10.1016/j.ijar.2010.05.003,
author = {Pizzi, Nick J. and Pedrycz, Witold},
title = {Aggregating multiple classification results using fuzzy integration and stochastic feature selection},
year = {2010},
issue_date = {October, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {51},
number = {8},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2010.05.003},
doi = {10.1016/j.ijar.2010.05.003},
abstract = {Classifying magnetic resonance spectra is often difficult due to the curse of dimensionality; scenarios in which a high-dimensional feature space is coupled with a small sample size. We present an aggregation strategy that combines predicted disease states from multiple classifiers using several fuzzy integration variants. Rather than using all input features for each classifier, these multiple classifiers are presented with different, randomly selected, subsets of the spectral features. Results from a set of detailed experiments using this strategy are carefully compared against classification performance benchmarks. We empirically demonstrate that the aggregated predictions are consistently superior to the corresponding prediction from the best individual classifier.},
journal = {Int. J. Approx. Reasoning},
month = oct,
pages = {883–894},
numpages = {12},
keywords = {Computational intelligence, Data classification, Feature selection, Fuzzy integrals, Fuzzy sets, Pattern recognition}
}

@inproceedings{10.1007/978-3-540-76390-1_86,
author = {Destrero, Augusto and De Mol, Christine and Odone, Francesca and Verri, Alessandro},
title = {A Regularized Approach to Feature Selection for Face Detection},
year = {2007},
isbn = {978-3-540-76389-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-76390-1_86},
doi = {10.1007/978-3-540-76390-1_86},
abstract = {In this paper we present a trainable method for selecting features from an overcomplete dictionary of measurements. The starting point is a thresholded version of the Landweber algorithm for providing a sparse solution to a linear system of equations. We consider the problem of face detection and adopt rectangular features as an initial representation for allowing straightforward comparisons with existing techniques. For computational efficiency and memory requirements, instead of implementing the full optimization scheme on tenths of thousands of features, we propose to first solve a number of smaller size optimization problems obtained by randomly sub-sampling the feature vector, and then recombining the selected features. The obtained set is still highly redundant, so we further apply feature selection. The final feature selection system is an efficient two-stages architecture. Experimental results of an optimized version of the method on face images and image sequences indicate that this method is a serious competitor of other feature selection schemes recently popularized in computer vision for dealing with problems of real time object detection.},
booktitle = {Computer Vision – ACCV 2007: 8th Asian Conference on Computer Vision, Tokyo, Japan, November 18-22, 2007, Proceedings, Part II},
pages = {881–890},
numpages = {10},
keywords = {Feature Selection, Face Detection, Regularize Approach, Overcomplete Dictionary, Face Detection System},
location = {Tokyo, Japan}
}

@article{10.1016/j.knosys.2015.11.013,
author = {Yijing, Li and Haixiang, Guo and Xiao, Liu and Yanan, Li and Jinling, Li},
title = {Adapted ensemble classification algorithm based on multiple classifier system and feature selection for classifying multi-class imbalanced data},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {94},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.11.013},
doi = {10.1016/j.knosys.2015.11.013},
abstract = {Learning from imbalanced data, where the number of observations in one class is significantly rarer than in other classes, has gained considerable attention in the data mining community. Most existing literature focuses on binary imbalanced case while multi-class imbalanced learning is barely mentioned. What's more, most proposed algorithms treated all imbalanced data consistently and aimed to handle all imbalanced data with a versatile algorithm. In fact, the imbalanced data varies in their imbalanced ratio, dimension and the number of classes, the performances of classifiers for learning from different types of datasets are different. In this paper we propose an adaptive multiple classifier system named of AMCS to cope with multi-class imbalanced learning, which makes a distinction among different kinds of imbalanced data. The AMCS includes three components, which are, feature selection, resampling and ensemble learning. Each component of AMCS is selected discriminatively for different types of imbalanced data. We consider two feature selection methods, three resampling mechanisms, five base classifiers and five ensemble rules to construct a selection pool, the adapting criterion of choosing each component from the selection pool to frame AMCS is analyzed through empirical study. In order to verify the effectiveness of AMCS, we compare AMCS with several state-of-the-art algorithms, the results show that AMCS can outperform or be comparable with the others. At last, AMCS is applied in oil-bearing reservoir recognition. The results indicate that AMCS makes no mistake in recognizing characters of layers for oilsk81-oilsk85 well logging data which is collected in Jianghan oilfield of China.},
journal = {Know.-Based Syst.},
month = feb,
pages = {88–104},
numpages = {17},
keywords = {Adaptive learning, Imbalanced data, Multiple classifier system, Oil reservoir}
}

@inproceedings{10.1145/1553374.1553427,
author = {Helleputte, Thibault and Dupont, Pierre},
title = {Partially supervised feature selection with regularized linear models},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553427},
doi = {10.1145/1553374.1553427},
abstract = {This paper addresses feature selection techniques for classification of high dimensional data, such as those produced by microarray experiments. Some prior knowledge may be available in this context to bias the selection towards some dimensions (genes) a priori assumed to be more relevant. We propose a feature selection method making use of this partial supervision. It extends previous works on embedded feature selection with linear models including regularization to enforce sparsity. A practical approximation of this technique reduces to standard SVM learning with iterative rescaling of the inputs. The scaling factors depend here on the prior knowledge but the final selection may depart from it. Practical results on several microarray data sets show the benefits of the proposed approach in terms of the stability of the selected gene lists with improved classification performances.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {409–416},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@article{10.1016/j.patrec.2004.09.044,
author = {Bhatt, Rajen B. and Gopal, M.},
title = {On fuzzy-rough sets approach to feature selection},
year = {2005},
issue_date = {15 May 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {26},
number = {7},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2004.09.044},
doi = {10.1016/j.patrec.2004.09.044},
abstract = {In this paper, we have shown that the fuzzy-rough set attribute reduction algorithm [Jenson, R., Shen, Q., 2002. Fuzzy-rough sets for descriptive dimensionality reduction. In: Proceedings of IEEE International Conference on Fuzzy Systems, FUZZ-IEEE'02, May 12-17, pp. 29-34] is not convergent on many real datasets due to its poorly designed termination criteria; and the computational complexity of the algorithm increases exponentially with increase in the number of input variables and in multiplication with the size of data patterns. Based on natural properties of fuzzy t-norm and t-conorm, we have put forward the concept of fuzzy-rough sets on compact computational domain, which is then utilized to improve the computational efficiency of FRSAR algorithm. Speed up factor as high as 622 have been achieved with this concept with improved accuracy. We also restructure the algorithm with efficient termination criteria to achieve the convergence on all the datasets and to improve the reliability of selected set of features.},
journal = {Pattern Recogn. Lett.},
month = may,
pages = {965–975},
numpages = {11},
keywords = {Computational complexity, Feature selection, Fuzzy sets, Rough sets}
}

@article{10.1016/j.eswa.2007.07.023,
author = {Cho, Hyun-Woo and Jeong, Myong K.},
title = {Enhanced prediction of misalignment conditions from spectral data using feature selection and filtering},
year = {2008},
issue_date = {July, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {1–2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.07.023},
doi = {10.1016/j.eswa.2007.07.023},
abstract = {This paper proposes a novel method for the use of genetic algorithm-based feature selection and signal filtering to construct reliable calibration models of shaft misalignment. Determination and selection of the key feature(s) is crucial to the predictive performance of calibration models. Even with proper feature selection, the predictive performance of calibration models can be enhanced by filtering the raw spectral data. This improvement results because a filter removes the unwanted variation of predictor variables that is orthogonal to response variables. This is the first work that attempts to develop a systematic calibration model based on genetic algorithm-based feature selection and orthogonal filtering. A case study shows that the proposed calibration model for shaft misalignment conditions produces better predictive performance than traditional multivariate statistical approaches such as principal component regression and partial least squares.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {451–458},
numpages = {8},
keywords = {Calibration model, Feature selection, Genetic algorithm (GA), Orthogonal signal filter, Parallel and angular misalignment, Partial least squares (PLS), Principal component regression (PCR)}
}

@article{10.1016/j.asoc.2007.03.013,
author = {Yue, Xun and Mo, Hongwei and Chi, Zhong-Xian},
title = {Immune-inspired incremental feature selection technology to data streams},
year = {2008},
issue_date = {March, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {2},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2007.03.013},
doi = {10.1016/j.asoc.2007.03.013},
abstract = {As data streams are gaining prominence in a growing number of emerging applications, advanced analysis and mining of data streams is becoming increasingly important. In this paper, an immune-inspired incremental feature selection algorithm called ISFaiNET is proposed as a solution for mining data streams, immune network memory antibody set which is far less than the size of data streams is design as a sketch data set. We can get the change features to the most extent by this set. ISFaiNET have the ability of feature extraction of dynamically tracking increasing huge size information by introducing increment strategy such as window mechanism. The empirical results for our algorithm are presented and discussed which demonstrate acceptable accuracy coupled with efficiency in running time.},
journal = {Appl. Soft Comput.},
month = mar,
pages = {1041–1049},
numpages = {9},
keywords = {Artificial immune network, Data streams, Incremental feature selection}
}

@article{10.1016/j.eswa.2012.05.011,
author = {Gorunescu, Florin and Belciug, Smaranda and Gorunescu, Marina and Badea, Radu},
title = {Intelligent decision-making for liver fibrosis stadialization based on tandem feature selection and evolutionary-driven neural network},
year = {2012},
issue_date = {December, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {17},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.05.011},
doi = {10.1016/j.eswa.2012.05.011},
abstract = {Hepatic fibrosis represents the principal pointer to the development of liver diseases. The correct evaluation of its degree, based on both recent non-invasive procedures and machine learning models, is of current major concern. One of the latest medical imaging methodologies for assessing it is the Fibroscan, supported by biochemical and clinical examinations. Since the complex interaction between the Fibroscan stiffness indicator and the biochemical and clinical results is hard to be manually managed towards the liver fibrosis stadialization, well-performing machine learning algorithms have been proposed to support an automatic diagnosis. We propose in this paper a tandem feature selection mechanism and evolutionary-driven neural network as a computer-based support for liver fibrosis stadialization in chronic hepatitis C. A synergetic system, based on both specific statistical tools and the sensitivity analysis provided by neural networks is used for reducing the dimension of the database from twenty-five to just six attributes. An evolutionary-trained neural network is developed afterwards for the classification of the liver fibrosis stages. The tandem approach is direct and simple, resulting from embedding the feature selection system into the method structure, in order to dynamically concentrate the search only on the most relevant attributes. Experimental results and a thorough statistical analysis clearly demonstrated the efficiency of the proposed intelligent system in comparison with other machine learning techniques reported in literature.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {12824–12832},
numpages = {9},
keywords = {Evolutionary-trained neural network, Feature selection, Liver fibrosis stadialization, Statistical assessment, Tandem intelligent system}
}

@article{10.1016/j.eswa.2012.11.007,
author = {Stoean, Ruxandra and Stoean, Catalin},
title = {Modeling medical decision making by support vector machines, explaining by rules of evolutionary algorithms with feature selection},
year = {2013},
issue_date = {June, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {7},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.11.007},
doi = {10.1016/j.eswa.2012.11.007},
abstract = {Machine learning support for medical decision making is truly helpful only when it meets two conditions: high prediction accuracy and a good explanation of how the diagnosis was reached. Support vector machines (SVMs) successfully achieve the first target due to a kernel-based engine; evolutionary algorithms (EAs) can greatly accomplish the second owing to their adaptable nature. In this context, the current paper puts forward a two-step hybridized methodology, where learning is accurately performed by the SVMs and a comprehensible emulation of the resulting decision model is generated by EAs in the form of propositional rules, while referring only those indicators that highly influence the class separation. An individual highlighting of the medical attributes that trigger a specific diagnosis for a current patient record is additionally obtained; this feature thus increases the confidence of the physician in the resulting automated diagnosis. Without loss of generality, we aim to model three breast cancer instances, for reasons of both high incidence of the disease and the large application of state of the art artificial intelligence methods for this medical task. As such, the prediction of a benign/malignant condition as well as the recurrence/nonrecurrence of a cancer event are studied on the Wisconsin corresponding data sets from the UCI Machine Learning Repository. The proposed hybridization reached its goals. Rule prototypes evolve against a SVM consistent training data, while diversity among the different classes is implicitly preserved. Feature selection eventually leads to a resulting rule set where only the significant medical indicators together with the discriminating threshold values are referred, while individual relevance of attributes can be additionally obtained for each patient. The gain is thus dual: the EA benefits from a noise-free SVM preprocessed data and the resulting SVM model is able to output rules in a comprehensible, concise format for the physician.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {2677–2686},
numpages = {10},
keywords = {Breast cancer diagnosis, Cooperative coevolution, Evolutionary algorithms, Feature selection, Rule extraction, Support vector machines}
}

@inproceedings{10.5555/2381147.2381156,
author = {Marini, S. and Patan\'{e}, G. and Spagnuolo, M. and Falcidieno, B.},
title = {Feature selection for enhanced spectral shape comparison},
year = {2010},
isbn = {9783905674224},
publisher = {Eurographics Association},
address = {Goslar, DEU},
abstract = {In the context of shape matching, this paper proposes a framework for selecting the Laplacian eigenvalues of 3D shapes that are more relevant for shape comparison and classification. Three approaches are compared to identify a specific set of eigenvalues such that they maximise the retrieval and/or the classification performance on the input benchmark data set: the first k eigenvalues, by varying k over the cardinality of the spectrum; the Hill Climbing technique; and the AdaBoost algorithm. In this way, we demonstrate that the information coded by the whole spectrum is unnecessary and we improve the shape matching results using only a set of selected eigenvalues. Finally, we test the efficacy of the selected eigenvalues by coupling shape classification and retrieval.},
booktitle = {Proceedings of the 3rd Eurographics Conference on 3D Object Retrieval},
pages = {31–38},
numpages = {8},
location = {Norrk\"{o}ping, Sweden},
series = {3DOR '10}
}

@inproceedings{10.5555/1779090.1779103,
author = {Weinshall, Daphna and Zamir, Lior},
title = {Image classification from small sample, with distance learning and feature selection},
year = {2007},
isbn = {3540768556},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Small sample is an acute problem in many application domains, which may be partially addressed by feature selection or dimensionality reduction. For the purpose of distance learning, we describe a method for feature selection using equivalence constraints between pairs of datapoints. The method is based on L1 regularization and optimization. Feature selection is then incorporated into an existing nonparametric method for distance learning, which is based on the boosting of constrained generative models. Thus the final algorithm employs dynamical feature selection, where features are selected anew in each boosting iteration based on the weighted training data. We tested our algorithm on the classification of facial images, using two public domain databases. We show the results of extensive experiments where our method performed much better than a number of competing methods, including the original boosting-based distance learning method and two commonly used Mahalanobis metrics.},
booktitle = {Proceedings of the 3rd International Conference on Advances in Visual Computing - Volume Part II},
pages = {106–115},
numpages = {10},
keywords = {L1 regularization, distance learning, feature selection, small sample},
location = {Lake Tahoe, NV, USA},
series = {ISVC'07}
}

@article{10.1016/j.neunet.2013.07.008,
author = {Ganivada, Avatharam and Ray, Shubhra Sankar and Pal, Sankar K.},
title = {Fuzzy rough sets, and a granular neural network for unsupervised feature selection},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {48},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2013.07.008},
doi = {10.1016/j.neunet.2013.07.008},
abstract = {A granular neural network for identifying salient features of data, based on the concepts of fuzzy set and a newly defined fuzzy rough set, is proposed. The formation of the network mainly involves an input vector, initial connection weights and a target value. Each feature of the data is normalized between 0 and 1 and used to develop granulation structures by a user defined @a-value. The input vector and the target value of the network are defined using granulation structures, based on the concept of fuzzy sets. The same granulation structures are also presented to a decision system. The decision system helps in extracting the domain knowledge about data in the form of dependency factors, using the notion of new fuzzy rough set. These dependency factors are assigned as the initial connection weights of the proposed network. It is then trained using minimization of a novel feature evaluation index in an unsupervised manner. The effectiveness of the proposed network, in evaluating selected features, is demonstrated on several real-life datasets. The results of FRGNN are found to be statistically more significant than related methods in 28 instances of 40 instances, i.e., 70% of instances, using the paired t-test.},
journal = {Neural Netw.},
month = dec,
pages = {91–108},
numpages = {18},
keywords = {Feature evaluation, Granular computing, Rough fuzzy computing, Rule based layered network}
}

@article{10.1016/j.compbiomed.2009.06.014,
author = {Khadivi Heris, Hossein and Seyed Aghazadeh, Babak and Nikkhah-Bahrami, Mansour},
title = {Optimal feature selection for the assessment of vocal fold disorders},
year = {2009},
issue_date = {October, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {10},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2009.06.014},
doi = {10.1016/j.compbiomed.2009.06.014},
abstract = {Unilateral vocal fold paralysis, vocal fold polyp, and vocal fold nodules are the most common types of neurogenic and organic vocal disorders. This article aims to distinguish these types of vocal diseases into four different classes for the purpose of automatic screening. Firstly, the reconstructed signal at each wavelet packet decomposition sub-band in five levels of decomposition with mother wavelet of (db10) is used to extract the nonlinear features of self-similarity and approximate entropy. Also, wavelet packet coefficients are used to measure energy and Shannon entropy features at different spectral sub-bands. Consequently, to find a discriminant feature vector, three different methods have been applied: Davies-Bouldin (DB) criteria, genetic algorithm (GA) with the fitness functions of support vector machine's (SVM) and k-nearest neighbor's (KNN) recognition rates. Finally, obtained feature vectors have been passed on to SVM and KNN classifiers. The results show that a feature vector of length 12 obtained by the optimization method of GA with the fitness function of SVM's recognition rate fed to SVM classifier achieves the highest classification accuracy of 91%. Furthermore, nonlinear features play an important role in pathological voice classification by participating rate of approximately 67% in the optimal feature vector.},
journal = {Comput. Biol. Med.},
month = oct,
pages = {860–868},
numpages = {9},
keywords = {Feature selection, Nonlinear analysis, Voice signal analysis, Wavelet packet}
}

@article{10.1016/j.patcog.2008.08.025,
author = {Bacauskiene, M. and Verikas, A. and Gelzinis, A. and Valincius, D.},
title = {A feature selection technique for generation of classification committees and its application to categorization of laryngeal images},
year = {2009},
issue_date = {May, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {42},
number = {5},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2008.08.025},
doi = {10.1016/j.patcog.2008.08.025},
abstract = {This paper is concerned with a two phase procedure to select salient features (variables) for classification committees. Both filter and wrapper approaches to feature selection are combined in this work. In the first phase, definitely redundant features are eliminated based on the paired t-test. The test compares the saliency of the candidate and the noise features. In the second phase, the genetic search is employed. The search integrates the steps of training, aggregation of committee members, selection of hyper-parameters, and selection of salient features into the same learning process. A small number of genetic iterations needed to find a solution is the characteristic feature of the genetic search procedure developed. The experimental tests performed on five real-world problems have shown that significant improvements in classification accuracy can be obtained in a small number of iterations if compared to the case of using all the features available.},
journal = {Pattern Recogn.},
month = may,
pages = {645–654},
numpages = {10},
keywords = {Classification committee, Feature selection, Genetic search, Laryngeal image, Support vector machine, Variable selection}
}

@article{10.1162/neco.2009.02-09-956,
author = {Famulare, Michael and Fairhall, Adrienne},
title = {Feature selection in simple neurons: How coding depends on spiking dynamics},
year = {2010},
issue_date = {March 2010},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {22},
number = {3},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2009.02-09-956},
doi = {10.1162/neco.2009.02-09-956},
abstract = {The relationship between a neuron's complex inputs and its spiking output defines the neuron's coding strategy. This is frequently and effectively modeled phenomenologically by one or more linear filters that extract the components of the stimulus that are relevant for triggering spikes and a nonlinear function that relates stimulus to firing probability. In many sensory systems, these two components of the coding strategy are found to adapt to changes in the statistics of the inputs in such a way as to improve information transmission. Here, we show for two simple neuron models how feature selectivity as captured by the spike-triggered average depends on both the parameters of the model and the statistical characteristics of the input.},
journal = {Neural Comput.},
month = mar,
pages = {581–598},
numpages = {18}
}

@inproceedings{10.1007/978-3-319-19066-2_19,
author = {Dess\`{\i}, Nicoletta and Pes, Barbara},
title = {Stability in Biomarker Discovery: Does Ensemble Feature Selection Really Help?},
year = {2015},
isbn = {9783319190655},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-19066-2_19},
doi = {10.1007/978-3-319-19066-2_19},
abstract = {Ensemble feature selection has been recently explored as a promising paradigm to improve the stability, i.e. the robustness with respect to sample variation, of subsets of informative features extracted from high-dimensional domains including genetics and medicine. Though recent literature discusses a number of cases where ensemble approaches seem to be capable of providing more stable results, especially in the context of biomarker discovery, there is a lack of systematic studies aiming at providing insight on when, and to which extent, the use of an ensemble method is to be preferred to a simple one. Using a well-known benchmark from the genomics domain, this paper presents an empirical study which evaluates ten selection methods, representatives of different selection approaches, investigating if they get significantly more stable when used in an ensemble fashion. Results of our study provide interesting indications on benefits and limitations of the ensemble paradigm in terms of stability.},
booktitle = {Proceedings of the 28th International Conference on Current Approaches in Applied Artificial Intelligence - Volume 9101},
pages = {191–200},
numpages = {10},
keywords = {Biomarker discovery, Ensemble feature selection, Feature selection stability, High-dimensional data}
}

@article{10.1016/j.eswa.2010.04.084,
author = {Deisy, C. and Baskar, S. and Ramraj, N. and Saravanan Koori, J. and Jeevanandam, P.},
title = {A novel information theoretic-interact algorithm (IT-IN) for feature selection using three machine learning algorithms},
year = {2010},
issue_date = {December, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {12},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.04.084},
doi = {10.1016/j.eswa.2010.04.084},
abstract = {The inclusion of irrelevant, redundant, and inconsistent features in the data-mining model results in poor predictions and high computational overhead. This paper proposes a novel information theoretic-based interact (IT-IN) algorithm, which concerns the relevance, redundancy, and consistency of the features. The proposed IT-IN algorithm is compared with existing Interact, FCBF, Relief and CFS feature selection algorithms. To evaluate the classification accuracy of IT-IN and remaining four feature selection algorithms, Naive Bayes, SVM, and ELM classifier are used for ten UCI repository datasets. The proposed IT-IN performs better than existing above algorithms in terms of number of features. The specially designed hash function is used to speed up the IT-IN algorithms and provides minimum computation time than the Interact algorithms. The result clearly reveals that the proposed feature selection algorithm improves the classification accuracy for ELM, Naive Bayes, and SVM classifiers. The performance of proposed IT-IN with ELM classifier is superior to other classifiers.},
journal = {Expert Syst. Appl.},
month = dec,
pages = {7589–7597},
numpages = {9},
keywords = {Consistency, Correlation, Feature selection, Redundancy, Relevance}
}

@article{10.5555/3225642.3225829,
author = {Das, Kamalika and Bhaduri, Kanishka and Kargupta, Hillol},
title = {A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks},
year = {2010},
issue_date = {September 2010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {0219-1377},
abstract = {In this paper we develop a local distributed privacy preserving algorithm for feature selection in a large peer-to-peer environment. Feature selection is often used in machine learning for data compaction and efficient learning by eliminating the curse of dimensionality. There exist many solutions for feature selection when the data are located at a central location. However, it becomes extremely challenging to perform the same when the data are distributed across a large number of peers or machines. Centralizing the entire dataset or portions of it can be very costly and impractical because of the large number of data sources, the asynchronous nature of the peer-to-peer networks, dynamic nature of the data/network, and privacy concerns. The solution proposed in this paper allows us to perform feature selection in an asynchronous fashion with a low communication overhead where each peer can specify its own privacy constraints. The algorithm works based on local interactions among participating nodes. We present results on real-world dataset in order to test the performance of the proposed algorithm.},
journal = {Knowl. Inf. Syst.},
month = sep,
pages = {341–367},
numpages = {27},
keywords = {Data mining, Distributed computation, Feature selection, Privacy preserving}
}

@article{10.1016/j.patcog.2006.12.009,
author = {Tsang, Chi-Ho and Kwong, Sam and Wang, Hanli},
title = {Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection},
year = {2007},
issue_date = {September, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {40},
number = {9},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2006.12.009},
doi = {10.1016/j.patcog.2006.12.009},
abstract = {Classification of intrusion attacks and normal network traffic is a challenging and critical problem in pattern recognition and network security. In this paper, we present a novel intrusion detection approach to extract both accurate and interpretable fuzzy IF-THEN rules from network traffic data for classification. The proposed fuzzy rule-based system is evolved from an agent-based evolutionary framework and multi-objective optimization. In addition, the proposed system can also act as a genetic feature selection wrapper to search for an optimal feature subset for dimensionality reduction. To evaluate the classification and feature selection performance of our approach, it is compared with some well-known classifiers as well as feature selection filters and wrappers. The extensive experimental results on the KDD-Cup99 intrusion detection benchmark data set demonstrate that the proposed approach produces interpretable fuzzy systems, and outperforms other classifiers and wrappers by providing the highest detection accuracy for intrusion attacks and low false alarm rate for normal network traffic with minimized number of features.},
journal = {Pattern Recogn.},
month = sep,
pages = {2373–2391},
numpages = {19},
keywords = {Feature selection, Fuzzy classifier, Genetic algorithms, Intrusion detection, Multi-objective optimization}
}

@inproceedings{10.1007/11494683_4,
author = {Redpath, D. B. and Lebart, K.},
title = {Observations on boosting feature selection},
year = {2005},
isbn = {3540263063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11494683_4},
doi = {10.1007/11494683_4},
abstract = {This paper presents a study of the Boosting Feature Selection (BFS) algorithm [1], a method which incorporates feature selection into Adaboost. Such an algorithm is interesting as it combines the methods studied by Boosting and ensemble feature selection researchers.Observations are made on generalisation, weighted error and error diversity to compare the algorithms performance to Adaboost while using a nearest mean base learner. Ensemble feature prominence is proposed as a stop criterion for ensemble construction. Its quality assessed using the former performance measures. BFS is found to compete with Adaboost in terms of performance, despite the reduced feature description for each base classifer. This is explained using weighted error and error diversity. Results show the proposed stop criterion to be useful for trading ensemble performance and complexity.},
booktitle = {Proceedings of the 6th International Conference on Multiple Classifier Systems},
pages = {32–41},
numpages = {10},
location = {Seaside, CA},
series = {MCS'05}
}

@article{10.5555/2365896.2365904,
author = {Delimata, Pawel and Suraj, Zbigniew},
title = {Feature Selection Algorithm for Multiple Classifier Systems: A Hybrid Approach},
year = {2008},
issue_date = {January 2008},
publisher = {IOS Press},
address = {NLD},
volume = {85},
number = {1–4},
issn = {0169-2968},
abstract = {Many problems in pattern classification and knowledge discovery require a selection of a subset of attributes or features to represent the patterns to be classified. The approach presented in this paper is designed mostly for multiple classifier systems with homogeneous (identical) classifiers. Such systems require many different subsets of the data set. The problem of finding the best subsets of a given feature set is of exponential complexity. The main aim of this paper is to present ways to improve RBFS algorithm which is a feature selection algorithm. RBFS algorithm is computationally quite complex because it uses all decision-relative reducts of a given decision table. In order to increase its speed, we propose a new algorithm called ARS algorithm. The task of this algorithm is to decrease the number of the decision-relative reducts for a decision table. Experiments have shown that ARS has greatly improved the execution time of the RBFS algorithm. A small loss on the classification accuracy of the multiple classifier used on the subset created by this algorithm has also been observed. To improve classification accuracy the simplified version of the bagging algorithm has been applied. Algorithms have been tested on some benchmarks.},
journal = {Fundam. Inf.},
month = jan,
pages = {97–110},
numpages = {14},
keywords = {diversity measures, feature selection, multiple classifiers, reducts}
}

@article{10.1007/s10489-007-0058-y,
author = {Shen, Qiang and Jensen, Richard},
title = {Approximation-based feature selection and application for algae population estimation},
year = {2008},
issue_date = {April     2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-007-0058-y},
doi = {10.1007/s10489-007-0058-y},
abstract = {This paper presents a data-driven approach for feature selection to address the common problem of dealing with high-dimensional data. This approach is able to handle the real-valued nature of the domain features, unlike many existing approaches. This is accomplished through the use of fuzzy-rough approximations. The paper demonstrates the effectiveness of this research by proposing an estimator of algae populations, a system that approximates, given certain water characteristics, the size of algae populations. This estimator significantly reduces computer time and space requirements, decreases the cost of obtaining measurements and increases runtime efficiency, making itself more viable economically. By retaining only information required for the estimation task, the system offers higher accuracy than conventional estimators. Finally, the system does not alter the domain semantics, making any distilled knowledge human-readable. The paper describes the problem domain, architecture and operation of the system, and provides and discusses detailed experimentation. The results show that algae estimators using a fuzzy-rough feature selection step produce more accurate predictions of algae populations in general.},
journal = {Applied Intelligence},
month = apr,
pages = {167–181},
numpages = {15},
keywords = {Algae population estimation, Classification, Data-driven knowledge acquisition, Feature evaluation and selection, Fuzzy-rough sets}
}

@article{10.5555/1516165.1516173,
author = {Delimata, Pawel and Suraj, Zbigniew},
title = {Feature Selection Algorithm for Multiple Classifier Systems: A Hybrid Approach},
year = {2008},
issue_date = {September 2008},
publisher = {IOS Press},
address = {NLD},
volume = {85},
number = {1–4},
issn = {0169-2968},
abstract = {Many problems in pattern classification and knowledge discovery require a selection of a subset of attributes or features to represent the patterns to be classified. The approach presented in this paper is designed mostly for multiple classifier systems with homogeneous (identical) classifiers. Such systems require many different subsets of the data set. The problem of finding the best subsets of a given feature set is of exponential complexity. The main aim of this paper is to present ways to improve RBFS algorithm which is a feature selection algorithm. RBFS algorithm is computationally quite complex because it uses all decision-relative reducts of a given decision table. In order to increase its speed, we propose a new algorithm called ARS algorithm. The task of this algorithm is to decrease the number of the decision-relative reducts for a decision table. Experiments have shown that ARS has greatly improved the execution time of the RBFS algorithm. A small loss on the classification accuracy of the multiple classifier used on the subset created by this algorithm has also been observed. To improve classification accuracy the simplified version of the bagging algorithm has been applied. Algorithms have been tested on some benchmarks.},
journal = {Fundam. Inf.},
month = jan,
pages = {97–110},
numpages = {14},
keywords = {diversity measures, feature selection, multiple classifiers, reducts}
}

@inproceedings{10.5555/2984093.2984111,
author = {Boutsidis, Christos and Mahoney, Michael W. and Drineas, Petros},
title = {Unsupervised feature selection for the k-means clustering problem},
year = {2009},
isbn = {9781615679119},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We present a novel feature selection algorithm for the k-means clustering problem. Our algorithm is randomized and, assuming an accuracy parameter ∊ ∈ (0,1), selects and appropriately rescales in an unsupervised manner Θ(k log(k/∊)/∊2) features from a dataset of arbitrary dimensions. We prove that, if we run any γ-approximate k-means algorithm (γ ≥ 1) on the features selected using our method, we can find a (1 + (1 + ∊) ≥)-approximate partition with high probability.},
booktitle = {Proceedings of the 23rd International Conference on Neural Information Processing Systems},
pages = {153–161},
numpages = {9},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'09}
}

@article{10.5555/1577069.1755828,
author = {Tuv, Eugene and Borisov, Alexander and Runger, George and Torkkola, Kari},
title = {Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination},
year = {2009},
issue_date = {12/1/2009},
publisher = {JMLR.org},
volume = {10},
issn = {1532-4435},
abstract = {Predictive models benefit from a compact, non-redundant subset of features that improves interpretability and generalization. Modern data sets are wide, dirty, mixed with both numerical and categorical predictors, and may contain interactive effects that require complex models. This is a challenge for filters, wrappers, and embedded feature selection methods. We describe details of an algorithm using tree-based ensembles to generate a compact subset of non-redundant features. Parallel and serial ensembles of trees are combined into a mixed method that can uncover masking and detect features of secondary effect. Simulated and actual examples illustrate the effectiveness of the approach.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1341–1366},
numpages = {26}
}

@article{10.1016/j.csi.2011.10.006,
author = {Tsai, Min-Jen and Wang, Chen-Sheng and Liu, Jung and Yin, Jin-Sheng},
title = {Using decision fusion of feature selection in digital forensics for camera source model identification},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {34},
number = {3},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2011.10.006},
doi = {10.1016/j.csi.2011.10.006},
abstract = {Digital forensics, which identifies the characteristics and origin of a digital device, has become a new field of research. If digital content will serve as evidence in court, similar to its non-digital counterparts, digital forensics can play a crucial role in identifying the source model or device. To achieve this goal, the relationship between an image and its camera model will be explored. Various image-related and hardware-related features are utilized in the proposed model by a support vector machine approach along with decision fusion techniques. Furthermore, the optimum feature subset to achieve the highest accuracy rate is also explored.},
journal = {Comput. Stand. Interfaces},
month = mar,
pages = {292–304},
numpages = {13},
keywords = {Camera model identification, Decision fusion, Digital image forensics, Feature selection}
}

@article{10.5555/2562348.2562719,
author = {Sharifzadeh, Sara and Clemmensen, Line H. and Borggaard, Claus and St\o{}ier, Susanne and Ersb\o{}ll, Bjarne K.},
title = {Supervised feature selection for linear and non-linear regression of L*a*b* color from multispectral images of meat},
year = {2014},
issue_date = {January, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {27},
issn = {0952-1976},
abstract = {In food quality monitoring, color is an important indicator factor of quality. The CIELab (L^@?a^@?b^@?) color space as a device independent color space is an appropriate means in this case. The commonly used colorimeter instruments can neither measure the L^@?a^@?b color in a wide area over the target surface nor in a contact-less mode. However, developing algorithms for conversion of food items images into L^@?a^@?b color space can solve both of these issues. This paper addresses the problem of L^@?a^@?b color prediction from multispectral images of different types of raw meat. The efficiency of using multispectral images instead of the standard RGB is investigated. In addition, it is demonstrated that due to the fiber structure and transparency of raw meat, the prediction models built on the standard color patches do not work for raw meat test samples. As a result, multispectral images of different types of meat samples (430-970nm) were used for training and testing of the L^@?a^@?b prediction models. Finding a sparse solution or the use of a minimum number of bands is of particular interest to make an industrial vision set-up simpler and cost effective. In this paper, a wide range of linear, non-linear, kernel-based regression and sparse regression methods are compared. In order to improve the prediction results of these models, we propose a supervised feature selection strategy which is compared with the Principal component analysis (PCA) as a pre-processing step. The results showed that the proposed feature selection method outperforms the PCA for both linear and non-linear methods. The highest performance was obtained by linear ridge regression applied on the selected features from the proposed Elastic net (EN) -based feature selection strategy. All the best models use a reduced number of wavelengths for each of the L^@?a^@?b components.},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
pages = {211–227},
numpages = {17},
keywords = {Artificial neural networks, L* a* b* color space, Multispectral imaging, Sparse regression, Supervised feature selection, Support vector machine}
}

@article{10.5555/2770958.2771036,
author = {Sharifzadeh, Sara and Clemmensen, Line H. and Borggaard, Claus and St\o{}ier, Susanne and Ersb\o{}ll, Bjarne K.},
title = {Supervised feature selection for linear and non-linear regression of L*a*b* color from multispectral images of meat},
year = {2014},
issue_date = {January 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {27},
number = {C},
issn = {0952-1976},
abstract = {In food quality monitoring, color is an important indicator factor of quality. The CIELab (L*a*b*) color space as a device independent color space is an appropriate means in this case. The commonly used colorimeter instruments can neither measure the L*a*b color in a wide area over the target surface nor in a contact-less mode. However, developing algorithms for conversion of food items images into L*a*b color space can solve both of these issues. This paper addresses the problem of L*a*b color prediction from multispectral images of different types of raw meat. The efficiency of using multispectral images instead of the standard RGB is investigated. In addition, it is demonstrated that due to the fiber structure and transparency of raw meat, the prediction models built on the standard color patches do not work for raw meat test samples. As a result, multispectral images of different types of meat samples (430-970nm) were used for training and testing of the L*a*b prediction models. Finding a sparse solution or the use of a minimum number of bands is of particular interest to make an industrial vision set-up simpler and cost effective. In this paper, a wide range of linear, non-linear, kernel-based regression and sparse regression methods are compared. In order to improve the prediction results of these models, we propose a supervised feature selection strategy which is compared with the Principal component analysis (PCA) as a pre-processing step. The results showed that the proposed feature selection method outperforms the PCA for both linear and non-linear methods. The highest performance was obtained by linear ridge regression applied on the selected features from the proposed Elastic net (EN) -based feature selection strategy. All the best models use a reduced number of wavelengths for each of the L*a*b components.},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
pages = {211–227},
numpages = {17},
keywords = {Artificial neural networks, L* a* b* color space, Multispectral imaging, Sparse regression, Supervised feature selection, Support vector machine}
}

@article{10.1016/j.patrec.2009.12.036,
author = {Ruvolo, Paul and Fasel, Ian and Movellan, Javier R.},
title = {A learning approach to hierarchical feature selection and aggregation for audio classification},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {12},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2009.12.036},
doi = {10.1016/j.patrec.2009.12.036},
abstract = {Audio classification typically involves feeding a fixed set of low-level features to a machine learning method, then performing feature aggregation before or after learning. Instead, we jointly learn a selection and hierarchical temporal aggregation of features, achieving significant performance gains.},
journal = {Pattern Recogn. Lett.},
month = sep,
pages = {1535–1542},
numpages = {8},
keywords = {Audio classification, Feature aggregation, Feature selection, Temporal modeling}
}

@article{10.1504/IJBIDM.2012.048730,
author = {Shanab, Ahmad Abu and Khoshgoftaar, Taghi M. and Wald, Randall and Hulse, Jason Van},
title = {Evaluation of the importance of data pre-processing order when combining feature selection and data sampling},
year = {2012},
issue_date = {August 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1/2},
issn = {1743-8195},
url = {https://doi.org/10.1504/IJBIDM.2012.048730},
doi = {10.1504/IJBIDM.2012.048730},
abstract = {Two problems often encountered in machine learning are class imbalance and high dimensionality. In this paper we compare three different approaches for addressing both problems simultaneously, by applying both data sampling and feature selection. With the first two approaches, sampling is followed by feature selection. In the first approach, the features are selected based on the sampled data, and then the unsampled data is used with just the selected features. The second approach is similar, but the sampled data is used. Finally, with the third approach, feature selection is performed prior to sampling. To compare the approaches, we use seven datasets from different domains, employ nine feature rankers from three different families, apply three sampling techniques, and inject class noise to better simulate real-world datasets. The results show that the second and third approaches are both very good, with the third approach showing a slight (but not statistically significant) lead.},
journal = {Int. J. Bus. Intell. Data Min.},
month = aug,
pages = {116–134},
numpages = {19}
}

@article{10.1016/j.neucom.2004.01.194,
author = {Huang, D. and Chow, Tommy W. S.},
title = {Effective feature selection scheme using mutual information},
year = {2005},
issue_date = {January, 2005},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {63},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2004.01.194},
doi = {10.1016/j.neucom.2004.01.194},
abstract = {This article proposes a novel mutual information-based feature selection scheme. In this scheme, the mutual information is estimated directly in an effective way even when one is handling a relative small data set. At the same time, the computation efficiency of the mutual information estimation is improved by proposing a supervised data compression algorithm. With these contributions, the proposed feature selection scheme is able to effectively identify the salience features. The proposed methodology is compared with the related study through applying to different classification problems in which the number of features ranged from less than 10 to over 12,600. The presented results are very promising and corroborate the contributions of the proposed methodology.},
journal = {Neurocomput.},
month = jan,
pages = {325–343},
numpages = {19},
keywords = {Feature selection, Kernel based density estimator, Quadratic mutual information, Supervised data compression}
}

@inproceedings{10.5555/1786574.1786614,
author = {Yang, Shuang-Hong and Hu, Bao-Gang},
title = {Feature selection by nonparametric Bayes error minimization},
year = {2008},
isbn = {3540681248},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an algorithmic framework for feature selection, which selects a subset of features by minimizing the nonparametric Bayes error. A set of existing algorithms as well as new ones can be derived naturally from this framework. For example, we show that the Relief algorithm greedily attempts to minimize the Bayes error estimated by k-Nearest-Neighbor method. This new interpretation not only reveals the secret behind Relief but also offers various opportunities to improve it or to establish new alternatives. In particular, we develop a new feature weighting algorithm, named Parzen-Relief, which minimizes the Bayes error estimated by Parzen method. Additionally, to enhance its ability to handle imbalanced and multiclass data, we integrate the class distribution with the max-margin objective function, leading to a new algorithm, named MAP-Relief. Comparison on benchmark data sets confirms the effectiveness of the proposed algorithms.},
booktitle = {Proceedings of the 12th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {417–428},
numpages = {12},
location = {Osaka, Japan},
series = {PAKDD'08}
}

@article{10.5555/1314498.1314517,
author = {Gadat, S\'{e}bastien and Younes, Laurent},
title = {A Stochastic Algorithm for Feature Selection in Pattern Recognition},
year = {2007},
issue_date = {12/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efficiency criterion, on the basis of specified classification or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efficient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of fit criterion for classifiers based on variable randomly chosen according to P. We then generate classifiers from the optimal distribution of weights learned on the training set. The method is first tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classification and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {509–547},
numpages = {39}
}

@article{10.5555/1248659.1248678,
author = {Gadat, S\'{e}bastien and Younes, Laurent},
title = {A Stochastic Algorithm for Feature Selection in Pattern Recognition},
year = {2007},
issue_date = {5/1/2007},
publisher = {JMLR.org},
volume = {8},
issn = {1532-4435},
abstract = {We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efficiency criterion, on the basis of specified classification or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efficient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of fit criterion for classifiers based on variable randomly chosen according to P. We then generate classifiers from the optimal distribution of weights learned on the training set. The method is first tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classification and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination.},
journal = {J. Mach. Learn. Res.},
month = may,
pages = {509–547},
numpages = {39}
}

@article{10.5555/1005332.1016787,
author = {Dy, Jennifer G. and Brodley, Carla E.},
title = {Feature Selection for Unsupervised Learning},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {845–889},
numpages = {45}
}

@inproceedings{10.5555/3023476.3023515,
author = {Kim, Seyoung and Xing, Eric},
title = {Feature selection via block-regularized regression},
year = {2008},
isbn = {0974903949},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Identifying co-varying causal elements in very high dimensional feature space with internal structures, e.g., a space with as many as millions of linearly ordered features, as one typically encounters in problems such as whole genome association (WGA) mapping, remains an open problem in statistical learning. We propose a block-regularized regression model for sparse variable selection in a high-dimensional space where the covariates are linearly ordered, and are possibly subject to local statistical linkages (e.g., block structures) due to spacial or temporal proximity of the features. Our goal is to identify a small subset of relevant covariates that are not merely from random positions in the ordering, but grouped as contiguous blocks from large number of ordered covariates. Following a typical linear regression framework between the features and the response, our proposed model employs a sparsity-enforcing Laplacian prior for the regression coefficients, augmented by a 1st-order Markovian process along the feature sequence that "activates" the regression coefficients in a coupled fashion. We describe a sampling-based learning algorithm and demonstrate the performance of our method on simulated and biological data for marker identification under WGA.},
booktitle = {Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence},
pages = {325–332},
numpages = {8},
location = {Helsinki, Finland},
series = {UAI'08}
}

@article{10.1016/j.eswa.2003.10.009,
author = {Buckinx, Wouter and Moons, Elke and Van den Poel, Dirk and Wets, Geert},
title = {Customer-adapted coupon targeting using feature selection},
year = {2004},
issue_date = {May, 2004},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2003.10.009},
doi = {10.1016/j.eswa.2003.10.009},
abstract = {The management of coupon promotions is an important issue for marketing managers since it still is the major promotion medium. However, the distribution of coupons does not go without problems. Although manufacturers and retailers are investing heavily in the attempt to convince as many customers as possible, overall coupon redemption rate is low. This study improves the strategy of retailers and manufacturers concerning their target selection since both parties often end up in a battle for customers. Two separate models are built: one model makes predictions concerning redemption behavior of coupons that are distributed by the retailer while another model does the same for coupons handed out by manufacturers. By means of the feature-selection technique 'Relief-F' the dimensionality of the models is reduced, since it searches for the variables that are relevant for predicting the outcome. In this way, redundant variables are not used in the model-building process. The model is evaluated on real-life data provided by a retailer in Fast Moving Consumer Goods (FMCG). The contributions of this study for retailers as well as manufacturers are three-fold. First, the possibility to classify customers concerning their coupon usage is shown. In addition, it is demonstrated that retailers and manufacturers can stay clear of each other in their marketing campaigns. Finally, the feature-selection technique Relief-F proves to facilitate and optimize the performance of the models.},
journal = {Expert Syst. Appl.},
month = may,
pages = {509–518},
numpages = {10},
keywords = {Classification, Data mining, Feature selection, Retailing}
}

@article{10.1023/A:1008363719778,
author = {Liu, Huan and Setiono, Rudy},
title = {Incremental Feature Selection},
year = {1998},
issue_date = {November-December 1998},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {9},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1023/A:1008363719778},
doi = {10.1023/A:1008363719778},
abstract = {Feature selection is a problem of finding relevant features.
When the number of features of a dataset is large and its number of patterns
is huge, an effective method of feature selection can help in dimensionality
reduction. An incremental probabilistic algorithm is designed and implemented
as an alternative to the exhaustive and heuristic approaches. 
Theoretical analysis is given to 
support the idea of the probabilistic algorithm in finding an optimal or 
near-optimal subset of features. Experimental results suggest that 
(1) the probabilistic algorithm is effective in 
obtaining optimal/suboptimal feature subsets; (2) its incremental version 
expedites feature selection further when the number of patterns is large
and can scale up without sacrificing the quality of selected 
features.},
journal = {Applied Intelligence},
month = nov,
pages = {217–230},
numpages = {14},
keywords = {dimensionality reduction, feature selection, machine learning, pattern recognition}
}

@article{10.1007/s00530-014-0400-2,
author = {Siddiqi, Muhammad Hameed and Ali, Rahman and Khan, Adil Mehmood and Kim, Eun Soo and Kim, Gerard Junghyun and Lee, Sungyoung},
title = {Facial expression recognition using active contour-based face detection, facial movement-based feature extraction, and non-linear feature selection},
year = {2015},
issue_date = {November  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {6},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-014-0400-2},
doi = {10.1007/s00530-014-0400-2},
abstract = {Knowledge about people's emotions can serve as an important context for automatic service delivery in context-aware systems. Hence, human facial expression recognition (FER) has emerged as an important research area over the last two decades. To accurately recognize expressions, FER systems require automatic face detection followed by the extraction of robust features from important facial parts. Furthermore, the process should be less susceptible to the presence of noise, such as different lighting conditions and variations in facial characteristics of subjects. Accordingly, this work implements a robust FER system, capable of providing high recognition accuracy even in the presence of aforementioned variations. The system uses an unsupervised technique based on active contour model for automatic face detection and extraction. In this model, a combination of two energy functions: Chan---Vese energy and Bhattacharyya distance functions are employed to minimize the dissimilarities within a face and maximize the distance between the face and the background. Next, noise reduction is achieved by means of wavelet decomposition, followed by the extraction of facial movement features using optical flow. These features reflect facial muscle movements which signify static, dynamic, geometric, and appearance characteristics of facial expressions. Post-feature extraction, feature selection, is performed using Stepwise Linear Discriminant Analysis, which is more robust in contrast to previously employed feature selection methods for FER. Finally, expressions are recognized using trained HMM(s). To show the robustness of the proposed system, unlike most of the previous works, which were evaluated using a single dataset, performance of the proposed system is assessed in a large-scale experimentation using five publicly available different datasets. The weighted average recognition rate across these datasets indicates the success of employing the proposed system for FER.},
journal = {Multimedia Syst.},
month = nov,
pages = {541–555},
numpages = {15},
keywords = {Active contour, Face detection, Facial expressions, Hidden Markov model, Level set, Optical flow, Stepwise linear discriminant analysis, Wavelet transform}
}

@article{10.1016/j.engappai.2010.11.009,
author = {Chatterjee, Snehamoy and Bhattacherjee, Ashis},
title = {Genetic algorithms for feature selection of image analysis-based quality monitoring model: An application to an iron mine},
year = {2011},
issue_date = {August, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {24},
number = {5},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2010.11.009},
doi = {10.1016/j.engappai.2010.11.009},
abstract = {Measuring the quality parameters of materials at mines is difficult and a costly job. In this paper, an image analysis-based method is proposed efficiently and cost effectively that determines the quality parameters of material. The image features are extracted from the samples collected from a mine and modeled using neural networks against the actual grade values of the samples generated by chemical analysis. The dimensions of the image features are reduced by applying the genetic algorithm. The results showed that only 39 features out of 189 features are sufficient to model the quality parameter. The model was tested with the testing data set and the result revealed that the estimated grade values are in good agreement with the real grade values (R^2=0.77). The developed method was then applied to a case study mine of iron ore. The case study results show that proposed image-based algorithm can be a good alternative for estimating quality parameters of materials at a mine site. The effectiveness of the proposed method was verified by applying it on a limestone deposit and the results revealed that the method performed equally well for the limestone deposit.},
journal = {Eng. Appl. Artif. Intell.},
month = aug,
pages = {786–795},
numpages = {10},
keywords = {Feature selection, Genetic algorithm, Image analysis, Iron ore, Neural network, Quality parameters}
}

@inproceedings{10.1145/1143997.1144248,
author = {Mierswa, Ingo and Wurst, Michael},
title = {Information preserving multi-objective feature selection for unsupervised learning},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144248},
doi = {10.1145/1143997.1144248},
abstract = {In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inherently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1545–1552},
numpages = {8},
keywords = {Pareto front segmentation, multi-objective feature selection, unsupervised learning},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@article{10.1016/j.csda.2010.02.014,
author = {Menjoge, Rajiv S. and Welsch, Roy E.},
title = {A diagnostic method for simultaneous feature selection and outlier identification in linear regression},
year = {2010},
issue_date = {December, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {12},
issn = {0167-9473},
url = {https://doi.org/10.1016/j.csda.2010.02.014},
doi = {10.1016/j.csda.2010.02.014},
abstract = {A diagnostic method along the lines of forward search is proposed to simultaneously study the effect of individual observations and features on the inferences made in linear regression. The method operates by appending dummy variables to the data matrix and performing backward selection on the augmented matrix. It outputs sequences of feature-outlier combinations which can be evaluated by plots similar to those of forward search and includes the capacity to incorporate prior knowledge, in order to mitigate issues such as collinearity. It also allows for alternative ways to understand the selection of the final model. The method is evaluated on five data sets and yields promising results.},
journal = {Comput. Stat. Data Anal.},
month = dec,
pages = {3181–3193},
numpages = {13},
keywords = {Forward search, Robust feature selection, Robust statistics}
}

@inproceedings{10.1007/11559887_11,
author = {Rogers, Jeremy D. and Gunn, Steve R.},
title = {Ensemble algorithms for feature selection},
year = {2004},
isbn = {3540290737},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11559887_11},
doi = {10.1007/11559887_11},
abstract = {Many feature selection algorithms are limited in that they attempt to identify relevant feature subsets by examining the features individually. This paper introduces a technique for determining feature relevance using the average information gain achieved during the construction of decision tree ensembles. The technique introduces a node complexity measure and a statistical method for updating the feature sampling distribution based upon confidence intervals to control the rate of convergence. A feature selection threshold is also derived, using the expected performance of an irrelevant feature. Experiments demonstrate the potential of these methods and illustrate the need for both feature weighting and selection.},
booktitle = {Proceedings of the First International Conference on Deterministic and Statistical Methods in Machine Learning},
pages = {180–198},
numpages = {19},
location = {Sheffield, UK}
}

@article{10.1016/j.jbi.2008.05.006,
author = {Exarchos, Konstantinos P. and Papaloukas, Costas and Exarchos, Themis P. and Troganis, Anastassios N. and Fotiadis, Dimitrios I.},
title = {Prediction of cis/trans isomerization using feature selection and support vector machines},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {42},
number = {1},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2008.05.006},
doi = {10.1016/j.jbi.2008.05.006},
abstract = {In protein structures the peptide bond is found to be in trans conformation in the majority of the cases. Only a small fraction of peptide bonds in proteins is reported to be in cis conformation. Most of these instances (&gt;90%) occur when the peptide bond is an imide (X-Pro) rather than an amide bond (X-nonPro). Due to the implication of cis/trans isomerization in many biologically significant processes, the accurate prediction of the peptide bond conformation is of high interest. In this study, we evaluate the effect of a wide range of features, towards the reliable prediction of both proline and non-proline cis/trans isomerization. We use evolutionary profiles, secondary structure information, real-valued solvent accessibility predictions for each amino acid and the physicochemical properties of the surrounding residues. We also explore the predictive impact of a modified feature vector, which consists of condensed position-specific scoring matrices (PSSMX), secondary structure and solvent accessibility. The best discriminating ability is achieved using the first feature vector combined with a wrapper feature selection algorithm and a support vector machine (SVM). The proposed method results in 70% accuracy, 75% sensitivity and 71% positive predictive value (PPV) in the prediction of the peptide bond conformation between any two amino acids. The output of the feature selection stage is investigated in order to identify discriminatory features as well as the contribution of each neighboring residue in the formation of the peptide bond, thus, advancing our knowledge towards cis/trans isomerization.},
journal = {J. of Biomedical Informatics},
month = feb,
pages = {140–149},
numpages = {10},
keywords = {Peptide bond, Support vector machines, cis/trans Isomerization}
}

@article{10.1162/neco.2009.02-08-702,
author = {Ting, Jo-Anne and D'Souza, Aaron and Vijayakumar, Sethu and Schaal, Stefan},
title = {Efficient learning and feature selection in high-dimensional regression},
year = {2010},
issue_date = {April 2010},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {22},
number = {4},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.2009.02-08-702},
doi = {10.1162/neco.2009.02-08-702},
abstract = {We present a novel algorithm for efficient learning and feature selection in high-dimensional regression problems. We arrive at this model through a modification of the standard regression model, enabling us to derive a probabilistic version of the well-known statistical regression technique of backfitting. Using the expectation-maximization algorithm, along with variational approximation methods to overcome intractability, we extend our algorithm to include automatic relevance detection of the input features. This variational Bayesian least squares (VBLS) approach retains its simplicity as a linear model, but offers a novel statistically robust black-box approach to generalized linear regression with high-dimensional inputs. It can be easily extended to nonlinear regression and classification problems. In particular, we derive the framework of sparse Bayesian learning, the relevance vector machine, with VBLS at its core, offering significant computational and robustness advantages for this class of methods. The iterative nature of VBLS makes it most suitable for real-time incremental learning, which is crucial especially in the application domain of robotics, brain-machine interfaces, and neural prosthetics, where real-time learning of models for control is needed. We evaluate our algorithm on synthetic and neurophysiological data sets, as well as on standard regression and classification benchmark data sets, comparing it with other competitive statistical approaches and demonstrating its suitability as a drop-in replacement for other generalized linear regression techniques.},
journal = {Neural Comput.},
month = apr,
pages = {831–886},
numpages = {56}
}

@inproceedings{10.5555/3121576.3121632,
author = {Helleputte, Thibault and Dupont, Pierre},
title = {Feature selection by transfer learning with linear regularized models},
year = {2009},
isbn = {3642041795},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a novel feature selection method for classification of high dimensional data, such as those produced by microarrays. It includes a partial supervision to smoothly favor the selection of some dimensions (genes) on a new dataset to be classified. The dimensions to be favored are previously selected from similar datasets in large microarray databases, hence performing inductive transfer learning at the feature level. This technique relies on a feature selection method embedded within a regularized linear model estimation. A practical approximation of this technique reduces to linear SVM learning with iterative input rescaling. The scaling factors depend on the selected dimensions from the related datasets. The final selection may depart from those whenever necessary to optimize the classification objective. Experiments on several microarray datasets show that the proposed method both improves the selected gene lists stability, with respect to sampling variation, as well as the classification performances.},
booktitle = {Proceedings of the 2009th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I},
pages = {533–547},
numpages = {15},
location = {Bled, Slovenia},
series = {ECMLPKDD'09}
}

@article{10.1016/j.eswa.2011.02.065,
author = {Hajnayeb, A. and Ghasemloonia, A. and Khadem, S. E. and Moradi, M. H.},
title = {Application and comparison of an ANN-based feature selection method and the genetic algorithm in gearbox fault diagnosis},
year = {2011},
issue_date = {August, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.02.065},
doi = {10.1016/j.eswa.2011.02.065},
abstract = {In this paper, a system based on artificial neural networks (ANNs) was designed to diagnose different types of fault in a gearbox. An experimental set of data was used to verify the effectiveness and accuracy of the proposed method. The system was optimized by eliminating unimportant features using a feature selection method (UTA method). Consequently, the fault detection system operates faster while the classification error decreases or remains constant in some other cases. This method of feature selection is compared with Genetic Algorithm (GA) results. The findings verify that the results of the UTA method are as accurate as GA, despite its simple algorithm.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {10205–10209},
numpages = {5},
keywords = {Artificial neural network, Diagnosis, Feature selection, Genetic Algorithm, Vibration analysis}
}

@inproceedings{10.5555/2981562.2981580,
author = {Blum, Ben and Jordan, Michael I. and Kim, David E. and Das, Rhiju and Bradley, Philip and Baker, David},
title = {Feature selection methods for improving protein structure prediction with Rosetta},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Rosetta is one of the leading algorithms for protein structure prediction today. It is a Monte Carlo energy minimization method requiring many random restarts to find structures with low energy. In this paper we present a resampling technique for structure prediction of small alpha/beta proteins using Rosetta. From an initial round of Rosetta sampling, we learn properties of the energy landscape that guide a subsequent round of sampling toward lower-energy structures. Rather than attempt to fit the full energy landscape, we use feature selection methods—both L1-regularized linear regression and decision trees—to identify structural features that give rise to low energy. We then enrich these structural features in the second sampling round. Results are presented across a benchmark set of nine small alpha/beta proteins demonstrating that our methods seldom impair, and frequently improve, Rosetta's performance.},
booktitle = {Proceedings of the 21st International Conference on Neural Information Processing Systems},
pages = {137–144},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'07}
}

@article{10.1016/j.asoc.2007.10.012,
author = {Lin, Shih-Wei and Lee, Zne-Jung and Chen, Shih-Chieh and Tseng, Tsung-Yuan},
title = {Parameter determination of support vector machine and feature selection using simulated annealing approach},
year = {2008},
issue_date = {September, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {4},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2007.10.012},
doi = {10.1016/j.asoc.2007.10.012},
abstract = {Support vector machine (SVM) is a novel pattern classification method that is valuable in many applications. Kernel parameter setting in the SVM training process, along with the feature selection, significantly affects classification accuracy. The objective of this study is to obtain the better parameter values while also finding a subset of features that does not degrade the SVM classification accuracy. This study develops a simulated annealing (SA) approach for parameter determination and feature selection in the SVM, termed SA-SVM. To measure the proposed SA-SVM approach, several datasets in UCI machine learning repository are adopted to calculate the classification accuracy rate. The proposed approach was compared with grid search which is a conventional method of performing parameter setting, and various other methods. Experimental results indicate that the classification accuracy rates of the proposed approach exceed those of grid search and other approaches. The SA-SVM is thus useful for parameter determination and feature selection in the SVM.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {1505–1512},
numpages = {8},
keywords = {Feature selection, Parameter determination, Simulated annealing, Support vector machines}
}

@inproceedings{10.5555/1880672.1880695,
author = {Auffarth, Benjamin and L\'{o}pez, Maite and Cerquides, Jes\'{u}s},
title = {Comparison of redundancy and relevance measures for feature selection in tissue classification of CT images},
year = {2010},
isbn = {3642143997},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we report on a study on feature selection within the minimum-redundancy maximum-relevance framework. Features are ranked by their correlations to the target vector. These relevance scores are then integrated with correlations between features in order to obtain a set of relevant and least-redundant features. Applied measures of correlation or distributional similarity for redunancy and relevance include Kolmogorov-Smirnov (KS) test, Spearman correlations, Jensen-Shannon divergence, and the sign-test. We introduce a metric called "value difference metric" (VDM) and present a simple measure, which we call "fit criterion" (FC). We draw conclusions about the usefulness of different measures. While KS-test and sign-test provided useful information, Spearman correlations are not fit for comparison of data of different measurement intervals. VDM was very good in our experiments as both redundancy and relevance measure. Jensen-Shannon and the sign-test are good redundancy measure alternatives and FC is a good relevance measure alternative.},
booktitle = {Proceedings of the 10th Industrial Conference on Advances in Data Mining: Applications and Theoretical Aspects},
pages = {248–262},
numpages = {15},
keywords = {distributional similarity, divergence measure, feature selection, relevance and redundancy},
location = {Berlin, Germany},
series = {ICDM'10}
}

@article{10.1016/j.eswa.2011.12.038,
author = {Bouguila, Nizar and Almakadmeh, Khaled and Boutemedjet, Sabri},
title = {A finite mixture model for simultaneous high-dimensional clustering, localized feature selection and outlier rejection},
year = {2012},
issue_date = {June, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {7},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.12.038},
doi = {10.1016/j.eswa.2011.12.038},
abstract = {Model-based approaches and in particular finite mixture models are widely used for data clustering which is a crucial step in several applications of practical importance. Indeed, many pattern recognition, computer vision and image processing applications can be approached as feature space clustering problems. For complex high-dimensional data, however, the use of these approaches presents several challenges such as the presence of many irrelevant features which may affect the speed and also compromise the accuracy of the used learning algorithm. Another problem is the presence of outliers which potentially influence the resulting model's parameters. For this purpose, we propose and discuss an algorithm that partitions a given data set without a priori information about the number of clusters, the saliency of the features or the number of outliers. We illustrate the performance of our approach using different applications involving synthetic data, real data and objects shape clustering.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {6641–6656},
numpages = {16},
keywords = {Clustering, EM, Feature selection, Finite mixture models, Gamma distribution, Integrated likelihood, Maximum likelihood, Outlier rejection, Shape modeling}
}

@article{10.1016/j.compbiomed.2009.09.003,
author = {Asadabadi, Ebrahim Barzegari and Abdolmaleki, Parviz and Barkooie, Seyyed Mohsen Hosseini and Jahandideh, Samad and Rezaei, Mohammad Ali},
title = {A combinatorial feature selection approach to describe the QSAR of dual site inhibitors of acetylcholinesterase},
year = {2009},
issue_date = {December, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {12},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2009.09.003},
doi = {10.1016/j.compbiomed.2009.09.003},
abstract = {Regarding the great potential of dual binding site inhibitors of acetylcholinesterase as the future potent drugs of Alzheimer's disease, this study was devoted to extraction of the most effective structural features of these inhibitors from among a large number of quantitative descriptors. To do this, we adopted a unique approach in quantitative structure-activity relationships. An efficient feature selection method was emphasized in such an approach, using the confirmative results of different routine and novel feature selection methods. The proposed methods generated quite consistent results ensuring the effectiveness of the selected structural features.},
journal = {Comput. Biol. Med.},
month = dec,
pages = {1089–1095},
numpages = {7},
keywords = {Acetylcholinesterase (AChE), Artificial intelligence, Dual (binding) site inhibitors, Feature selection}
}

@inproceedings{10.1007/978-3-642-31346-2_52,
author = {Chen, Cuixian and Wang, Yishi and Chang, Yaw and Ricanek, Karl},
title = {Sensitivity analysis with cross-validation for feature selection and manifold learning},
year = {2012},
isbn = {9783642313455},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31346-2_52},
doi = {10.1007/978-3-642-31346-2_52},
abstract = {The performance of a learning algorithm is usually measured in terms of prediction error. It is important to choose an appropriate estimator of the prediction error. This paper analyzes the statistical properties of the K-fold cross-validation prediction error estimator. It investigates how to compare two algorithms statistically. It also analyzes the sensitivity to the changes in the training/test set. Our main contribution is to experimentally study the statistical property of repeated cross-validation to stabilize the prediction error estimation, and thus to reduce the variance of the prediction error estimator. Our simulation results provide an empirical evidence to this conclusion. The experimental study has been performed on PAL dataset for age estimation task.},
booktitle = {Proceedings of the 9th International Conference on Advances in Neural Networks - Volume Part I},
pages = {458–467},
numpages = {10},
location = {Shenyang, China},
series = {ISNN'12}
}

@inproceedings{10.1145/2425836.2425874,
author = {Bai, Shuang and Matsumoto, Tetsuya and Kudo, Hiroaki and Ohnishi, Noboru and Takeuchi, Yoshinori},
title = {Scene classification based on category-specific representations created through prototype feature selection},
year = {2012},
isbn = {9781450314732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2425836.2425874},
doi = {10.1145/2425836.2425874},
abstract = {In this paper, we propose a novel approach to coping with scene classification. In natural scenes, images from different categories often share similar components. As a result, it is difficult to distinguish them directly. In order to overcome this problem, for an input image, we propose to create a set of category-specific representations and use them to model the image as a probability distribution over the categories. Specifically, we first create a prototype for each scene category by pooling local features of its sample images together. Then, based on the category prototypes, given an input image, its salient local features corresponding to each category are extracted and used for creating a category-specific representation, respectively. Here, salient features for a scene category are defined as features that appear frequently in its instance images. In the classifier training stage, corresponding to one category prototype, the category-specific representations of a set of training images are used to train a multi-class SVM classifier. Thereafter, the obtained SVM classifiers are used to classify another set of training images in a probabilistic way, so that for each category-specific representation of the second set of training images, a probability vector is able to be obtained. Subsequently, all probability vectors of a training image are concatenated as its final representation, which are used to train another SVM classifier. For an unknown image, the same process is applied to it for classification. The proposed method is evaluated on datasets scene categories 8 and scene categories 15, experiment results demonstrated the effectiveness of the proposed method.},
booktitle = {Proceedings of the 27th Conference on Image and Vision Computing New Zealand},
pages = {174–179},
numpages = {6},
keywords = {category prototype, category-specific representation, scene classification},
location = {Dunedin, New Zealand},
series = {IVCNZ '12}
}

@article{10.1155/2015/265637,
author = {Tomar, Divya and Agarwal, Sonali},
title = {Hybrid feature selection based weighted least squares twin support vector machine approach for diagnosing Breast Cancer, Hepatitis, and Diabetes},
year = {2015},
issue_date = {January 2015},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2015},
issn = {1687-7594},
url = {https://doi.org/10.1155/2015/265637},
doi = {10.1155/2015/265637},
abstract = {There is a necessity for analysis of a large amount of data in many fields such as healthcare, business, industries, and agriculture. Therefore, the need of the feature selection (FS) technique for the researchers is quite evident in many fields of science, especially in computer science. Furthermore, an effective FS technique that is best suited to a particular learning algorithm is of great help for the researchers. Hence, this paper proposes a hybrid feature selection (HFS) based efficient disease diagnostic model for Breast Cancer, Hepatitis, and Diabetes. A HFS is an efficient method that combines the positive aspects of both Filter and Wrapper FS approaches. The proposed model adopts weighted least squares twin support vector machine (WLSTSVM) as a classification approach, sequential forward selection (SFS) as a search strategy, and correlation feature selection (CFS) to evaluate the importance of each feature. This model not only selects relevant feature subset but also efficiently deals with the data imbalance problem. The effectiveness of the HFS based WLSTSVM approach is examined on three well-known disease datasets taken from UCI repository with the help of predictive accuracy, sensitivity, specificity, and geometric mean. The experiment confirms that our proposed HFS based WLSTSVM disease diagnostic model can result in positive outcomes.},
journal = {Adv. Artif. Neu. Sys.},
month = jan,
articleno = {1},
numpages = {1}
}

@inproceedings{10.1145/1321440.1321478,
author = {Metzler, Donald A.},
title = {Automatic feature selection in the markov random field model for information retrieval},
year = {2007},
isbn = {9781595938039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321440.1321478},
doi = {10.1145/1321440.1321478},
abstract = {Previous applications of the Markov random field model for information retrieval have used manually chosen features. However, it is often difficult or impossible to know, a priori, the best set of features to use for a given task or data set. Therefore, there is a need to develop automatic feature selection techniques. In this paper we describe a greedy procedure for automatically selecting features to use within the Markov random field model for information retrieval. We also propose a novel, robust method for describing classes of textual information retrieval features. Experimental results, evaluated on standard TREC test collections, show that our feature selection algorithm produces models that are either significantly more effective than, or equally effective as, models with manually selected features, such as those used in the past.},
booktitle = {Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management},
pages = {253–262},
numpages = {10},
keywords = {feature selection, markov random field model, parameter estimation},
location = {Lisbon, Portugal},
series = {CIKM '07}
}

@article{10.1016/j.patcog.2007.05.010,
author = {Lung, Shung-Yung},
title = {Efficient text independent speaker recognition with wavelet feature selection based multilayered neural network using supervised learning algorithm},
year = {2007},
issue_date = {December, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {40},
number = {12},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2007.05.010},
doi = {10.1016/j.patcog.2007.05.010},
abstract = {A wavelet packet feature selection derived by using multilayered neural network for speaker identification is described. The concept of a multilayered neural network is without using a gradient method. First, the outputs of each hidden unit are algebraically determined by an error backpropagation method. Then, the weight parameters are determined by using an exponentially weighted least squares method. Our results have shown that this feature selection introduced better performance than the other methods with respect to the percentages of recognition.},
journal = {Pattern Recogn.},
month = dec,
pages = {3616–3620},
numpages = {5},
keywords = {Neural network, Speaker recognition, Wavelet feature selection}
}

@article{10.1016/j.neucom.2011.03.043,
author = {Zhang, Kui and Li, Yuhua and Scarf, Philip and Ball, Andrew},
title = {Feature selection for high-dimensional machinery fault diagnosis data using multiple models and Radial Basis Function networks},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {17},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2011.03.043},
doi = {10.1016/j.neucom.2011.03.043},
abstract = {The technique of machinery fault diagnosis has been greatly enhanced over recent years with the application of many pattern classification methods. However, these classification methods suffer from the ''curse of dimensionality'' when applied to high-dimensional fault diagnosis data. In order to solve the problem, this paper proposes a hybrid model which combines multiple feature selection models to select the most significant input features from all potentially relevant features. Among the models, eight filter models are used to pre-rank the candidate features. They include data variance, Pearson correlation coefficient, the Relief algorithm, Fisher score, class separability, chi-squared, information gain and gain ratio. These variable ranking models measure features from various perspectives, and lead to different ranking results. Based on the effect of the ranking results on the Radial Basis Function (RBF) classification, a weighted voting scheme is then introduced to re-rank features. Furthermore, two wrapper models, a Binary Search (BS) model and a Sequential Backward Search (SBS) model are utilized to minimize the number of relevant features. To demonstrate the potential for applying the method to machinery fault diagnosis, two case studies are discussed. The experiment results support the conclusion that this method is useful for revealing fault-related frequency features.},
journal = {Neurocomput.},
month = oct,
pages = {2941–2952},
numpages = {12},
keywords = {Binary Search, Fault diagnosis, Feature selection, Radial Basis Function networks, Sequential Backward Search}
}

@article{10.5555/2567709.2567740,
author = {Mairal, Julien and Yu, Bin},
title = {Supervised feature selection in graphs with path coding penalties and network flows},
year = {2013},
issue_date = {January 2013},
publisher = {JMLR.org},
volume = {14},
number = {1},
issn = {1532-4435},
abstract = {We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called "path coding" penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efficiently solve by leveraging network flow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {2449–2485},
numpages = {37},
keywords = {convex and non-convex optimization, graph sparsity, network flow optimization}
}

@article{10.1016/j.neucom.2015.04.069,
title = {Intelligent fault diagnosis of rotating machinery using support vector machine with ant colony algorithm for synchronous feature selection and parameter optimization},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.04.069},
doi = {10.1016/j.neucom.2015.04.069},
abstract = {The failure of rotating machinery can result in fatal damage and economic loss since rotating machinery plays an important role in the modern manufacturing industry. The development of a reliable and efficient intelligent fault diagnosis approach is an ongoing attempt. Support vector machine (SVM) is a widely used machine learning method in intelligent fault diagnosis. But finding out good features that can discriminate different fault conditions and optimizing parameters for support vector machine can be regarded as the most two important problems that can highly affect the final diagnosis accuracy of support vector machine. Until now, the two issues of feature selection and parameter optimization are usually treated separately, weakening the effects of both efforts. Therefore, an ant colony algorithm for synchronous feature selection and parameter optimization for support vector machine in intelligent fault diagnosis of rotating machinery is presented. Comparing with other methods, the advantages of the proposed method are evaluated on an experiment of rotor system and an engineering application of locomotive roller bearings, which proves it can attain much better results.},
journal = {Neurocomput.},
month = nov,
pages = {260–279},
numpages = {20}
}

@article{10.1016/j.artmed.2007.07.002,
author = {Huang, Yue and McCullagh, Paul and Black, Norman and Harper, Roy},
title = {Feature selection and classification model construction on type 2 diabetic patients' data},
year = {2007},
issue_date = {November, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {41},
number = {3},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2007.07.002},
doi = {10.1016/j.artmed.2007.07.002},
abstract = {Objective: Diabetes affects between 2% and 4% of the global population (up to 10% in the over 65 age group), and its avoidance and effective treatment are undoubtedly crucial public health and health economics issues in the 21st century. The aim of this research was to identify significant factors influencing diabetes control, by applying feature selection to a working patient management system to assist with ranking, classification and knowledge discovery. The classification models can be used to determine individuals in the population with poor diabetes control status based on physiological and examination factors. Methods: The diabetic patients' information was collected by Ulster Community and Hospitals Trust (UCHT) from year 2000 to 2004 as part of clinical management. In order to discover key predictors and latent knowledge, data mining techniques were applied. To improve computational efficiency, a feature selection technique, feature selection via supervised model construction (FSSMC), an optimisation of ReliefF, was used to rank the important attributes affecting diabetic control. After selecting suitable features, three complementary classification techniques (Naive Bayes, IB1 and C4.5) were applied to the data to predict how well the patients' condition was controlled. Results: FSSMC identified patients' 'age', 'diagnosis duration', the need for 'insulin treatment', 'random blood glucose' measurement and 'diet treatment' as the most important factors influencing blood glucose control. Using the reduced features, a best predictive accuracy of 95% and sensitivity of 98% was achieved. The influence of factors, such as 'type of care' delivered, the use of 'home monitoring', and the importance of 'smoking' on outcome can contribute to domain knowledge in diabetes control. Conclusion: In the care of patients with diabetes, the more important factors identified: patients' 'age', 'diagnosis duration' and 'family history', are beyond the control of physicians. Treatment methods such as 'insulin', 'diet' and 'tablets' (a variety of oral medicines) may be controlled. However lifestyle indicators such as 'body mass index' and 'smoking status' are also important and may be controlled by the patient. This further underlines the need for public health education to aid awareness and prevention. More subtle data interactions need to be better understood and data mining can contribute to the clinical evidence base. The research confirms and to a lesser extent challenges current thinking. Whilst fully appreciating the requirement for clinical verification and interpretation, this work supports the use of data mining as an exploratory tool, particularly as the domain is suffering from a data explosion due to enhanced monitoring and the (potential) storage of this data in the electronic health record. FSSMC has proved a useful feature estimator for large data sets, where processing efficiency is an important factor.},
journal = {Artif. Intell. Med.},
month = nov,
pages = {251–262},
numpages = {12},
keywords = {Blood glucose, Classification, Data mining, Feature selection, Type 2 diabetes}
}

@article{10.1155/S111086570321115X,
author = {Slonim, Noam and Bejerano, Gill and Fine, Shai and Tishby, Naftali},
title = {Discriminative feature selection via multiclass variable memory Markov model},
year = {2003},
issue_date = {January 2003},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2003},
issn = {1110-8657},
url = {https://doi.org/10.1155/S111086570321115X},
doi = {10.1155/S111086570321115X},
abstract = {We propose a novel feature selection method based on a variable memory Markov (VMM) model. The VMM was originally proposed as a generative model trying to preserve the original source statistics from training data. We extend this technique to simultaneously handle several sources, and further apply a new criterion to prune out nondiscriminative features out of the model. This results in a multiclass discriminative VMM (DVMM), which is highly efficient, scaling linearly with data size. Moreover, we suggest a natural scheme to sort the remaining features based on their discriminative power with respect to the sources at hand. We demonstrate the utility of our method for text and protein classification tasks.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {93–102},
numpages = {10},
keywords = {feature selection, multiclass discriminative analysis, variable memory Markov (VMM) model}
}

@article{10.1016/j.neucom.2010.01.017,
author = {Crone, Sven F. and Kourentzes, Nikolaos},
title = {Feature selection for time series prediction - A combined filter and wrapper approach for neural networks},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {73},
number = {10–12},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2010.01.017},
doi = {10.1016/j.neucom.2010.01.017},
abstract = {Modelling artificial neural networks for accurate time series prediction poses multiple challenges, in particular specifying the network architecture in accordance with the underlying structure of the time series. The data generating processes may exhibit a variety of stochastic or deterministic time series patterns of single or multiple seasonality, trends and cycles, overlaid with pulses, level shifts and structural breaks, all depending on the discrete time frequency in which it is observed. For heterogeneous datasets of time series, such as the 2008 ESTSP competition, a universal methodology is required for automatic network specification across varying data patterns and time frequencies. We propose a fully data driven forecasting methodology that combines filter and wrapper approaches for feature selection, including automatic feature evaluation, construction and transformation. The methodology identifies time series patterns, creates and transforms explanatory variables and specifies multilayer perceptrons for heterogeneous sets of time series without expert intervention. Examples of the valid and reliable performance in comparison to established benchmark methods are shown for a set of synthetic time series and for the ESTSP'08 competition dataset, where the proposed methodology obtained second place.},
journal = {Neurocomput.},
month = jun,
pages = {1923–1936},
numpages = {14},
keywords = {Artificial neural networks, Automatic model specification, Feature selection, Forecasting, Input variable selection, Time series prediction}
}

@inproceedings{10.1145/1008992.1009034,
author = {Mladeni\'{c}, Dunja and Brank, Janez and Grobelnik, Marko and Milic-Frayling, Natasa},
title = {Feature selection using linear classifier weights: interaction with classification models},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009034},
doi = {10.1145/1008992.1009034},
abstract = {This paper explores feature scoring and selection based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Na\"{\i}ve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {234–241},
numpages = {8},
keywords = {SVM normal, feature scoring, feature selection, information retrieval, linear SVM, text classification, vector representation},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}

@article{10.1007/s11263-008-0180-2,
author = {Destrero, Augusto and Mol, Christine and Odone, Francesca and Verri, Alessandro},
title = {A Regularized Framework for Feature Selection in Face Detection and Authentication},
year = {2009},
issue_date = {June      2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {83},
number = {2},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-008-0180-2},
doi = {10.1007/s11263-008-0180-2},
abstract = {This paper proposes a general framework for selecting features in the computer vision domain--i.e., learning descriptions from data--where the prior knowledge related to the application is confined in the early stages. The main building block is a regularization algorithm based on a penalty term enforcing sparsity. The overall strategy we propose is also effective for training sets of limited size and reaches competitive performances with respect to the state-of-the-art. To show the versatility of the proposed strategy we apply it to both face detection and authentication, implementing two modules of a monitoring system working in real time in our lab. Aside from the choices of the feature dictionary and the training data, which require prior knowledge on the problem, the proposed method is fully automatic. The very good results obtained in different applications speak for the generality and the robustness of the framework.},
journal = {Int. J. Comput. Vision},
month = jun,
pages = {164–177},
numpages = {14},
keywords = {Face authentication, Face detection, Feature selection, Lasso regression, Learning from examples, Real-time system, Regularized methods, Thresholded Landweber}
}

@article{10.1016/j.artmed.2007.09.005,
author = {Cho, Baek Hwan and Yu, Hwanjo and Kim, Kwang-Won and Kim, Tae Hyun and Kim, In Young and Kim, Sun I.},
title = {Application of irregular and unbalanced data to predict diabetic nephropathy using visualization and feature selection methods},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {42},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2007.09.005},
doi = {10.1016/j.artmed.2007.09.005},
abstract = {Objective: Diabetic nephropathy is damage to the kidney caused by diabetes mellitus. It is a common complication and a leading cause of death in people with diabetes. However, the decline in kidney function varies considerably between patients and the determinants of diabetic nephropathy have not been clearly identified. Therefore, it is very difficult to predict the onset of diabetic nephropathy accurately with simple statistical approaches such as t-test or @g^2-test. To accurately predict the onset of diabetic nephropathy, we applied various machine learning techniques to irregular and unbalanced diabetes dataset, such as support vector machine (SVM) classification and feature selection methods. Visualization of the risk factors was another important objective to give physicians intuitive information on each patient's clinical pattern. Methods and materials: We collected medical data from 292 patients with diabetes and performed preprocessing to extract 184 features from the irregular data. To predict the onset of diabetic nephropathy, we compared several classification methods such as logistic regression, SVM, and SVM with a cost sensitive learning method. We also applied several feature selection methods to remove redundant features and improve the classification performance. For risk factor analysis with SVM classifiers, we have developed a new visualization system which uses a nomogram approach. Results: Linear SVM classifiers combined with wrapper or embedded feature selection methods showed the best results. Among the 184 features, the classifiers selected the same 39 features and gave 0.969 of the area under the curve by receiver operating characteristics analysis. The visualization tool was able to present the effect of each feature on the decision via graphical output. Conclusions: Our proposed method can predict the onset of diabetic nephropathy about 2-3 months before the actual diagnosis with high prediction performance from an irregular and unbalanced dataset, which statistical methods such as t-test and logistic regression could not achieve. Additionally, the visualization system provides physicians with intuitive information for risk factor analysis. Therefore, physicians can benefit from the automatic early warning of each patient and visualize risk factors, which facilitate planning of effective and proper treatment strategies.},
journal = {Artif. Intell. Med.},
month = jan,
pages = {37–53},
numpages = {17},
keywords = {Decision support systems, Diabetic nephropathy, Feature selection, Risk factor analysis, Support vector machines, Visualization}
}

@inproceedings{10.5555/2981562.2981585,
author = {Boutemedjet, Sabri and Ziou, Djemel and Bouguila, Nizar},
title = {Unsupervised feature selection for accurate recommendation of high-dimensional image data},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Content-based image suggestion (CBIS) targets the recommendation of products based on user preferences on the visual content of images. In this paper, we motivate both feature selection and model order identification as two key issues for a successful CBIS. We propose a generative model in which the visual features and users are clustered into separate classes. We identify the number of both user and image classes with the simultaneous selection of relevant visual features using the message length approach. The goal is to ensure an accurate prediction of ratings for multidimensional non-Gaussian and continuous image descriptors. Experiments on a collected data have demonstrated the merits of our approach.},
booktitle = {Proceedings of the 21st International Conference on Neural Information Processing Systems},
pages = {177–184},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'07}
}

@inproceedings{10.5555/2968618.2968698,
author = {Law, Martin H. and Jain, Anil K. and Figueiredo, M\'{a}rio A. T.},
title = {Feature selection in mixture-based clustering},
year = {2002},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {There exist many approaches to clustering, but the important issue of feature selection, i.e., selecting the data attributes that are relevant for clustering, is rarely addressed. Feature selection for clustering is difficult due to the absence of class labels. We propose two approaches to feature selection in the context of Gaussian mixture-based clustering. In the first one, instead of making hard selections, we estimate feature saliencies. An expectation-maximization (EM) algorithm is derived for this task. The second approach extends Koller and Sahami's mutual-information-based feature relevance criterion to the unsupervised case. Feature selection is then carried out by a backward search scheme. This scheme can be classified as a "wrapper", since it wraps mixture estimation in an outer layer that performs feature selection. Experimental results on synthetic and real data show that both methods have promising performance.},
booktitle = {Proceedings of the 16th International Conference on Neural Information Processing Systems},
pages = {641–648},
numpages = {8},
series = {NIPS'02}
}

@inproceedings{10.5555/2390524.2390527,
author = {Simianer, Patrick and Riezler, Stefan and Dyer, Chris},
title = {Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT},
year = {2012},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {With a few exceptions, discriminative training in statistical machine translation (SMT) has been content with tuning weights for large feature sets on small development data. Evidence from machine learning indicates that increasing the training sample size results in better prediction. The goal of this paper is to show that this common wisdom can also be brought to bear upon SMT. We deploy local features for SCFG-based SMT that can be read off from rules at runtime, and present a learning algorithm that applies l1/l2 regularization for joint feature selection over distributed stochastic learning processes. We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.},
booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1},
pages = {11–21},
numpages = {11},
location = {Jeju Island, Korea},
series = {ACL '12}
}

@article{10.1016/j.compbiomed.2010.10.003,
author = {Pereira, Wagner Coelho A. and Alvarenga, Andr\'{e} V. and Infantosi, Antonio Fernando C. and Macrini, Leonardo and Pedreira, Carlos E.},
title = {A non-linear morphometric feature selection approach for breast tumor contour from ultrasonic images},
year = {2010},
issue_date = {November, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {11–12},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2010.10.003},
doi = {10.1016/j.compbiomed.2010.10.003},
abstract = {Ultrasound breast images have been used to improve diagnostics and decrease the number of unneeded biopsies. Malignant breast tumors tend to present irregular and blurred contours while benign ones are usually round, smooth and well-defined. Accordingly, investigating the tumor contour may help in establishing diagnosis. Herein, Mutual Information and Linear Discriminant Analysis were implemented to rank morphometric features in discriminating breast tumors in ultrasound images. Seven features were extracted from Convex Polygon and the Normalized Radial Length techniques. By applying a Mutual Information based approach, it was possible to identity features with possibly non-linear contributions to the outcome.},
journal = {Comput. Biol. Med.},
month = nov,
pages = {912–918},
numpages = {7},
keywords = {Breast cancer, Feature selection, Morphometric features, Mutual information, Tumor, Ultrasonic images}
}

@inproceedings{10.5555/1757898.1757932,
author = {Li, Guo-Zheng and Liu, Tian-Yu},
title = {Feature selection for bagging of support vector machines},
year = {2006},
isbn = {9783540366676},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature selection for the individuals of bagging is studied in this paper. Ensemble learning like bagging can effectively improve the performance of single learning machines, and so can feature selection, but few has studied whether feature selection could improve bagging of single learning machines. Therefore, two typical feature selection approaches namely the embedded feature selection model with the prediction risk criteria and the filter model with the mutual information criteria are used for the bagging of support vector machines respectively. Experiments performed on the UCI data sets show the effectiveness of feature selection for the bagging of support vector machines.},
booktitle = {Proceedings of the 9th Pacific Rim International Conference on Artificial Intelligence},
pages = {271–277},
numpages = {7},
location = {Guilin, China},
series = {PRICAI'06}
}

@inproceedings{10.1007/11551188_44,
author = {Guo, Gongde and Neagu, Daniel and Cronin, Mark T. D.},
title = {Using kNN model for automatic feature selection},
year = {2005},
isbn = {3540287574},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11551188_44},
doi = {10.1007/11551188_44},
abstract = {This paper proposes a kNN model-based feature selection method aimed at improving the efficiency and effectiveness of the ReliefF method by: (1) using a kNN model as the starter selection, aimed at choosing a set of more meaningful representatives to replace the original data for feature selection; (2) integration of the Heterogeneous Value Difference Metric to handle heterogeneous applications – those with both ordinal and nominal features; and (3) presenting a simple method of difference function calculation based on inductive information in each representative obtained bykNN model. We have evaluated the performance of the proposed kNN model-based feature selection method on toxicity dataset Phenols with two different endpoints. Experimental results indicate that the proposed feature selection method has a significant improvement in the classification accuracy for the trial dataset.},
booktitle = {Proceedings of the Third International Conference on Advances in Pattern Recognition - Volume Part I},
pages = {410–419},
numpages = {10},
location = {Bath, UK},
series = {ICAPR'05}
}

@inproceedings{10.5555/1775728.1775832,
author = {Destrero, Augusto and De Mol, Christine and Odone, Francesca and Verri, Alessandro},
title = {A regularized approach to feature selection for face detection},
year = {2007},
isbn = {3540763899},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we present a trainable method for selecting features from an overcomplete dictionary of measurements. The starting point is a thresholded version of the Landweber algorithm for providing a sparse solution to a linear system of equations. We consider the problem of face detection and adopt rectangular features as an initial representation for allowing straightforward comparisons with existing techniques. For computational efficiency and memory requirements, instead of implementing the full optimization scheme on tenths of thousands of features, we propose to first solve a number of smaller size optimization problems obtained by randomly sub-sampling the feature vector, and then recombining the selected features. The obtained set is still highly redundant, so we further apply feature selection. The final feature selection system is an efficient two-stages architecture. Experimental results of an optimized version of the method on face images and image sequences indicate that this method is a serious competitor of other feature selection schemes recently popularized in computer vision for dealing with problems of real time object detection.},
booktitle = {Proceedings of the 8th Asian Conference on Computer Vision - Volume Part II},
pages = {881–890},
numpages = {10},
location = {Tokyo, Japan},
series = {ACCV'07}
}

@inproceedings{10.1145/1553374.1553442,
author = {Kolter, J. Zico and Ng, Andrew Y.},
title = {Regularization and feature selection in least-squares temporal difference learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553442},
doi = {10.1145/1553374.1553442},
abstract = {We consider the task of reinforcement learning with linear value function approximation. Temporal difference algorithms, and in particular the Least-Squares Temporal Difference (LSTD) algorithm, provide a method for learning the parameters of the value function, but when the number of features is large this algorithm can over-fit to the data and is computationally expensive. In this paper, we propose a regularization framework for the LSTD algorithm that overcomes these difficulties. In particular, we focus on the case of l1 regularization, which is robust to irrelevant features and also serves as a method for feature selection. Although the l1 regularized LSTD solution cannot be expressed as a convex optimization problem, we present an algorithm similar to the Least Angle Regression (LARS) algorithm that can efficiently compute the optimal solution. Finally, we demonstrate the performance of the algorithm experimentally.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {521–528},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@article{10.1016/j.sigpro.2008.07.001,
author = {Ververidis, Dimitrios and Kotropoulos, Constantine},
title = {Fast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition},
year = {2008},
issue_date = {December, 2008},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {88},
number = {12},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2008.07.001},
doi = {10.1016/j.sigpro.2008.07.001},
abstract = {This paper addresses subset feature selection performed by the sequential floating forward selection (SFFS). The criterion employed in SFFS is the correct classification rate of the Bayes classifier assuming that the features obey the multivariate Gaussian distribution. A theoretical analysis that models the number of correctly classified utterances as a hypergeometric random variable enables the derivation of an accurate estimate of the variance of the correct classification rate during cross-validation. By employing such variance estimate, we propose a fast SFFS variant. Experimental findings on Danish emotional speech (DES) and speech under simulated and actual stress (SUSAS) databases demonstrate that SFFS computational time is reduced by 50% and the correct classification rate for classifying speech into emotional states for the selected subset of features varies less than the correct classification rate found by the standard SFFS. Although the proposed SFFS variant is tested in the framework of speech emotion recognition, the theoretical results are valid for any classifier in the context of any wrapper algorithm.},
journal = {Signal Process.},
month = dec,
pages = {2956–2970},
numpages = {15},
keywords = {Bayes classifier, Cross-validation, Feature selection, Variance of the correct classification rate of the Bayes classifier, Wrappers}
}

@article{10.1016/S0167-8655(00)00085-4,
author = {Kawamoto, Kazuhiko and Imiya, Atsushi},
title = {Detection of spatial points and lines by random sampling and voting procedure},
year = {2001},
issue_date = {Feb. 2001},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {22},
number = {2},
issn = {0167-8655},
url = {https://doi.org/10.1016/S0167-8655(00)00085-4},
doi = {10.1016/S0167-8655(00)00085-4},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {199–207},
numpages = {9},
keywords = {Hough transform, image sequence, projective geometry, random sampling, shape recovery, voting}
}

@inproceedings{10.1007/978-3-319-03756-1_48,
author = {Bhattacharyya, Saugat and Rakshit, Pratyusha and Konar, Amit and Tibarewala, D. N. and Janarthanan, Ramadoss},
title = {Feature Selection of Motor Imagery EEG Signals Using Firefly Temporal Difference Q-Learning and Support Vector Machine},
year = {2013},
isbn = {9783319037554},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-03756-1_48},
doi = {10.1007/978-3-319-03756-1_48},
abstract = {Electroencephalograph (EEG) based Brain-computer Inter- face (BCI) research provides a non-muscular communication to drive assistive devices using movement related signals, generated from the motor activation areas of the brain. The dimensions of the feature vector play an important role in BCI research, which not only increases the computational time but also reduces the accuracy of the classifiers. In this paper, we aim to reduce the redundant features of a feature vector obtained from motor imagery EEG signals to improve their corresponding classification. In this paper we have proposed a feature selection method based on Firefly Algorithm and Temporal Difference Q-Learning. Here, we have applied our proposed method to the wavelet transform features of a standard BCI competition dataset. Support Vector Machines have been employed to determine the fitness function of the proposed method and obtain the resultant classification accuracy. We have shown that the accuracy of the reduced feature are considerably higher than the original features. This paper also demonstrates the superiority of the new method to its competitor algorithms.},
booktitle = {Proceedings of the 4th International Conference on Swarm, Evolutionary, and Memetic Computing - Volume 8298},
pages = {534–545},
numpages = {12},
keywords = {Brain-Computer Interfacing, Electroencephalography, Firefly Algorithm, Support Vector Machines, Temporal Difference Q-Learning, Wavelet Transforms},
location = {Chennai, India},
series = {SEMCCO 2013}
}

@article{10.1093/bioinformatics/btp630,
author = {Abeel, Thomas and Helleputte, Thibault and Van de Peer, Yves and Dupont, Pierre and Saeys, Yvan},
title = {Robust biomarker identification for cancer diagnosis with ensemble feature selection methods},
year = {2010},
issue_date = {February 2010},
publisher = {Oxford University Press, Inc.},
address = {USA},
volume = {26},
number = {3},
issn = {1367-4803},
url = {https://doi.org/10.1093/bioinformatics/btp630},
doi = {10.1093/bioinformatics/btp630},
abstract = {Motivation: Biomarker discovery is an important topic in biomedical applications of computational biology, including applications such as gene and SNP selection from high-dimensional data. Surprisingly, the stability with respect to sampling variation or robustness of such selection processes has received attention only recently. However, robustness of biomarkers is an important issue, as it may greatly influence subsequent biological validations. In addition, a more robust set of markers may strengthen the confidence of an expert in the results of a selection method.Results: Our first contribution is a general framework for the analysis of the robustness of a biomarker selection algorithm. Secondly, we conducted a large-scale analysis of the recently introduced concept of ensemble feature selection, where multiple feature selections are combined in order to increase the robustness of the final set of selected features. We focus on selection methods that are embedded in the estimation of support vector machines (SVMs). SVMs are powerful classification models that have shown state-of-the-art performance on several diagnosis and prognosis tasks on biological data. Their feature selection extensions also offered good results for gene selection tasks. We show that the robustness of SVMs for biomarker discovery can be substantially increased by using ensemble feature selection techniques, while at the same time improving upon classification performances. The proposed methodology is evaluated on four microarray datasets showing increases of up to almost 30% in robustness of the selected biomarkers, along with an improvement of ~15% in classification performance. The stability improvement with ensemble methods is particularly noticeable for small signature sizes (a few tens of genes), which is most relevant for the design of a diagnosis or prognosis model from a gene signature. Contact: yvan.saeys@psb.ugent.be Supplementary information: Supplementary data are available at Bioinformatics online.},
journal = {Bioinformatics},
month = feb,
pages = {392–398},
numpages = {7}
}

@article{10.1016/j.asoc.2007.10.007,
author = {Huang, Cheng-Lung and Dun, Jian-Fan},
title = {A distributed PSO-SVM hybrid system with feature selection and parameter optimization},
year = {2008},
issue_date = {September, 2008},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {4},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2007.10.007},
doi = {10.1016/j.asoc.2007.10.007},
abstract = {This study proposed a novel PSO-SVM model that hybridized the particle swarm optimization (PSO) and support vector machines (SVM) to improve the classification accuracy with a small and appropriate feature subset. This optimization mechanism combined the discrete PSO with the continuous-valued PSO to simultaneously optimize the input feature subset selection and the SVM kernel parameter setting. The hybrid PSO-SVM data mining system was implemented via a distributed architecture using the web service technology to reduce the computational time. In a heterogeneous computing environment, the PSO optimization was performed on the application server and the SVM model was trained on the client (agent) computer. The experimental results showed the proposed approach can correctly select the discriminating input features and also achieve high classification accuracy.},
journal = {Appl. Soft Comput.},
month = sep,
pages = {1381–1391},
numpages = {11},
keywords = {Data mining, Distributed computing, Feature selection, Particle swarm optimization, Support vector machines, Web service}
}

@article{10.1007/s11042-010-0546-7,
author = {Krishnamoorthy, P. and Kumar, Sarvesh},
title = {Hierarchical audio content classification system using an optimal feature selection algorithm},
year = {2011},
issue_date = {August    2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {2},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-010-0546-7},
doi = {10.1007/s11042-010-0546-7},
abstract = {This paper proposes a hierarchical time-efficient method for audio classification and also presents an automatic procedure to select the best set of features for audio classification using Kolmogorov-Smirnov test (KS-test). The main motivation for our study is to propose a framework of general genre (e.g., action, comedy, drama, documentary, musical, etc...) movie video abstraction scheme for embedded devices-based only on the audio component. Accordingly simple audio features are extracted to ensure the feasibility of real-time processing. Five audio classes are considered in this paper: pure speech, pure music or songs, speech with background music, environmental noise and silence. Audio classification is processed in three stages, (i) silence or environmental noise detection, (ii) speech and non-speech classification and (iii) pure music or songs and speech with background music classification. The proposed system has been tested on various real time audio sources extracted from movies and TV programs. Our experiments in the context of real time processing have shown the algorithms produce very satisfactory results.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {415–444},
numpages = {30},
keywords = {Audio classification, Audio content analysis, General genre video}
}

@inproceedings{10.1007/11829898_25,
author = {Strickert, Marc and Sreenivasulu, Nese and Peterek, Silke and Weschke, Winfriede and Mock, Hans-Peter and Seiffert, Udo},
title = {Unsupervised feature selection for biomarker identification in chromatography and gene expression data},
year = {2006},
isbn = {3540379517},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11829898_25},
doi = {10.1007/11829898_25},
abstract = {A novel approach to feature selection from unlabeled vector data is presented. It is based on the reconstruction of original data relationships in an auxiliary space with either weighted or omitted features. Feature weighting, on one hand, is related to the return forces of factors in a parametric data similarity measure as response to disturbance of their optimum values. Feature omission, on the other hand, inducing measurable loss of reconstruction quality, is realized in an iterative greedy way. The proposed framework allows to apply custom data similarity measures. Here, adaptive Euclidean distance and adaptive Pearson correlation are considered, the former serving as standard reference, the latter being usefully for intensity data. Results of the different strategies are given for chromatography and gene expression data.},
booktitle = {Proceedings of the Second International Conference on Artificial Neural Networks in Pattern Recognition},
pages = {274–285},
numpages = {12},
keywords = {adaptive similarity measures, feature selection},
location = {Ulm, Germany},
series = {ANNPR'06}
}

@article{10.1016/j.artmed.2010.04.011,
author = {Lee, Michael C. and Boroczky, Lilla and Sungur-Stasik, Kivilcim and Cann, Aaron D. and Borczuk, Alain C. and Kawut, Steven M. and Powell, Charles A.},
title = {Computer-aided diagnosis of pulmonary nodules using a two-step approach for feature selection and classifier ensemble construction},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {50},
number = {1},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2010.04.011},
doi = {10.1016/j.artmed.2010.04.011},
abstract = {Objective: Accurate classification methods are critical in computer-aided diagnosis (CADx) and other clinical decision support systems. Previous research has reported on methods for combining genetic algorithm (GA) feature selection with ensemble classifier systems in an effort to increase classification accuracy. In this study, we describe a CADx system for pulmonary nodules using a two-step supervised learning system combining a GA with the random subspace method (RSM), with the aim of exploring algorithm design parameters and demonstrating improved classification performance over either the GA or RSM-based ensembles alone. Methods and materials: We used a retrospective database of 125 pulmonary nodules (63 benign; 62 malignant) with CT volumes and clinical history. A total of 216 features were derived from the segmented image data and clinical history. Ensemble classifiers using RSM or GA-based feature selection were constructed and tested via leave-one-out validation with feature selection and classifier training executed within each iteration. We further tested a two-step approach using a GA ensemble to first assess the relevance of the features, and then using this information to control feature selection during a subsequent RSM step. The base classification was performed using linear discriminant analysis (LDA). Results: The RSM classifier alone achieved a maximum leave-one-out Az of 0.866 (95% confidence interval: 0.794-0.919) at a subset size of s=36 features. The GA ensemble yielded an Az of 0.851 (0.775-0.907). The proposed two-step algorithm produced a maximum Az value of 0.889 (0.823-0.936) when the GA ensemble was used to completely remove less relevant features from the second RSM step, with similar results obtained when the GA-LDA results were used to reduce but not eliminate the occurrence of certain features. After accounting for correlations in the data, the leave-one-out Az in the two-step method was significantly higher than in the RSM and the GA-LDA. Conclusions: We have developed a CADx system for evaluation of pulmonary nodule based on a two-step feature selection and ensemble classifier algorithm. We have shown that by combining classifier ensemble algorithms in this two-step manner, it is possible to predict the malignancy for solitary pulmonary nodules with a performance exceeding that of either of the individual steps.},
journal = {Artif. Intell. Med.},
month = sep,
pages = {43–53},
numpages = {11},
keywords = {Computer-aided diagnosis, Feature selection, Genetic algorithms, Linear discriminant analysis, Pulmonary nodules, Random subspace}
}

@inproceedings{10.1145/2507157.2507168,
author = {Koenigstein, Noam and Paquet, Ulrich},
title = {Xbox movies recommendations: variational bayes matrix factorization with embedded feature selection},
year = {2013},
isbn = {9781450324090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2507157.2507168},
doi = {10.1145/2507157.2507168},
abstract = {We present a matrix factorization model inspired by challenges we encountered while working on the Xbox movies recommendation system. The item catalog in a recommender system is typically equipped with meta-data features in the form of labels. However, only part of these features are informative or useful with regard to collaborative filtering. By incorporating a novel sparsity prior on feature parameters, the model automatically discerns and utilizes informative features while simultaneously pruning non-informative features.The model is designed for binary feedback, which is common in many real-world systems where numeric rating data is scarce or non-existent. However, the overall framework is applicable to any likelihood function. Model parameters are estimated with a Variational Bayes inference algorithm, which is robust to over-fitting and does not require cross-validation and fine tuning of regularization coefficients. The efficacy of our method is illustrated on a sample from the Xbox movies dataset as well as on the publicly available MovieLens dataset. In both cases, the proposed solution provides superior predictive accuracy, especially for long-tail items. We then demonstrate the feature selection capabilities and compare against the common case of simple Gaussian priors. Finally, we show that even without features, our model performs better than a baseline model trained with the popular stochastic gradient descent approach.},
booktitle = {Proceedings of the 7th ACM Conference on Recommender Systems},
pages = {129–136},
numpages = {8},
keywords = {feature selection, recommender system},
location = {Hong Kong, China},
series = {RecSys '13}
}

@article{10.1016/j.compeleceng.2015.05.003,
title = {Investigating the effect of fixing the subset length on the performance of ant colony optimization for feature selection for supervised learning},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {45},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2015.05.003},
doi = {10.1016/j.compeleceng.2015.05.003},
abstract = {This paper studies the effect of fixing the length of the selected feature subsets on the performance of ant colony optimization (ACO) for feature selection (FS) for supervised learning. It addresses this concern by investigating: (1) determining the optimal feature subset from datamining perspective, (2) demonstrating the solution convergence in case of fixing the length of the selected feature subsets, (3) determining the subset length in ACO for subset selection problems, and (4) different stopping criteria when solving FS by ACO. Besides, two types of experiments on ACO algorithms for FS for classification and regression problems using artificial and real world datasets in two cases fixing and not fixing the length of the selected feature subsets with the use of a support vector machine. The obtained results showed that not fixing the length of the selected feature subsets is better than fixing the length of the selected feature subsets.},
journal = {Comput. Electr. Eng.},
month = jul,
pages = {1–9},
numpages = {9}
}

@article{10.5555/1122544.1122551,
author = {Kawakami, Hiroshi and Ito, Yoshihiro and Kanazawa, Yasushi},
title = {A robust method for detecting planar regions based on random sampling using distributions of feature points},
year = {2006},
issue_date = {April 2006},
publisher = {Wiley-Interscience},
address = {USA},
volume = {37},
number = {4},
abstract = {We propose a robust method for detecting local planar regions in a scene with an uncalibrated stereo. Here, we assume that the correspondences between the two images have been established. Our method is based on RANSAC for estimating homographies to the local planar regions in the scene. For doing this, we adopt double random sampling scheme by a uniform distribution and the local probability distribution of each pair, which is defined by the distances from the point to the others in one image. We first choose a pair as a seed by the uniform distribution, and then choose four pairs by the local probability distribution with respect to the seed. By introducing the local probability distribution, we can efficiently choose four potentially coplanar pairs in the scene. The same scheme can be applicable to detect line segments in an image. We demonstrate that our method is robust to the outliers in a scene by simulations and real image examples. © 2006 Wiley Periodicals, Inc. Syst Comp Jpn, 37(4): 11–22, 2006; Published online in Wiley InterScience (). DOI 10.1002/scj.20492},
journal = {Syst. Comput. Japan},
month = apr,
pages = {11–22},
numpages = {12},
keywords = {RANSAC, homography, planar region detection, uncalibrated stereo}
}

@inproceedings{10.5555/2976040.2976109,
author = {Guyon, Isabelle and Gunn, Steve and Hur, Asa Ben and Dror, Gideon},
title = {Result analysis of the NIPS 2003 feature selection challenge},
year = {2004},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {The NIPS 2003 workshops included a feature selection competition organized by the authors. We provided participants with five datasets from different application domains and called for classification results using a minimal number of features. The competition took place over a period of 13 weeks and attracted 78 research groups. Participants were asked to make on-line submissions on the validation and test sets, with performance on the validation set being presented immediately to the participant and performance on the test set presented to the participants at the workshop. In total 1863 entries were made on the validation sets during the development period and 135 entries on all test sets for the final competition. The winners used a combination of Bayesian neural networks with ARD priors and Dirichlet diffusion trees. Other top entries used a variety of methods for feature selection, which combined filters and/or wrapper or embedded methods using Random Forests, kernel methods, or neural networks as a classification engine. The results of the benchmark (including the predictions made by the participants and the features they selected) and the scoring software are publicly available. The benchmark is available at www.nipsfsc.ecs.soton.ac.uk for post-challenge submissions to stimulate further research.},
booktitle = {Proceedings of the 18th International Conference on Neural Information Processing Systems},
pages = {545–552},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'04}
}

@article{10.1016/j.patcog.2015.05.006,
author = {Jing, Liping and Tian, Kuang and Huang, Joshua Z.},
title = {Stratified feature sampling method for ensemble clustering of high dimensional data},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {11},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2015.05.006},
doi = {10.1016/j.patcog.2015.05.006},
abstract = {High dimensional data with thousands of features present a big challenge to current clustering algorithms. Sparsity, noise and correlation of features are common characteristics of such data. Another common phenomenon is that clusters in such high dimensional data often exist in different subspaces. Ensemble clustering is emerging as a prominent technique for improving robustness, stability and accuracy of high dimensional data clustering. In this paper, we propose a stratified sampling method for generating subspace component data sets in ensemble clustering of high dimensional data. Instead of randomly sampling a subset of features for each component data set, in this method we first cluster the features of high dimensional data into a few feature groups called feature strata. Using stratified sampling, we randomly sample some features from each feature stratum and merge the sampled features from different feature strata to generate a component data set. In this way, the component data sets have better representations of the clustering structure in the original data set. Comparing with random sampling and random projection methods in synthetic data analysis, the component clustering by stratified sampling has demonstrated that the average clustering accuracy was increased without sacrificing clustering diversity. We carried out a series of experiments on eight real world data sets from microarray, text and image domains to evaluate ensemble clustering methods using three subspace component data generation methods and four consensus functions. The experimental results consistently showed that the stratified sampling method produced the best ensemble clustering results in all data sets. The ensemble clustering with stratified sampling also outperformed three other ensemble clustering methods which generate component clusters from the entire space of the original data. HighlightsA new component generation approach is proposed to produce ensemble components.Stratified sampling is used to generate subspace component data sets.The component data can well represent the characteristics of the original data.The proposed method can achieve a consistent performance in ensemble clustering.The proposed method is easy to implement.},
journal = {Pattern Recogn.},
month = nov,
pages = {3688–3702},
numpages = {15},
keywords = {Consensus function, Ensemble clustering, High dimensional data, Stratified sampling}
}

@article{10.1155/ASP.2005.3128,
author = {Peterson, David A. and Knight, James N. and Kirby, Michael J. and Anderson, Charles W. and Thaut, Michael H.},
title = {Feature selection and blind source separation in an EEG-based brain-computer interface},
year = {2005},
issue_date = {1 January 2005},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2005},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP.2005.3128},
doi = {10.1155/ASP.2005.3128},
abstract = {Most EEG-based BCI systems make use of well-studied patterns of brain activity. However, those systems involve tasks that indirectly map to simple binary commands such as "yes" or "no" or require many weeks of biofeedback training. We hypothesized that signal processing and machine learning methods can be used to discriminate EEG in a direct "yes"/"no" BCI from a single session. Blind source separation (BSS) and spectral transformations of the EEG produced a 180-dimensional feature space. We used a modified genetic algorithm (GA) wrapped around a support vector machine (SVM) classifier to search the space of feature subsets. The GA-based search found feature subsets that outperform full feature sets and random feature subsets. Also, BSS transformations of the EEG outperformed the original time series, particularly in conjunction with a subset search of both spaces. The results suggest that BSS and feature selection can be used to improve the performance of even a "direct," single-session BCI.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {3128–3140},
numpages = {13},
keywords = {brain-computer interface, electroencephalogram, feature selection, genetic algorithm, independent components analysis, support vector machine}
}

@article{10.1016/j.eswa.2008.12.036,
author = {Chen, You-Shyang and Cheng, Ching-Hsue},
title = {Evaluating industry performance using extracted RGR rules based on feature selection and rough sets classifier},
year = {2009},
issue_date = {July, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.12.036},
doi = {10.1016/j.eswa.2008.12.036},
abstract = {In strategy of investment, an important thing for investors is to correctly predict firm's revenue growth rate (RGR), which is an effective evaluation indicator for them to see how big the potential power of future development is and measure how about the growth of future development for a target firm that may be selected to investment portfolios. However, conventional methods of forecasting RGR have some shortcomings such as statistical methods based on strict assumptions of linearity and/or normality limit applications in real world. Additionally, due to rapid changing of information technology (IT) today, some techniques (i.e. rough sets and data mining tools) have become important research trends to both practitioners and academicians. With these reasons above, a new procedure, using the feature selection method and rough sets classifier, is proposed to extract decision rules and improve accuracy rate for classifying RGR. In empirical study, an actual RGR dataset collected from publicly traded company of stock markets is employed to illustrate the proposed procedure. The experimental results of RGR dataset analyses indicate that the proposed procedure surpasses the listing methods in terms of both higher accuracy and fewer attributes, and the output of proposed procedure is to generate a set of easily understandable decision rules that are readily applied in knowledge-based investment systems by investors.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {9448–9456},
numpages = {9},
keywords = {Data mining techniques, Feature selection, Fundamental analysis, Revenue growth rate (RGR), Rough sets classifier}
}

@article{10.1155/ASP/2006/30274,
author = {Shen, Linlin and Bai, Li},
title = {Information theory for Gabor feature selection for face recognition},
year = {2006},
issue_date = {01 January},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2006},
issn = {1110-8657},
url = {https://doi.org/10.1155/ASP/2006/30274},
doi = {10.1155/ASP/2006/30274},
abstract = {A discriminative and robust feature--kernel enhanced informative Gabor feature--is proposed in this paper for face recognition. Mutual information is applied to select a set of informative and nonredundant Gabor features, which are then further enhanced by kernel methods for recognition. Compared with one of the top performing methods in the 2004 Face Verification Competition cost. The proposed method has been fully tested on the FERET database using the FERET evaluation protocol. Significant improvements on three of the test data sets are observed. Compared with the classical Gabor wavelet-based approaches using a huge number of features, our method requires less than 4 milliseconds to retrieve a few hundreds of features. Due to the substantially reduced feature dimension, only 4 seconds are required to recognize 200 face images. The paper also unified different Gabor filter definitions and proposed a training sample generation algorithm to reduce the effects caused by unbalanced number of samples available in different classes.},
journal = {EURASIP J. Adv. Signal Process},
month = jan,
pages = {8},
numpages = {1}
}

@article{10.1016/j.compbiomed.2013.09.016,
author = {Korfiatis, Vasileios Ch. and Asvestas, Pantelis A. and Delibasis, Konstantinos K. and Matsopoulos, George K.},
title = {A classification system based on a new wrapper feature selection algorithm for the diagnosis of primary and secondary polycythemia},
year = {2013},
issue_date = {December, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {43},
number = {12},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2013.09.016},
doi = {10.1016/j.compbiomed.2013.09.016},
abstract = {Primary and Secondary Polycythemia are diseases of the bone marrow that affect the blood's composition and prohibit patients from becoming blood donors. Since these diseases may become fatal, their early diagnosis is important. In this paper, a classification system for the diagnosis of Primary and Secondary Polycythemia is proposed. The proposed system classifies input data into three classes; Healthy, Primary Polycythemic (PP) and Secondary Polycythemic (SP) and is implemented using two separate binary classification levels. The first level performs the Healthy/non-Healthy classification and the second level the PP/SP classification. To this end, a novel wrapper feature selection algorithm, called the LM-FM algorithm, is presented in order to maximize the classifier's performance. The algorithm is comprised of two stages that are applied sequentially: the Local Maximization (LM) stage and the Floating Maximization (FM) stage. The LM stage finds the best possible subset of a fixed predefined size, which is then used as an input for the next stage. The FM stage uses a floating size technique to search for an even better solution by varying the initially provided subset size. Then, the Support Vector Machine (SVM) classifier is used for the discrimination of the data at each classification level. The proposed classification system is compared with various well-established feature selection techniques such as the Sequential Floating Forward Selection (SFFS) and the Maximum Output Information (MOI) wrapper schemes, and with standalone classification techniques such as the Multilayer Perceptron (MLP) and SVM classifier. The proposed LM-FM feature selection algorithm combined with the SVM classifier increases the overall performance of the classification system, scoring up to 98.9% overall accuracy at the first classification level and up to 96.6% at the second classification level. Moreover, it provides excellent robustness regardless of the size of the input feature subset used.},
journal = {Comput. Biol. Med.},
month = dec,
pages = {2118–2126},
numpages = {9},
keywords = {Classification system, LM-FM wrapper, Machine learning, Maximum output information, Multiclass SVM, Polycythemia}
}

@article{10.5555/3030638.3030860,
title = {Fully automatic segmentation of AP pelvis X-rays via random forest regression with efficient feature selection and hierarchical sparse shape composition},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {1077-3142},
abstract = {A fully automatic approach for landmark detection and shape segmentation.The random forest regression framework with the sparse shape composition model.Effective feature selection that reduces dimension without sacrificing the accuracy. In clinical practice, traditional X-ray radiography is widely used, and knowledge of landmarks and contours in anteroposterior (AP) pelvis X-rays is invaluable for computer aided diagnosis, hip surgery planning and image-guided interventions. This paper presents a fully automatic approach for landmark detection and shape segmentation of both pelvis and femur in conventional AP X-ray images. Our approach is based on the framework of landmark detection via Random Forest (RF) regression and shape regularization via hierarchical sparse shape composition. We propose a visual feature FL-HoG (Flexible-Level Histogram of Oriented Gradients) and a feature selection algorithm based on trace radio optimization to improve the robustness and the efficacy of RF-based landmark detection. The landmark detection result is then used in a hierarchical sparse shape composition framework for shape regularization. Finally, the extracted shape contour is fine-tuned by a post-processing step based on low level image features. The experimental results demonstrate that our feature selection algorithm reduces the feature dimension in a factor of 40 and improves both training and test efficiency. Further experiments conducted on 436 clinical AP pelvis X-rays show that our approach achieves an average point-to-curve error around 1.2mm for femur and 1.9mm for pelvis.},
journal = {Comput. Vis. Image Underst.},
month = sep,
pages = {1–10},
numpages = {10}
}

@article{10.1016/j.asoc.2011.05.010,
author = {Manimala, K. and Selvi, K. and Ahila, R.},
title = {Hybrid soft computing techniques for feature selection and parameter optimization in power quality data mining},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {11},
number = {8},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2011.05.010},
doi = {10.1016/j.asoc.2011.05.010},
abstract = {Recognition of the presence of any power disturbance and classifying any existing disturbance into a particular type is the first step in combating the power quality problem. In spite of the extensive number of power disturbances classification methods, a research on the selection of useful features from the existing feature set and the parameter optimization for specific classifiers was omitted. The kernel parameters setting for support vector machine (SVM) classifier in training process along with feature selection will significantly impact the classification accuracy. Two novel wrapper based hybrid soft computing techniques are proposed in this paper for feature selection and parameters optimization to classify nine types of power disturbances without degrading the SVM classification accuracy. The feature items were selected from discrete wavelet transform across several decomposition levels of the disturbance signals and from the duration of disturbance occurrence. This analysis selects the more useful feature set and optimized parameters for two types of kernels namely the polynomial kernel and radial basis function kernel for SVM. Compared with the traditional grid algorithm the proposed genetic algorithm and simulated annealing based approach significantly improves the classification accuracy rate by eliminating relatively useless feature items and proper parameter selection for the classifier.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {5485–5497},
numpages = {13},
keywords = {Genetic algorithm, Power quality, Simulated annealing, Support vector machine, Wavelet transform}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@inproceedings{10.1007/978-3-030-78811-7_39,
author = {Cheng, Jian and Jiao, Botao and Guo, Yinan and Wang, Shijie},
title = {Ensemble Recognition Based on the Harmonic Information Gain Ratio for Unsafe Behaviors in Coal Mines},
year = {2021},
isbn = {978-3-030-78810-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78811-7_39},
doi = {10.1007/978-3-030-78811-7_39},
abstract = {More than 90% accidents occurred in coal mine are caused by unsafe behaviors of human. How to effectively identify unsafe behaviors and decrease the possibility of their occurrence is the fundamental of avoiding accidents. However, the number of unsafe behaviors is far less than that of safe ones in a behavior dataset of coal mine. Serious imbalance has a negative impact on recognition efficiency and accuracy. To address the problem, the harmonic information gain ratio is defined by introducing the degree of imbalance into traditional information gain, and the corresponding feature selection method is presented. By integrating it into Underbagging, a novel ensemble recognition based on the harmonic information gain ratio for unsafe behaviors is presented, with the purpose of avoiding information loss caused by feature reduction and guaranteeing recognition accuracy. Based on a sub-dataset obtained by undersampling, the optimal features subset is selected by the proposed feature selection method, and employed to train a base classifier built by support vector machine. The weighted sum of all base classifiers output forms final recognition result. Each weight is calculated from the corresponding harmonic information gain ratio. Experimental results on UCI dataset and a behavior dataset for a particular coal mine indicate that the proposed ensemble recognition method outperforms the others, especially for a dataset with high imbalance ratio.},
booktitle = {Advances in Swarm Intelligence: 12th International Conference, ICSI 2021, Qingdao, China, July 17–21, 2021, Proceedings, Part II},
pages = {420–429},
numpages = {10},
keywords = {Feature selection, Imbalanced data, Ensemble learning, Information gain ratio},
location = {Qingdao, China}
}

@article{10.5555/1218685.1218690,
author = {Kalapanidas, Elias and Avouris, Nikolaos},
title = {Feature selection for air quality forecasting: a genetic algorithm approach},
year = {2003},
issue_date = {December 2003},
publisher = {IOS Press},
address = {NLD},
volume = {16},
number = {4},
issn = {0921-7126},
abstract = {Feature selection is a process of determining the most relevant features of a given problem in order to improve the generalization and the performance of a relevant classification or regression algorithm.This paper focuses on the exploitation of a genetic algorithm following a wrapping iterative approach used to extract an optimal feature subset of a large database containing pollutant concentration measurements. The feature subset is fed to a machine learning algorithm in order to predict the daily maximum concentration of two air pollutants.The encoding problem of the complexity of representation of the features in the genomes is tackled. Results of the experimentation on a specific dataset of an air quality forecasting problem are presented, as well as some proposed alterations on the standard genetic algorithm that guided the process to a mature convergence and gave good solutions for this problem. A modified version of the initial algorithm is presented as well, implemented for the purpose of being compared on an equal basis with other feature selection methods. Two such methods of the filtering type, CFS and ReliefF, are being compared with.The comparative results suggest that the wrapping type technique described in this paper is significantly better in the specific problem at hand, but this conclusion is limited to the machine learning algorithm that the technique uses at its core in the feature selection phase.},
journal = {AI Commun.},
month = dec,
pages = {235–251},
numpages = {17},
keywords = {Feature selection, air quality forecasting, environmental informatics, genetic algorithm, machine learning}
}

@article{10.1016/j.patrec.2014.06.017,
author = {Chen, Zhong and Xiong, Shengwu and Fang, Zhixiang and Li, Qingquan and Wang, Baolin and Zou, Qin},
title = {A kernel support vector machine-based feature selection approach for recognizing Flying Apsaras' streamers in the Dunhuang Grotto Murals, China},
year = {2014},
issue_date = {November 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {49},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2014.06.017},
doi = {10.1016/j.patrec.2014.06.017},
abstract = {Define the shape-based features of Flying Apsaras' streamers.Propose a morphological descriptor of incorporating these features for KSVM.Demonstrate the suitability of the descriptor and KSVM for streamer recognition. Recognizing Flying Apsaras' streamers is of great importance in analyzing Chinese cultural background and art forms form the early Chinese dynasties. This analysis is very valuable for cultural protection and heritage. However, few studies have focused on recognition of Flying Apsaras in the Dunhuang Grotto Murals, China, which record elements of Chinese culture in different Chinese dynasties. By introducing a set of feature descriptors for Flying Apsaras' streamers, this paper proposes a morphological streamer feature descriptor to describe the shape-based features (i.e., slenderness, posture ratio, area ratio, and intensity) of Flying Apsaras' streamers. Then, a Kernel Support Vector Machine (KSVM) is implemented to locate and recognize Flying Apsaras' streamers using the proposed feature descriptor. This machine is composed of two important parts: region segmentation of the images in the Dunhuang Grotto Murals, and KSVM-based feature selection for streamer recognition. The implemented KSVM approach incorporating the proposed morphological feature descriptor can classify streamer regions with 89.56% accuracy. Comparing the results of different classifiers and different feature descriptors demonstrates that the proposed morphological feature descriptor is a suitable morphological operator and that the KSVM is a suitable classifier for Flying Apsaras' streamers in the Dunhuang Grotto Murals, China.},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {107–113},
numpages = {7},
keywords = {Feature extraction, Flying Apsaras, Image segmentation, Kernel function, Mean shift, Support vector machine}
}

@inproceedings{10.1007/978-3-540-30502-6_34,
author = {Sung, Andrew H. and Mukkamala, Srinivas},
title = {The feature selection and intrusion detection problems},
year = {2004},
isbn = {354024087X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30502-6_34},
doi = {10.1007/978-3-540-30502-6_34},
abstract = {Cyber security is a serious global concern. The potential of cyber terrorism has posed a threat to national security; meanwhile the increasing prevalence of malware and incidents of cyber attacks hinder the utilization of the Internet to its greatest benefit and incur significant economic losses to individuals, enterprises, and public organizations. This paper presents some recent advances in intrusion detection, feature selection, and malware detection.In intrusion detection, stealthy and low profile attacks that include only few carefully crafted packets over an extended period of time to delude firewalls and the intrusion detection system (IDS) have been difficult to detect. In protection against malware (trojans, worms, viruses, etc.), how to detect polymorphic and metamorphic versions of recognized malware using static scanners is a great challenge.We present in this paper an agent based IDS architecture that is capable of detecting probe attacks at the originating host and denial of service (DoS) attacks at the boundary controllers. We investigate and compare the performance of different classifiers implemented for intrusion detection purposes. Further, we study the performance of the classifiers in real-time detection of probes and DoS attacks, with respect to intrusion data collected on a real operating network that includes a variety of simulated attacks.Feature selection is as important for IDS as it is for many other modeling problems. We present several techniques for feature selection and compare their performance in the IDS application. It is demonstrated that, with appropriately chosen features, both probes and DoS attacks can be detected in real time or near real time at the originating host or at the boundary controllers.We also briefly present some encouraging recent results in detecting polymorphic and metamorphic malware with advanced static, signature-based scanning techniques.},
booktitle = {Proceedings of the 9th Asian Computing Science Conference on Advances in Computer Science: Dedicated to Jean-Louis Lassez on the Occasion of His 5th Cycle Birthday},
pages = {468–482},
numpages = {15},
location = {Chiang Mai, Thailand},
series = {ASIAN'04}
}

@inproceedings{10.5555/646259.684605,
author = {Sierra, Alejandro and Corbacho, Fernando J.},
title = {Input and Output Feature Selection},
year = {2002},
isbn = {3540440747},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature selection is called wrapper whenever the classification algorithm is used in the selection procedure. Our approach makes use of linear classifiers wrapped into a genetic algorithm. As a proof of concept we check its performance against the UCI spam filtering problem showing that the wrapping of linear neural networks is the best. However,making sense of data involves not only selecting input features but also output features. Generally,this is considered too much of a human task to be addressed by computers. Only a few algorithms,suc h as association rules,allo w the output to change. One of the advantages of our approach is that it can be easily generalized to search for outputs and relevant inputs at the same time. This is addressed at the end of the paper and it is currently being investigated.},
booktitle = {Proceedings of the International Conference on Artificial Neural Networks},
pages = {625–630},
numpages = {6},
series = {ICANN '02}
}

@article{10.1016/j.patrec.2006.04.008,
author = {Park, Sangmin and Bajaj, Chandrajit},
title = {Feature selection of 3D volume data through multi-dimensional transfer functions},
year = {2007},
issue_date = {February, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {28},
number = {3},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2006.04.008},
doi = {10.1016/j.patrec.2006.04.008},
abstract = {Direct volume rendering maps data values to visual properties such as transparency and color through transfer functions. Traditional multi-dimensional functions are generated based on a 2D histogram of function value and gradient magnitude. When two different features overlap in the 2D histogram, the traditional transfer functions cannot visually distinguish the features, since overlapped areas have similar visual properties. In this paper, we describe a new multi-dimensional transfer function that enables visual differentiation of features even in the case when two different features overlap in the 2D histogram. Furthermore, we provide details of an implementation of our transfer function on modern programmable graphics hardware.},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {367–374},
numpages = {8},
keywords = {Transfer functions, Volume rendering}
}

@inproceedings{10.1007/978-3-642-53917-6_38,
author = {Zhong, Minjuan and Wan, Changxuan and Liu, Dexi and Liao, Shumei and Luo, Siwen},
title = {Cluster Labeling Extraction and Ranking Feature Selection for High Quality XML Pseudo Relevance Feedback Fragments Set},
year = {2013},
isbn = {9783642539169},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-53917-6_38},
doi = {10.1007/978-3-642-53917-6_38},
abstract = {In traditional pseudo feedback, the main reason of the  topic drift  is the low quality of the feedback source. Clustering search results is an effective way to improve the quality of feedback set. For XML data, how to effectively perform clustering algorithm and then identify good xml fragments from the clustering results is a intricate problem. This paper mainly focus on the latter problem. Based on k-mediod clustering results, This work firstly proposes an cluster label extraction method to select candidate relevant clusters. Secondly, multiple ranking features are introduced to assist the related xml fragments identification from the candidate clusters. Top  N  fragments compose the high quality pseudo feedback set finally. Experimental results on standard INEX test data show that in one hand, the proposed cluster label extraction method could obtain proper cluster key terms and lead to appropriate candidate cluster selection. On the other hand, the presented ranking features are beneficial to the relevant xml fragments identification. The quality of feedback set is ensured.},
booktitle = {Part II of the Proceedings of the 9th International Conference on Advanced Data Mining and Applications - Volume 8347},
pages = {423–432},
numpages = {10},
keywords = {Pseudo Relevance Feedback, cluster label, ranking features, xml search results clustering},
location = {Hangzhou, China},
series = {ADMA 2013}
}

@inproceedings{10.5555/645531.756485,
author = {Liu, Huan and Motoda, Hiroshi and Yu, Lei},
title = {Feature Selection with Selective Sampling},
year = {2002},
isbn = {1558608737},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
pages = {395–402},
numpages = {8},
series = {ICML '02}
}

@inproceedings{10.1145/1273463.1273482,
author = {Cohen, Myra B. and Dwyer, Matthew B. and Shi, Jiangfan},
title = {Interaction testing of highly-configurable systems in the presence of constraints},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273482},
doi = {10.1145/1273463.1273482},
abstract = {Combinatorial interaction testing (CIT) is a method to sample configurations of a software system systematically for testing. Many algorithms have been developed that create CIT samples, however few have considered the practical concerns that arise when adding constraints between combinations of options. In this paper, we survey constraint handling techniques in existing algorithms and discuss the challenges that they present. We examine two highly-configurable software systems to quantify the nature of constraints in real systems. We then present a general constraint representation and solving technique that can be integrated with existing CIT algorithms and compare two constraint-enhanced algorithm implementations with existing CIT tools to demonstrate feasibility.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {129–139},
numpages = {11},
keywords = {SAT, combinatorial interaction testing, constraints, covering arrays},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inproceedings{10.1145/2683483.2683529,
author = {Babu, S. Hitesh and Birajdhar, Sachin A. and Tambad, Samarth},
title = {Face Recognition using Entropy based Face Segregation as a Pre-processing Technique and Conservative BPSO based Feature Selection},
year = {2014},
isbn = {9781450330619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2683483.2683529},
doi = {10.1145/2683483.2683529},
abstract = {The statistical appearance of the face can vary due to various factors such as pose, occlusion, expression and background which makes it a challenging task to have an efficient Face Recognition (FR) system. This paper proposes 4 novel techniques viz., Entropy based Face Segregation (EFS) as pre-processing technique, Double Wavelet Noise Removal (DWNR) as pre-processing technique, 1D Stationary Wavelet Transform (SWT) as Feature Extractor and Conservative Binary Particle Optimization (CBPSO) as Feature Selector to enhance the performance of the system. EFS is used to segregate the facial region, thus removing the cluttered background. DWNR has unique combination of 2D Discrete Wavelet Transform (DWT), Wiener Filter and 2D SWT for image denoising and contrast enhancement. The pre-processed image is then fed to unique combination of 1D DWT, 1D SWT and 1D Discrete Cosine Transform (DCT) to extract essential features. CBPSO is used to select very optimum feature subset and significantly reduce the computation time. The proposed algorithm is experimented on four benchmark databases viz., Color FERET, CMU PIE, Pointing Head Pose and Georgia Tech.},
booktitle = {Proceedings of the 2014 Indian Conference on Computer Vision Graphics and Image Processing},
articleno = {46},
numpages = {8},
keywords = {Entropy, Face Recognition, Feature Extraction, Image pre-processing, Particle Swarm Optimization, Stationary Wavelet Transform},
location = {Bangalore, India},
series = {ICVGIP '14}
}

@article{10.1016/j.compbiolchem.2010.07.002,
author = {He, Zengyou and Yu, Weichuan},
title = {Review Article: Stable feature selection for biomarker discovery},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {34},
number = {4},
issn = {1476-9271},
url = {https://doi.org/10.1016/j.compbiolchem.2010.07.002},
doi = {10.1016/j.compbiolchem.2010.07.002},
abstract = {Feature selection techniques have been used as the workhorse in biomarker discovery applications for a long time. Surprisingly, the stability of feature selection with respect to sampling variations has long been under-considered. It is only until recently that this issue has received more and more attention. In this article, we review existing stable feature selection methods for biomarker discovery using a generic hierarchical framework. We have two objectives: (1) providing an overview on this new yet fast growing topic for a convenient reference; (2) categorizing existing methods under an expandable framework for future research and development.},
journal = {Comput. Biol. Chem.},
month = aug,
pages = {215–225},
numpages = {11},
keywords = {Biomarker discovery, Feature selection, Machine learning, Stability}
}

@article{10.5555/1005332.1044711,
author = {Fleuret, Fran\c{c}ois},
title = {Fast Binary Feature Selection with Conditional Mutual Information},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {We propose in this paper a very fast feature selection technique based on conditional mutual information. By picking features which maximize their mutual information with the class to predict conditional to any feature already picked, it ensures the selection of features which are both individually informative and two-by-two weakly dependant. We show that this feature selection method outperforms other classical algorithms, and that a naive Bayesian classifier built with features selected that way achieves error rates similar to those of state-of-the-art methods such as boosting or SVMs. The implementation we propose selects 50 features among 40,000, based on a training set of 500 examples in a tenth of a second on a standard 1Ghz PC.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1531–1555},
numpages = {25}
}

@article{10.1155/2007/16354,
author = {Xiao, Yufei and Hua, Jianping and Dougherty, Edward R.},
title = {Quantification of the impact of feature selection on the variance of cross-validation error estimation},
year = {2007},
issue_date = {January 2007},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2007},
issn = {1687-4145},
url = {https://doi.org/10.1155/2007/16354},
doi = {10.1155/2007/16354},
abstract = {Given the relatively small number of microarrays typically used in gene-expression-based classification, all of the data must be used to train a classifier and therefore the same training data is used for error estimation. The key issue regarding the quality of an error estimator in the context of small samples is its accuracy, and this is most directly analyzed via the deviation distribution of the estimator, this being the distribution of the difference between the estimated and true errors. Past studies indicate that given a prior set of features, cross-validation does not perform as well in this regard as some other training-data-based error estimators. The purpose of this study is to quantify the degree to which feature selection increases the variation of the deviation distribution in addition to the variation in the absence of feature selection. To this end, we propose the coefficient of relative increase in deviation dispersion (CRIDD), which gives the relative increase in the deviation-distribution variance using feature selection as opposed to using an optimal feature set without feature selection. The contribution of feature selection to the variance of the deviation distribution can be significant, contributing to over half of the variance in many of the cases studied. We consider linear-discriminant analysis, 3-nearest-neighbor, and linear support vector machines for classification; sequential forward selection, sequential forward floating selection, and the t-test for feature selection; and k-fold and leave-one-out cross-validation for error estimation. We apply these to three feature-label models and patient data from a breast cancer study. In sum, the cross-validation deviation distribution is significantly flatter when there is feature selection, compared with the case when cross-validation is performed on a given feature set. This is reflected by the observed positive values of the CRIDD, which is defined to quantify the contribution of feature selection towards the deviation variance.},
journal = {EURASIP J. Bioinformatics Syst. Biol.},
month = jan,
pages = {1},
numpages = {1}
}

@inproceedings{10.1145/1389095.1389362,
author = {Chow, Rick and Zhong, Wei and Blackmon, Michael and Stolz, Richard and Dowell, Marsha},
title = {An efficient SVM-GA feature selection model for large healthcare databases},
year = {2008},
isbn = {9781605581309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1389095.1389362},
doi = {10.1145/1389095.1389362},
abstract = {This paper presents an efficient hybrid feature selection model based on Support Vector Machine (SVM) and Genetic Algorithm (GA) for large healthcare databases. Even though SVM and GA are robust computational paradigms, the combined iterative nature of a SVM-GA hybrid system makes runtime costs infeasible when using large databases. This paper utilizes hierarchical clustering to reduce dataset size and SVM training time, multi-resolution parameter search for efficient SVM model selection, and chromosome caching to avoid redundant fitness evaluations. This approach significantly reduces runtime and improves classification performance.},
booktitle = {Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation},
pages = {1373–1380},
numpages = {8},
keywords = {classifier systems, data mining, genetic algorithms, machine learning, optimization, parameter tuning, support vector machines.},
location = {Atlanta, GA, USA},
series = {GECCO '08}
}

@article{10.1016/j.eswa.2009.10.027,
author = {Huang, Bingquan and Buckley, B. and Kechadi, T. -M.},
title = {Multi-objective feature selection by using NSGA-II for customer churn prediction in telecommunications},
year = {2010},
issue_date = {May, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {37},
number = {5},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.10.027},
doi = {10.1016/j.eswa.2009.10.027},
abstract = {This paper proposes a new multiobjective feature selection approach for churn prediction in telecommunication service field, based on the optimisation approach NSGA-II. The basic idea of this approach is to modify the approach NSGA-II to select local feature subsets of various sizes, and then to use the method of searching nondominated solutions to select the global nondominated feature subsets. Finally, the method FBSM which yields the fitness thresholds is proposed to choose the global solutions with the lowest ranks as the final solutions. In order to evaluate the proposed approach, experiments were carried out and the experimental results show that the proposed feature selection approach is efficient for churn prediction with multiobjectives.},
journal = {Expert Syst. Appl.},
month = may,
pages = {3638–3646},
numpages = {9},
keywords = {Churn prediction, Decision trees, Feature extraction and selection, Multiobjectives, NSGA-II, Nondominated solutions}
}

@article{10.1007/s10664-017-9557-6,
author = {Dintzner, Nicolas and Deursen, Arie and Pinzger, Martin},
title = {FEVER: An approach to analyze feature-oriented changes and artefact co-evolution in highly configurable systems},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9557-6},
doi = {10.1007/s10664-017-9557-6},
abstract = {The evolution of highly configurable systems is known to be a challenging task. Thorough understanding of configuration options their relationships, and their implementation in various types of artefacts (variability model, mapping, and implementation) is required to avoid compilation errors, invalid products, or dead code. Recent studies focusing on co-evolution of artefacts detailed feature-oriented change scenarios, describing how related artefacts might change over time. However, relying on manual analysis of commits, such work do not provide the means to obtain quantitative information on the frequency of described scenarios nor information on the exhaustiveness of the presented scenarios for the evolution of a large scale system. In this work, we propose FEVER and its instantiation for the Linux kernel. FEVER extracts detailed information on changes in variability models (KConfig files), assets (preprocessor based C code), and mappings (Makefiles). We apply this methodology to the Linux kernel and build a dataset comprised of 15 releases of the kernel history. We performed an evaluation of the FEVER approach by manually inspecting the data and compared it with commits in the system's history. The evaluation shows that FEVER accurately captures feature related changes for more than 85% of the 810 manually inspected commits. We use the collected data to reflect on occurrences of co-evolution in practice. Our analysis shows that complex co-evolution scenarios occur in every studied release but are not among the most frequent change scenarios, as they only occur for 8 to 13% of the evolving features. Moreover, only a minority of developers working on a given release will make changes to all artefacts related to a feature (between 10% and 13% of authors). While our conclusions are derived from observations on the evolution of the Linux kernel, we believe that they may have implications for tool developers as well as guide further research in the field of co-evolution of artefacts.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {905–952},
numpages = {48},
keywords = {Co-evolution, Feature, Highly variable systems, Variability}
}

@inproceedings{10.1007/11805816_14,
author = {Gupta, Kalyan Moy and Aha, David W. and Moore, Philip},
title = {Rough set feature selection algorithms for textual case-based classification},
year = {2006},
isbn = {3540368434},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11805816_14},
doi = {10.1007/11805816_14},
abstract = {Feature selection algorithms can reduce the high dimensionality of textual cases and increase case-based task performance. However, conventional algorithms (e.g., information gain) are computationally expensive. We previously showed that, on one dataset, a rough set feature selection algorithm can reduce computational complexity without sacrificing task performance. Here we test the generality of our findings on additional feature selection algorithms, add one data set, and improve our empirical methodology. We observed that features of textual cases vary in their contribution to task performance based on their part-of-speech, and adapted the algorithms to include a part-of-speech bias as background knowledge. Our evaluation shows that injecting this bias significantly increases task performance for rough set algorithms, and that one of these attained significantly higher classification accuracies than information gain. We also confirmed that, under some conditions, randomized training partitions can dramatically reduce training times for rough set algorithms without compromising task performance.},
booktitle = {Proceedings of the 8th European Conference on Advances in Case-Based Reasoning},
pages = {166–181},
numpages = {16},
location = {Fethiye, Turkey},
series = {ECCBR'06}
}

@article{10.1016/j.cageo.2009.02.004,
author = {K\"{o}hler, Andreas and Ohrnberger, Matthias and Scherbaum, Frank},
title = {Unsupervised feature selection and general pattern discovery using Self-Organizing Maps for gaining insights into the nature of seismic wavefields},
year = {2009},
issue_date = {September, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {9},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2009.02.004},
doi = {10.1016/j.cageo.2009.02.004},
abstract = {This study presents an unsupervised feature selection and learning approach for the discovery and intuitive imaging of significant temporal patterns in seismic single-station or network recordings. For this purpose, the data are parametrized by real-valued feature vectors for short time windows using standard analysis tools for seismic data, such as frequency-wavenumber, polarization, and spectral analysis. We use Self-Organizing Maps (SOMs) for a data-driven feature selection, visualization and clustering procedure, which is in particular suitable for high-dimensional data sets. Our feature selection method is based on significance testing using the Wald-Wolfowitz runs test for individual features and on correlation hunting with SOMs in feature subsets. Using synthetics composed of Rayleigh and Love waves and real-world data, we show the robustness and the improved discriminative power of that approach compared to feature subsets manually selected from individual wavefield parametrization methods. Furthermore, the capability of the clustering and visualization techniques to investigate the discrimination of wave phases is shown by means of synthetic waveforms and regional earthquake recordings.},
journal = {Comput. Geosci.},
month = sep,
pages = {1757–1767},
numpages = {11},
keywords = {Clustering, Data mining, Feature selection, Seismic features, Unsupervised learning}
}

@inproceedings{10.1007/11758501_27,
author = {Cr\u{a}ciun, Marian Viorel and Cocu, Adina and Dumitriu, Lumini\c{t}a and Segal, Cristina},
title = {A hybrid feature selection algorithm for the QSAR problem},
year = {2006},
isbn = {3540343792},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11758501_27},
doi = {10.1007/11758501_27},
abstract = {In this paper we discuss a hybrid feature selection algorithm for the Quantitative Structure Activity Relationship (QSAR) modelling. This is one of the goals in Predictive Toxicology domain, aiming to describe the relations between the chemical structure of a molecule and its biological or toxicological effects, in order to predict the behaviour of new, unknown chemical compounds. We propose a hybridization of the ReliefF algorithm based on a simple fuzzy extension of the value difference metric. The experimental results both on benchmark and real world applications suggest more stability in dealing with noisy data and our preliminary tests give a promising starting point for future research.},
booktitle = {Proceedings of the 6th International Conference on Computational Science - Volume Part I},
pages = {172–178},
numpages = {7},
location = {Reading, UK},
series = {ICCS'06}
}

@article{10.5555/2372179.2372185,
author = {Puuronen, Seppo and Tsymbal, Alexey},
title = {Local Feature Selection with Dynamic Integration of Classifiers},
year = {2001},
issue_date = {January 2001},
publisher = {IOS Press},
address = {NLD},
volume = {47},
number = {1–2},
issn = {0169-2968},
abstract = {Multidimensional data is often feature space heterogeneous so that individual features have unequal importance in different sub areas of the feature space. This motivates to search for a technique that provides a strategic splitting of the instance space being able to identify the best subset of features for each instance to be classified. Our technique applies the wrapper approach where a classification algorithm is used as an evaluation function to differentiate between different feature subsets. In order to make the feature selection local, we apply the recent technique for dynamic integration of classifiers. This allows to determine which classifier and which feature subset should be used for each new instance. Decision trees are used to help to restrict the number of feature combinations analyzed. For each new instance we consider only those feature combinations that include the features present in the path taken by the new instance in the decision tree built on the whole feature set. We evaluate our technique on data sets from the UCI machine learning repository. In our experiments, we use the C4.5 algorithm as the learning algorithm for base classifiers and for the decision trees that guide the local feature selection. The experiments show some advantages of the local feature selection with dynamic integration of classifiers in comparison with the selection of one feature subset for the whole space.},
journal = {Fundam. Inf.},
month = jan,
pages = {91–117},
numpages = {27},
keywords = {Feature selection, data mining, dynamic integration, ensemble of classifiers, machine learning}
}

@inproceedings{10.5555/1764441.1764564,
author = {Yu, Ting and Simoff, Simeon J. and Stokes, Donald},
title = {Incorporating prior domain knowledge into a kernel based feature selection algorithm},
year = {2007},
isbn = {9783540717003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes a new method of incorporating prior domain knowledge into a kernel based feature selection algorithm. The proposed feature selection algorithm combines the Fast Correlation-Based Filter (FCBF) and the kernel methods in order to uncover an optimal subset of features for the support vector regression. In the proposed algorithm, the Kernel Canonical Correlation Analysis (KCCA) is employed as a measurement of mutual information between feature candidates. Domain knowledge in forms of constraints is used to guide the tuning of the KCCA. In the second experiments, the audit quality research carried by Yang Li and Donald Stokes [1] provides the domain knowledge, and the result extends the original subset of features.},
booktitle = {Proceedings of the 11th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {1064–1071},
numpages = {8},
location = {Nanjing, China},
series = {PAKDD'07}
}

@article{10.5555/2736760.2736841,
author = {Windridge, David and Bowden, Richard},
title = {Hidden Markov chain estimation and parameterisation via ICA-based feature-selection},
year = {2005},
issue_date = {September 2005},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1–2},
issn = {1433-7541},
abstract = {We set out a methodology for the automated generation of hidden Markov models (HMMs) of observed feature-space transitions in a noisy experimental environment that is maximally generalising under the assumed experimental constraints. Specifically, we provide an ICA-based feature-selection technique for determining the number, and the transition sequence of the underlying hidden states, along with the observed-state emission characteristics when the specified noise model assumptions are fulfilled. In retaining correlation information between features, the method is potentially more general than the commonly employed Gaussian mixture model HMM parameterisation methods, to which we demonstrate that our method reduces when an arbitrary separation of features, or an experimentally-limited feature-space is imposed. A practical demonstration of the application of this method to automated sign-language classification is given, for which we demonstrate that a performance improvement of the order of 40% over naive Markovian modelling of the observed transitions is possible.},
journal = {Pattern Anal. Appl.},
month = sep,
pages = {115–124},
numpages = {10},
keywords = {Feature-selection, Feature-spaces, Hidden Markov models, ICA, Parameterisation, Sign-recognition}
}

@inproceedings{10.1145/1273496.1273516,
author = {Chen, Xue-wen and Jeong, Jong Cheol},
title = {Minimum reference set based feature selection for small sample classifications},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273516},
doi = {10.1145/1273496.1273516},
abstract = {We address feature selection problems for classification of small samples and high dimensionality. A practical example is microarray-based cancer classification problems, where sample size is typically less than 100 and number of features is several thousands or higher. One of the commonly used methods in addressing this problem is recursive feature elimination (RFE) method, which utilizes the generalization capability embedded in support vector machines and is thus suitable for small samples problems. We propose a novel method using minimum reference set (MRS) generated by the nearest neighbor rule. MRS is the set of minimum number of samples that correctly classify all the training samples. It is related to structural risk minimization principle and thus leads to good generalization. The proposed MRS based method is compared to RFE method with several real datasets, and experimental results show that the MRS method produces better classification performance.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {153–160},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@inproceedings{10.1007/11890348_39,
author = {Pirttikangas, Susanna and Fujinami, Kaori and Nakajima, Tatsuo},
title = {Feature selection and activity recognition from wearable sensors},
year = {2006},
isbn = {3540462872},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11890348_39},
doi = {10.1007/11890348_39},
abstract = {We describe our data collection and results on activity recognition with wearable, coin-sized sensor devices. The devices were attached to four different parts of the body: right thigh and wrist, left wrist and to a necklace on 13 different testees. In this experiment, data was from 17 daily life examples from male and female subjects. Features were calculated from triaxial accelerometer and heart rate data within different sized time windows. The best features were selected with forward-backward sequential search algorithm. Interestingly, acceleration mean values from the necklace were selected as important features. Two classifiers (multilayer perceptrons and kNN classifiers) were tested for activity recognition, and the best result (90.61 % aggregate recognition rate for 4-fold cross validation) was achieved with a kNN classifier.},
booktitle = {Proceedings of the Third International Conference on Ubiquitous Computing Systems},
pages = {516–527},
numpages = {12},
location = {Seoul, Korea},
series = {UCS'06}
}

@inproceedings{10.5555/3104322.3104433,
author = {Petrik, Marek and Taylor, Gavin and Parr, Ron and Zilberstein, Shlomo},
title = {Feature selection using regularization in approximate linear programs for Markov decision processes},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using L1 regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {871–878},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{10.5555/646418.693328,
author = {Dash, Manoranjan and Liu, Huan},
title = {Feature Selection for Clustering},
year = {2000},
isbn = {3540673822},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Clustering is an important data mining task. Data mining often concerns large and high-dimensional data but unfortunately most of the clustering algorithms in the literature are sensitive to largeness or high-dimensionality or both. Different features affect clusters differently, some are important for clusters while others may hinder the clustering task. An efficient way of handling it is by selecting a subset of important features. It helps in finding clusters efficiently, understanding the data better and reducing data size for efficient storage, collection and processing. The task of finding original important features for unsupervised data is largely untouched. Traditional feature selection algorithms work only for supervised data where class information is available. For unsupervised data, without class information, often principal components (PCs) are used, but PCs still require all features and they may be difficult to understand. Our approach: first features are ranked according to their importance on clustering and then a subset of important features are selected. For large data we use a scalable method using sampling. Empirical evaluation shows the effectiveness and scalability of our approach for benchmark and synthetic data sets.},
booktitle = {Proceedings of the 4th Pacific-Asia Conference on Knowledge Discovery and Data Mining, Current Issues and New Applications},
pages = {110–121},
numpages = {12},
series = {PADKK '00}
}

@article{10.1145/1007730.1007741,
author = {Zheng, Zhaohui and Wu, Xiaoyun and Srihari, Rohini},
title = {Feature selection for text categorization on imbalanced data},
year = {2004},
issue_date = {June 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/1007730.1007741},
doi = {10.1145/1007730.1007741},
abstract = {A number of feature selection metrics have been explored in text categorization, among which information gain (IG), chi-square (CHI), correlation coefficient (CC) and odds ratios (OR) are considered most effective. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicative of membership only, while feature selection using two-sided metrics implicitly combines the features most indicative of membership (e.g. positive features) and non-membership (e.g. negative features) by ignoring the signs of features. The former never consider the negative features, which are quite valuable, while the latter cannot ensure the optimal combination of the two kinds of features especially on imbalanced data. In this work, we investigate the usefulness of explicit control of that combination within a proposed feature selection framework. Using multinomial na\"{\i}ve Bayes and regularized logistic regression as classifiers, our experiments show both great potential and actual merits of explicitly combining positive and negative features in a nearly optimal fashion according to the imbalanced data.},
journal = {SIGKDD Explor. Newsl.},
month = jun,
pages = {80–89},
numpages = {10}
}

@inproceedings{10.1145/1454115.1454122,
author = {Coons, Katherine E. and Robatmili, Behnam and Taylor, Matthew E. and Maher, Bertrand A. and Burger, Doug and McKinley, Kathryn S.},
title = {Feature selection and policy optimization for distributed instruction placement using reinforcement learning},
year = {2008},
isbn = {9781605582825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454115.1454122},
doi = {10.1145/1454115.1454122},
abstract = {Communication overheads are one of the fundamental challenges in a multiprocessor system. As the number of processors on a chip increases, communication overheads and the distribution of computation and data become increasingly important performance factors. Explicit Dataflow Graph Execution (EDGE) processors, in which instructions communicate with one another directly on a distributed substrate, give the compiler control over communication overheads at a fine granularity. Prior work shows that compilers can effectively reduce fine-grained communication overheads in EDGE architectures using a spatial instruction placement algorithm with a heuristic-based cost function. While this algorithm is effective, the cost function must be painstakingly tuned. Heuristics tuned to perform well across a variety of applications leave users with little ability to tune performance-critical applications, yet we find that the best placement heuristics vary significantly with the application.First, we suggest a systematic feature selection method that reduces the feature set size based on the extent to which features affect performance. To automatically discover placement heuristics, we then use these features as input to a reinforcement learning technique, called Neuro-Evolution of Augmenting Topologies (NEAT), that uses a genetic algorithm to evolve neural networks. We show that NEAT outperforms simulated annealing, the most commonly used optimization technique for instruction placement. We use NEAT to learn general heuristics that are as effective as hand-tuned heuristics, but we find that improving over highly hand-tuned general heuristics is difficult. We then suggest a hierarchical approach to machine learning that classifies segments of code with similar characteristics and learns heuristics for these classes. This approach performs closer to the specialized heuristics. Together, these results suggest that learning compiler heuristics may benefit from both improved feature selection and classification.},
booktitle = {Proceedings of the 17th International Conference on Parallel Architectures and Compilation Techniques},
pages = {32–42},
numpages = {11},
keywords = {compiler heuristics, genetic algorithms, instruction scheduling, machine learning, neural networks},
location = {Toronto, Ontario, Canada},
series = {PACT '08}
}

@inproceedings{10.5555/1565835.1565869,
author = {Barandela, R. and Hern\'{a}ndez, J. K. and S\'{a}nchez, J. S. and Ferri, F. J.},
title = {Imbalanced Training Set Reduction and Feature Selection Through Genetic Optimization},
year = {2005},
isbn = {1586035606},
publisher = {IOS Press},
address = {NLD},
abstract = {Despite its simplicity and good classification performance, the Nearest Neighbor (NN) rule is not applied in many practical tasks because of the high amount of computational resources that it requires. Besides, when working with imbalanced training samples, its classification accuracy can be seriously degraded. In the present paper we propose two genetic algorithms to cope with these two issues. The purpose is to obtain complexity reduction while at the same time, to get a better balance in the training sample. Experimental results showing the benefits of our proposals are also reported.},
booktitle = {Proceedings of the 2005 Conference on Artificial Intelligence Research and Development},
pages = {215–222},
numpages = {8},
keywords = {Genetic Algorithms, Imbalanced Sample, Prototype And Feature Selection}
}

@article{10.5555/1005332.1044700,
author = {Yu, Lei and Liu, Huan},
title = {Efficient Feature Selection via Analysis of Relevance and Redundancy},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1205–1224},
numpages = {20}
}

@article{10.5555/3225663.3225979,
author = {Mavroeidis, Dimitrios and Bingham, Ella},
title = {Enhancing the stability and efficiency of spectral ordering with partial supervision and feature selection},
year = {2010},
issue_date = {May       2010},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {2},
issn = {0219-1377},
abstract = {Several studies have demonstrated the prospects of spectral ordering for data mining. One successful application is seriation of paleontological findings, i.e. ordering the sites of excavation, using data on mammal co-occurrences only. However, spectral ordering ignores the background knowledge that is naturally present in the domain: paleontologists can derive the ages of the sites within some accuracy. On the other hand, the age information is uncertain, so the best approach would be to combine the background knowledge with the information on mammal co-occurrences. Motivated by this kind of partial supervision we propose a novel semi-supervised spectral ordering algorithm that modifies the Laplacian matrix such that domain knowledge is taken into account. Also, it performs feature selection by discarding features that contribute most to the unwanted variability of the data in bootstrap sampling. Moreover, we demonstrate the effectiveness of the proposed framework on the seriation of Usenet newsgroup messages, where the task is to find out the underlying flow of discussion. The theoretical properties of our algorithm are thoroughly analyzed and it is demonstrated that the proposed framework enhances the stability of the spectral ordering output and induces computational gains.},
journal = {Knowl. Inf. Syst.},
month = may,
pages = {243–265},
numpages = {23},
keywords = {Eigengap, Laplacian, Matrix perturbation theory, Paleontology, Semi-supervised learning, Seriation, Spectral ordering}
}

@inproceedings{10.5555/1986381.1986424,
author = {Li, Shutao and Wang, Yaonan},
title = {Feature selection and fusion for texture classification},
year = {2005},
isbn = {3540259139},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, a novel texture classification method using selected and combined features from wavelet frame and steerable pyramid decompositions has been proposed. Firstly, wavelet frame and steerable pyramid decompositions are used to extract complementary features from texture regions. Then the number of features is reduced by selection using maximal information compression index. Finally the reduced features are combined and forwarded to SVM classifiers. The experimental results show that the proposed method used selected and fused features can achieve good classification accuracy and have low computational complexity.},
booktitle = {Proceedings of the Second International Conference on Advances in Neural Networks - Volume Part II},
pages = {268–273},
numpages = {6},
location = {Chongqing, China},
series = {ISNN'05}
}

@inproceedings{10.1145/1183614.1183711,
author = {Lu, Yumao and Peng, Fuchun and Li, Xin and Ahmed, Nawaaz},
title = {Coupling feature selection and machine learning methods for navigational query identification},
year = {2006},
isbn = {1595934332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1183614.1183711},
doi = {10.1145/1183614.1183711},
abstract = {It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector machine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the problem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web's relational content. A multi-level feature extraction system is constructed.Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, coupled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identified with 88.1% F1 score, which is 33% error rate reduction compared to the best uncoupled system, and 40% error rate reduction compared to a well tuned system without feature selection.},
booktitle = {Proceedings of the 15th ACM International Conference on Information and Knowledge Management},
pages = {682–689},
numpages = {8},
keywords = {machine learning, navigational query classification},
location = {Arlington, Virginia, USA},
series = {CIKM '06}
}

@article{10.5555/1756006.1756013,
author = {Aliferis, Constantin F. and Statnikov, Alexander and Tsamardinos, Ioannis and Mani, Subramani and Koutsoukos, Xenofon D.},
title = {Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {We present an algorithmic framework for learning local causal structure around target variables of interest in the form of direct causes/effects and Markov blankets applicable to very large data sets with relatively small samples. The selected feature sets can be used for causal discovery and classification. The framework (Generalized Local Learning, or GLL) can be instantiated in numerous ways, giving rise to both existing state-of-the-art as well as novel algorithms. The resulting algorithms are sound under well-defined sufficient conditions. In a first set of experiments we evaluate several algorithms derived from this framework in terms of predictivity and feature set parsimony and compare to other local causal discovery methods and to state-of-the-art non-causal feature selection methods using real data. A second set of experimental evaluations compares the algorithms in terms of ability to induce local causal neighborhoods using simulated and resimulated data and examines the relation of predictivity with causal induction performance.Our experiments demonstrate, consistently with causal feature selection theory, that local causal feature selection methods (under broad assumptions encompassing appropriate family of distributions, types of classifiers, and loss functions) exhibit strong feature set parsimony, high predictivity and local causal interpretability. Although non-causal feature selection methods are often used in practice to shed light on causal relationships, we find that they cannot be interpreted causally even when they achieve excellent predictivity. Therefore we conclude that only local causal techniques should be used when insight into causal structure is sought. In a companion paper we examine in depth the behavior of GLL algorithms, provide extensions, and show how local techniques can be used for scalable and accurate global causal graph learning.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {171–234},
numpages = {64}
}

@inproceedings{10.5555/3041838.3041900,
author = {Liu, Tao and Liu, Shengping and Chen, Zheng and Ma, Wei-Ying},
title = {An evaluation on feature selection for text clustering},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {Feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information. In this paper, we first give empirical evidence that feature selection methods can improve the efficiency and performance of text clustering algorithm. Then we propose a new feature selection method called "Term Contribution (TC)" and perform a comparative study on a variety of feature selection methods for text clustering, including Document Frequency (DF), Term Strength (TS), Entropy-based (En), Information Gain (IG) and χ2 statistic (CHI). Finally, we propose an "Iterative Feature Selection (IF)" method that addresses the unavailability of label problem by utilizing effective supervised feature selection method to iteratively select features and perform clustering. Detailed experimental results on Web Directory data are provided in the paper.},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {488–495},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}

@inproceedings{10.5555/1888593.1888609,
author = {Cheng, Jinyong and Liu, Yihui and Sang, Jun and Liu, Qiang and Wang, Shaoqing},
title = {Diagnosis of liver diseases from P31 MRS data based on feature selection using genetic algorithm},
year = {2010},
isbn = {3642156142},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {P31 MRS technique is important either in diagnosis or in treatment of many hepatic diseases for it can provides non-invasive information about the chemical content of the energy metabolism in cellular level. The data samples from P31 MRS are classified into three types of hepatocellular carcinoma, hepatic cirrhosis and normal hepatic tissue using computational intelligence methods. A genetic algorithm is used as main feature selection method and the Gaussian model is selected in the mutation operation. Two classification algorithms are used which consist of fisher linear discriminant analysis and quadratic discriminant analysis. Experiments show that the application of genetic algorithm and fisher linear classifier offers more reliable information for diagnostic prediction of liver cancer in vivo. And when the cross-validation method is 10-fold model, this algorithm can improve the average recognition correction rate of three types to 94.28%.},
booktitle = {Proceedings of the 2010 International Conference on Life System Modeling and Simulation and Intelligent Computing, and 2010 International Conference on Intelligent Computing for Sustainable Energy and Environment: Part III},
pages = {122–130},
numpages = {9},
keywords = {P31 MRS, fisher linear classifier, genetic algorighm, hepatic cirrhosis, hepatocellular carcinoma},
location = {Wuxi, China},
series = {LSMS/ICSEE'10}
}

@inproceedings{10.1145/3479645.3479664,
author = {Hutamaputra, William and Mawarni, Marrisaeka and Krisnabayu, Rifky Yunus and Mahmudy, Wayan Firdaus},
title = {Detection of Coronary Heart Disease Using Modified K-NN Method with Recursive Feature Elimination},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479664},
doi = {10.1145/3479645.3479664},
abstract = {Heart disease has infected many people in the world. One of the most common and deadly heart diseases is coronary artery disease (CAD). Several parameters can diagnose a patient who is positive for coronary artery disease (CAD). These parameters based on demographic, symptom and examination, ECG, and laboratory and echo features. For prevention needs to be done by early detection of patients who have the potential to have CAD. One way to do early detection is by building a system for making predictions. In this study, researchers used the KNN method by developing the weight of each class to increase accuracy. Before entering the KNN, the data will perform feature selection using SVM-RFE to find the ideal features and speed up computing time. The results of the KNN without feature selection are 82.65% of accuracy, the KNN with feature selection achieves 86.33% of accuracy, the Weighted KNN without feature selection achieves 83.45% of accuracy, and the Weighted KNN with feature selection achieves 90.88% of accuracy. The results prove the effectiveness of the Weighted KNN with feature selection.},
booktitle = {Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology},
pages = {146–150},
numpages = {5},
keywords = {Heart disease detection, KNN, Modified KNN, SVM-RFE feature selection},
location = {Malang, Indonesia},
series = {SIET '21}
}

@article{10.3233/JIFS-210466,
author = {Zixian, Zhang and Xuning, Liu and Zhixiang, Li and Hongqiang, Hu},
title = {Outburst prediction and influencing factors analysis based on Boruta-Apriori and BO-SVM algorithms},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210466},
doi = {10.3233/JIFS-210466},
abstract = {The influencing factors of coal and gas outburst are complex, and now the accuracy and efficiency of outburst prediction are not high. In order to obtain the effective features from influencing factors and realize the accurate and fast dynamic prediction of coal and gas outburst, this article proposes an outburst prediction model based on the coupling of feature selection and intelligent optimization classifier. Firstly, in view of the redundancy and irrelevance of the influencing factors of coal and gas outburst, we use Boruta feature selection method to obtain the optimal feature subset from influencing factors of coal and gas outburst. Secondly, based on Apriori association rules mining method, the internal association relationship between coal and gas outburst influencing factors is mined, and the strong association rules existing in the influencing factors and samples that affect the classification of coal and gas outburst are extracted. Finally, svm is used to classify coal and gas outburst based on the above obtained optimal feature subset and sample data, and Bayesian optimization algorithm is used to optimize the kernel parameters of svm, and the coal and gas outburst pattern recognition prediction model is established, which is compared with the existing coal and gas outburst prediction model in literatures. Compared with the method of feature selection and association rules mining alone, the proposed model achieves the highest prediction accuracy of 93% when the feature dimension is 3, which is higher than that of Apriori association rules and Boruta feature selection, and the classification accuracy is significantly improved. However, the feature dimension decreased significantly, the results show that the proposed model is better than other prediction models, which further verifies the accuracy and applicability of the coupling prediction model.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3201–3218},
numpages = {18},
keywords = {Coal and gas outburst, Feature selection, Boruta, Apriori, Bayesian optimization, SVM}
}

@article{10.1016/j.cmpb.2008.01.003,
author = {Yan, Zhiguo and Wang, Zhizhong and Xie, Hongbo},
title = {The application of mutual information-based feature selection and fuzzy LS-SVM-based classifier in motion classification},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {90},
number = {3},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2008.01.003},
doi = {10.1016/j.cmpb.2008.01.003},
abstract = {This paper presents an effective mutual information-based feature selection approach for EMG-based motion classification task. The wavelet packet transform (WPT) is exploited to decompose the four-class motion EMG signals to the successive and non-overlapped sub-bands. The energy characteristic of each sub-band is adopted to construct the initial full feature set. For reducing the computation complexity, mutual information (MI) theory is utilized to get the reduction feature set without compromising classification accuracy. Compared with the extensively used feature reduction methods such as principal component analysis (PCA), sequential forward selection (SFS) and backward elimination (BE) etc., the comparison experiments demonstrate its superiority in terms of time-consuming and classification accuracy. The proposed strategy of feature extraction and reduction is a kind of filter-based algorithms which is independent of the classifier design. Considering the classification performance will vary with the different classifiers, we make the comparison between the fuzzy least squares support vector machines (LS-SVMs) and the conventional widely used neural network classifier. In the further study, our experiments prove that the combination of MI-based feature selection and SVM techniques outperforms other commonly used combination, for example, the PCA and NN. The experiment results show that the diverse motions can be identified with high accuracy by the combination of MI-based feature selection and SVM techniques. Compared with the combination of PCA-based feature selection and the classical Neural Network classifier, superior performance of the proposed classification scheme illustrates the potential of the SVM techniques combined with WPT and MI in EMG motion classification.},
journal = {Comput. Methods Prog. Biomed.},
month = jun,
pages = {275–284},
numpages = {10},
keywords = {Classification, EMG, Mutual information, SVM, Separability, Wavelet packet}
}

@inproceedings{10.5555/3041838.3041946,
author = {Yu, Lei and Liu, Huan},
title = {Feature selection for high-dimensional data: a fast correlation-based filter solution},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {856–863},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}

@inproceedings{10.5555/2976248.2976373,
author = {Navot, Amir and Shpigelman, Lavi and Tishby, Naftali and Vaadia, Eilon},
title = {Nearest neighbor based feature selection for regression and its application to neural activity},
year = {2005},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present a non-linear, simple, yet effective, feature subset selection method for regression and use it in analyzing cortical neural activity. Our algorithm involves a feature-weighted version of the k-nearest-neighbor algorithm. It is able to capture complex dependency of the target function on its input and makes use of the leave-one-out error as a natural regularization. We explain the characteristics of our algorithm on synthetic problems and use it in the context of predicting hand velocity from spikes recorded in motor cortex of a behaving monkey. By applying feature selection we are able to improve prediction quality and suggest a novel way of exploring neural data.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
pages = {996–1002},
numpages = {7},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'05}
}

@inproceedings{10.5555/1779837.1779852,
author = {Yan, Luo and Changrui, Yu},
title = {A new hybrid algorithm for feature selection and its application to customer recognition},
year = {2007},
isbn = {3540735550},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes a novel hybrid algorithm for feature selection. This algorithm combines a global optimization algorithm called the simulated annealing algorithm based nested partitions (NP/SA). The resulting hybrid algorithm NP/SA retains the global perspective of the nested partitions algorithm and the local search capabilities of the simulated annealing method. We also present a detailed application of the new algorithm to a customer feature selection problem in customer recognition of a life insurance company and it is found to have great computation efficiency and convergence speed.},
booktitle = {Proceedings of the 1st International Conference on Combinatorial Optimization and Applications},
pages = {102–111},
numpages = {10},
location = {Xi'an, China},
series = {COCOA'07}
}

@article{10.5555/1756006.1756014,
author = {Aliferis, Constantin F. and Statnikov, Alexander and Tsamardinos, Ioannis and Mani, Subramani and Koutsoukos, Xenofon D.},
title = {Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {In part I of this work we introduced and evaluated the Generalized Local Learning (GLL) framework for producing local causal and Markov blanket induction algorithms. In the present second part we analyze the behavior of GLL algorithms and provide extensions to the core methods. Specifically, we investigate the empirical convergence of GLL to the true local neighborhood as a function of sample size. Moreover, we study how predictivity improves with increasing sample size. Then we investigate how sensitive are the algorithms to multiple statistical testing, especially in the presence of many irrelevant features. Next we discuss the role of the algorithm parameters and also show that Markov blanket and causal graph concepts can be used to understand deviations from optimality of state-of-the-art non-causal algorithms. The present paper also introduces the following extensions to the core GLL framework: parallel and distributed versions of GLL algorithms, versions with false discovery rate control, strategies for constructing novel heuristics for specific domains, and divide-and-conquer local-to-global learning (LGL) strategies. We test the generality of the LGL approach by deriving a novel LGL-based algorithm that compares favorably to the state-of-the-art global learning algorithms. In addition, we investigate the use of non-causal feature selection methods to facilitate global learning. Open problems and future research paths related to local and local-to-global causal learning are discussed.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {235–284},
numpages = {50}
}

@inproceedings{10.1145/1390156.1390251,
author = {Parr, Ronald and Li, Lihong and Taylor, Gavin and Painter-Wakefield, Christopher and Littman, Michael L.},
title = {An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390251},
doi = {10.1145/1390156.1390251},
abstract = {We show that linear value-function approximation is equivalent to a form of linear model approximation. We then derive a relationship between the model-approximation error and the Bellman error, and show how this relationship can guide feature selection for model improvement and/or value-function improvement. We also show how these results give insight into the behavior of existing feature-selection algorithms.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {752–759},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{10.1016/j.engappai.2015.03.013,
author = {Ben Ali, Jaouher and Saidi, Lotfi and Mouelhi, Aymen and Chebel-Morello, Brigitte and Fnaiech, Farhat},
title = {Linear feature selection and classification using PNN and SFAM neural networks for a nearly online diagnosis of bearing naturally progressing degradations},
year = {2015},
issue_date = {June 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2015.03.013},
doi = {10.1016/j.engappai.2015.03.013},
abstract = {In this work, an effort is made to characterize seven bearing states depending on the energy entropy of Intrinsic Mode Functions (IMFs) resulted from the Empirical Modes Decomposition (EMD). Three run-to-failure bearing vibration signals representing different defects either degraded or different failing components (roller, inner race and outer race) with healthy state lead to seven bearing states under study. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are used for feature reduction. Then, six classification scenarios are processed via a Probabilistic Neural Network (PNN) and a Simplified Fuzzy Adaptive Resonance Theory Map (SFAM) neural network. In other words, the three extracted feature data bases (EMD, PCA and LDA features) are processed firstly with SFAM and secondly with a combination of PNN-SFAM. The computation of classification accuracy and scattering criterion for each scenario shows that the EMD-LDA-PNN-SFAM combination is the suitable strategy for online bearing fault diagnosis. The proposed methodology reveals better generalization capability compared to previous works and it is validated by an online bearing fault diagnosis. The proposed strategy can be applied for the decision making of several assets. Display Omitted A new methodology proposed for a nearly online damage stage detection.The proposed strategy is based on neural networks and EMD method.The nearly online detection of bearing health status is done thanks to various damage stages.Experimental results show that this methodology is very effective.Unlike previous works, the proposed method is tested for the diagnosis of naturally progressing bearing degradations.},
journal = {Eng. Appl. Artif. Intell.},
month = jun,
pages = {67–81},
numpages = {15},
keywords = {Empirical Mode Decomposition (EMD), Linear Discriminant Analysis (LDA), Principal Component Analysis (PCA), Probabilistic Neural Network (PNN), Scatter matrix, Simplified Fuzzy Adaptive Resonance Theory Map (SFAM)}
}

@inproceedings{10.1007/11527503_37,
author = {Liu, Xiaoyan and Wang, Huaiqing and Xu, Dongming},
title = {The application of adaptive partitioned random search in feature selection problem},
year = {2005},
isbn = {354027894X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527503_37},
doi = {10.1007/11527503_37},
abstract = {Feature selection is one of important and frequently used techniques in data preprocessing. It can improve the efficiency and the effectiveness of data mining by reducing the dimensions of feature space and removing the irrelevant and redundant information. Feature selection can be viewed as a global optimization problem of finding a minimum set of M relevant features that describes the dataset as well as the original N attributes. In this paper, we apply the adaptive partitioned random search strategy into our feature selection algorithm. Under this search strategy, the partition structure and evaluation function is proposed for feature selection problem. This algorithm ensures the global optimal solution in theory and avoids complete randomness in search direction. The good property of our algorithm is shown through the theoretical analysis.},
booktitle = {Proceedings of the First International Conference on Advanced Data Mining and Applications},
pages = {307–314},
numpages = {8},
location = {Wuhan, China},
series = {ADMA'05}
}

@inproceedings{10.1145/1543834.1543925,
author = {Xu, Su and Zhou, Xu and Sun, Yi-ning},
title = {A genetic algorithm-based feature selection method for human identification based on ground reaction force},
year = {2009},
isbn = {9781605583266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1543834.1543925},
doi = {10.1145/1543834.1543925},
abstract = {Biometrics-based identification is a promising technology. Ground reaction force (GRF), with its characteristics of non-invasion, easily measurement and low environment-affection, shows a potential in this field. Feature selection is an important step in biometrics-based identification. In this paper, a genetic algorithm-based feature selection method was discussed. The proposed algorithm has the advantage of finding small subsets of features that perform well in identification. Two contrast experiments were conducted to show the effectiveness of the algorithm, which shows that with GA, higher identification accuracy and smaller feature size were reached},
booktitle = {Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation},
pages = {665–670},
numpages = {6},
keywords = {biometric identification, genetic algorithm, ground reaction force},
location = {Shanghai, China},
series = {GEC '09}
}

@inproceedings{10.5555/1781238.1781247,
author = {Ghodke, Sumukh and Baldwin, Timothy},
title = {An investigation into the interaction between feature selection and discretization: learning how and when to read numbers},
year = {2007},
isbn = {3540769269},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Pre-processing is an important part of machine learning, and has been shown to significantly improve the performance of classifiers. In this paper, we take a selection of pre-processing methods--focusing specifically on discretization and feature selection--and empirically examine their combined effect on classifier performance. In our experiments learning algorithms, namely one-R, ID3, naive Bayes, and IB1, and explore the impact of different forms of preprocessing on each combination of dataset and algorithm. We find that in general the combination of wrapper-based forward selection and naive supervised methods of discretization yield consistently above-baseline results.},
booktitle = {Proceedings of the 20th Australian Joint Conference on Advances in Artificial Intelligence},
pages = {48–57},
numpages = {10},
location = {Gold Coast, Australia},
series = {AI'07}
}

@inproceedings{10.5555/846227.848525,
author = {G\"{o}kberk, B. and Alpayd\'{y}n, E.},
title = {Feature Selection for Pose Invariant Face Recognition},
year = {2002},
isbn = {076951695X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {One of the major difficulties in face recognition systems is the in-depth pose variation problem. Most face recognition approaches assume that the pose of the face is known. In this work, we have designed a feature based pose estimation and face recognition system using 2D Gabor wavelets as local feature information. The difference of our system from the existing ones lies in its simplicity and its intelligent sampling of local features. Intelligent feature selection can be carried out by learning a set of parameters where the aim is the optimal performance of the overall system. In this paper, we give comparative analysis of the performance of our system with the standard modular Eigenfaces approach and show that local feature based approach improved the performance of both pose estimation and face recognition. For efficient coding, we have employed Principal Component Analysis (PCA) to the outputs of local feature vectors. Intelligent feature selection also reduced the space and time complexity of the system while retaining almost the same estimation and recognition accuracy.},
booktitle = {Proceedings of the 16 Th International Conference on Pattern Recognition (ICPR'02) Volume 4 - Volume 4},
pages = {40306},
series = {ICPR '02}
}

@inproceedings{10.1007/11538356_7,
author = {Conilione, Paul C. and Wang, Dianhui},
title = {E-Coli promoter recognition using neural networks with feature selection},
year = {2005},
isbn = {3540282270},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11538356_7},
doi = {10.1007/11538356_7},
abstract = {This paper investigates the effects on neural classification performance of biological data by features selection. Where the Relief-F and Symmetrical Tau feature selection algorithms were employed on a set of high level features of DNA and structural profiles. It was observed that even with a small percentage of the features used in neural classifiers, the recognition rate of E.coli promoters was not degraded significantly.},
booktitle = {Proceedings of the 2005 International Conference on Advances in Intelligent Computing - Volume Part II},
pages = {61–70},
numpages = {10},
location = {Hefei, China},
series = {ICIC'05}
}

@inproceedings{10.1145/3422392.3422409,
author = {Medeiros, Fl\'{a}vio and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Braz, Larissa and K\"{a}stner, Christian and Apel, Sven and Santos, Kleber},
title = {An Empirical Study on Configuration-Related Code Weaknesses},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422409},
doi = {10.1145/3422392.3422409},
abstract = {Developers often use the C preprocessor to handle variability and portability. However, many researchers and practitioners criticize the use of preprocessor directives because of their negative effect on code understanding, maintainability, and error proneness. This negative effect may lead to configuration-related code weaknesses, which appear only when we enable or disable certain configuration options. A weakness is a type of mistake in software that, in proper conditions, could contribute to the introduction of vulnerabilities within that software. Configuration-related code weaknesses may be harder to detect and fix than weaknesses that appear in all configurations, because variability increases complexity. To address this problem, we propose a sampling-based white-box technique to detect configuration-related weaknesses in configurable systems. To evaluate our technique, we performed an empirical study with 24 popular highly configurable systems that make heavy use of the C preprocessor, such as Apache Httpd and Libssh. Using our technique, we detected 57 configuration-related weaknesses in 16 systems. In total, we found occurrences of the following five kinds of weaknesses: 30 memory leaks, 10 uninitialized variables, 9 null pointer dereferences, 6 resource leaks, and 2 buffer overflows. The corpus of these weaknesses is a valuable source to better support further research on configuration-related code weaknesses.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {193–202},
numpages = {10},
keywords = {Code Weaknesses, Configurable Systems, Preprocessors},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1007/978-3-642-35380-2_43,
author = {Ghosh, Shameek and Ramachandran, Nayana and Venkateshwari, C. and Jayaraman, V. K.},
title = {Hybrid Biogeography Based Simultaneous Feature Selection and Prediction of N-Myristoylation Substrate Proteins Using Support Vector Machines and Random Forest Classifiers},
year = {2012},
isbn = {978-3-642-35379-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35380-2_43},
doi = {10.1007/978-3-642-35380-2_43},
abstract = {Majority of proteins undergo important post-translational modifications (PTM) that may alter physical and chemical properties of the protein and mainly their functions. Laboratory processes of determining PTM sites in proteins are laborious and expensive. On the contrary, computational approaches are far swifter and economical; and the models for prediction of PTMs can be quite accurate too. Among the PTMs, Protein N- terminal N-myristoylation by myristoyl-CoA protein N-myristoyltransferase (NMT) is an important lipid anchor modification of eukaryotic and viral proteins; occurring in about 0.5% encoded NMT substrates. Reliable recognition of myristoylation capability from the substrate amino acid sequence is useful for proteomic functional annotation projects as also in building therapeutics targeting the NMT. Using computational techniques, prediction-based models can be developed and new functions of protein substrates can be identified.In this study, we employ Biogeography based Optimization (BBO) for feature selection along with Support Vector Machines (SVM) and Random Forest for classification of N-myristoylation sequences. The simulations indicate that N-myristoylation sites can be identified with high accuracy using hybrid BBO wrappers in combination with weighted filter methods.},
booktitle = {Swarm, Evolutionary, and Memetic Computing: Third International Conference, SEMCCO 2012, Bhubaneswar, India, December 20-22, 2012. Proceedings},
pages = {364–371},
numpages = {8},
keywords = {Post-translational modifications (PTM), N-myristoylation, Biogeography based Optimization (BBO), Support Vector Machines (SVM), Random Forest classifier, Amino Acid Indices, dbPTM, SwissProt},
location = {Bhubaneswar, India}
}

@inproceedings{10.5555/1768409.1768482,
author = {G\'{o}mez-Verdejo, Vanessa and Verleysen, Michel and Fleury, J\'{e}r\^{o}me},
title = {Information-theoretic feature selection for the classification of hysteresis curves},
year = {2007},
isbn = {9783540730064},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a methodology for functional data analysis. It consists in extracting a large number of features with maximal content of information and then selecting the appropriate ones through a Mutual Information criterion; next, this reduced set of features is used to build a classifier. The methodology is applied to an industrial problem: the classification of the dynamic properties of elastomeric material characterized by rigidity and hysteresis curves.},
booktitle = {Proceedings of the 9th International Work Conference on Artificial Neural Networks},
pages = {522–529},
numpages = {8},
location = {San Sebasti\'{a}n, Spain},
series = {IWANN'07}
}

@inproceedings{10.1007/11589990_107,
author = {Chen, Haixia and Yuan, Senmiao and Jiang, Kai},
title = {Fitness approximation in estimation of distribution algorithms for feature selection},
year = {2005},
isbn = {3540304622},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11589990_107},
doi = {10.1007/11589990_107},
abstract = {Estimation of distribution algorithms (EDAs) are popular and robust algorithms that combine two technical disciplines of soft computing methodologies, probabilistic reasoning and evolutionary computing, for optimization problems. Several algorithms have already been proposed by different authors. However, these algorithms may require huge computation power, which is seldom considered in those applications. This paper introduces a “fast estimation of distribution algorithm” (FEDA) for feature selection that does not evaluate all new individuals by actual fitness function, thus reducing the computational cost and improve the performance. Bayesian networks are used to model the probabilistic distribution and generate new individuals in the optimization process. Moreover, fitness value is assigned to each new individual using the extended Bayesian network as an approximate model to fitness function. Implementation issues such as individual control strategy, model management are addressed. Promising results are achieved in experiments on 5 UCI datasets. The results indicate that, as population-sizing requirements for building appropriate models of promising solutions lead to good fitness estimates, more compact feature subsets that give more accurate result can be found.},
booktitle = {Proceedings of the 18th Australian Joint Conference on Advances in Artificial Intelligence},
pages = {904–909},
numpages = {6},
location = {Sydney, Australia},
series = {AI'05}
}

@article{10.1016/j.knosys.2010.07.003,
author = {Li, Shijin and Wu, Hao and Wan, Dingsheng and Zhu, Jiali},
title = {An effective feature selection method for hyperspectral image classification based on genetic algorithm and support vector machine},
year = {2011},
issue_date = {February, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {24},
number = {1},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2010.07.003},
doi = {10.1016/j.knosys.2010.07.003},
abstract = {With the development and popularization of the remote-sensing imaging technology, there are more and more applications of hyperspectral image classification tasks, such as target detection and land cover investigation. It is a very challenging issue of urgent importance to select a minimal and effective subset from those mass of bands. This paper proposed a hybrid feature selection strategy based on genetic algorithm and support vector machine (GA-SVM), which formed a wrapper to search for the best combination of bands with higher classification accuracy. In addition, band grouping based on conditional mutual information between adjacent bands was utilized to counter for the high correlation between the bands and further reduced the computational cost of the genetic algorithm. During the post-processing phase, the branch and bound algorithm was employed to filter out those irrelevant band groups. Experimental results on two benchmark data sets have shown that the proposed approach is very competitive and effective.},
journal = {Know.-Based Syst.},
month = feb,
pages = {40–48},
numpages = {9},
keywords = {Band selection, Branch and bound algorithm, Conditional mutual information, Genetic algorithm, Hyperspectral remote sensing, Support vector machine}
}

@article{10.1016/j.specom.2010.04.002,
author = {Chakroborty, Sandipan and Saha, Goutam},
title = {Feature selection using singular value decomposition and QR factorization with column pivoting for text-independent speaker identification},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {9},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2010.04.002},
doi = {10.1016/j.specom.2010.04.002},
abstract = {Selection of features is one of the important tasks in the application like Speaker Identification (SI) and other pattern recognition problems. When multiple features are extracted from the same frame of speech, it is expected that a feature vector would contain redundant features. Redundant features confuse the speaker model in multidimensional space resulting in degraded performance by the system. Careful selection of potential features can remove this redundancy while helping to achieve the higher rate of accuracy at lower computational cost. Although the selection of features is difficult without having exhaustive search, this paper proposes an alternative and straight forward technique for feature selection using Singular Value Decomposition (SVD) followed by QR Decomposition with Column Pivoting (QRcp). The idea is to capture the most salient part of the information from the speakers' data by choosing those features that can explain different dimensions showing minimal similarities (or maximum acoustic variability) among them in orthogonal sense. The performances after selection of features using proposed criterion have been compared with using Mel-frequency Cepstral Coefficients (MFCC), Linear Frequency (LF) Cepstral Coefficients (LFCC) and a new feature proposed in this paper that is based on Gaussian shaped filters on mel-scale. It is shown that proposed SVD-QRcp based feature selection outperforms F-Ratio based method and the proposed feature extraction tool is superior to baseline MFCC &amp; LFCC.},
journal = {Speech Commun.},
month = sep,
pages = {693–709},
numpages = {17},
keywords = {Correlation, Divergence, F-Ratio, GMFCC, GMM, LFCC, MFCC, QRcp, SVD, Speaker identification, Subband}
}

@inproceedings{10.5555/1791734.1791797,
author = {Feng, Shi and Liu, Yan and Wang, Daling and Shen, Derong},
title = {A novel and effective method for web system tuning based on feature selection},
year = {2008},
isbn = {3540788484},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Web has become the main platform for the interchange of information and the transaction of commerce. The performance of a Web system can be greatly improved by tuning its configuration parameters. However, there are dozens or even hundreds of tunable parameters in one Web system, and tuning can be the tough work even for the most experienced server administrators. Traditional Web tuning methods only focus on two or three specified parameters, and can not provide an effective solution to the tuning problem when the number of parameters is large. In this paper, we propose a feature selection algorithm based on Information Gain criterion to find the key parameters of a Web system. The algorithm can pick out the parameters that significantly affect Web system performance. Therefore, the tuning approach can be simplified dramatically. We have carried out extensive experiments with different Web systems. The results show that the algorithm is effective in searching the most important parameters under different conditions and reducing the time cost of next tuning steps.},
booktitle = {Proceedings of the 10th Asia-Pacific Web Conference on Progress in WWW Research and Development},
pages = {537–547},
numpages = {11},
location = {Shenyang, China},
series = {APWeb'08}
}

@inproceedings{10.5555/1893496.1893515,
author = {Mart\'{\i}n-F\'{e}lez, Ra\'{u}l and Mollineda, Ram\'{o}n A.},
title = {On the suitability of combining feature selection and resampling to manage data complexity},
year = {2009},
isbn = {364214263X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The effectiveness of a learning task depends on data complexity (class overlap, class imbalance, irrelevant features, etc.). When more than one complexity factor appears, two or more preprocessing techniques should be applied. Nevertheless, no much effort has been devoted to investigate the importance of the order in which they can be used. This paper focuses on the joint use of feature reduction and balancing techniques, and studies which could be the application order that leads to the best classification results. This analysis was made on a specific problem whose aim was to identify the melodic track given a MIDI file. Several experiments were performed from different imbalanced 38- dimensional training sets with many more accompaniment tracks than melodic tracks, and where features were aggregated without any correlation study. Results showed that the most effective combination was the ordered use of resampling and feature reduction techniques.},
booktitle = {Proceedings of the Current Topics in Artificial Intelligence, and 13th Conference on Spanish Association for Artificial Intelligence},
pages = {141–150},
numpages = {10},
keywords = {class imbalance problem, data complexity, feature reduction, melody finding, music information retrieval},
location = {Seville, Spain},
series = {CAEPIA'09}
}

@inproceedings{10.1007/978-3-319-09192-1_6,
author = {Rocha Vicente, F\'{a}bio Fernandes and Lopes, Fabr\'{\i}cio Martins},
title = {SFFS-SW: A Feature Selection Algorithm Exploring the Small-World Properties of GNs},
year = {2014},
isbn = {9783319091914},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-09192-1_6},
doi = {10.1007/978-3-319-09192-1_6},
abstract = {In recent years, several methods for gene networks (GNs) inference from expression data have been developed. Also, models of data integration (as protein-protein and protein-DNA) are nowadays broadly used to face the problem of few amount of expression data. Moreover, it is well known that biological networks conserve some topological properties. The small-world topology is a common arrangement in nature found both in biological and non-biological phenomena. However, in general this information is not used by GNs inference methods. In this work we proposed a new GNs inference algorithm that combines topological features and expression data. The algorithm outperforms the approach that uses only expression data both in accuracy and measures of recovered network.},
booktitle = {Proceedings of the 9th IAPR International Conference on Pattern Recognition in Bioinformatics - Volume 8626},
pages = {60–71},
numpages = {12},
keywords = {bioinformatics, feature selection, gene networks, graph theory, pattern recognition, small-world},
location = {Stockholm, Sweden},
series = {PRIB 2014}
}

@inproceedings{10.1007/11691730_13,
author = {Li, Guo-Zheng and Liu, Tian-Yu and Cheng, Victor S.},
title = {Classification of brain glioma by using SVMs bagging with feature selection},
year = {2006},
isbn = {3540331042},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11691730_13},
doi = {10.1007/11691730_13},
abstract = {The degree of malignancy in brain glioma needs to be assessed by MRI findings and clinical data before operations. There have been previous attempts to solve this problem by using fuzzy max-min neural networks and support vector machines (SVMs), while in this paper, a novel algorithm named PRIFEB is proposed by combining bagging of SVMs with embedded feature selection for its individuals. PRIFEB is compared with the general case of bagging on UCI data sets, experimental results show PRIFEB can obtain better performance than the general case of bagging. Then, PRIFEB is used to predict the degree of malignancy in brain glioma, computation results show that PRIFEB obtains better accuracy than other several methods like bagging of SVMs and single SVMs does.},
booktitle = {Proceedings of the 2006 International Conference on Data Mining for Biomedical Applications},
pages = {124–130},
numpages = {7},
location = {Singapore},
series = {BioDM'06}
}

@article{10.1016/j.artmed.2007.02.002,
author = {Luts, Jan and Heerschap, Arend and Suykens, Johan A. K. and Van Huffel, Sabine},
title = {A combined MRI and MRSI based multiclass system for brain tumour recognition using LS-SVMs with class probabilities and feature selection},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {40},
number = {2},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2007.02.002},
doi = {10.1016/j.artmed.2007.02.002},
abstract = {Objective: This study investigates the use of automated pattern recognition methods on magnetic resonance data with the ultimate goal to assist clinicians in the diagnosis of brain tumours. Recently, the combined use of magnetic resonance imaging (MRI) and magnetic resonance spectroscopic imaging (MRSI) has demonstrated to improve the accuracy of classifiers. In this paper we extend previous work that only uses binary classifiers to assess the type and grade of a tumour to a multiclass classification system obtaining class probabilities. The important problem of input feature selection is also addressed. Methods and material: Least squares support vector machines (LS-SVMs) with radial basis function kernel are applied and compared with linear discriminant analysis (LDA). Both a Bayesian framework and cross-validation are used to infer the parameters of the LS-SVM classifiers. Four different techniques to obtain multiclass probabilities as a measure of accuracy are compared. Four variable selection methods are explored. MRI and MRSI data are selected from the INTERPRET project database. Results: The results illustrate the significantly better performance of automatic relevance determination (ARD), in combination with LS-SVMs in a Bayesian framework and coupling of class probabilities, compared to classical LDA. Conclusion: It is demonstrated that binary LS-SVMs can be extended to a multiclass classifier system obtaining class probabilities by Bayesian techniques and pairwise coupling. Feature selection based on ARD further improves the results. This classifier system can be of great help in the diagnosis of brain tumours.},
journal = {Artif. Intell. Med.},
month = jun,
pages = {87–102},
numpages = {16},
keywords = {Brain tumours, Class probabilities, Feature selection, Magnetic resonance imaging (MRI), Magnetic resonance spectroscopic imaging (MRSI), Multiclass classification}
}

@article{10.1007/s11042-020-10011-7,
author = {N, Thomas Rincy and Gupta, Roopam},
title = {An efficient feature subset selection approach for machine learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {8},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10011-7},
doi = {10.1007/s11042-020-10011-7},
abstract = {This literature introduces&nbsp;CAPPER, an efficient hybrid feature subset selection approach which is the combination of feature subsets from the Correlation-based feature selection (CFS) and Wrapper approaches. CFS is basically a filter approach that appraises the merits of subset attributes by classifying the feature ability according to the amount of redundancy between them and the feature subset selection for Wrappers that examines the attributes by applying the induction of various machine learning algorithms. For the evaluation of metrics, the CAPPER&nbsp;approach is tested on the different domains of datasets. The reduced, highly merit and accurate feature subsets obtained from CAPPER approach are then trained with the machine learning algorithm and evaluated by cross-validation for the set of attributes. Moreover, a statistical approach is applied for the significance of the&nbsp;result. It was observed that the CAPPER approach surpasses the CFS and Wrapper approaches on different domains of datasets.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {12737–12830},
numpages = {94},
keywords = {Feature selection, Machine learning algorithms, Proposed CAPPER approach, CFS approach, Wrapper approach, Datasets}
}

@inproceedings{10.3115/1218955.1219035,
author = {Niu, Zheng-Yu and Ji, Dong-Hong and Tan, Chew-Lim},
title = {Learning word senses with feature selection and order identification capabilities},
year = {2004},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1218955.1219035},
doi = {10.3115/1218955.1219035},
abstract = {This paper presents an unsupervised word sense learning algorithm, which induces senses of target word by grouping its occurrences into a "natural" number of clusters based on the similarity of their contexts. For removing noisy words in feature set, feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner. Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number. Experimental results show that our algorithm can find important feature subset, estimate model order (cluster number) and achieve better performance than another algorithm which requires cluster number to be provided.},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
pages = {629–es},
location = {Barcelona, Spain},
series = {ACL '04}
}

@inproceedings{10.1145/3473258.3473276,
author = {Li, Jiajia and Qiao, Chen and Ren, Kai and Chen, Jian and Fang, Hanfeng},
title = {Biomarkers Selection based on FS-TNNR in Schizophrenia},
year = {2021},
isbn = {9781450389655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473258.3473276},
doi = {10.1145/3473258.3473276},
abstract = {Schizophrenia (SZ) is a chronic mental illness that severely affects people's thoughts, feelings and behaviors. As time goes by, the symptoms will become more and more serious, clinically manifested as the confusion of thinking and speech, delusions, hallucinations, etc., which seriously endanger human health and life. Studies have shown that SZ is associated with abnormal brain regions and abnormal functional connections in brain regions. In recent years, magnetic resonance imaging (MRI) technology has been widely used in the medical field and scientific research, which contributes to the study of SZ. However, MRI data usually has a small sample size but a large number of features. How to effectively reduce the data dimension and accurately detect the biomarkers of SZ is of great significance in the diagnosis and further research of SZ. In this paper, truncated nuclear norm regularization (TNNR) is introduced into feature selection (FS) to form an embedded feature selection method. The non-convex low-rank advantage of the truncated nuclear norm as a regularization term is used to filter out those features that contribute the most to the identification data in SZ magnetic resonance data containing a large number of features and study the abnormal regions and abnormal functional connections in the SZ brain. The results show that the abnormal brain regions of SZ are mainly distributed in the supramarginal gyrus, cingulate gyrus, frontal gyrus, precuneus, frontal gyrus and caudate, and the abnormal functional connections are mainly related to the superior parietal gyrus, superior occipital gyrus, caudate nucleus, middle frontal gyrus, insula, supplementary motor area and precuneus. These abnormalities may lead to incorrect information processing in the brain, which provides further evidence for cognitive impairment in SZ.},
booktitle = {Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology},
pages = {116–122},
numpages = {7},
keywords = {feature selection, magnetic resonance imaging, schizophrenia, truncated nuclear norm regularization},
location = {Xi'an, China},
series = {ICBBT '21}
}

@inproceedings{10.1007/11919476_58,
author = {Jia, Jingping and Feng, David and Chai, Yanmei and Zhao, Rongchun and Chi, Zheru},
title = {Blob tracking with adaptive feature selection and accurate scale determination},
year = {2006},
isbn = {3540486283},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11919476_58},
doi = {10.1007/11919476_58},
abstract = {We propose a novel color based tracking framework in which an object configuration and color feature are simultaneously determined via scale space filtration. The tracker can automatically select discriminative color feature that well distinguishes foreground from background. According to that feature, a likelihood image of the target is generated for each incoming frame. The target’s area turns into a blob in the likelihood image. The scale of this blob can be determined based on the local maximum of differential scale-space filters. We employ the QP_TR trust region algorithm to search for the local maximum of multi-scale normalized Laplacian filter of the likelihood image to locate the target as well as determine its scale. Based on the tracking results of sequence examples, the proposed method has been proven to be resilient to the color and lighting changes, be capable of describing the target more accurately and achieve much better tracking precision.},
booktitle = {Proceedings of the Second International Conference on Advances in Visual Computing - Volume Part I},
pages = {579–588},
numpages = {10},
location = {Lake Tahoe, NV},
series = {ISVC'06}
}

@article{10.1016/j.eswa.2007.08.010,
author = {Chen, Liang-Hsuan and Hsiao, Huey-Der},
title = {Feature selection to diagnose a business crisis by using a real GA-based support vector machine: An empirical study},
year = {2008},
issue_date = {October, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {35},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.08.010},
doi = {10.1016/j.eswa.2007.08.010},
abstract = {This research is aimed at establishing the diagnosis models for business crises through integrating a real-valued genetic algorithm to determine the optimum parameters and SVM to perform learning and classification on data. After finishing the training processes, the proposed GA-SVM can reach a prediction accuracy of up to 95.56% for all the tested business data. Particularly, only six influential features are included in the proposed model with intellectual capital and financial features after the 2-phase selecting process; the six features are ordinary and widely available from public business reports. The proposed GA-SVM is available for business managers to conduct self-diagnosis in order to realize whether business units are really facing a crisis.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {1145–1155},
numpages = {11},
keywords = {Business crisis, GA, Intellectual capital, SVM}
}

@article{10.1016/j.eswa.2006.05.013,
author = {Polat, Kemal and G\"{u}ne\c{s}, Salih},
title = {Medical decision support system based on artificial immune recognition immune system (AIRS), fuzzy weighted pre-processing and feature selection},
year = {2007},
issue_date = {August, 2007},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {33},
number = {2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2006.05.013},
doi = {10.1016/j.eswa.2006.05.013},
abstract = {In this study, diagnosis of hepatitis disease, which is a very common and important disease, was conducted with a machine learning system. The proposed machine learning approach has three stages. The first stage, the feature number of hepatitis disease dataset was reduced to 10 from 19 in the feature selection (FS) sub-program by means of C 4.5 decision tree algorithm. Then, hepatitis disease dataset is normalized in the range of [0,1] and is weighted with fuzzy weighted pre-processing. Then, weighted input values obtained from fuzzy weighted pre-processing is classified by using AIRS classifier system. In this study, fuzzy weighted pre-processing, which can improved by ours, is a new method and firstly, it is applied to hepatitis disease dataset. We took the dataset used in our study from the UCI machine learning database. The obtained classification accuracy of our system was 94.12% and it was very promising with regard to the other classification applications in the literature for this problem.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {484–490},
numpages = {7},
keywords = {Artificial immune recognition immune system, Feature selection, Fuzzy weighted pre-processing, Hepatitis disease, Medical diagnosis}
}

@inproceedings{10.1145/1401890.1401910,
author = {Chen, Xue-wen and Wasikowski, Michael},
title = {FAST: a roc-based feature selection metric for small samples and imbalanced data classification problems},
year = {2008},
isbn = {9781605581934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401890.1401910},
doi = {10.1145/1401890.1401910},
abstract = {The class imbalance problem is encountered in a large number of practical applications of machine learning and data mining, for example, information retrieval and filtering, and the detection of credit card fraud. It has been widely realized that this imbalance raises issues that are either nonexistent or less severe compared to balanced class cases and often results in a classifier's suboptimal performance. This is even more true when the imbalanced data are also high dimensional. In such cases, feature selection methods are critical to achieve optimal performance. In this paper, we propose a new feature selection method, Feature Assessment by Sliding Thresholds (FAST), which is based on the area under a ROC curve generated by moving the decision boundary of a single feature classifier with thresholds placed using an even-bin distribution. FAST is compared to two commonly-used feature selection methods, correlation coefficient and RELevance In Estimating Features (RELIEF), for imbalanced data classification. The experimental results obtained on text mining, mass spectrometry, and microarray data sets showed that the proposed method outperformed both RELIEF and correlation methods on skewed data sets and was comparable on balanced data sets; when small number of features is preferred, the classification performance of the proposed method was significantly improved compared to correlation and RELIEF-based methods.},
booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {124–132},
numpages = {9},
keywords = {ROC, feature selection, imbalanced data classification},
location = {Las Vegas, Nevada, USA},
series = {KDD '08}
}

@inproceedings{10.5555/2969033.2969054,
author = {Wang, Qian and Zhang, Jiaxing and Song, Sen and Zhang, Zheng},
title = {Attentional neural network: feature selection using cognitive feedback},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Attentional Neural Network is a new framework that integrates top-down cognitive bias and bottom-up feature extraction in one coherent architecture. The top-down influence is especially effective when dealing with high noise or difficult segmentation problems. Our system is modular and extensible. It is also easy to train and cheap to run, and yet can accommodate complex behaviors. We obtain classification accuracy better than or competitive with state of art results on the MNIST variation dataset, and successfully disentangle overlaid digits with high success rates. We view such a general purpose framework as an essential foundation for a larger system emulating the cognitive abilities of the whole brain.},
booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2033–2041},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@article{10.5555/1313030.1313038,
author = {Ruiz, Roberto and Riquelme, Jos\'{e} C. and Aguilar-Ruiz, Jes\'{u}s S.},
title = {Projection-based measure for efficient feature selection},
year = {2002},
issue_date = {December 2002},
publisher = {IOS Press},
address = {NLD},
volume = {12},
number = {3,4},
issn = {1064-1246},
abstract = {The attribute selection techniques for supervised learning, used in the preprocessing phase to emphasize the most relevant attributes, allow making models of classification simpler and easy to understand. Depending on the method to apply: starting point, search organization, evaluation strategy, and the stopping criterion, there is an added cost to the classification algorithm that we are going to use, that normally will be compensated, in greater or smaller extent, by the attribute reduction in the classification model. The method proposed in this work utilizes a measure based on projections to guide the selection of the attributes. The algorithm (SOAP: Selection of Attributes by Projection) has some interesting characteristics: lower computational cost (O(mn log n) m attributes and n examples in the data set) with respect to other typical algorithms due to the absence of distance and statistical calculations; its applicability to any labelled data set, that is to say, it can contain continuous and discrete variables, with no need for transformation. The performance of SOAP is analysed in two ways: percentage of reduction and classification. SOAP has been compared to CFS [4] and ReliefF [8]. The results are generated by C4.5 before and after the application of the algorithms.},
journal = {J. Intell. Fuzzy Syst.},
month = dec,
pages = {175–183},
numpages = {9}
}

@article{10.1016/j.engappai.2005.08.004,
author = {Ac\i{}r, Nurettin and \"{O}zdamar, \"{O}zcan and G\"{u}zeli\c{s}, C\"{u}neyt},
title = {Automatic classification of auditory brainstem responses using SVM-based feature selection algorithm for threshold detection},
year = {2006},
issue_date = {March, 2006},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {19},
number = {2},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2005.08.004},
doi = {10.1016/j.engappai.2005.08.004},
abstract = {This paper presents a novel system for automatic recognition of auditory brainstem responses (ABR) to detect hearing threshold. ABR is an important potential signal for determining objective audiograms. Its detection is usually performed by medical experts with often basic signal processing techniques. The proposed system comprises of two stages. In the first stage, for feature extraction, a set of raw amplitude values, a set of discrete cosine transform (DCT) coefficients and a set of discrete wavelet transform (DWT) approximation coefficients are calculated and extracted from signals separately as three different sets of feature vectors. These features are then selected by a modified adaptive method, which mainly supports to the input dimension reduction via selecting the most significant feature components. In the second stage, the feature vectors are classified by a support vector machine (SVM) classifier which is a powerful advanced technique for solving supervised binary classification problem due to its generalization ability. After that the proposed system is applied to real ABR data and it is resulted in a very good sensitivity, specificity and accuracy levels for DCT coefficients such as 99.2%, 94.0% and 96.2%, respectively. Consequently, the proposed system can be used for recognition of ABRs for hearing threshold detection.},
journal = {Eng. Appl. Artif. Intell.},
month = mar,
pages = {209–218},
numpages = {10},
keywords = {Auditory evoked potentials, Feature selection, Support vector machines}
}

@inproceedings{10.5555/2073946.2073958,
author = {Coetzee, Frans M. and Lawrence, Steve and Giles, C. Lee},
title = {Bayesian classification and feature selection from finite data sets},
year = {2000},
isbn = {1558607099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Feature selection aims to select the smallest subset of features for a specified level of performance. The optimal achievable classification performance on a feature subset is summarized by its Receiver Operating Curve (ROC). When infinite data is available, the Neyman-Pearson (NP) design procedure provides the most efficient way of obtaining this curve. In practice the design procedure is applied to density estimates from finite data sets. We perform a detailed statistical analysis of the resulting error propagation on finite alphabets. We show that the estimated performance curve (EPC) produced by the design procedure is arbitrarily accurate given sufficient data, independent of the size of the feature set. However, the underlying likelihood ranking procedure is highly sensitive to errors that reduces the probability that the EPC is in fact the ROC. In the worst case, guaranteeing that the EPC is equal to the ROC may require data sizes exponential in the size of the feature set. These results imply that in theory the NP design approach may only be valid for characterizing relatively small feature subsets, even when the performance of any given classifier can be estimated very accurately. We discuss the practical limitations for on-line methods that ensures that the NP procedure operates in a statistically valid region.},
booktitle = {Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
pages = {89–97},
numpages = {9},
location = {Stanford, California},
series = {UAI'00}
}

@article{10.1016/j.eswa.2010.09.135,
author = {Warren Liao, T.},
title = {Short communication: Diagnosis of bladder cancers with small sample size via feature selection},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.135},
doi = {10.1016/j.eswa.2010.09.135},
abstract = {This paper proposes feature selection as an approach to deal with a bladder cancer data set with small sample size. Three feature selection methods and four classifiers were used to determine the best feature subsets that produce perfect classification accuracy. The smallest best feature subsets are used to build neural models with the small data set to achieve 100% training and testing accuracies. Therefore, the mega-trend-diffusion technique proposed by Li et al. to produce artificial samples is actually unnecessary. The similarity classifier proposed by Luukka was also applied to the small data set with the smallest best feature subsets to achieve 100% accuracy using only 4 samples (two with bladder cancer and two normal) for two selected p and m values. Given the same accuracy, using the best feature subsets selected is better than using all 13 features as done by Luukka. Furthermore, several indexes/methods commonly used in filtering feature selection methods were tested for their ability to find the best feature subsets for this particular small bladder cancer data set.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4649–4654},
numpages = {6},
keywords = {Artificial neural network, Bladder cancer, Classifier, Feature selection, Filtering method, Small sample size, Wrapper method}
}

@article{10.1016/j.compeleceng.2012.09.001,
author = {Idris, Adnan and Rizwan, Muhammad and Khan, Asifullah},
title = {Churn prediction in telecom using Random Forest and PSO based data balancing in combination with various feature selection strategies},
year = {2012},
issue_date = {November, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {6},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2012.09.001},
doi = {10.1016/j.compeleceng.2012.09.001},
abstract = {The telecommunication industry faces fierce competition to retain customers, and therefore requires an efficient churn prediction model to monitor the customer's churn. Enormous size, high dimensionality and imbalanced nature of telecommunication datasets are main hurdles in attaining the desired performance for churn prediction. In this study, we investigate the significance of a Particle Swarm Optimization (PSO) based undersampling method to handle the imbalance data distribution in collaboration with different feature reduction techniques such as Principle Component Analysis (PCA), Fisher's ratio, F-score and Minimum Redundancy and Maximum Relevance (mRMR). Whereas Random Forest (RF) and K Nearest Neighbour (KNN) classifiers are employed to evaluate the performance on optimally sampled and reduced features dataset. Prediction performance is evaluated using sensitivity, specificity and Area under the curve (AUC) based measures. Finally, it is observed through simulations that our proposed approach based on PSO, mRMR, and RF termed as Chr-PmRF, performs quite well for predicting churners and therefore can be beneficial for highly competitive telecommunication industry.},
journal = {Comput. Electr. Eng.},
month = nov,
pages = {1808–1819},
numpages = {12}
}

@article{10.1504/ijict.2020.105606,
author = {Wang, Ping-Ping and Ma, Lei and Lv, Yun-Hui and Xiang, Yan and Shao, Dang-Guo and Xiong, Xin},
title = {Deep forest-based hypertension and OSAHS patient screening model},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {2},
issn = {1466-6642},
url = {https://doi.org/10.1504/ijict.2020.105606},
doi = {10.1504/ijict.2020.105606},
abstract = {Incidence of OSAHS is high in hypertension patients. To make the OSAHS diagnosis more precise and simple, an OSAHS screening model is built hereof by deep forest algorithm with the collected information of hypertension and OSHAS patients from the Sleep and Respiration Centre of a hospital. Firstly, variation in index and dimensions and inter-class imbalance in sample dataset is resolved by normalisation and SMOTE method; and OSAHS screening model is built by deep forest method (gcForest) after redundant information in features is removed with modified chi-square test single feature selection. The results show that with modified chi-square test single feature selection method, the redundant features can be effectively removed and performance of classifier can be improved; deep forest-based OSAHS screening model is superior to other classification models in classification performance and can effectively improve the precision of OSAHS patient screening, reduce the incidence of OSAHS missed diagnosis.},
journal = {Int. J. Inf. Commun. Techol.},
month = jan,
pages = {112–122},
numpages = {10},
keywords = {hypertension, obstructive sleep apnea-hypopnea syndrome, OSAHS, unbalanced data, feature selection, deep forest, screening model}
}

@inproceedings{10.5555/647967.741754,
author = {Puuronen, Seppo and Tsymbal, Alexey and Skrypnyk, Iryna},
title = {Correlation-Based and Contextual Merit-Based Ensemble Feature Selection},
year = {2001},
isbn = {3540425810},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Recent research has proved the benefits of using an ensemble of diverse and accurate base classifiers for classification problems. In this paper the focus is on producing diverse ensembles with the aid of three feature selection heuristics based on two approaches: correlation and contextual merit -based ones. We have developed an algorithm and experimented with it to evaluate and compare the three feature selection heuristics on ten data sets from UCI Repository. On average, simple correlation-based ensemble has the superiority in accuracy. The contextual merit -based heuristics seem to include too many features in the initial ensembles and iterations were most successful with it.},
booktitle = {Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis},
pages = {135–144},
numpages = {10},
series = {IDA '01}
}

@inproceedings{10.1007/11903697_73,
author = {Yan, Guo-zheng and Wu, Ting and Yang, Bang-hua},
title = {Automated feature selection based on an adaptive genetic algorithm for brain-computer interfaces},
year = {2006},
isbn = {3540473319},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11903697_73},
doi = {10.1007/11903697_73},
abstract = {In brain-computer interfaces (BCIs), a feature selection approach using an adaptive genetic algorithm (AGA) is described in this paper. In the AGA, each individual among the population has its own crossover probability and mutation probability. The probabilities of crossover and mutation are varied depending on the fitness values of the individuals. The adaptive probabilities of crossover and mutation are propitious to maintain diversity in the population and sustain the convergence capacity of the genetic algorithms (GAs). The performance of the AGA is compared with those of the Standard GA (SGA) and the Filter method in selecting feature subset for BCIs. The results show that the classification accuracy obtained by the AGA is significantly higher than those obtained by other methods. Furthermore, the AGA has a higher convergence rate than the SGA.},
booktitle = {Proceedings of the 6th International Conference on Simulated Evolution And Learning},
pages = {575–582},
numpages = {8},
location = {Hefei, China},
series = {SEAL'06}
}

@article{10.1016/j.neunet.2012.12.007,
author = {Roy, Asim and Mackin, Patrick D. and Mukhopadhyay, Somnath},
title = {2013 Special Issue: Methods for pattern selection, class-specific feature selection and classification for automated learning},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {41},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2012.12.007},
doi = {10.1016/j.neunet.2012.12.007},
abstract = {This paper presents methods for training pattern (prototype) selection, class-specific feature selection and classification for automated learning. For training pattern selection, we propose a method of sampling that extracts a small number of representative training patterns (prototypes) from the dataset. The idea is to extract a set of prototype training patterns that represents each class region in a classification problem. In class-specific feature selection, we try to find a separate feature set for each class such that it is the best one to separate that class from the other classes. We then build a separate classifier for that class based on its own feature set. The paper also presents a new hypersphere classification algorithm. Hypersphere nets are similar to radial basis function (RBF) nets and belong to the group of kernel function nets. Polynomial time complexity of the methods is proven. Polynomial time complexity of learning algorithms is important to the field of neural networks. Computational results are provided for a number of well-known datasets. None of the parameters of the algorithm were fine tuned for any of the problems solved and this supports the idea of automation of learning methods. Automation of learning is crucial to wider deployment of learning technologies.},
journal = {Neural Netw.},
month = may,
pages = {113–129},
numpages = {17},
keywords = {Automated learning, Classification algorithm, Complexity of learning, Feature selection, Hypersphere net, Polynomial time complexity, Training pattern selection}
}

@inproceedings{10.1145/3382025.3414725,
author = {Ferreira, Fischer and Viggiato, Markos and Souza, Maur\'{\i}cio and Figueiredo, Eduardo},
title = {Testing configurable software systems: the failure observation challenge},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414725},
doi = {10.1145/3382025.3414725},
abstract = {Configurable software systems can be adapted or configured according to a set of features to increase reuse and productivity. The testing process is essential because configurations that fail may potentially hurt user experience and degrade the reputation of a project. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. To encourage the research area on testing configurable systems, researchers and practitioners should be able to try out their solutions in common datasets. In this paper, we propose a dataset with 22 configurable software systems and an extensive test suite. Moreover, we report failures found in these systems and source code metrics to allow evaluating candidate solutions. We hope to engage the community and stimulate new and existing approaches to the problem of testing configurable systems.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {28},
numpages = {6},
keywords = {software product line, testing configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {configurable systems, feature models, monte carlo tree search, software product lines, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.patcog.2006.02.004,
author = {Lung, Shung-Yung},
title = {Wavelet feature selection based neural networks with application to the text independent speaker identification},
year = {2006},
issue_date = {August, 2006},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {39},
number = {8},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2006.02.004},
doi = {10.1016/j.patcog.2006.02.004},
abstract = {A wavelet packets feature selection derived by using neuro-fuzzy evaluation index for speaker identification is described. The concept of a flexible membership function incorporating weighed distance is introduced in the evaluation index to make the modeling of clusters more appropriate. Experimental evaluation of the systems performance was conducted on three speech databases. Our results have shown that this feature selection introduced better performance than the wavelet features with respect to the percentages of recognition.},
journal = {Pattern Recogn.},
month = aug,
pages = {1518–1521},
numpages = {4},
keywords = {Fuzzy, Neural network, Speaker recognition, Wavelet packets}
}

@article{10.1016/j.ins.2008.12.001,
author = {Catal, Cagatay and Diri, Banu},
title = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {8},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.12.001},
doi = {10.1016/j.ins.2008.12.001},
abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
journal = {Inf. Sci.},
month = mar,
pages = {1040–1058},
numpages = {19},
keywords = {Artificial Immune Systems, J48, Machine learning, Naive Bayes, Random Forests, Software fault prediction}
}

@inproceedings{10.1145/347090.347168,
author = {Dy, Jennifer G. and Brodley, Carla E.},
title = {Visualization and interactive feature selection for unsupervised data},
year = {2000},
isbn = {1581132336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/347090.347168},
doi = {10.1145/347090.347168},
booktitle = {Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {360–364},
numpages = {5},
location = {Boston, Massachusetts, USA},
series = {KDD '00}
}

@inproceedings{10.5555/1764441.1764453,
author = {Guo, Dihua and Xiong, Hui and Atluri, Vijay and Adam, Nabil},
title = {Semantic feature selection for object discovery in high-resolution remote sensing imagery},
year = {2007},
isbn = {9783540717003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Given its importance, the problem of object discovery in High-Resolution Remote-Sensing (HRRS) imagery has been given a lot of attention by image retrieval researchers. Despite the vast amount of expert endeavor spent on this problem, more effort has been expected to discover and utilize hidden semantics of images for image retrieval. To this end, in this paper, we exploit a hyperclique pattern discovery method to find complex objects that consist of several co-existing individual objects that usually form a unique semantic concept. We consider the identified groups of co-existing objects as new feature sets and feed them into the learning model for better performance of image retrieval. Experiments with real-world datasets show that, with new semantic features as starting points, we can improve the performance of object discovery in terms of various external criteria.},
booktitle = {Proceedings of the 11th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining},
pages = {71–83},
numpages = {13},
location = {Nanjing, China},
series = {PAKDD'07}
}

@inproceedings{10.1007/11494683_3,
author = {Nishida, Kenji and Kurita, Takio},
title = {Boosting soft-margin SVM with feature selection for pedestrian detection},
year = {2005},
isbn = {3540263063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11494683_3},
doi = {10.1007/11494683_3},
abstract = {We present an example-based algorithm for detecting objects in images by integrating component-based classifiers, which automaticaly select the best feature for each classifier and are combined according to the AdaBoost algorithm. The system employs a soft-margin SVM for the base learner, which is trained for all features and the optimal feature is selected at each stage of boosting. We employed two features such as a histogram-equalization and an edge feature in our experiment. The proposed method was applied to the MIT CBCL pedestrian image database, and 100 sub-regions were extracted from each image as local-features. The experimental results showed fairly good classification ratio with selecting sub-regions, while some improvement attained by combining the two features, histogram-equalization and edge. However, the combination of features could to select good local-features for base learners.},
booktitle = {Proceedings of the 6th International Conference on Multiple Classifier Systems},
pages = {22–31},
numpages = {10},
location = {Seaside, CA},
series = {MCS'05}
}

@inproceedings{10.5555/646360.690408,
author = {Tsymbal, Alexey and Puuronen, Seppo and Patterson, David},
title = {Feature Selection for Ensembles of Simple Bayesian Classifiers},
year = {2002},
isbn = {3540437851},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A popular method for creating an accurate classifier from a set of training data is to train several classifiers, and then to combine their predictions. The ensembles of simple Bayesian classifiers have traditionally not been a focus of research. However, the simple Bayesian classifier has much broader applicability than previously thought. Besides its high classification accuracy, it also has advantages in terms of simplicity, learning speed, classification speed, storage space, and incrementality. One way to generate an ensemble of simple Bayesian classifiers is to use different feature subsets as in the random subspace method. In this paper we present a technique for building ensembles of simple Bayesian classifiers in random subspaces. We consider also a hill-climbing-based refinement cycle, which improves accuracy and diversity of the base classifiers. We conduct a number of experiments on a collection of real-world and synthetic data sets. In many cases the ensembles of simple Bayesian classifiers have significantly higher accuracy than the single "global" simple Bayesian classifier. We consider several methods for integration of simple Bayesian classifiers. The dynamic integration better utilizes ensemble diversity than the static integration.},
booktitle = {Proceedings of the 13th International Symposium on Foundations of Intelligent Systems},
pages = {592–600},
numpages = {9},
series = {ISMIS '02}
}

@inproceedings{10.1007/11427391_84,
author = {Chen, Haixia and Yuan, Senmiao and Jiang, Kai},
title = {Wrapper approach for learning neural network ensemble by feature selection},
year = {2005},
isbn = {3540259120},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11427391_84},
doi = {10.1007/11427391_84},
abstract = {A new algorithm for learning neural network ensemble is introduced in this paper. The proposed algorithm, called NNEFS, exploits the synergistic power of neural network ensemble and feature subset selection to fully exploit the information encoded in the original dataset. All the neural network components in the ensemble are trained with feature subsets selected from the total number of available features by wrapper approach. Classification for a given intance is decided by weighted majority votes of all available components in the ensemble. Experiments on two UCI datasets show the superiority of the algorithm to other two state of art algorithms. In addition, the induced neural network ensemble has more consistent performance for incomplete datasets, without any assumption of the missing mechanism.},
booktitle = {Proceedings of the Second International Conference on Advances in Neural Networks - Volume Part I},
pages = {526–531},
numpages = {6},
location = {Chongqing, China},
series = {ISNN'05}
}

@article{10.1016/j.cogsys.2010.07.004,
author = {Hidaka, Shohei and Smith, Linda B.},
title = {Packing: A geometric analysis of feature selection and category formation},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {12},
number = {1},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2010.07.004},
doi = {10.1016/j.cogsys.2010.07.004},
abstract = {This paper presents a geometrical analysis of how local interactions in a large population of categories packed into a feature space create a global structure of feature relevance. The theory is a formal proof that the joint optimization of discrimination and inclusion creates a smooth space of categories such that near categories in the similarity space have similar generalization gradients. Packing theory offers a unified account of several phenomena in human categorization including the differential importance of different features for different kinds of categories, the dissociation between judgments of similarity and judgments of category membership, and children's ability to generalize a category from very few examples.},
journal = {Cogn. Syst. Res.},
month = mar,
pages = {1–18},
numpages = {18},
keywords = {Categorization, Cognitive development, Feature selection, Word learning}
}

@inproceedings{10.1145/1015330.1015435,
author = {Ng, Andrew Y.},
title = {Feature selection, L1 vs. L2 regularization, and rotational invariance},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015435},
doi = {10.1145/1015330.1015435},
abstract = {We consider supervised learning in the presence of very many irrelevant features, and study two different regularization methods for preventing overfitting. Focusing on logistic regression, we show that using L1 regularization of the parameters, the sample complexity (i.e., the number of training examples required to learn "well,") grows only logarithmically in the number of irrelevant features. This logarithmic rate matches the best known bounds for feature selection, and indicates that L1 regularized logistic regression can be effective even if there are exponentially many irrelevant features as there are training examples. We also give a lower-bound showing that any rotationally invariant algorithm---including logistic regression with L2 regularization, SVMs, and neural networks trained by backpropagation---has a worst case sample complexity that grows at least linearly in the number of irrelevant features.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {78},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{10.1145/3439797,
author = {Fan, Xiaoqian and Yang, Bowen and Chen, Wenzhi and Fan, Quanfang},
title = {Deep Neural Network Based Noised Asian Speech Enhancement and Its Implementation on a Hearing Aid App},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3439797},
doi = {10.1145/3439797},
abstract = {This article studies noised Asian speech enhancement based on the deep neural network (DNN) and its implementation on an app. We use the THCHS-30 speech dataset and the common noise dataset in daily life as training and testing data of the DNN. To stack the frequency data of multiple audio frames to improve the effect of speech enhancement, the system compares the best number of stacked frames during training and testing. At the same time, the influence of training rounds on the PESQ is compared, and the best number of rounds is obtained. On this basis, the best model is implemented on the hearing aid app, and the real-time performance of the device is tested. The experiment shows that based on the DNN, using an appropriate number of rounds for training and using an appropriate number of audio frames stacking to improve the speech enhancement effect, and transplanting this speech enhancement model to the hearing aid app, can effectively improve speech clarity and intelligibility within a reasonable time delay range.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {78},
numpages = {14},
keywords = {Feature selection, neural networks, noise reduction, health care information systems, natural language processing}
}

@article{10.1007/BF03325090,
author = {Hu, Xiaohua and Cercone, Nick},
title = {Data Mining via Discretization, Generalization and Rough Set Feature Selection},
year = {1999},
issue_date = {Feb 1999},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/BF03325090},
doi = {10.1007/BF03325090},
abstract = {We present a data mining method which integrates discretization, generalization and rough set feature selection. Our method reduces the data horizontally and vertically. In the first phase, discretization and generalization are integrated. Numeric attributes are discretized into a few intervals. The primitive values of symbolic attributes are replaced by high level concepts and some obvious superfluous or irrelevant symbolic attributes are also eliminated. The horizontal reduction is done by merging identical tuples after substituting an attribute value by its higher level value in a pre- defined concept hierarchy for symbolic attributes, or the discretization of continuous (or numeric) attributes. This phase greatly decreases the number of tuples we consider further in the database(s). In the second phase, a novel context- sensitive feature merit measure is used to rank features, a subset of relevant attributes is chosen, based on rough set theory and the merit values of the features. A reduced table is obtained by removing those attributes which are not in the relevant attributes subset and the data set is further reduced vertically without changing the interdependence relationships between the classes and the attributes. Finally, the tuples in the reduced relation are transformed into different knowledge rules based on different knowledge discovery algorithms. Based on these principles, a prototype knowledge discovery system DBROUGH-II has been constructed by integrating discretization, generalization, rough set feature selection and a variety of data mining algorithms. Tests on a telecommunication customer data warehouse demonstrates that different kinds of knowledge rules, such as characteristic rules, discriminant rules, maximal generalized classification rules, and data evolution regularities, can be discovered efficiently and effectively.},
journal = {Knowl. Inf. Syst.},
month = feb,
pages = {33–60},
numpages = {28},
keywords = {Data mining, knowledge rules, data warehouse, discretization, generalization, feature selection, rough set}
}

@article{10.3233/JIFS-191461,
author = {Ul Haq, Amin and Li, JianPing and Memon, Muhammad Hammad and Khan, Jalaluddin and Ud Din, Salah},
title = {A novel integrated diagnosis method for breast cancer detection},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-191461},
doi = {10.3233/JIFS-191461},
abstract = {The effective detection of breast cancer is particularly essential for recovery and treatment in the initial phases. The existing methods are not successfully diagnosis breast cancer in the initial phases. Thus the initial recognition of breast cancer is expressively a great challenge for health professionals and scientists. To resolve the problem of initial stages recognition of breast cancer, we recommended a machine learning based diagnosis method which will excellently classify the malignant and benign persons. In the designing of our method machine learning model support vector machine has been applied to classify the malignant and benign persons. To increase the classification performances of the method, we used Minimal Redundancy Maximal Relevance and Chi-square algorithms to choose more appropriate features from the breast cancer dataset. The training/testing splitting technique is used for training and testing of the model. Additionally, the performance of the model has been evaluated by performance assessment metrics. The experimental results demonstrated that the classifier support vector machine obtained best classification performance on the selected subset of features as selected by Minimal Redundancy Maximal Relevance feature selection algorithm. The performances of support vector machine on selected features by Chi square feature selection algorithm are low as compared to Minimal Redundancy Maximal Relevance algorithm. From experimental results analysis, we determined that the integrated system based on Minimal Redundancy Maximal Relevance and support vector machine performances are high due to the selection of more suitable features and obtained 99.71% accuracy. According to McNemar’s statistical test the proposed method is more significant then existing methods. Thus, we recommend that the proposed diagnosis method for effective detection of breast cancer.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {2383–2398},
numpages = {16},
keywords = {Breast cancer, feature selection, machine learning predictive model, diagnosis system, classification}
}

@inproceedings{10.5555/1894214.1894265,
author = {Kachel, Adam and Biesiada, Jacek and Blachnik, Marcin and Duch, W\l{}odzis\l{}aw},
title = {Infosel++: information based feature selection C++ library},
year = {2010},
isbn = {3642132073},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A large package of algorithms for feature ranking and selection has been developed. Infosel++, Information Based Feature Selection C++ Library, is a collection of classes and utilities based on probability estimation that can help developers of machine learning methods in rapid interfacing of feature selection algorithms, aid users in selecting an appropriate algorithm for a given task (embed feature selection in machine learning task), and aid researchers in developing new algorithms, especially hybrid algorithms for feature selection. A few examples of such possibilities are presented.},
booktitle = {Proceedings of the 10th International Conference on Artificial Intelligence and Soft Computing: Part I},
pages = {388–396},
numpages = {9},
location = {Zakopane, Poland},
series = {ICAISC'10}
}

@article{10.1007/s00500-014-1443-1,
author = {Gupta, Akshansh and Agrawal, R. K. and Kaur, Baljeet},
title = {Performance enhancement of mental task classification using EEG signal: a study of multivariate feature selection methods},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {10},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-014-1443-1},
doi = {10.1007/s00500-014-1443-1},
abstract = {In the recent years, the research community has shown interest in the development of brain---computer interface applications which assist physically challenged people to communicate with their brain electroencephalogram (EEG) signal. Representation of these EEG signals for mental task classification in terms of relevant features is important to achieve higher performance in terms of accuracy and computation time. For feature extraction from the EEG, empirical mode decomposition and wavelet transform are more appropriate as they are suitable for the analysis of non-linear and non-stationary time series signals. However, the size of the feature vector obtained from them is huge and may hinder the performance of mental task classification. To obtain a minimal set of relevant and non-redundant features for classification, six popular multivariate filter methods have been investigated which are based on different criteria: distance measure, causal effect and mutual information. Experimental results demonstrate that the classification accuracy improves while the computation time reduces considerably with the use of each of the six multivariate feature selection methods. Among all the combinations of feature extraction and selection methods that are investigated, the combination of wavelet transform and linear regression performs the best. Ranking analysis and statistical tests are also performed to validate the empirical results.},
journal = {Soft Comput.},
month = oct,
pages = {2799–2812},
numpages = {14},
keywords = {Bhattacharyya distance, Empirical mode decomposition, Kullback---Leibler distance, Linear regression, Minimum redundancy and maximum relevance, Ratio of scatter matrices, Wavelet transform}
}

@article{10.1016/j.datak.2012.08.001,
author = {Alibeigi, Mina and Hashemi, Sattar and Hamzeh, Ali},
title = {DBFS: An effective Density Based Feature Selection scheme for small sample size and high dimensional imbalanced data sets},
year = {2012},
issue_date = {November, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81–82},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2012.08.001},
doi = {10.1016/j.datak.2012.08.001},
abstract = {Nowadays, imbalanced data sets are pervasive in real world human practices, and hence, become a very interesting research area within machine learning communities. Imbalanced data sets introduce a significant reduction in performance of standard classifiers when they are invoked to learn data underlying concepts. The problem becomes even more sever when imbalanced data sets are involved with high dimensions. This paper presents a novel feature ranking approach based on the probability density estimation to cope with these issues. The idea behind our approach, named Density Based Feature Selection (DBFS), is that features' distributions over classes can bring significant benefits to feature selection algorithms. In other words, to explore the contribution of each attribute and assign it an appropriate rank, DBFS takes into account features' corresponding distributions over all classes along with their correlations. To show the effectiveness of the presented approach, well-known feature ranking methods are implemented and compared with our approach across varieties of small sample size and high dimensional data sets from microarray, mass spectrometry and text mining domains. Our theoretical analysis and experimental observations reveal that our approach is the method of choice by offering a simple yet effective feature ranking method based on well-known statistical evaluation measures.},
journal = {Data Knowl. Eng.},
month = nov,
pages = {67–103},
numpages = {37},
keywords = {Feature selection, Imbalanced data set, Probability density function (PDF)}
}

@article{10.5555/944919.944974,
author = {Forman, George},
title = {An extensive empirical study of feature selection metrics for text classification},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair---e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1289–1305},
numpages = {17}
}

@inproceedings{10.1007/978-3-540-30185-1_17,
author = {Huang, Yue and McCullagh, Paul and Black, Norman and Harper, Roy},
title = {Feature selection and classification model construction on type 2 diabetic patient’s data},
year = {2004},
isbn = {3540240543},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30185-1_17},
doi = {10.1007/978-3-540-30185-1_17},
abstract = {Diabetes is a disorder of the metabolism where the amount of glucose in the blood is too high because the body cannot produce or properly use insulin. In order to achieve more effective diabetes clinic management, data mining techniques have been applied to a patient database. In an attempt to improve the efficiency of data mining algorithms, a feature selection technique ReliefF is used with the data, which can rank the important attributes affecting Type 2 diabetes control. After selecting suitable attributes, classification techniques are applied to the data to predict how well the patients are controlling their condition. Preliminary results have been confirmed by the clinician and this provides optimism that data mining can be used to generate prediction models.},
booktitle = {Proceedings of the 4th International Conference on Advances in Data Mining: Applications in Image Mining, Medicine and Biotechnology, Management and Environmental Control, and Telecommunications},
pages = {153–162},
numpages = {10},
location = {Leipzig, Germany},
series = {ICDM'04}
}

@inproceedings{10.1145/3436209.3436388,
author = {Dai, Hong},
title = {Network Intrusion Detection Method Based on AO-BP Framework.},
year = {2021},
isbn = {9781450388573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436209.3436388},
doi = {10.1145/3436209.3436388},
abstract = {Aiming at the low detection rate of network intrusion detection, a intrusion detection framework AO-BP is presented. It combines feature selection with artificial neural network. Firstly, SMOTE technology and random sampling technology are adopted to equalize data. Secondly, applying crucial features deal with data dimension reduction with the integration method in internet intrusion data. Finally, a classified experiment on the intrusion data is conducted using the optimized BP neural network. The experiment results express that the presented model shorten modeling time of the traditional BP neural network. It increases the detection accuracy of U2R and R2L. Compared with the SVM and NaiveBayes classification methods, experiments prove that the suggested method also has a highest accuracy, precision and recall.},
booktitle = {Proceedings of the 2020 4th International Conference on E-Business and Internet},
pages = {52–56},
numpages = {5},
keywords = {AO-BP framework, Artificial neural network, Feature selection, Network intrusion detection},
location = {Singapore, Singapore},
series = {ICEBI '20}
}

@article{10.1016/j.dsp.2006.07.005,
author = {Polat, Kemal and G\"{u}ne\c{s}, Salih},
title = {Hepatitis disease diagnosis using a new hybrid system based on feature selection (FS) and artificial immune recognition system with fuzzy resource allocation},
year = {2006},
issue_date = {November, 2006},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {16},
number = {6},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2006.07.005},
doi = {10.1016/j.dsp.2006.07.005},
abstract = {This paper presents a novel method for diagnosis of hepatitis disease. The proposed method is based on a hybrid method that uses feature selection (FS) and artificial immune recognition system (AIRS) with fuzzy resource allocation mechanism. AIRS has showed an effective performance on several problems such as machine learning benchmark problems and medical classification problems like breast cancer, diabets, liver disorders classification. By hybridizing FS and AIRS with fuzzy resource allocation mechanism, a method is obtained to solve this diagnosis problem via classifying. The robustness of this method with regard to sampling variations is examined using a cross-validation method. We used hepatitis disease dataset which is taken from UCI machine learning repository. We obtained a classification accuracy of 92.59%, which is the highest one reached so far. The classification accuracy was obtained via 10-fold cross validation. The obtained classification accuracy of our system was 92.59% and it was very promising with regard to the other classification applications in literature for this problem. Also, sensitivity, and specificity values for hepatitis disease dataset were obtained as 100 and 85%.},
journal = {Digit. Signal Process.},
month = nov,
pages = {889–901},
numpages = {13},
keywords = {AIRS, Feature selection, Fuzzy resource allocation, Hepatitis disease, Medical diagnosis}
}

@inproceedings{10.1007/978-3-030-27629-4_37,
author = {Pfeifer, Daniel and Leidner, Jochen L.},
title = {A Study on Topic Modeling for Feature Space Reduction in Text Classification},
year = {2019},
isbn = {978-3-030-27628-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27629-4_37},
doi = {10.1007/978-3-030-27629-4_37},
abstract = {We examine two topic modeling approaches as feature space reduction techniques for text classification and compare their performance with two standard feature selection techniques, namely Information Gain (IG) and and Document Frequency (DF). Feature selection techniques are commonly applied in order to avoid the well-known “curse of dimensionality” in machine learning. Regarding text classification, traditional techniques achieve this by selecting words from the training vocabulary. In contrast, topic models compute topics as multinomial distributions over words and reduce each document to a distribution over such topics. Corresponding topic-to-document distributions may act as input data to train a document classifier. Our comparison includes two topic modeling approaches – Latent Dirichlet Allocation (LDA) and Topic Grouper. Our results are based on classification accuracy and suggest that topic modeling is far superior to IG and DF at a very low number of reduced features. However, if the number of reduced features is still large, IG becomes competitive and the cost of computing topic models is considerable. We conclude by giving basic recommendations on when to consider which type of method.},
booktitle = {Flexible Query Answering Systems: 13th International Conference, FQAS 2019, Amantea, Italy, July 2–5, 2019, Proceedings},
pages = {403–412},
numpages = {10},
keywords = {Topic modeling, Text classification, Feature selection, Feature space reduction},
location = {Amantea, Italy}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {configurable system, decision propagation, software product lines, t-wise sampling, uniform random sampling},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11042-020-09424-1,
author = {Gidaye, Girish and Nirmal, Jagannath and Ezzine, Kadria and Frikha, Mondher},
title = {Wavelet sub-band features for voice disorder detection and classification},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {39–40},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09424-1},
doi = {10.1007/s11042-020-09424-1},
abstract = {Acoustic analysis of the speech signal enables non-intrusive, affordable, unbiased and fast assessment of voice pathologies. This assessment provides complimentary information to otolaryngologist for preliminary diagnosis of pathological larynx. Several voice impairment assessment systems focused on acoustic analysis have been introduced in recent years. Nevertheless, these systems are tested using only one or two datasets and are not independent of database and human bias. In this paper, a unified wavelet based framework is suggested for evaluating voice disorders, which is independent of database and human bias. Stationary wavelet transform (SWT) is used to decompose the speech signal, since it offers good time and frequency localization. Energy and statistical features are extracted from each sub-band after multilevel decomposition. Higher the decomposition level, higher is the order of feature vector. To decrease the dimension of the feature vector, information gain (IG) based feature selection technique is harnessed for selecting most relevant and discarding redundant features. The enriched feature vector is assessed using support vector machine (SVM), stochastic gradient descent (SGD) and artificial neural network (ANN) classifiers. Records of vowel /a/, vocalized at natural pitch for both healthy and pathological subjects, are mined from German, English, Arabic and Spanish speech databases. During the first phase of experiments, input speech signal is detected as healthy or pathological. Second phase classifies input speech samples into healthy, cyst, paralysis or polyp. Experimental results demonstrate that, the extracted energy and statistical features can be used as possible clues for voice disorder evaluation. The most important aspect of the proposed method is that the features are independent of the fundamental frequency. The detection and classification rates attained are comparable to other state-of-the-art approaches.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {28499–28523},
numpages = {25},
keywords = {Voice disorder detection, Stationary wavelet transform, Voice pathology, Statistical features, Feature selection, Information gain}
}

@article{10.1016/j.neunet.2009.06.039,
author = {Bailly, Kevin and Milgram, Maurice},
title = {2009 Special Issue: Boosting feature selection for Neural Network based regression},
year = {2009},
issue_date = {July, 2009},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {22},
number = {5–6},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2009.06.039},
doi = {10.1016/j.neunet.2009.06.039},
abstract = {The head pose estimation problem is well known to be a challenging task in computer vision and is a useful tool for several applications involving human-computer interaction. This problem can be stated as a regression one where the input is an image and the output is pan and tilt angles. Finding the optimal regression is a hard problem because of the high dimensionality of the input (number of image pixels) and the large variety of morphologies and illumination. We propose a new method combining a boosting strategy for feature selection and a neural network for the regression. Potential features are a very large set of Haar-like wavelets which are well known to be adapted to face image processing. To achieve the feature selection, a new Fuzzy Functional Criterion (FFC) is introduced which is able to evaluate the link between a feature and the output without any estimation of the joint probability density function as in the Mutual Information. The boosting strategy uses this criterion at each step: features are evaluated by the FFC using weights on examples computed from the error produced by the neural network trained at the previous step. Tests are carried out on the commonly used Pointing 04 database and compared with three state-of-the-art methods. We also evaluate the accuracy of the estimation on FacePix, a database with a high angular resolution. Our method is compared positively to a Convolutional Neural Network, which is well known to incorporate feature extraction in its first layers.},
journal = {Neural Netw.},
month = jul,
pages = {748–756},
numpages = {9},
keywords = {Boosting, Fuzzy functional criterion, Input feature selection, Regression}
}

@inproceedings{10.5555/1996889.1996919,
author = {Jagarlamudi, Jagadeesh and Bennett, Paul N.},
title = {Fractional similarity: cross-lingual feature selection for search},
year = {2011},
isbn = {9783642201608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Training data as well as supplementary data such as usagebased click behavior may abound in one search market (i.e., a particular region, domain, or language) and be much scarcer in another market. Transfer methods attempt to improve performance in these resourcescarce markets by leveraging data across markets. However, differences in feature distributions across markets can change the optimal model. We introduce a method called Fractional Similarity, which uses query-based variance within a market to obtain more reliable estimates of feature deviations across markets. An empirical analysis demonstrates that using this scoring method as a feature selection criterion in cross-lingual transfer improves relevance ranking in the foreign language and compares favorably to a baseline based on KL divergence},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
pages = {226–237},
numpages = {12},
location = {Dublin, Ireland},
series = {ECIR'11}
}

@inproceedings{10.1145/3292500.3330868,
author = {Liu, Kunpeng and Fu, Yanjie and Wang, Pengfei and Wu, Le and Bo, Rui and Li, Xiaolin},
title = {Automating Feature Subspace Exploration via Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330868},
doi = {10.1145/3292500.3330868},
abstract = {Feature selection is the preprocessing step in machine learning which tries to select the most relevant features for the subsequent prediction task. Effective feature selection could help reduce dimensionality, improve prediction accuracy and increase result comprehensibility. It is very challenging to find the optimal feature subset from the subset space as the space could be very large. While much effort has been made by existing studies, reinforcement learning can provide a new perspective for the searching strategy in a more global way. In this paper, we propose a multi-agent reinforcement learning framework for the feature selection problem. Specifically, we first reformulate feature selection with a reinforcement learning framework by regarding each feature as an agent. Then, we obtain the state of environment in three ways, i.e., statistic description, autoencoder and graph convolutional network (GCN), in order to make the algorithm better understand the learning progress. We show how to learn the state representation in a graph-based way, which could tackle the case when not only the edges, but also the nodes are changing step by step. In addition, we study how the coordination between different features would be improved by more reasonable reward scheme. The proposed method could search the feature subset space globally and could be easily adapted to the real-time case (real-time feature selection) due to the nature of reinforcement learning. Also, we provide an efficient strategy to accelerate the convergence of multi-agent reinforcement learning. Finally, extensive experimental results show the significant improvement of the proposed method over conventional approaches.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {207–215},
numpages = {9},
keywords = {automated exploration, feature selection, multi-agent reinforcement learning},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.5555/2969442.2969492,
author = {Kim, Been and Shah, Julie and Doshi-Velez, Finale},
title = {Mind the Gap: a generative approach to interpretable feature selection and extraction},
year = {2015},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection. By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.},
booktitle = {Proceedings of the 29th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2260–2268},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'15}
}

@article{10.1016/j.compag.2018.07.002,
author = {Chen, Dongmei and Shi, Yeyin and Huang, Wenjiang and Zhang, Jingcheng and Wu, Kaihua},
title = {Mapping wheat rust based on high spatial resolution satellite imagery},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {152},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2018.07.002},
doi = {10.1016/j.compag.2018.07.002},
journal = {Comput. Electron. Agric.},
month = sep,
pages = {109–116},
numpages = {8},
keywords = {Wheat rust, Multispectral remote sensing, Mapping, Feature selection, Support vector machine}
}

@inproceedings{10.1145/3422392.3422418,
author = {Rocha, Larissa and Machado, Ivan and Almeida, Eduardo and K\"{a}stner, Christian and Nadi, Sarah},
title = {A semi-automated iterative process for detecting feature interactions},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422418},
doi = {10.1145/3422392.3422418},
abstract = {For configurable systems, features developed and tested separately may present a different behavior when combined in a system. Since software products might be composed of thousands of features, developers should guarantee that all valid combinations work properly. However, features can interact in undesired ways, resulting in failures. A feature interaction is an unpredictable behavior that cannot be easily deduced from the individual features involved. We proposed VarXplorer to inspect feature interactions as they are detected and incrementally classify them as benign or problematic. Our approach provides an iterative analysis of feature interactions allowing developers to focus on suspicious cases. In this paper, we present an experimental study to evaluate our iterative process of tests execution. We aim to understand how VarXplorer could be used for a faster and more objective feature interaction analysis. Our results show that VarXplorer may reduce up to 50% the amount of interactions a developer needs to check during the testing process.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {778–787},
numpages = {10},
keywords = {Configurable Systems, Experimental Study, Feature interaction, Runtime Analysis},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.5555/2029604.2029629,
author = {Kauppi, Jukka-Pekka and Huttunen, Heikki and Korkala, Heikki and J\"{a}\"{a}skel\"{a}inen, Iiro P. and Sams, Mikko and Tohka, Jussi},
title = {Face prediction from fMRI data during movie stimulus: strategies for feature selection},
year = {2011},
isbn = {9783642217371},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We investigate the suitability of the multi-voxel pattern analysis approach to analyze diverse movie stimulus functional magnetic resonance imaging (fMRI) data. We focus on predicting the presence of faces in the drama movie based on the fMRI measurements of 12 subjects watching the movie. We pose the prediction as a regression problem where regression coefficients estimated from the training data are used to estimate the presence of faces in the stimulus for the test data. Because the number of features (voxels) exceeds the number of training samples, an emphasis is placed on the feature selection. We compare four automatic feature selection approaches. The best results were achieved by sparse regression models. The correlations between the face presence time-course predicted from fMRI data and manual face annotations were in the range from 0.43 to 0.62 depending on the subject and pre-processing options, i.e., the prediction was successful. This suggests that proposed methods are useful in testing novel research hypotheses with natural stimulus fMRI data.},
booktitle = {Proceedings of the 21st International Conference on Artificial Neural Networks - Volume Part II},
pages = {189–196},
numpages = {8},
keywords = {brain imaging, natural stimulation, regression},
location = {Espoo, Finland},
series = {ICANN'11}
}

@article{10.5555/1553642.1553650,
author = {Chang, Hsin-Yun},
title = {Employee turnover: a novel prediction solution with effective feature selection},
year = {2009},
issue_date = {March 2009},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
volume = {6},
number = {3},
issn = {1790-0832},
abstract = {This study proposed to address a new method that could select subsets more efficiently. In addition, the reasons why employers voluntarily turnover were also investigated in order to increase the classification accuracy and to help managers to prevent employers' turnover. The mixed feature subset selection used in this study combined Taguchi method and Nearest Neighbor Classification Rules to select feature subset and analyze the factors to find the best predictor of employer turnover. All the samples used in this study were from industry A, in which the employers left their job during 1st of February, 2001 to 31st of December, 2007, compared with those incumbents. The results showed that through the mixed feature subset selection method, total 18 factors were found that are important to the employers. In addition, the accuracy of correct selection was 87.85% which was higher than before using this feature subset selection method (80.93%). The new feature subset selection method addressed in this study does not only provide industries to understand the reasons of employers' turnover, but also could be a long-term classification prediction for industries.},
journal = {WSEAS Trans. Info. Sci. and App.},
month = mar,
pages = {417–426},
numpages = {10},
keywords = {Taguchi methods, feature subset selection, nearest neighbor classification rules, training pattern, voluntary turnover}
}

@article{10.3233/JIFS-162431,
author = {Ji, Wei and Huang, Yixiang and Qiang, Baohua and Li, Yun},
title = {Min-Max Ensemble Feature Selection1},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {33},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-162431},
doi = {10.3233/JIFS-162431},
abstract = {Feature selection is one of the key problems in machine learning and data mining. It involves identifying a subset of the most useful features that produces compatible results as the original entire set of features. It can reduce the dimensionality of original data, speed up the learning process and build comprehensible learning models with good generalization performance. Nowadays, ensemble idea has been used to improve the performance of feature selection by integrating multiple base feature selection models into an ensemble one. In this paper, in order to improve the efficiency of feature selection in dealing with large scale, high dimension and imbalanced problems, a Min-Max Ensemble Feature Selection (M2-EFS) is proposed, which is based on balanced data partition and min-max ensemble strategy. The experimental results demonstrate that the M2-EFS can obtain higher performance than other classical ensemble methods in most cases, especially for large scale, high dimension and imbalanced data.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3441–3450},
numpages = {10},
keywords = {Feature selection, Min-Max strategy, ensemble, data partition}
}

@article{10.5555/1046920.1194906,
author = {Wolf, Lior and Shashua, Amnon},
title = {Feature Selection for Unsupervised and Supervised Inference: The Emergence of Sparsity in a Weight-Based Approach},
year = {2005},
issue_date = {12/1/2005},
publisher = {JMLR.org},
volume = {6},
issn = {1532-4435},
abstract = {The problem of selecting a subset of relevant features in a potentially overwhelming quantity of data is classic and found in many branches of science. Examples in computer vision, text processing and more recently bio-informatics are abundant. In text classification tasks, for example, it is not uncommon to have 104 to 107 features of the size of the vocabulary containing word frequency counts, with the expectation that only a small fraction of them are relevant. Typical examples include the automatic sorting of URLs into a web directory and the detection of spam email.In this work we present a definition of "relevancy" based on spectral properties of the Laplacian of the features' measurement matrix. The feature selection process is then based on a continuous ranking of the features defined by a least-squares optimization process. A remarkable property of the feature relevance function is that sparse solutions for the ranking values naturally emerge as a result of a "biased non-negativity" of a key matrix in the process. As a result, a simple least-squares optimization process converges onto a sparse solution, i.e., a selection of a subset of features which form a local maximum over the relevance function. The feature selection algorithm can be embedded in both unsupervised and supervised inference problems and empirical evidence show that the feature selections typically achieve high accuracy even when only a small fraction of the features are relevant.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {1855–1887},
numpages = {33}
}

@article{10.1007/s13748-021-00238-2,
author = {Tripathi, Diwakar and Edla, Damodar Reddy and Bablani, Annushree and Shukla, Alok Kumar and Reddy, B. Ramachandra},
title = {Experimental analysis of machine learning methods for credit score classification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {3},
url = {https://doi.org/10.1007/s13748-021-00238-2},
doi = {10.1007/s13748-021-00238-2},
abstract = {Credit scoring concerns with emerging empirical model to assist the financial institutions for financial decision-making process. Credit risk analysis plays a vital role for decision-making process; statistical and machine learning approaches are utilized to estimate the risk associated with a credit applicant. Enhancing the performance of credit scoring model, particularly toward non-trustworthy “or non-creditworthy” group, may result incredible effect for financial institution. However, credit scoring data may have excess and unimportant data and features which degrades the performance of model. So, selection of important features (or reduction in irrelevant and redundant features) may play the key role for improving the effectiveness and reducing the complexity of the model. This study presents a experimental results analysis of various combinations of feature selection approaches with various classification approaches and impact of feature selection approaches. For experimental results analysis, nine feature selection and sixteen classification state-of-the-art approaches have been applied on seven benched marked credit scoring datasets.},
journal = {Prog. in Artif. Intell.},
month = sep,
pages = {217–243},
numpages = {27},
keywords = {Classification, Credit scoring, Feature selection}
}

@inproceedings{10.1007/978-3-642-29946-9_24,
author = {Snel, Matthijs and Whiteson, Shimon},
title = {Multi-Task reinforcement learning: shaping and feature selection},
year = {2011},
isbn = {9783642299452},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29946-9_24},
doi = {10.1007/978-3-642-29946-9_24},
abstract = {Shaping functions can be used in multi-task reinforcement learning (RL) to incorporate knowledge from previously experienced source tasks to speed up learning on a new target task. Earlier work has not clearly motivated choices for the shaping function. This paper discusses and empirically compares several alternatives, and demonstrates that the most intuive one may not always be the best option. In addition, we extend previous work on identifying good representations for the value and shaping functions, and show that selecting the right representation results in improved generalization over tasks.},
booktitle = {Proceedings of the 9th European Conference on Recent Advances in Reinforcement Learning},
pages = {237–248},
numpages = {12},
location = {Athens, Greece},
series = {EWRL'11}
}

@inproceedings{10.1145/3302425.3302428,
author = {Shi, Linlin and Jia, Hanguang and Zhou, Zhenwei and Yu, Pengfei and Cheng, Liye and Huang, Yun},
title = {Health and Effectiveness Assessment of Aeronautical General Processing System Based on Feature Analysis of State Parameters},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302428},
doi = {10.1145/3302425.3302428},
abstract = {This paper uses fault injection to simulate the degradation state of each module of P2020. Based on the time-domain data of the sensitivity parameters of each module under normal and fault injection conditions, a health and effectiveness evaluation method for general aviation processing system based on parameter feature analysis isproposed. Firstly, feature extraction, feature selection and principal component analysis are used to form a sample set of health assessment parameters for each module of P2020 system. Then, mahalanobis distance is used to evaluate the health status of each key module and calculate the health degree. Finally, the system effectiveness of P2020 is obtained by AHP (The analytic hierarchy process) and ADC model. The analysis of test data shows that this method has high accuracy and accuracy for health and effectiveness evaluation of general aviation processing system.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {2},
numpages = {6},
keywords = {Feature extraction, PCA, effectiveness assessment, feature selection, mahalanobis distance},
location = {Sanya, China},
series = {ACAI '18}
}

@article{10.1016/j.compag.2019.104942,
author = {Jia, Min and Li, Wei and Wang, Kangkang and Zhou, Chen and Cheng, Tao and Tian, YongChao and Zhu, Yan and Cao, Weixing and Yao, Xia},
title = {A newly developed method to extract the optimal hyperspectral feature for monitoring leaf biomass in wheat},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {165},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.104942},
doi = {10.1016/j.compag.2019.104942},
journal = {Comput. Electron. Agric.},
month = oct,
numpages = {7},
keywords = {Feature selection, Hyperspectral image, Wheat leaf biomass, Synergy interval partial least squares–successive projections algorithm (SIPLS-SPA)}
}

@inproceedings{10.5555/1783034.1783177,
author = {Buchenrieder, Klaus},
title = {Processing of myoelectric signals by feature selection and dimensionality reduction for the control of powered upper-limb prostheses},
year = {2007},
isbn = {3540758666},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The extraction of features from myoelectric signals (MES) for the classification of prehensile motions is difficult to achieve. The optimal selection of features, extracted from a MES and the reduction of dimensions is even more challenging. In the context of prosthetic control, dimensionality reduction means to retain MES information, that is important for class discrimination and to discard irrelevant data. Dimensionality reduction strategies are categorized into feature selection and feature projection methods according to their objective functions. In this contribution, we bring forward a statistical cluster analysis technique, which we call the "Guilin Hills Selection Method". It combines selection plus projection and can be applied in the time- and in the frequency-domain. The goal is to control an electrically-powered upper-limb prostheses, the UniBw-Hand, with a minimum number of sensors and a low-power processor. We illustrate the technique with time-domain features derived from the MES of two sensors to clearly differentiate four hand-positions.},
booktitle = {Proceedings of the 11th International Conference on Computer Aided Systems Theory},
pages = {1057–1065},
numpages = {9},
location = {Las Palmas de Gran Canaria, Spain},
series = {EUROCAST'07}
}

@inproceedings{10.5555/1519432.1519481,
author = {Chang, Hsin-Yun},
title = {Employee turnover: a novel prediction solution with effective feature selection},
year = {2009},
isbn = {978960474413},
publisher = {World Scientific and Engineering Academy and Society (WSEAS)},
address = {Stevens Point, Wisconsin, USA},
abstract = {This study proposed to address a new method that could select subsets more efficiently. In addition, the reasons why employers voluntarily turnover were also investigated in order to increase the classification accuracy and to help managers to prevent employers' turnover. The mixed subset selection used in this study combined Taguchi method and Nearest Neighbor Classification Rules to select subset and analyze the factors to find the best predictor of employer turnover. All the samples used in this study were from industry A, in which the employers left their job during 1st of February, 2001 to 31st of December, 2007, compared with those incumbents. The results showed that through the mixed subset selection method, total 18 factors were found that are important to the employers. In addition, the accuracy of correct selection was 87.85% which was higher than before using this subset selection (80.93%). The new subset selection method addressed in this study does not only provide industries to understand the reasons of employers' turnover, but also could be a long-term classification prediction for industries.},
booktitle = {Proceedings of the 3rd WSEAS International Conference on Computer Engineering and Applications},
pages = {252–256},
numpages = {5},
keywords = {Taguchi methods, nearest neighbor classification rules, subset selection, training pattern, voluntary turnover},
location = {Ningbo, China},
series = {CEA'09}
}

@inproceedings{10.1145/3377024.3377025,
author = {Sundermann, Chico and Th\"{u}m, Thomas and Schaefer, Ina},
title = {Evaluating #SAT solvers on industrial feature models},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377025},
doi = {10.1145/3377024.3377025},
abstract = {Configurable systems are widely used for families of products that share multiple configuration options. These systems often induce a large configuration space. Handling the variability of such a system is difficult without being able to measure its complexity. Several methods depend on computing the number of valid configurations, such as estimating the effort of an update or effectively reducing the variability of a system. In many cases, it is possible to map a configurable system to propositional logic. Therefore, we use #SAT in order to evaluate variability of such systems. A #SAT solver computes the number of valid assignments of a propositional formula. However, this problem is even harder than SAT. The main contribution of our work is an investigation of the scalability of off-the-shelf #SAT solvers on industrial feature models. Additionally, we examine the correlation between size of a system and the runtime of a solver computing the number of valid configurations. In this paper, we empirically evaluate nine publicly available #SAT solvers on 127 industrial feature models. Our results indicate that current solvers master a majority of the evaluated systems. However, there are large models, for which none of the evaluated solvers scales. Nevertheless, there are even larger and more complex systems for which the solvers scale.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {9},
keywords = {#SAT, configurable systems, configuration counting, feature models, model counting, product lines},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3233027.3233044,
author = {Al-Hajjaji, Mustafa and Schulze, Michael and Ryssel, Uwe},
title = {Similarity analysis of product-line variants},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233044},
doi = {10.1145/3233027.3233044},
abstract = {Many existing approaches have exploited the similarity notion to analyze software systems. In product-line engineering, similarity notion has been considered to fulfill analysis objectives, such as improving the testing effectiveness and reducing the testing efforts. However, most of the existing approaches consider in the similarity measurement only information of high level of abstraction, such as the feature selections of variants. In this paper, we present the notion of similarity in product-line engineering using different types of problem-space as well as solution-space information. In particular, we discuss different scenarios of measuring the similarity between variants and the possibility of combining different types of information to output the similarity between the compared variants. Moreover, we realized these scenarios in the industrial variant management tool pure::variants to fulfill analysis functionalities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {226–235},
numpages = {10},
keywords = {highly configurable systems, similarity, software product lines, variants analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3167132.3167234,
author = {Mohammad, Yasser and Matsumoto, Kazunori and Hoashi, Keiichiro},
title = {Deep feature learning and selection for activity recognition},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167234},
doi = {10.1145/3167132.3167234},
abstract = {Human physical activity recognition from sensor data is a growing area of research due to the widespread adoption of sensor-rich wearable and smart devices. The growing interest resulted in several formulations with multiple proposals for each of them. This paper is interested in activity recognition from short sequences of sensor readings. Traditionally, solutions to this problem have relied on handcrafted features and feature selection from large predefined feature sets. More recently, deep methods have been employed to provide an end-to-end classification system for activity recognition with higher accuracy at the expense of much slower performance. This paper proposes a middle ground in which a deep neural architecture is employed for feature learning followed by traditional feature selection and classification. This approach is shown to outperform state-of-the-art systems on six out of seven experiments using publicly available datasets.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {930–939},
numpages = {10},
keywords = {activity recognition, deep learning, feature selection, mobile computing},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1145/3488280,
author = {Sowah, Robert A. and Kuditchar, Bernard and Mills, Godfrey A. and Acakpovi, Amevi and Twum, Raphael A. and Buah, Gifty and Agboyi, Robert},
title = {HCBST: An Efficient Hybrid Sampling Technique for Class Imbalance Problems},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4681},
url = {https://doi.org/10.1145/3488280},
doi = {10.1145/3488280},
abstract = {Class imbalance problem is prevalent in many real-world domains. It has become an active area of research. In binary classification problems, imbalance learning refers to learning from a dataset with a high degree of skewness to the negative class. This phenomenon causes classification algorithms to perform woefully when predicting positive classes with new examples. Data resampling, which involves manipulating the training data before applying standard classification techniques, is among the most commonly used techniques to deal with the class imbalance problem. This article presents a new hybrid sampling technique that improves the overall performance of classification algorithms for solving the class imbalance problem significantly. The proposed method called the Hybrid Cluster-Based Undersampling Technique (HCBST) uses a combination of the cluster undersampling technique to under-sample the majority instances and an oversampling technique derived from Sigma Nearest Oversampling based on Convex Combination, to oversample the minority instances to solve the class imbalance problem with a high degree of accuracy and reliability. The performance of the proposed algorithm was tested using 11 datasets from the National Aeronautics and Space Administration Metric Data Program data repository and University of California Irvine Machine Learning data repository with varying degrees of imbalance. Results were compared with classification algorithms such as the K-nearest neighbours, support vector machines, decision tree, random forest, neural network, AdaBoost, na\"{\i}ve Bayes, and quadratic discriminant analysis. Tests results revealed that for the same datasets, the HCBST performed better with average performances of 0.73, 0.67, and 0.35 in terms of performance measures of area under curve, geometric mean, and Matthews Correlation Coefficient, respectively, across all the classifiers used for this study. The HCBST has the potential of improving the performance of the class imbalance problem, which by extension, will improve on the various applications that rely on the concept for a solution.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {57},
numpages = {37},
keywords = {Class imbalance, data sampling, cluster undersampling technique, clustering, classification}
}

@article{10.1007/s11045-019-00632-z,
author = {Aggarwal, Namita and Rana, Bharti and Agrawal, R. K.},
title = {Role of surfacelet transform in diagnosing Alzheimer’s disease},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {30},
number = {4},
issn = {0923-6082},
url = {https://doi.org/10.1007/s11045-019-00632-z},
doi = {10.1007/s11045-019-00632-z},
abstract = {The clinical diagnosis of Alzheimer’s disease (AD) is based on the tedious questionnaire and prolonged tests, which require involvement of the patient, his/her family, and the clinician. Ultimately, the success of subjective evaluation depends on the expertise of the clinician. To eradicate this subjectivity, the present work proposes a method to automate the diagnosis of AD. The proposed method represents complex brain structure of the subject (captured by MRI, a safe and non-invasive medical imaging modality) in terms of quantifiable features and then performs classification of normal and AD subjects. It does so in three phases: (1) MRI volume is normalized and the gray matter, which is the most affected tissue in AD is extracted from the regions of interest. (2) Surfacelet transform, a 3D multi-resolution transform that captures intricate directional details, is applied on the volume to extract features. The features are then reduced by selecting relevant and non-redundant features using a combination of Fisher discriminant ratio, and minimum redundancy and maximum relevance feature selection methods. (3) Based on these features, AD patients and normal subjects are classified using support vector machine classifier with 10 runs of 10 fold cross-validation. The performance of the method was computed using three measures (sensitivity, specificity, classification accuracy) on three datasets, constructed from the publically available database. It achieved 78.04%, 98.00% and 84.37% classification accuracies on the three datasets respectively. The effectiveness of the proposed method is validated by comparing it with other existing methods under identical experimental settings using the above three performance measures as well as receiver operating characteristic curves, ranking and statistical tests. Overall, it significantly outperformed the existing methods. It indicates that the proposed method has the potential to assist clinicians in the decision making for the diagnosis of AD.},
journal = {Multidimensional Syst. Signal Process.},
month = oct,
pages = {1839–1858},
numpages = {20},
keywords = {Alzheimer’s disease, Classification, Feature selection, Minimum redundancy maximum relevance (mRMR), Structural magnetic resonance imaging (MRI), Surfacelet transform}
}

@inproceedings{10.5555/645405.651754,
author = {Moser, Andreas and Murthy, M. Narasimha},
title = {On the Scalability of Genetic Algorithms to Very Large-Scale Feature Selection},
year = {2000},
isbn = {3540673539},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature Selection is a very promising optimisation strategy for Pattern Recognition systems. But, as an NP-complete task, it is extremely dificult to carry out. Past studies therefore were rather limited in either the cardinality of the feature space or the number of patterns utilised to assess the feature subset performance.This study examines the scalability of Distributed Genetic Algorithms to very large-scale Feature Selection. As domain of application, a classification system for Optical Characters is chosen. The system is tailored to classify hand-written digits, involving 768 binary features. Due to the vastness of the investigated problem, this study forms a step into new realms in Feature Selection for classification.We present a set of customisations of GAs that provide for an application of known concepts to Feature Selection problems of practical interest. Some limitations of GAs in the domain of Feature Selection are unrevealed and improvements are suggested. A widely used strategy to accelerate the optimisation process, Training Set Sampling, was observed to fail in this domain of application.Experiments on unseen validation data suggest that Distributed GAs are capable of reducing the problemcomplexity significantly. The results show that the classification accuracy can be maintained while reducing the feature space cardinality by about 50%. Genetic Algorithms are demonstrated to scale well to very large-scale problems in Feature Selection.},
booktitle = {Real-World Applications of Evolutionary Computing, EvoWorkshops 2000: EvoIASP, EvoSCONDI, EvoTel, EvoSTIM, EvoROB, and EvoFlight},
pages = {77–86},
numpages = {10}
}

@inproceedings{10.1109/ICASSP.1999.758184,
author = {Chen, S. F. and Rosenfeld, R.},
title = {Efficient sampling and feature selection in whole sentence maximum entropy language models},
year = {1999},
isbn = {0780350413},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICASSP.1999.758184},
doi = {10.1109/ICASSP.1999.758184},
abstract = {Conditional maximum entropy models have been successfully applied to estimating language model probabilities of the form P(w|h), but are often to demanding computationally. Furthermore, the conditional framework does not lend itself to expressing global sentential phenomena. We have previously introduced a non-conditional maximum entropy language model which directly models the probability of an entire sentence or utterance. The model treats each utterance as a "bag of features", where features are arbitrary computable properties of the sentence. Using the model is computationally straightforward since it does not require normalization. Training the model requires efficient sampling of sentences from an exponential distribution. In this paper, we further develop the model and demonstrate its feasibility and power. We compare the efficiency of several sampling techniques. implement smoothing to accommodate rare features, and suggest an efficient algorithm for improving the convergence rate. We then present a novel procedure for feature selection, which exploits discrepancies between the existing model and the training corpus. We demonstrate our ideas by constructing and analyzing competitive modes in the Switchboard domain.},
booktitle = {Proceedings of the Acoustics, Speech, and Signal Processing, 1999. on 1999 IEEE International Conference - Volume 01},
pages = {549–552},
numpages = {4},
series = {ICASSP '99}
}

@article{10.1016/j.patcog.2009.12.012,
author = {Derrac, Joaqu\'{\i}n and Garc\'{\i}a, Salvador and Herrera, Francisco},
title = {IFS-CoCo: Instance and feature selection based on cooperative coevolution with nearest neighbor rule},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {43},
number = {6},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2009.12.012},
doi = {10.1016/j.patcog.2009.12.012},
abstract = {Feature and instance selection are two effective data reduction processes which can be applied to classification tasks obtaining promising results. Although both processes are defined separately, it is possible to apply them simultaneously. This paper proposes an evolutionary model to perform feature and instance selection in nearest neighbor classification. It is based on cooperative coevolution, which has been applied to many computational problems with great success. The proposed approach is compared with a wide range of evolutionary feature and instance selection methods for classification. The results contrasted through non-parametric statistical tests show that our model outperforms previously proposed evolutionary approaches for performing data reduction processes in combination with the nearest neighbor rule.},
journal = {Pattern Recogn.},
month = jun,
pages = {2082–2105},
numpages = {24},
keywords = {Cooperative coevolution, Evolutionary algorithms, Feature selection, Instance selection, Nearest neighbor}
}

@article{10.1007/s00500-021-06080-x,
author = {Gillala, Rekha and Vuyyuru, Krishna Reddy and Jatoth, Chandrashekar and Fiore, Ugo},
title = {An efficient chaotic salp swarm optimization approach based on ensemble algorithm for class imbalance problems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06080-x},
doi = {10.1007/s00500-021-06080-x},
abstract = {Class imbalance problems have attracted the research community, but a few works have focused on feature selection with imbalanced datasets. To handle class imbalance problems, we developed a novel fitness function for feature selection using the chaotic salp swarm optimization algorithm, an efficient meta-heuristic optimization algorithm that has been successfully used in a wide range of optimization problems. This paper proposes an AdaBoost algorithm with chaotic salp swarm optimization. The most discriminating features are selected using salp swarm optimization, and AdaBoost classifiers are thereafter trained on the features selected. Experiments show the ability of the proposed technique to find the optimal features with performance maximization of AdaBoost.},
journal = {Soft Comput.},
month = dec,
pages = {14955–14965},
numpages = {11},
keywords = {Imbalanced data, Feature selection, Ensemble algorithms, Classification, Salp swarm algorithm}
}

@inproceedings{10.1145/3417990.3421263,
author = {Pett, Tobias and Eichhorn, Domenik and Schaefer, Ina},
title = {Risk-based compatibility analysis in automotive systems engineering},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421263},
doi = {10.1145/3417990.3421263},
abstract = {Software is the new leading factor for innovation in the automotive industry. With the increase of software in road vehicles new business models, such as after-sale updates (i.e., Function-on-Demand) and Over-the-Air-Updates come into focus of manufacturers. When updating a road vehicle in the field, it is required to ensure functional safety. An update shall not influence existing functionality and break its safety. Hence, it must be compatible with the existing software. The compatibility of an update is ensured by testing. However, testing all variants of a highly configurable system, such as a modern car's software, is infeasible, due to the combinatorial explosion. To address this problem, in this paper, we propose a risk-based change-impact analysis to identify system variants relevant for retesting after an update. We combine existing concepts from product sampling, risk-based testing, and configuration prioritization and apply them to automotive architectures. For validating our concept, we use the Body Comfort System case study from the automotive industry. Our evaluation reveals that the concept backed by tool support may reduce testing effort by identifying and prioritizing incompatible variants wrt to a system update.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {34},
numpages = {10},
keywords = {automotive engineering, configurable systems, risk-based analysis},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1007/s11042-020-09538-6,
author = {Tripathi, Diwakar and Edla, Damodar Reddy and Kuppili, Venkatanareshbabu and Dharavath, Ramesh},
title = {Binary BAT algorithm and RBFN based hybrid credit scoring model},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09538-6},
doi = {10.1007/s11042-020-09538-6},
abstract = {Credit scoring is a process of calculating the risk associated with an applicant on the basis of applicant’s credentials such as social status, financial status, etc. and it plays a vital role to improve cash flow for financial industry. However, the credit scoring dataset may have a large number of irrelevant or redundant features which leads to poorer classification performances and higher complexity. So, by removing redundant and irrelevant features may overcome the problem with huge number of features. This work emphasized on the role of feature selection and proposed a hybrid model by combining feature selection by utilizing Binary BAT optimization technique with a novel fitness function and aggregated with for Radial Basis Function Neural Network (RBFN) for credit score classification. Further, proposed feature selection approach is aggregated with Support Vector Machine (SVM) &amp; Random Forest (RF), and other optimization approaches namely: Hybrid Particle Swarm Optimization and Gravitational Search Algorithm (PSOGSA), Hybrid Particle Swarm Optimization and Genetic Algorithm (PSOGA), Improved Krill Herd (IKH), Improved Cuckoo Search (ICS), Firefly Algorithm (FF) and Differential Evolution (DE) are also applied for comparative analysis.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {31889–31912},
numpages = {24},
keywords = {Optimization, Credit scoring, Feature selection, Classification}
}

@article{10.1016/j.compbiomed.2020.104128,
author = {Chalakkal, Renoh and Hafiz, Faizal and Abdulla, Waleed and Swain, Akshya},
title = {An efficient framework for automated screening of Clinically Significant Macular Edema},
year = {2021},
issue_date = {Mar 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {130},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104128},
doi = {10.1016/j.compbiomed.2020.104128},
journal = {Comput. Biol. Med.},
month = mar,
numpages = {10},
keywords = {Diabetic retinopathy, Feature selection, Fundus imaging, Skewed datasets}
}

@article{10.1007/s10489-013-0440-x,
author = {Idris, Adnan and Khan, Asifullah and Lee, Yeon Soo},
title = {Intelligent churn prediction in telecom: employing mRMR feature selection and RotBoost based ensemble classification},
year = {2013},
issue_date = {October   2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {39},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-013-0440-x},
doi = {10.1007/s10489-013-0440-x},
abstract = {Churn prediction in telecom has recently gained substantial interest of stakeholders because of associated revenue losses.Predicting telecom churners, is a challenging problem due to the enormous nature of the telecom datasets. In this regard, we propose an intelligent churn prediction system for telecom by employing efficient feature extraction technique and ensemble method. We have used Random Forest, Rotation Forest, RotBoost and DECORATE ensembles in combination with minimum redundancy and maximum relevance (mRMR), Fisher's ratio and F-score methods to model the telecom churn prediction problem. We have observed that mRMR method returns most explanatory features compared to Fisher's ratio and F-score, which significantly reduces the computations and help ensembles in attaining improved performance. In comparison to Random Forest, Rotation Forest and DECORATE, RotBoost in combination with mRMR features attains better prediction performance on the standard telecom datasets. The better performance of RotBoost ensemble is largely attributed to the rotation of feature space, which enables the base classifier to learn different aspects of the churners and non-churners. Moreover, the Adaboosting process in RotBoost also contributes in achieving higher prediction accuracy by handling hard instances. The performance evaluation is conducted on standard telecom datasets using AUC, sensitivity and specificity based measures. Simulation results reveal that the proposed approach based on RotBoost in combination with mRMR features (CP-MRB) is effective in handling high dimensionality of the telecom datasets. CP-MRB offers higher accuracy in predicting churners and thus is quite prospective in modeling the challenging problems of customer churn prediction in telecom.},
journal = {Applied Intelligence},
month = oct,
pages = {659–672},
numpages = {14},
keywords = {Churn prediction, DECORATE, RotBoost, Telecom, mRMR}
}

@article{10.1016/j.ins.2010.05.037,
author = {Unler, Alper and Murat, Alper and Chinnam, Ratna Babu},
title = {mr2PSO: A maximum relevance minimum redundancy feature selection method based on swarm intelligence for support vector machine classification},
year = {2011},
issue_date = {October, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {20},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.05.037},
doi = {10.1016/j.ins.2010.05.037},
abstract = {This paper presents a hybrid filter-wrapper feature subset selection algorithm based on particle swarm optimization (PSO) for support vector machine (SVM) classification. The filter model is based on the mutual information and is a composite measure of feature relevance and redundancy with respect to the feature subset selected. The wrapper model is a modified discrete PSO algorithm. This hybrid algorithm, called maximum relevance minimum redundancy PSO (mr^2PSO), is novel in the sense that it uses the mutual information available from the filter model to weigh the bit selection probabilities in the discrete PSO. Hence, mr^2PSO uniquely brings together the efficiency of filters and the greater accuracy of wrappers. The proposed algorithm is tested over several well-known benchmarking datasets. The performance of the proposed algorithm is also compared with a recent hybrid filter-wrapper algorithm based on a genetic algorithm and a wrapper algorithm based on PSO. The results show that the mr^2PSO algorithm is competitive in terms of both classification accuracy and computational performance.},
journal = {Inf. Sci.},
month = oct,
pages = {4625–4641},
numpages = {17},
keywords = {Classification, Feature selection, Filters, Mutual information, Particle swarm optimization, Support vector machine, Wrappers}
}

@article{10.1145/3434775,
author = {Kumar, Rahul and Gupta, Ankur and Arora, Harkirat Singh and Raman, Balasubramanian},
title = {IBRDM: An Intelligent Framework for Brain Tumor Classification Using Radiomics- and DWT-based Fusion of MRI Sequences},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3434775},
doi = {10.1145/3434775},
abstract = {Brain tumors are one of the critical malignant neurological cancers with the highest number of deaths and injuries worldwide. They are categorized into two major classes, high-grade glioma (HGG) and low-grade glioma (LGG), with HGG being more aggressive and malignant, whereas LGG tumors are less aggressive, but if left untreated, they get converted to HGG. Thus, the classification of brain tumors into the corresponding grade is a crucial task, especially for making decisions related to treatment. Motivated by the importance of such critical threats to humans, we propose a novel framework for brain tumor classification using discrete wavelet transform-based fusion of MRI sequences and Radiomics feature extraction. We utilized the Brain Tumor Segmentation 2018 challenge training dataset for the performance evaluation of our approach, and we extract features from three regions of interest derived using a combination of several tumor regions. We used wrapper method-based feature selection techniques for selecting a significant set of features and utilize various machine learning classifiers, Random Forest, Decision Tree, and Extra Randomized Tree for training the model. For proper validation of our approach, we adopt the five-fold cross-validation technique. We achieved state-of-the-art performance considering several performance metrics, 〈Acc, Sens, Spec, F1-score, MCC, AUC 〉 ≡ 〈 98.60%, 99.05%, 97.33%, 99.05%, 96.42%, 98.19% 〉, where Acc, Sens, Spec, F1-score, MCC, and AUC represents the accuracy, sensitivity, specificity, F1-score, Matthews correlation coefficient, and area-under-the-curve, respectively. We believe our proposed approach will play a crucial role in the planning of clinical treatment and guidelines before surgery.},
journal = {ACM Trans. Internet Technol.},
month = sep,
articleno = {9},
numpages = {30},
keywords = {Gliomas, decision tree and random forest, extremely randomized tree}
}

@article{10.1504/ijdmb.2021.116891,
author = {Buathong, Wipawan and Jarupunphol, Pita},
title = {Dengue fever prediction modelling using data mining techniques},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {25},
number = {1–2},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2021.116891},
doi = {10.1504/ijdmb.2021.116891},
abstract = {This research experiments on several combinations of feature selection and classifier to obtain the most efficient classification model for predicting dengue fever. The features of relationship patterns for predicting dengue fever were investigated. In order to obtain the most effective classification model, several feature selection techniques were ranked and experimented with well-recognised classifiers. The measurement results of different models were illustrated and compared. The most efficient model is the neural network with three layers. Each layer contains 100 nodes with ReLu activation function. Five features were classified using information gain with 64.9% accuracy, 71.8% F-measure, 65.7% precision, and 79.0% recall. Other competitive machine learning models with slightly similar efficiency are: (1) the combined Naive Bayes and information gain; (2) the combined neural network and ReliefF; (3) the combined Naive Bayes and FCBF. SVM, on the other hand, is considered as the least efficient model when experimented with selected feature selection techniques.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {103–127},
numpages = {24},
keywords = {dengue fever, data mining, classification, feature selection, ranking}
}

@inproceedings{10.1145/3302425.3302471,
author = {Akkaradamrongrat, Suphamongkol and Kachamas, Pornpimon and Sinthupinyo, Sukree},
title = {Classification of Advertisement Text on Facebook Using Synthetic Minority Over-Sampling Technique},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302471},
doi = {10.1145/3302425.3302471},
abstract = {Understanding in consumer behavior is an important task in the field of marketing. Dentsu's AISAS model is a model that has been proposed to describe consumer behavior. The model defines reaction when the consumer has seen advertising into five stages: attention, interest, search, action, and share. In this paper, advertisement text datasets from Facebook were labelled as the stages of AISAS model and learned to be classified by machine learning algorithms. Nevertheless, like many other real-world data, our dataset had imbalanced class distribution. The classifier algorithms tend to predict mostly the majority class. To overcome this problem, synthetic minority over-sampling technique (SMOTE) was adopted and also combined with chi-square based feature selection technique. Varieties of feature sizes based on various classifier algorithms were compared. In the appropriate feature size, SMOTE could improve the classification performance in terms of recall and F1 score.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {67},
numpages = {6},
keywords = {AISAS model, Feature selection, SMOTE},
location = {Sanya, China},
series = {ACAI '18}
}

@inproceedings{10.5555/2394970.2395063,
author = {Hern\'{a}ndez-Cisneros, Rolando R. and Terashima-Mar\'{\i}n, Hugo and Conant-Pablos, Santiago E.},
title = {Comparison of class separability, forward sequential search and genetic algorithms for feature selection in the classification of individual and clustered microcalcifications in digital mammograms},
year = {2007},
isbn = {3540742581},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The presence of microcalcification clusters in digital mammograms is a primary indicator of early stages of malignant types of breast cancer and its detection is important to prevent the disease. This paper uses a procedure for the classification of microcalcification clusters in mammograms using sequential Difference of Gaussian filters (DoG) and feedforward Neural Networks (NN). Three methods using class separability, forward sequential search and genetic algorithms for feature selection are compared. We found that the use of Genetic Algorithms (GAs) for selecting the features from microcalcifications and microcalcification clusters that will be the inputs of a feedforward Neural Network (NN) results mainly in improvements in overall accuracy, sensitivity and specificity of the classification.},
booktitle = {Proceedings of the 4th International Conference on Image Analysis and Recognition},
pages = {911–922},
numpages = {12},
location = {Montreal, Canada},
series = {ICIAR'07}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/973264.973281,
author = {Grimaldi, Marco and Cunningham, P\'{a}draig and Kokaram, Anil},
title = {A Wavelet Packet representation of audio signals for music genre classification using different ensemble and feature selection techniques},
year = {2003},
isbn = {1581137788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/973264.973281},
doi = {10.1145/973264.973281},
abstract = {The vast amount of music available electronically presents considerable challenges for information retrieval. There is a need to annotate music items with descriptors in order to facilitate retrieval. In this paper we present a process for determining the music genre of an item using a new set of descriptors. A Wavelet Packet Transform is applied to obtain the signal representation at different levels. Time and frequency features are extracted from these levels taking into account the nature of music. Using round-robin and one-against-all ensembles of simple classifiers, together with feature selection methods, we evaluate the best signal representation for music genre classification. Ensembles based on different feature sub-spaces are explored as well in order to overcome over-fitting issues. Our evaluation shows that Wavelet Packet analysis together with ensemble methods achieves very good classification accuracy.},
booktitle = {Proceedings of the 5th ACM SIGMM International Workshop on Multimedia Information Retrieval},
pages = {102–108},
numpages = {7},
keywords = {Wavelet analysis, ensemble techniques, features selection, music information retrieval},
location = {Berkeley, California},
series = {MIR '03}
}

@inproceedings{10.1145/2808492.2808523,
author = {Fang, Yuchun and Zheng, Yandan and Yu, Chanjuan},
title = {Manifold learning in sparse selected feature subspaces},
year = {2015},
isbn = {9781450335287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808492.2808523},
doi = {10.1145/2808492.2808523},
abstract = {Feature selection and extraction are often combined for learning effective representation in image recognition. In this paper, we propose a serial combination method for feature representation in face image recognition. The algorithm first selects a subset in high-dimensional low level feature spaces with sparse learning and then feature extraction is further performed with manifold learning. Both feature selection and extraction are performed evolving class-specific information to learn task-specific feature representation. The serial combination serves to form the final lower dimensional representation in a multi-layer structure. Experimental analysis for multiple face image recognition tasks prove the effectiveness of the proposed serial combination of feature learning.},
booktitle = {Proceedings of the 7th International Conference on Internet Multimedia Computing and Service},
articleno = {31},
numpages = {5},
keywords = {face image recognition, feature extraction, feature selection},
location = {Zhangjiajie, Hunan, China},
series = {ICIMCS '15}
}

@article{10.3233/IDA-150390,
author = {Makrehchi, Masoud and Kamel, Mohamed S.},
title = {Extracting domain-specific stopwords for text classifiers},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {21},
number = {1},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-150390},
doi = {10.3233/IDA-150390},
abstract = {In this paper, an automatic generation of domain-specific stopwords
from a large labeled corpus is proposed. In the majority of text mining tasks, stopwords are removed according to a standard stopword list and/or using  high and low document frequencies. In this paper,
a new approach for stopword extraction, based on the notion of
backward filter-level performance and data sparsity index, is
proposed. First, based on the proposed model to evaluate the
extracted stopwords, we examine high document frequency filtering
for stopword reduction. Secondly, a new algorithm for building
general and domain-specific stopword lists is proposed. For the
method, it is assumed that a set of candidate stopwords must have a
minimum information content and prediction capacity that is measured
by the performance of a classifier. We show that to avoid obtaining
the classifier performance, it can be estimated by the sparsity of the training dataset. Moreover, it is confirmed that even if a given term ranking measure can perform well for the feature
selection, the measure is not necessarily efficient for selecting
poor features (stopwords). According to the comparative study, the newly devised
approach offers more promising results that guarantee a minimum
information loss by filtering out most stopwords.},
journal = {Intell. Data Anal.},
month = jan,
pages = {39–62},
numpages = {24},
keywords = {Text classification, stopwords, stopword reduction, feature selection}
}

@inproceedings{10.1145/3423457.3429362,
author = {Karatzoglou, Antonios},
title = {Applying topographic features for identifying speed patterns using the example of critical driving},
year = {2020},
isbn = {9781450381666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423457.3429362},
doi = {10.1145/3423457.3429362},
abstract = {Finding the right features represents an essential part when trying to identify patterns in spatiotemporal signals. This paper describes the concept of using the topographic properties prominence and isolation for recognizing critical driving patterns in speed signals. Experiments show that both features can help identify specific driving segments in the users' speed data such as harsh acceleration, abrupt braking as well as over-speeding phases.},
booktitle = {Proceedings of the 13th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
articleno = {6},
numpages = {4},
keywords = {driving behavior, feature selection, speed signal Patterns, topographic isolation, topographic prominence},
location = {Seattle, Washington},
series = {IWCTS '20}
}

@article{10.1134/S1054661820040033,
author = {Al-Frady, Laith and Al-Taei, Ali},
title = {Wrapper Filter Approach for Accelerometer-Based Human Activity Recognition},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {4},
issn = {1054-6618},
url = {https://doi.org/10.1134/S1054661820040033},
doi = {10.1134/S1054661820040033},
journal = {Pattern Recognit. Image Anal.},
month = oct,
pages = {757–764},
numpages = {8},
keywords = {human activity recognition, accelerometer, feature selection, machine learning}
}

@article{10.1016/j.asoc.2020.107064,
author = {An, Shuang and Hu, Qinghua and Wang, Changzhong},
title = {Probability granular distance-based fuzzy rough set model},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.107064},
doi = {10.1016/j.asoc.2020.107064},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {13},
keywords = {Fuzzy rough sets, Probability granular distance, Noisy samples, Feature selection, Robustness}
}

@article{10.3233/JIFS-201395,
author = {Malarvizhi, K. and Amshakala, K.},
title = {Feature Linkage Weight Based Feature Reduction using Fuzzy Clustering Method},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-201395},
doi = {10.3233/JIFS-201395},
abstract = {In this paper, a novel Feature-Reduction Fuzzy C-means (FRFCM) with Feature Linkage Weight (FRFCM-FLW) algorithm is introduced. By the combination of FRFCM and feature linkage weight,  a new feature selection model is developed, called a Feature Linkage Weight Based FRFCM using fuzzy clustering. The larger amounts of features are superior to the complication of the problem, and the larger the time that is exhausted in creating the outcome of the classifier or the model. Feature selection has been established as a high-quality method for preferring features that best describes the data under certain criteria or measure. The proposed method presents three stages namely, 1) Data Formation: The process of data collection and data cleaning; 2) FRFCM-FLW. The proposed method can decrease feature elements routinely, and also construct excellent clustering results. The proposed method calculates a novel weight for every feature by combining modified Mahalanobis distance with feature δm variance in FRFCM algorithm; 3) Fuzzy C-means (FCM) cluster. The proposed FRFCM-FLW method proves high Accuracy Rate (AR), Rand Index (RI) and Jaccard Index (JI) ratio when compared to other feature reduction algorithms like WFCM, EWKM, WKM, FCM and FRFCM algorithms.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {4563–4572},
numpages = {10},
keywords = {Data mining, fuzzy logic, feature selection, FCM}
}

@article{10.1016/j.eswa.2021.114658,
author = {Lavanya, P.G. and Kouser, K. and Suresha, Mallappa},
title = {Effective feature representation using symbolic approach for classification and clustering of big data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114658},
doi = {10.1016/j.eswa.2021.114658},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {16},
keywords = {Big data, Feature selection, Sensor data, Summarization, Symbolic data, Twitter data}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {Software performance prediction, adversarial learning, configurable systems, regularization},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1016/j.knosys.2014.06.004,
author = {Wu, Qingyao and Ye, Yunming and Zhang, Haijun and Ng, Michael K. and Ho, Shen-Shyang},
title = {ForesTexter: An efficient random forest algorithm for imbalanced text categorization},
year = {2014},
issue_date = {September, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {67},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2014.06.004},
doi = {10.1016/j.knosys.2014.06.004},
abstract = {In this paper, we propose a new random forest (RF) based ensemble method, ForesTexter, to solve the imbalanced text categorization problems. RF has shown great success in many real-world applications. However, the problem of learning from text data with class imbalance is a relatively new challenge that needs to be addressed. A RF algorithm tends to use a simple random sampling of features in building their decision trees. As a result, it selects many subspaces that contain few, if any, informative features for the minority class. Furthermore, the Gini measure for data splitting is considered to be skew sensitive and bias towards the majority class. Due to the inherent complex characteristics of imbalanced text datasets, learning RF from such data requires new approaches to overcome challenges related to feature subspace selection and cut-point choice while performing node splitting. To this end, we propose a new tree induction method that selects splits, both feature subspace selection and splitting criterion, for RF on imbalanced text data. The key idea is to stratify features into two groups and to generate effective term weighting for the features. One group contains positive features for the minority class and the other one contains the negative features for the majority class. Then, for feature subspace selection, we effectively select features from each group based on the term weights. The advantage of our approach is that each subspace contains adequate informative features for both minority and majority classes. One difference between our proposed tree induction method and the classical RF method is that our method uses Support Vector Machines (SVM) classifier to split the training data into smaller and more balance subsets at each tree node, and then successively retrains the SVM classifiers on the data partitions to refine the model while moving down the tree. In this way, we force the classifiers to learn from refined feature subspaces and data subsets to fit the imbalanced data better. Hence, the tree model becomes more robust for text categorization task with imbalanced dataset. Experimental results on various benchmark imbalanced text datasets (Reuters-21578, Ohsumed, and imbalanced 20 newsgroup) consistently demonstrate the effectiveness of our proposed ForesTexter method. The performance of our proposed approach is competitive against the standard random forest and different variants of SVM algorithms.},
journal = {Know.-Based Syst.},
month = sep,
pages = {105–116},
numpages = {12},
keywords = {Imbalanced classification, Random forests, SVM, Stratified sampling, Text categorization}
}

@article{10.1016/j.knosys.2021.106875,
author = {Wang, Ning and Zhao, Senyao and Cui, Shaoze and Fan, Weiguo},
title = {A hybrid ensemble learning method for the identification of gang-related arson cases},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {218},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.106875},
doi = {10.1016/j.knosys.2021.106875},
journal = {Know.-Based Syst.},
month = apr,
numpages = {16},
keywords = {Arson cases, Gang crime, Ensemble learning, Unbalanced data, Feature selection, Differential evolution}
}

@article{10.1007/s00500-020-05152-8,
author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam and Aleem, Muhammad},
title = {A load balance multi-scheduling model for OpenCL kernel tasks in an integrated cluster},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {1},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05152-8},
doi = {10.1007/s00500-020-05152-8},
abstract = {Nowadays, embedded systems are comprised of heterogeneous multi-core architectures, i.e., CPUs and GPUs. If the application is mapped to an appropriate processing core, then these architectures provide many performance benefits to applications. Typically, programmers map sequential applications to CPU and parallel applications to GPU. The task mapping becomes challenging because of the usage of evolving and complex CPU- and GPU-based architectures. This paper presents an approach to map the OpenCL application to heterogeneous multi-core architecture by determining the application suitability and processing capability. The classification is achieved by developing a machine learning-based device suitability classifier that predicts which processor has the highest computational compatibility to run OpenCL applications. In this paper, 20 distinct features are proposed that are extracted by using the developed LLVM-based static analyzer. In order to select the best subset of features, feature selection is performed by using both correlation analysis and the feature importance method. For the class imbalance problem, we use and compare synthetic minority over-sampling method with and without feature selection. Instead of hand-tuning the machine learning classifier, we use the tree-based pipeline optimization method to select the best classifier and its hyper-parameter. We then compare the optimized selected method with traditional algorithms, i.e., random forest, decision tree, Na\"{\i}ve Bayes and KNN. We apply our novel approach on extensively used OpenCL benchmarks, i.e., AMD and Polybench. The dataset contains 653 training and 277 testing applications. We test the classification results using four performance metrics, i.e., F-measure, precision, recall and R2. The optimized and reduced feature subset model achieved a high F-measure of 0.91 and R2 of 0.76. The proposed framework automatically distributes the workload based on the application requirement and processor compatibility.},
journal = {Soft Comput.},
month = jan,
pages = {407–420},
numpages = {14},
keywords = {Machine learning, Classification, Feature selection, OpenCL, Optimization}
}

@article{10.1109/TCBB.2021.3053429,
author = {I\c{s}\i{}k, Yunus Emre and G\"{o}rmez, Yasin and Ayd\i{}n, Zafer and Bakir-Gungor, Burcu},
title = {The Determination of Distinctive Single Nucleotide Polymorphism Sets for the Diagnosis of Behçet's Disease},
year = {2021},
issue_date = {May-June 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3053429},
doi = {10.1109/TCBB.2021.3053429},
abstract = {Behçet's Disease (BD) is a multi-system inflammatory disorder in which the etiology remains unclear. The most probable hypothesis is that genetic tendency and environmental factors play roles in the development of BD. In order to find the essential reasons, genetic changes on thousands of genes should be analyzed. Besides, there is a need for extra analysis to find out which genetic factor affects the disease. Machine learning approaches have high potential for extracting the knowledge from genomics and selecting the representative Single Nucleotide Polymorphisms (SNPs) as the most effective features for the clinical diagnosis process. In this study, we have attempted to identify representative SNPs using feature selection methods, incorporating biological information and aimed to develop a machine-learning model for diagnosing Behçet's disease. By combining biological information and machine learning classifiers, up to 99.64 percent accuracy of disease prediction is achieved using only 13,611 out of 311,459 SNPs. In addition, we revealed the SNPs that are most distinctive by performing repeated feature selection in cross-validation experiments.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {1909–1918},
numpages = {10}
}

@article{10.1007/s10878-020-00578-0,
author = {Nguyen, Thi Thanh Sang and Do, Pham Minh Thu},
title = {Classification optimization for training a large dataset with Na\"{\i}ve Bayes},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {40},
number = {1},
issn = {1382-6905},
url = {https://doi.org/10.1007/s10878-020-00578-0},
doi = {10.1007/s10878-020-00578-0},
abstract = {Book classification is very popular in digital libraries. Book rating prediction is crucial to improve the care of readers. The commonly used techniques are decision tree, Na\"{\i}ve Bayes (NB), neural networks, etc. Moreover, mining book data depends on feature selection, data pre-processing, and data preparation. This paper proposes the solutions of knowledge representation optimization as well as feature selection to enhance book classification and point out appropriate classification algorithms. Several experiments have been conducted and it has been found that NB could provide best prediction results. The accuracy and performance of NB can be improved and outperform other classification algorithms by applying appropriate strategies of feature selections, data type selection as well as data transformation.},
journal = {J. Comb. Optim.},
month = jul,
pages = {141–169},
numpages = {29},
keywords = {Data mining, Na\"{\i}ve Bayes, Word embedding, Feature selection}
}

@inproceedings{10.1007/978-3-030-86890-1_21,
author = {Wu, Yalun and Song, Minglu and Li, Yike and Tian, Yunzhe and Tong, Endong and Niu, Wenjia and Jia, Bowei and Huang, Haixiang and Li, Qiong and Liu, Jiqiang},
title = {Improving Convolutional Neural Network-Based Webshell Detection Through Reinforcement Learning},
year = {2021},
isbn = {978-3-030-86889-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86890-1_21},
doi = {10.1007/978-3-030-86890-1_21},
abstract = {Webshell detection is highly important for network security protection. Conventional methods are based on keywords matching, which heavily relies on experiences of domain experts when facing emerging malicious webshells of various kinds. Recently, machine learning, especially supervised learning, is introduced for webshell detection and has proved to be a great success. As one of state-of-the-art work, neural network (NN) is designed to input a large number of features and enable deep learning. Thus, how to properly combine the advantages of automatic feature selection and the advantages of expert knowledge-based way has become a key issue. Considering that special features to indicate unexpected webshell behaviors for a target business system are usually simple but effective, in this work, we propose a novel approach for improving webshell detection based on convolutional neural network (CNN) through reinforcement learning. We utilize the reinforcement learning of asynchronous advantage actor-critic (A3C) for automatic feature selection, aiming to maximize the expected accuracy of the CNN classifier on a validation dataset by sequentially interacting with the feature space. Moreover, considering the sparseness of feature values, we build the CNN classifier with two convolutional layers and a global pooling. Extensive experiments and analysis have been conducted to demonstrate the effectiveness of our proposed method.},
booktitle = {Information and Communications Security: 23rd International Conference, ICICS 2021, Chongqing, China, November 19-21, 2021, Proceedings, Part I},
pages = {368–383},
numpages = {16},
keywords = {Webshell detection, Feature selection, Unexpected behavior feature, Reinforcement learning, Convolutional neural network},
location = {Chongqing, China}
}

@inproceedings{10.1145/3205455.3205638,
author = {Zhang, Boyu and Qin, A. K. and Sellis, Timos},
title = {Evolutionary feature subspaces generation for ensemble classification},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205455.3205638},
doi = {10.1145/3205455.3205638},
abstract = {Ensemble learning is a powerful machine learning paradigm which leverages a collection of diverse base learners to achieve better prediction performance than that could be achieved by any individual base learner. This work proposes an evolutionary feature subspaces generation based ensemble learning framework, which formulates the tasks of searching for the most suitable feature subspace for each base learner into a multi-task optimization problem and solve it via an evolutionary multi-task optimizer. Multiple such problems which correspond to different base learners are solved simultaneously via an evolutionary multi-task feature selection algorithm such that solving one problem may help solve some other problems via implicit knowledge transfer. The quality of thus generated feature subspaces is supposed to outperform those obtained by individually seeking the optimal feature subspace for each base learner. We implement the proposed framework by using SVM, KNN, and decision tree as the base learners, proposing a multi-task binary particle swarm optimization algorithm for evolutionary multi-task feature selection, and utilizing the major voting scheme to combine the outputs of the base learners. Experiments on several UCI datasets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {577–584},
numpages = {8},
keywords = {ensemble, feature selection, multi-task optimization},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.compind.2021.103394,
author = {Dias, Andre Luis and Turcato, Afonso Celso and Sestito, Guilherme Serpa and Brandao, Dennis and Nicoletti, Rodrigo},
title = {A cloud-based condition monitoring system for fault detection in rotating machines using PROFINET process data},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {126},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2021.103394},
doi = {10.1016/j.compind.2021.103394},
journal = {Comput. Ind.},
month = apr,
numpages = {13},
keywords = {Condition monitoring, Rotating machines, Feature Selection, Support vector machine, PROFINET, Cloud computing}
}

@article{10.1016/j.comnet.2021.108613,
author = {D’hooge, Laurens and Verkerken, Miel and Wauters, Tim and Volckaert, Bruno and De Turck, Filip},
title = {Hierarchical feature block ranking for data-efficient intrusion detection modeling},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108613},
doi = {10.1016/j.comnet.2021.108613},
journal = {Comput. Netw.},
month = dec,
numpages = {17},
keywords = {Network security, Intrusion detection, Hybrid feature selection}
}

@inproceedings{10.1007/978-3-030-29911-8_27,
author = {Hara, Satoshi and Maehara, Takanori},
title = {Convex Hull Approximation of Nearly Optimal Lasso Solutions},
year = {2019},
isbn = {978-3-030-29910-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29911-8_27},
doi = {10.1007/978-3-030-29911-8_27},
abstract = {In an ordinary feature selection procedure, a set of important features is obtained by solving an optimization problem such as the Lasso regression problem, and we expect that the obtained features explain the data well. In this study, instead of the single optimal solution, we consider finding a set of diverse yet nearly optimal solutions. To this end, we formulate the problem as finding a small number of solutions such that the convex hull of these solutions approximates the set of nearly optimal solutions. The proposed algorithm consists of two steps: First, we randomly sample the extreme points of the set of nearly optimal solutions. Then, we select a small number of points using a greedy algorithm. The experimental results indicate that the proposed algorithm can approximate the solution set well. The results also indicate that we can obtain Lasso solutions with a large diversity.},
booktitle = {PRICAI 2019: Trends in Artificial Intelligence: 16th Pacific Rim International Conference on Artificial Intelligence, Cuvu, Yanuca Island, Fiji, August 26–30, 2019, Proceedings, Part II},
pages = {350–363},
numpages = {14},
keywords = {Feature selection, Lasso, Diversity},
location = {Cuvu, Yanuka Island, Fiji}
}

@article{10.5555/3546258.3546411,
author = {Atarashi, Kyohei and Oyama, Satoshi and Kurihara, Masahito},
title = {Factorization machines with regularization for sparse feature interactions},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Factorization machines (FMs) are machine learning predictive models based on second-order feature interactions and FMs with sparse regularization are called sparse FMs. Such regularizations enable feature selection, which selects the most relevant features for accurate prediction, and therefore they can contribute to the improvement of the model accuracy and interpretability. However, because FMs use second-order feature interactions, the selection of features often causes the loss of many relevant feature interactions in the resultant models. In such cases, FMs with regularization specially designed for feature interaction selection trying to achieve interaction-level sparsity may be preferred instead of those just for feature selection trying to achieve feature-level sparsity. In this paper, we present a new regularization scheme for feature interaction selection in FMs. For feature interaction selection, our proposed regularizer makes the feature interaction matrix sparse without a restriction on sparsity patterns imposed by the existing methods. We also describe efficient proximal algorithms for the proposed FMs and how our ideas can be applied or extended to feature selection and other related models such as higher-order FMs and the all-subsets model. The analysis and experimental results on synthetic and real-world datasets show the effectiveness of the proposed methods.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {153},
numpages = {50},
keywords = {factorization machines, sparse regularization, feature interaction selection, feature selection, proximal algorithm}
}

@article{10.1016/j.asoc.2021.107989,
author = {Cui, Shaoze and Qiu, Huaxin and Wang, Sutong and Wang, Yanzhang},
title = {Two-stage stacking heterogeneous ensemble learning method for gasoline octane number loss prediction},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107989},
doi = {10.1016/j.asoc.2021.107989},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {15},
keywords = {RON prediction, Gasoline, Stacking method, Heterogeneous ensemble, Feature selection, Differential evolution}
}

@inproceedings{10.1007/978-3-030-87007-2_28,
author = {Kumar, Lov and Dastidar, Triyasha Ghosh and Murthy Neti, Lalitha Bhanu and Satapathy, Shashank Mouli and Misra, Sanjay and Kocher, Vipul and Padmanabhuni, Srinivas},
title = {Deep-Learning Approach with DeepXplore for Software Defect Severity Level Prediction},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_28},
doi = {10.1007/978-3-030-87007-2_28},
abstract = {Fixing the defects of earlier releases and working on fast and efficient fixing of those software defects is detrimental for the release of further versions. Bug tracking systems like Bugzilla get thousands of software defect reports every day. Manually handling those report to assign severity to the defects is not feasible. Earlier traditional Machine Learning methods have been used to predict the severity level from the defect description. This paper presents different deep learning models to predict defect severity level. Furthermore, the deep neural network was tested using a framework developed similar to that DeepXplore. Different word-embedding techniques, feature-selection techniques, sampling techniques and deep learning models are analyzed and compared for this study. In this paper, we have considered Descriptive statistics, Box-plot, and Significant tests to compare the developed models for defect severity level prediction. The three performance metrics used for testing the models are AUC, Accuracy and Neuron Coverage. This is a preliminary study on DNN testing on this dataset. Thus, the paper focuses on DeepXplore DNN testing technique. However further studies would be undertaken on comparative analysis of different DNN testing techniques on this dataset.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {398–410},
numpages = {13},
keywords = {Severity prediction, Imbalance handling, Feature selection, Deep learning, Word embedding},
location = {Cagliari, Italy}
}

@article{10.1016/j.cose.2021.102374,
author = {McGahagan, John and Bhansali, Darshan and Pinto-Coelho, Ciro and Cukier, Michel},
title = {Discovering features for detecting malicious websites: An empirical study},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {109},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102374},
doi = {10.1016/j.cose.2021.102374},
journal = {Comput. Secur.},
month = oct,
numpages = {21},
keywords = {Feature-based malicious website detection, Web security, Supervised learning, Feature selection}
}

@article{10.1007/s10586-017-0950-0,
author = {Zhang, X. M. and He, G. J. and Zhang, Z. M. and Peng, Y. and Long, T. F.},
title = {Spectral-spatial multi-feature classification of remote sensing big data based on a random forest classifier for land cover mapping},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-017-0950-0},
doi = {10.1007/s10586-017-0950-0},
abstract = {Supplementary information, such as multi-temporal spectral data and textural features, has the potential to improve land cover classification accuracy. However, given the larger volumes of remote sensing data, it is difficult to utilize all the features of remote sensing big data having different times and spatial resolutions. Inefficiency is also a large problem when dealing with large area land cover mapping. In this study, a new mode of incorporating spatial and temporal dependencies in a complex region employing the random forests (RFs) classifier was utilized. To map land covers, spring and autumn spectral images and their spectral indexes, textural features obtained from Landsat 5 were selected, and an importance measure variable was used to reduce the data's dimension. In addition to randomly selecting the variable, we used random sampling to furthest decrease the generalization error in RF. The results showed that utilizing random sampling, multi-temporal spectral image and texture features, the classification of the Wuhan urban agglomeration, China, using RF performed well. The RF algorithm yielded an overall accuracy of 89.2% and a Kappa statistic of 0.8522, indicating high model performance. In addition, the variable importance measures demonstrated that the type of textural features was extremely important for intra-class separability. The RF model has transitivity. The algorithm can be extended by choosing a set of appropriate features for signature extension over large areas or in time-series of Landsat imagery. Land cover mapping might be more economical and efficient if no-cost imagery is used.},
journal = {Cluster Computing},
month = sep,
pages = {2311–2321},
numpages = {11},
keywords = {Feature selection (FS), Land cover mapping, Random forests (RFs), Random sampling, Spectral features, Temporal features, Textural features, Wuhan urban agglomeration}
}

@article{10.1007/s11277-020-07742-z,
author = {Singh, Gurwinder and Kaur, Manpreet and Singh, Birmohan},
title = {Detection of Epileptic Seizure EEG Signal Using Multiscale Entropies and Complete Ensemble Empirical Mode Decomposition},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {116},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-020-07742-z},
doi = {10.1007/s11277-020-07742-z},
abstract = {Epilepsy is a severe neurological disease which is diagnosed by analyzing Electroencephalogram. The epileptic seizure detection technique based on multiscale entropies and complete ensemble empirical mode decomposition (CEEMD) is proposed in this paper. CEEMD is used for the estimation of sub-bands and two multiscale entropies; multiscale dispersion entropy (MDE) and refined composite MDE are extracted from the sub-bands. The feature selection method, configured by hybridizing the filter based and wrapper based method, is used to select relevant multiscale entropies. The hybrid method has not only reduced features but also improved classification performance. An artificial neural network is trained with relevant features and performance is measured using classification accuracy, sensitivity and specificity. Five clinically relevant classification problems are used to assess the proposed technique. The performance is also compared with the state of the art techniques. The proposed technique has shown an improvement in detection of seizures and can be used to build the clinical system for epileptic seizure detection.},
journal = {Wirel. Pers. Commun.},
month = jan,
pages = {845–864},
numpages = {20},
keywords = {Epilepsy, Electroencephalogram, Multiscale entropies, Complete ensemble empirical mode decomposition (CEEMD), Artificial neural network}
}

@inproceedings{10.1007/978-3-030-91608-4_7,
author = {Liuliakov, Aleksei and Hammer, Barbara},
title = {AutoML Technologies for the Identification of Sparse Models},
year = {2021},
isbn = {978-3-030-91607-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91608-4_7},
doi = {10.1007/978-3-030-91608-4_7},
abstract = {Automated machine learning (AutoML) technologies constitute promising tools to automatically infer model architecture, meta-parameters or processing pipelines for specific machine learning tasks given suitable training data. At present, the main objective of such technologies typically relies on the accuracy of the resulting model. Additional objectives such as sparsity can be integrated by pre-processing steps or according penalty terms in the objective function. Yet, sparsity and model accuracy are often contradictory goals, and optimum solutions form a Pareto front. Thereby, it is not guaranteed that solutions at different positions of the Pareto front share the same architectural choices, hence current AutoML technologies might yield sub-optimal results. In this contribution, we propose a novel method, based on the AutoML method TPOT, which enables an automated optimization of ML pipelines with sparse input features along the whole Pareto front. We demonstrate that, indeed, different architectures are found at different points of the Pareto front for benchmark examples from the domain of systems security.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2021: 22nd International Conference, IDEAL 2021, Manchester, UK, November 25–27, 2021, Proceedings},
pages = {65–75},
numpages = {11},
keywords = {AutoML, Feature selection, TPOT},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1145/3278122.3278127,
author = {Soares, Larissa Rocha and Meinicke, Jens and Nadi, Sarah and K\"{a}stner, Christian and de Almeida, Eduardo Santana},
title = {Exploring feature interactions without specifications: a controlled experiment},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278127},
doi = {10.1145/3278122.3278127},
abstract = {In highly configurable systems, features may interact unexpectedly and produce faulty behavior. Those faults are not easily identified from the analysis of each feature separately, especially when feature specifications are missing. We propose VarXplorer, a dynamic and iterative approach to detect suspicious interactions. It provides information on how features impact the control and data flow of the program. VarXplorer supports developers with a graph that visualizes this information, mainly showing suppress and require relations between features. To evaluate whether VarXplorer helps improve the performance of identifying suspicious interactions, we perform a controlled study with 24 subjects. We find that with our proposed feature-interaction graphs, participants are able to identify suspicious interactions more than 3 times faster compared to the state-of-the-art tool.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {40–52},
numpages = {13},
keywords = {Controlled Experiment, Feature Interaction, Highly Configurable Systems},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1016/j.ins.2018.09.045,
author = {Bol\'{o}n-Canedo, Ver\'{o}nica and Sechidis, Konstantinos and S\'{a}nchez-Maro\~{n}o, Noelia and Alonso-Betanzos, Amparo and Brown, Gavin},
title = {Insights into distributed feature ranking},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {496},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.09.045},
doi = {10.1016/j.ins.2018.09.045},
journal = {Inf. Sci.},
month = sep,
pages = {378–398},
numpages = {21},
keywords = {Feature selection, Feature ranking, Distributed learning}
}

@article{10.1016/j.patrec.2020.11.015,
author = {Vouros, Avgoustinos and Vasilaki, Eleni},
title = {A semi-supervised sparse K-Means algorithm},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {142},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2020.11.015},
doi = {10.1016/j.patrec.2020.11.015},
journal = {Pattern Recogn. Lett.},
month = feb,
pages = {65–71},
numpages = {7},
keywords = {Semi-supervised clustering, sparse clustering, feature selection}
}

@article{10.1186/s13636-020-00182-4,
author = {Demiroglu, Cenk and Be\c{s}irli, Asl\i{} and Ozkanca, Yasin and \c{C}elik, Selime},
title = {Depression-level assessment from multi-lingual conversational speech data using acoustic and text features},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-4714},
url = {https://doi.org/10.1186/s13636-020-00182-4},
doi = {10.1186/s13636-020-00182-4},
abstract = {Depression is a widespread mental health problem around the world with a significant burden on economies. Its early diagnosis and treatment are critical to reduce the costs and even save lives. One key aspect to achieve that goal is to use technology and monitor depression remotely and relatively inexpensively using automated agents. There has been numerous efforts to automatically assess depression levels using audiovisual features as well as text-analysis of conversational speech transcriptions. However, difficulty in data collection and the limited amounts of data available for research present challenges that are hampering the success of the algorithms. One of the two novel contributions in this paper is to exploit databases from multiple languages for acoustic feature selection. Since a large number of features can be extracted from speech, given the small amounts of training data available, effective data selection is critical for success. Our proposed multi-lingual method was effective at selecting better features than the baseline algorithms, which significantly improved the depression assessment accuracy. The second contribution of the paper is to extract text-based features for depression assessment and use a novel algorithm to fuse the text- and speech-based classifiers which further boosted the performance.},
journal = {EURASIP J. Audio Speech Music Process.},
month = nov,
numpages = {17},
keywords = {Depression detection, Acoustic features, Feature selection}
}

@inproceedings{10.1145/3373724.3373725,
author = {Hoblitzell, Andrew and Babbar-Sebens, Meghna and Mukhopadhyay, Snehasis},
title = {Non-Stationary Reinforcement-Learning Based Dimensionality Reduction for Multi-objective Optimization of Wetland Design},
year = {2020},
isbn = {9781450372350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373724.3373725},
doi = {10.1145/3373724.3373725},
abstract = {This paper outlines a method of non-stationary reinforcement-based learning for feature selection. The method was developed for the Watershed REstoration using Spatio-Temporal Optimization of REsources (WRESTORE) system, which is a decision support system used for wetland design on the Eagle Creek Watershed, northwest of Indianapolis, Indiana. Our results show measurable impact for maximizing reward efficiently for the feature selection task. This work describes the existing WRESTORE system, provides an overview of related work in reinforcement learning and dimensionality reduction, and shows the impact of our work in the multi-objective optimization process of WRESTORE. The contribution of this work is the application of an RL-based feature selection technique in interactive optimization of watershed design.},
booktitle = {Proceedings of the 5th International Conference on Robotics and Artificial Intelligence},
pages = {82–86},
numpages = {5},
keywords = {Reinforcement based learning, artificial neural network, decision support system, feature selection, interactive optimization, online learning, user model},
location = {Singapore, Singapore},
series = {ICRAI '19}
}

@article{10.1007/s00357-019-09330-8,
author = {Yu, Jaehong and Zhong, Hua and Kim, Seoung Bum},
title = {An Ensemble Feature Ranking Algorithm for Clustering Analysis},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {37},
number = {2},
issn = {0176-4268},
url = {https://doi.org/10.1007/s00357-019-09330-8},
doi = {10.1007/s00357-019-09330-8},
abstract = {Feature ranking is a widely used feature selection method. It uses importance scores to evaluate features and selects those with high scores. Conventional unsupervised feature ranking methods do not consider the information on cluster structures; therefore, these methods may be unable to select the relevant features for clustering analysis. To address this limitation, we propose a feature ranking algorithm based on silhouette decomposition. The proposed algorithm calculates the ensemble importance scores by decomposing the average silhouette widths of random subspaces. By doing so, the contribution of a feature in generating cluster structures can be represented more clearly. Experiments on different benchmark data sets examined the properties of the proposed algorithm and compared it with the existing ensemble-based feature ranking methods. The experiments demonstrated that the proposed algorithm outperformed its existing counterparts.},
journal = {J. Classif.},
month = jul,
pages = {462–489},
numpages = {28},
keywords = {Ensemble importance score, Random subspace method, Silhouette decomposition, Unsupervised feature ranking}
}

@inproceedings{10.1145/3132847.3132994,
author = {Pang, Guansong and Xu, Hongzuo and Cao, Longbing and Zhao, Wentao},
title = {Selective Value Coupling Learning for Detecting Outliers in High-Dimensional Categorical Data},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132994},
doi = {10.1145/3132847.3132994},
abstract = {This paper introduces a novel framework, namely SelectVC and its instance POP, for learning selective value couplings (i.e., interactions between the full value set and a set of outlying values) to identify outliers in high-dimensional categorical data. Existing outlier detection methods work on a full data space or feature subspaces that are identified independently from subsequent outlier scoring. As a result, they are significantly challenged by overwhelming irrelevant features in high-dimensional data due to the noise brought by the irrelevant features and its huge search space. In contrast, SelectVC works on a clean and condensed data space spanned by selective value couplings by jointly optimizing outlying value selection and value outlierness scoring. Its instance POP defines a value outlierness scoring function by modeling a partial outlierness propagation process to capture the selective value couplings. POP further defines a top-k outlying value selection method to ensure its scalability to the huge search space. We show that POP (i) significantly outperforms five state-of-the-art full space- or subspace-based outlier detectors and their combinations with three feature selection methods on 12 real-world high-dimensional data sets with different levels of irrelevant features; and (ii) obtains good scalability, stable performance w.r.t. k, and fast convergence rate.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {807–816},
numpages = {10},
keywords = {categorical data, coupling learning, feature selection, high-dimensional data, outlier detection},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/3379156.3391379,
author = {Szalma, J\'{a}nos and Weiss, B\'{e}la},
title = {Data-Driven Classification of Dyslexia Using Eye-Movement Correlates of Natural Reading},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391379},
doi = {10.1145/3379156.3391379},
abstract = {Developmental dyslexia is a reading disability estimated to affect between 5 to 10 percent of the population. Current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Investigation of eye-movement correlates of reading using machine learning could enhance detection of dyslexia. Here we used eye-tracking data collected during natural reading of 48 young adults (24 dyslexic, 24 control). We established a set of 67 features containing saccade-, glissade-, fixation-related measures and the reading speed. To detect participants with dyslexic reading patterns, we used a linear support vector machine with 10-fold stratified cross-validation repeated 10 times. For feature selection we used a recursive feature elimination method, and we also considered hyperparameter optimization, both with nested and regular cross-validation. The overall best model achieved a 90.1% classification accuracy, while the best nested model achieved a 75.75% accuracy.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {40},
numpages = {4},
keywords = {Classification, Dyslexia, Eye tracking, Feature selection, Machine learning, Reading},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}

@article{10.1016/j.compbiomed.2021.104985,
author = {Ali, Md Mamun and Ahmed, Kawsar and Bui, Francis M. and Paul, Bikash Kumar and Ibrahim, Sobhy M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
title = {Machine learning-based statistical analysis for early stage detection of cervical cancer},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104985},
doi = {10.1016/j.compbiomed.2021.104985},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {17},
keywords = {Cervical cancer, Biopsy, Cytology, Hinselmann, Schiller, Random tree}
}

@article{10.1016/j.ins.2021.07.050,
author = {Niu, Xinzheng and Wang, Shimin and Wu, Chase Q. and Li, Yuran and Wu, Peng and Zhu, Jiahui},
title = {On a clustering-based mining approach with labeled semantics for significant place discovery},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {578},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.07.050},
doi = {10.1016/j.ins.2021.07.050},
journal = {Inf. Sci.},
month = nov,
pages = {37–63},
numpages = {27},
keywords = {Spatio-temporal trajectory clustering, Semantic trajectory, Feature selection, Significant place discovery}
}

@inproceedings{10.5555/3091574.3091610,
author = {Skalak, David B.},
title = {Prototype and feature selection by sampling and random mutation hill climbing algorithms},
year = {1994},
isbn = {1558603352},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eleventh International Conference on International Conference on Machine Learning},
pages = {293–301},
numpages = {9},
location = {New Brunswick, NJ, USA},
series = {ICML'94}
}

@article{10.1016/j.neunet.2005.06.044,
author = {Gold, Carl and Holub, Alex and Sollich, Peter},
title = {2005 Special Issue: Bayesian approach to feature selection and parameter tuning for support vector machine classifiers},
year = {2005},
issue_date = {June 2005},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {18},
number = {5–6},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2005.06.044},
doi = {10.1016/j.neunet.2005.06.044},
abstract = {A Bayesian point of view of SVM classifiers allows the definition of a quantity analogous to the evidence in probabilistic models. By maximizing this one can systematically tune hyperparameters and, via automatic relevance determination (ARD), select relevant input features. Evidence gradients are expressed as averages over the associated posterior and can be approximated using Hybrid Monte Carlo (HMC) sampling. We describe how a Nystrom approximation of the Gram matrix can be used to speed up sampling times significantly while maintaining almost unchanged classification accuracy. In experiments on classification problems with a significant number of irrelevant features this approach to ARD can give a significant improvement in classification performance over more traditional, non-ARD, SVM systems. The final tuned hyperparameter values provide a useful criterion for pruning irrelevant features, and we define a measure of relevance with which to determine systematically how many features should be removed. This use of ARD for hard feature selection can improve classification accuracy in non-ARD SVMs. In the majority of cases, however, we find that in data sets constructed by human domain experts the performance of non-ARD SVMs is largely insensitive to the presence of some less relevant features. Eliminating such features via ARD then does not improve classification accuracy, but leads to impressive reductions in the number of features required, by up to 75%.},
journal = {Neural Netw.},
month = jun,
pages = {693–701},
numpages = {9}
}

@article{10.1007/s10489-012-0391-7,
author = {Chatterjee, Snehamoy},
title = {Vision-based rock-type classification of limestone using multi-class support vector machine},
year = {2013},
issue_date = {July      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {39},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-012-0391-7},
doi = {10.1007/s10489-012-0391-7},
abstract = {Rock-type classification is a challenging and difficult job due to the heterogeneous properties of rocks. In this paper, an image-based rock-type analysis and classification method is proposed. The study was conducted at a limestone mine in western India using stratified random sampling from a case study mine. The analysis of collected sample images was performed in laboratory. Color, morphology, and textural features were extracted from the captured image and a total of 189 features were recorded. The multi-class support vector machine (SVM) algorithm was then applied for rock-type classification. The hyper-parameters and the number of input features of the SVM model were selected by genetic algorithm. The results revealed that the SVM model performed best when 40 features were selected out of the 189 extracted features. The results demonstrated that the overall accuracy of the proposed technique for rock type classification is 96.2 %. A comparative study shows that the proposed SVM model performed better than a competing neural network model in this case study mine.},
journal = {Applied Intelligence},
month = jul,
pages = {14–27},
numpages = {14},
keywords = {Feature selection, Genetic algorithm, Image classification, Support vector machine}
}

@article{10.1016/j.jksuci.2019.07.008,
author = {Hacine-Gharbi, Abdenour and Ravier, Philippe},
title = {On the optimal number estimation of selected features using joint histogram based mutual information for speech emotion recognition},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {9},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2019.07.008},
doi = {10.1016/j.jksuci.2019.07.008},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = nov,
pages = {1074–1083},
numpages = {10},
keywords = {Speech emotion recognition, Mutual information, Binning of joint histogram, Features selection, MFCC coefficients, GMM models}
}

@article{10.1145/3488369,
author = {Reddy, A. Pramod and V., Vijayarajan},
title = {Fusion Based AER System Using Deep Learning Approach for Amplitude and Frequency Analysis},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3488369},
doi = {10.1145/3488369},
abstract = {Automatic emotion recognition from Speech (AERS) systems based on acoustical analysis reveal that some emotional classes persist with ambiguity. This study employed an alternative method aimed at providing deep understanding into the amplitude–frequency, impacts of various emotions in order to aid in the advancement of near term, more effectively in classifying AER approaches. The study was undertaken by converting narrow 20 ms frames of speech into RGB or grey-scale spectrogram images. The features have been used to fine-tune a feature selection system that had previously been trained to recognise emotions. Two different Linear and Mel spectral scales are used to demonstrate a spectrogram. An inductive approach for in sighting the amplitude and frequency features of various emotional classes. We propose a two-channel profound combination of deep fusion network model for the efficient categorization of images. Linear and Mel- spectrogram is acquired from Speech-signal, which is prepared in the recurrence area to input Deep Neural Network. The proposed model Alex-Net with five convolutional layers and two fully connected layers acquire most vital features form spectrogram images plotted on the amplitude-frequency scale. The state-of-the-art is compared with benchmark dataset (EMO-DB). RGB and saliency images are fed to pre-trained Alex-Net tested both EMO-DB and Telugu dataset with an accuracy of 72.18% and fused image features less computations reaching to an accuracy 75.12%. The proposed model show that Transfer learning predict efficiently than Fine-tune network. When tested on Emo-DB dataset, the prop\.{o}sed system adequately learns discriminant features from speech spectr\.{o}grams and outperforms many stundefinedte-of-the-art techniques.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {54},
numpages = {19},
keywords = {Fusion, Deep-CNN, Emotion recognition, spectrogram’s, AlexNet}
}

@article{10.1007/s11063-017-9746-8,
author = {Zuo, Xin and Shen, Jifeng and Yu, Hualong and Xu, Dan and Qian, Chengshan and Shan, Yongwei},
title = {Fast Pedestrian Detection Based on the Selective Window Differential Filter},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-017-9746-8},
doi = {10.1007/s11063-017-9746-8},
abstract = {Following the recent progress of the pixel-level filtering for pedestrian detection, we propose a window differential feature (WDF) based on the multiple channel maps. More specifically, WDF encodes first-order statistics between artitary two pixels in the whole detection window, thus obtaining larger receptive field for achor pixel than other filtering methods. Despite obtaining more discriminative information for pedestrian, WDF suffers expensive space complexity due to the high feature dimensionality. Quantitive analysis for the arbitrary pairwise elements in the WDF vector demonstrates the weak correlations existing in the proposed feature, thus motivate dimension reduction with feature selection to be the top choice. Three different dimension reduction methods for the WDF demonstrate that feature selection with mutual information achieves superior result. In addition, we find the complementary characteristics between the baseline feature and selective window differential feature, thus combining both can obtain further performance improvement. Extensive experiments on the INRIA, Caltech, ETH, and TUD-Brussel datasets consistently show superior performance of the proposed method to state-of-the-art methods with a 22 fps running speed for 640 $$times $$ 480 images.},
journal = {Neural Process. Lett.},
month = aug,
pages = {403–417},
numpages = {15},
keywords = {Feature selection, Pedestrian detection, Windows differential feature}
}

@inproceedings{10.1145/1452392.1452426,
author = {Morency, Louis-Philippe and de Kok, Iwan and Gratch, Jonathan},
title = {Context-based recognition during human interactions: automatic feature selection and encoding dictionary},
year = {2008},
isbn = {9781605581989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452392.1452426},
doi = {10.1145/1452392.1452426},
abstract = {During face-to-face conversation, people use visual feedback such as head nods to communicate relevant information and to synchronize rhythm between participants. In this paper we describe how contextual information from other participants can be used to predict visual feedback and improve recognition of head gestures in human-human interactions. For example, in a dyadic interaction, the speaker contextual cues such as gaze shifts or changes in prosody will influence listener backchannel feedback (e.g., head nod). To automatically learn how to integrate this contextual information into the listener gesture recognition framework, this paper addresses two main challenges: optimal feature representation using an encoding dictionary and automatic selection of optimal feature-encoding pairs. Multimodal integration between context and visual observations is performed using a discriminative sequential model (Latent-Dynamic Conditional Random Fields) trained on previous interactions. In our experiments involving 38 storytelling dyads, our context-based recognizer significantly improved head gesture recognition performance over a vision-only recognizer.},
booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
pages = {181–188},
numpages = {8},
keywords = {contextual information, head nod recognition, human-human interaction, visual gesture recognition},
location = {Chania, Crete, Greece},
series = {ICMI '08}
}

@article{10.1016/j.artmed.2021.102162,
author = {Naranjo, Lizbeth and P\'{e}rez, Carlos J. and Campos-Roca, Yolanda and Madruga, Mario},
title = {Replication-based regularization approaches to diagnose Reinke's edema by using voice recordings},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {120},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102162},
doi = {10.1016/j.artmed.2021.102162},
journal = {Artif. Intell. Med.},
month = oct,
numpages = {10},
keywords = {Acoustic features, Classification, Reinke's edema, Regularization, Replicated measurements, Variable selection}
}

@article{10.1007/s10462-020-09928-0,
author = {Ray, Papia and Reddy, S. Surender and Banerjee, Tuhina},
title = {Various dimension reduction techniques for high dimensional data analysis: a review},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09928-0},
doi = {10.1007/s10462-020-09928-0},
abstract = {In the era of healthcare, and its related research fields, the dimensionality problem of high dimensional data is a massive challenge as it contains a huge number of variables forming complex data matrices. The demand for dimension reduction of complex data is growing immensely to improvise data prediction, analysis and visualization. In general, dimension reduction techniques are defined as a compression of dataset from higher dimensional matrix to lower dimensional matrix. Several computational techniques have been implemented for data dimension reduction, which is further segregated into two categories such as feature extraction and feature selection. In this review, a detailed investigation of various feature extraction and feature selection methods has been carried out with a systematic comparison of several dimension reduction techniques for the analysis of high dimensional data and to overcome the problem of data loss. Then, some case studies are also cited to verify the better approach for data dimension reduction by considering few advances described in the technical literature. This review paper may guide researchers to choose the most effective method for satisfactory analysis of high dimensional data.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {3473–3515},
numpages = {43},
keywords = {Canonical correlation analysis, Feature extraction, Feature selection, Local Fisher’s discriminate analysis, Locally linear embedding, Principle component analysis}
}

@article{10.1016/j.jvcir.2018.07.006,
author = {Perrot, Romuald and Bourdon, Pascal and Helbert, David},
title = {Sampling strategies for performance improvement in cascaded face regression},
year = {2018},
issue_date = {Aug 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {55},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2018.07.006},
doi = {10.1016/j.jvcir.2018.07.006},
journal = {J. Vis. Comun. Image Represent.},
month = aug,
pages = {841–852},
numpages = {12},
keywords = {Face landmarking, Regression, Sampling, Data augmentation}
}

@article{10.1145/3386249,
author = {Srivastava, Gargi and Srivastava, Rajeev},
title = {Design, Analysis, and Implementation of Efficient Framework for Image Annotation},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1551-6857},
url = {https://doi.org/10.1145/3386249},
doi = {10.1145/3386249},
abstract = {In this article, a general framework of image annotation is proposed by involving salient object detection (SOD), feature extraction, feature selection, and multi-label classification. For SOD, Augmented-Gradient Vector Flow (A-GVF) is proposed, which fuses benefits of GVF and Minimum Directional Contrast. The article also proposes to control the background information to be included for annotation. This article brings about a comprehensive study of all major feature selection methods for a study on four publicly available datasets. The study concludes with the proposition of using Fisher’s method for reducing the dimension of features. Moreover, this article also proposes a set of features that are found to be strong discriminants by most of the methods. This reduced set for image annotation gives 3--4% better accuracy across all the four datasets. This article also proposes an improved multi-label classification algorithm C-MLFE.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {89},
numpages = {24},
keywords = {Image annotation, feature selection, scene analysis, salient object detection, multi-label classification}
}

@article{10.1145/3280986,
author = {Rhein, Alexander Von and Liebig, J\"{o}RG and Janker, Andreas and K\"{a}stner, Christian and Apel, Sven},
title = {Variability-Aware Static Analysis at Scale: An Empirical Study},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280986},
doi = {10.1145/3280986},
abstract = {The advent of variability management and generator technology enables users to derive individual system variants from a configurable code base by selecting desired configuration options. This approach gives rise to the generation of possibly billions of variants, which, however, cannot be efficiently analyzed for bugs and other properties with classic analysis techniques. To address this issue, researchers and practitioners have developed sampling heuristics and, recently, variability-aware analysis techniques. While sampling reduces the analysis effort significantly, the information obtained is necessarily incomplete, and it is unknown whether state-of-the-art sampling techniques scale to billions of variants. Variability-aware analysis techniques process the configurable code base directly, exploiting similarities among individual variants with the goal of reducing analysis effort. However, while being promising, so far, variability-aware analysis techniques have been applied mostly only to small academic examples. To learn about the mutual strengths and weaknesses of variability-aware and sample-based static-analysis techniques, we compared the two by means of seven concrete control-flow and data-flow analyses, applied to five real-world subject systems: Busybox, OpenSSL, SQLite, the x86 Linux kernel, and uClibc. In particular, we compare the efficiency (analysis execution time) of the static analyses and their effectiveness (potential bugs found). Overall, we found that variability-aware analysis outperforms most sample-based static-analysis techniques with respect to efficiency and effectiveness. For example, checking all variants of OpenSSL with a variability-aware static analysis is faster than checking even only two variants with an analysis that does not exploit similarities among variants.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {18},
numpages = {33},
keywords = {variability-aware analysis, configuration sampling, TypeChef, Highly configurable systems}
}

@article{10.1007/s11042-020-09013-2,
author = {Khurana, Anshu and Verma, Om Prakash},
title = {Novel approach with nature-inspired and ensemble techniques for optimal text classification},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {33–34},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09013-2},
doi = {10.1007/s11042-020-09013-2},
abstract = {Text classification reduces the time complexity and space complexity by dividing the complete task into the different classes. The main problem with text classification is a vast number of features extracted from the textual data. Pre-processed dataset have many features, some of which are not desirable and act only like noise. In this paper, a novel approach for optimal text classification based on nature-inspired algorithm and ensemble classifier is proposed. In the proposed model, feature selection was performed with Biogeography Based Optimization (BBO) algorithm along with ensemble classifiers (Bagging). The use of ensemble classifiers for classification delivers better performance for optimal text classification as compared to an individual classifier, and hence, improving the accuracy. Ensemble classifiers combines the weakness of individual classifiers. The individual classifiers are unable to improve the classification results when compared to ensemble classifier. The selected features, after feature selection using BBO algorithm, are classified into various classes using six machine learning classifier. The experimental results are computed on ten text classification datasets taken from UCI repository and one real-time dataset of an airlines. The four different measures namely; Accuracy, Precision, Recall and F- measure are used to validate performance of our model with ten-fold cross-validation. For feature selection process, a comparison is performed among state-of-the-art algorithms available in the literature. Results shows that BBO for feature selection outperforms the other similar nature-based optimization techniques. Our proposed approach of BBO with ensemble classifier is also compared with techniques proposed by other researchers and we analyzed the results quantitatively and qualitatively.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {23821–23848},
numpages = {28},
keywords = {Biogeography based optimization, Ensemble classifier, Machine learning classifier, Nature-based optimization, Feature selection, Text classification}
}

@article{10.1016/j.specom.2021.07.010,
author = {Li, Jialu and Hasegawa-Johnson, Mark and McElwain, Nancy L.},
title = {Analysis of acoustic and voice quality features for the classification of infant and mother vocalizations},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {133},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2021.07.010},
doi = {10.1016/j.specom.2021.07.010},
journal = {Speech Commun.},
month = oct,
pages = {41–61},
numpages = {21},
keywords = {Feature selection, Global feature, Self-attention, Convolutional neural networks, Infant-directed speech, Infant vocalizations, Emotion classifier}
}

@article{10.1007/s10664-018-9679-5,
author = {Kondo, Masanari and Bezemer, Cor-Paul and Kamei, Yasutaka and Hassan, Ahmed E. and Mizuno, Osamu},
title = {The impact of feature reduction techniques on defect prediction models},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9679-5},
doi = {10.1007/s10664-018-9679-5},
abstract = {Defect prediction is an important task for preserving software quality. Most prior work on defect prediction uses software features, such as the number of lines of code, to predict whether a file or commit will be defective in the future. There are several reasons to keep the number of features that are used in a defect prediction model small. For example, using a small number of features avoids the problem of multicollinearity and the so-called `curse of dimensionality'. Feature selection and reduction techniques can help to reduce the number of features in a model. Feature selection techniques reduce the number of features in a model by selecting the most important ones, while feature reduction techniques reduce the number of features by creating new, combined features from the original features. Several recent studies have investigated the impact of feature selection techniques on defect prediction. However, there do not exist large-scale studies in which the impact of multiple feature reduction techniques on defect prediction is investigated. In this paper, we study the impact of eight feature reduction techniques on the performance and the variance in performance of five supervised learning and five unsupervised defect prediction models. In addition, we compare the impact of the studied feature reduction techniques with the impact of the two best-performing feature selection techniques (according to prior work). The following findings are the highlights of our study: (1) The studied correlation and consistency-based feature selection techniques result in the best-performing supervised defect prediction models, while feature reduction techniques using neural network-based techniques (restricted Boltzmann machine and autoencoder) result in the best-performing unsupervised defect prediction models. In both cases, the defect prediction models that use the selected/generated features perform better than those that use the original features (in terms of AUC and performance variance). (2) Neural network-based feature reduction techniques generate features that have a small variance across both supervised and unsupervised defect prediction models. Hence, we recommend that practitioners who do not wish to choose a best-performing defect prediction model for their data use a neural network-based feature reduction technique.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1925–1963},
numpages = {39},
keywords = {Restricted Boltzmann machine, Neural network, Feature selection, Feature reduction, Defect prediction}
}

@inproceedings{10.1145/2814204.2814206,
author = {Medeiros, Fl\'{a}vio and Rodrigues, Iran and Ribeiro, M\'{a}rcio and Teixeira, Leopoldo and Gheyi, Rohit},
title = {An empirical study on configuration-related issues: investigating undeclared and unused identifiers},
year = {2015},
isbn = {9781450336871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814204.2814206},
doi = {10.1145/2814204.2814206},
abstract = {The variability of configurable systems may lead to configuration-related issues (i.e., faults and warnings) that appear only when we select certain configuration options. Previous studies found that issues related to configurability are harder to detect than issues that appear in all configurations, because variability increases the complexity. However, little effort has been put into understanding configuration-related faults (e.g., undeclared functions and variables) and warnings (e.g., unused functions and variables). To better understand the peculiarities of configuration-related undeclared/unused variables and functions, in this paper we perform an empirical study of 15 systems to answer research questions related to how developers introduce these issues, the number of configuration options involved, and the time that these issues remain in source files. To make the analysis of several projects feasible, we propose a strategy that minimizes the initial setup problems of variability-aware tools. We detect and confirm 2 undeclared variables, 14 undeclared functions, 16 unused variables, and 7 unused functions related to configurability. We submit 30 patches to fix issues not fixed by developers. Our findings support the effectiveness of sampling (i.e., analysis of only a subset of valid configurations) because most issues involve two or less configuration options. Nevertheless, by analyzing the version history of the projects, we observe that a number of issues remain in the code for several years. Furthermore, the corpus of undeclared/unused variables and functions gathered is a valuable source to study these issues, compare sampling algorithms, and test and improve variability-aware tools.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {35–44},
numpages = {10},
keywords = {Faults and Warnings, Configurable Systems},
location = {Pittsburgh, PA, USA},
series = {GPCE 2015}
}

@inproceedings{10.1007/978-3-030-66843-3_27,
author = {Park, Yae Won and Park, Ji Eun and Ahn, Sung Soo and Kim, Hwiyoung and Kim, Ho Sung and Lee, Seung-Koo},
title = {Differentiation of Recurrent Glioblastoma from Radiation Necrosis Using Diffusion Radiomics: Machine Learning Model Development and External Validation},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_27},
doi = {10.1007/978-3-030-66843-3_27},
abstract = {We aimed to establish a high-performing radiomics strategy to differentiate recurrent glioblastoma (GBM) from radiation necrosis (RN). Eighty-six patients with posttreatment GBM were enrolled in the training set (63 recurrent GBM and 23 RN). Another 41 patients (23 recurrent GBM and 18 RN) from a different institution were enrolled in the test set. Conventional MRI sequences (T2-weighted and postcontrast T1-weighted images) and ADC were analyzed to extract 263 radiomic features. After feature selection, various machine learning models were trained with combinations of MRI sequences and validated in the test set. In the independent test set, the model using ADC sequence showed the best diagnostic performance, with an AUC, accuracy, sensitivity, specificity of 0.800, 78%, 66.7%, and 87%, respectively. The radiomics models using other MRI sequences showed AUCs ranging from 0.650 to 0.662 in the test set. In conclusion, the diffusion radiomics may be helpful in differentiating recurrent GBM from RN.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {276–283},
numpages = {8},
keywords = {Radiomics, Magnetic resonance imaging, Machine learning, Glioblastoma},
location = {Lima, Peru}
}

@article{10.5555/2842045.2842371,
author = {Feng, Dingcheng and Chen, Feng and Xu, Wenli},
title = {Supervised feature subset selection with ordinal optimization},
year = {2014},
issue_date = {January 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {56},
number = {C},
issn = {0950-7051},
abstract = {The ultimate goal of supervised feature selection is to select a feature subset such that the prediction accuracy is maximized. Most of the existing methods, such as filter and embedded models, can be viewed as using approximate objectives which are different from the prediction accuracy. The wrapper models maximize the prediction accuracy directly, but the optimization has very high computational complexity. To address the limitations, we present an ordinal optimization perspective for feature selection (OOFS). Feature subset evaluation is formulated as a system simulation process with randomness. Supervised feature selection becomes maximizing the expected performance of the system, where ordinal optimization can be applied to identify a set of order-good-enough solutions with much reduced complexity and parallel computing. These solutions correspond to the really good enough (value-good-enough) solutions when the solution space structure, characterized by ordered performance curve (OPC), exhibits concave shapes. We analyze that this happens in some important applications such as image classification, where a large number features have relatively similar abilities in discrimination. We further improve the OOFS method with a feature scoring algorithm, called OOFSs. We prove that, when the performance difference of solutions increases monotonically with respect to the solution difference, the expectation of the scores reflects useful information for estimating the globally optimal solution. Experimental results in sixteen real-world datasets show that our method provides a good trade-off between prediction accuracy and computational complexity.},
journal = {Know.-Based Syst.},
month = jan,
pages = {123–140},
numpages = {18},
keywords = {Value-good enough, Uniform sampling, Supervised feature selection, Ordinal optimization, Ordered performance curve, Order-good-enough, Feature scoring}
}

@article{10.3233/IDA-205331,
author = {Khan, Ahmad Jaffar and Raza, Basit and Shahid, Ahmad Raza and Kumar, Yogan Jaya and Faheem, Muhammad and Alquhayz, Hani},
title = {Handling incomplete data classification using imputed feature selected bagging (IFBag) method},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {4},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-205331},
doi = {10.3233/IDA-205331},
abstract = {Almost all real-world datasets contain missing values. Classification of data with missing values can adversely affect the performance of a classifier if not handled correctly. A common approach used for classification with incomplete data is imputation. Imputation transforms incomplete data with missing values to complete data. Single imputation methods are mostly less accurate than multiple imputation methods which are often computationally much more expensive. This study proposes an imputed feature selected bagging (IFBag) method which uses multiple imputation, feature selection and bagging ensemble learning approach to construct a number of base classifiers to classify new incomplete instances without any need for imputation in testing phase. In bagging ensemble learning approach, data is resampled multiple times with substitution, which can lead to diversity in data thus resulting in more accurate classifiers. The experimental results show the proposed IFBag method is considerably fast and gives 97.26% accuracy for classification with incomplete data as compared to common methods used.},
journal = {Intell. Data Anal.},
month = jan,
pages = {825–846},
numpages = {22},
keywords = {ensemble learning, feature selection, data classification, machine learning, Incomplete data}
}

@article{10.1016/j.asoc.2019.105538,
author = {Gangavarapu, Tushaar and Patil, Nagamma},
title = {A novel filter–wrapper hybrid greedy ensemble approach optimized using the genetic algorithm to reduce the dimensionality of high-dimensional biomedical datasets},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105538},
doi = {10.1016/j.asoc.2019.105538},
journal = {Appl. Soft Comput.},
month = aug,
numpages = {15},
keywords = {Parameter optimization, Hybrid feature selection, High-dimensional data, Greedy ensemble, Genetic algorithm, Biomedical data}
}

@article{10.1016/j.asoc.2021.107938,
author = {Zapf, Fabian and Wallek, Thomas},
title = {Comparison of data selection methods for modeling chemical processes with artificial neural networks},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107938},
doi = {10.1016/j.asoc.2021.107938},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {16},
keywords = {Wolfram Mathematica, Chemical processes, Regression, Artificial neural networks, Instance selection, Subset selection, Data selection}
}

@article{10.1007/s00530-020-00709-x,
author = {Liu, Zhe and Han, Kai and Wang, Zhaohui and Zhang, Jing and Song, Yuqing and Yao, Xu and Yuan, Deqi and Sheng, Victor S.},
title = {Automatic liver segmentation from abdominal CT volumes using improved convolution neural networks},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {1},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-020-00709-x},
doi = {10.1007/s00530-020-00709-x},
abstract = {Segmentation of the liver from abdominal CT images is an essential step for computer-aided diagnosis and surgery planning. The U-Net architecture is one of the most well-known CNN architectures which achieved remarkable successes in both medical and biological image segmentation domain. However, it does not perform well when the target area is small or partitioned. In this paper, we propose a novel architecture, called dense feature selection U-Net (DFS U-Net), which addresses this challenging problem. Specifically, The Hounsfield unit values were windowed in a range to exclude irrelevant organs, and then use the pre-processed data to train our proposed DFS U-Net model. To further improve the segmentation accuracy of the small region and disconnected regions of interests with limited training datasets, we improve the loss function by adding a parameter to the formula. With respect to the ground truth, the Dice score ratio can reach over 94.9% for the liver. Our experimental results demonstrate its potential in clinical usage with high effectiveness, robustness and efficiency.},
journal = {Multimedia Syst.},
month = feb,
pages = {111–124},
numpages = {14},
keywords = {Threshold deconvolution, Feature selection, DFS U-Net, Automatic liver segmentation}
}

@article{10.1007/s10618-021-00750-y,
author = {Pang, Guansong and Cao, Longbing and Chen, Ling},
title = {Homophily outlier detection in non-IID categorical data},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-021-00750-y},
doi = {10.1007/s10618-021-00750-y},
abstract = {Most of existing outlier detection methods assume that the outlier factors (i.e., outlierness scoring measures) of data entities (e.g., feature values and data objects) are Independent and Identically Distributed (IID). This assumption does not hold in real-world applications where the outlierness of different entities is dependent on each other and/or taken from different probability distributions (non-IID). This may lead to the failure of detecting important outliers that are too subtle to be identified without considering the non-IID nature. The issue is even intensified in more challenging contexts, e.g., high-dimensional data with many noisy features. This work introduces a novel outlier detection framework and its two instances to identify outliers in categorical data by capturing non-IID outlier factors. Our approach first defines and incorporates distribution-sensitive outlier factors and their interdependence into a value-value graph-based representation. It then models an outlierness propagation process in the value graph to learn the outlierness of feature values. The learned value outlierness allows for either direct outlier detection or outlying feature selection. The graph representation and mining approach is employed here to well capture the rich non-IID characteristics. Our empirical results on 15 real-world data sets with different levels of data complexities show that (i) the proposed outlier detection methods significantly outperform five state-of-the-art methods at the 95%/99% confidence level, achieving 10–28% AUC improvement on the 10 most complex data sets; and (ii) the proposed feature selection methods significantly outperform three competing methods in enabling subsequent outlier detection of two different existing detectors.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1163–1224},
numpages = {62},
keywords = {Coupling learning, Random walk, Homophily relation, Categorical data, Non-IID learning, Feature selection, Outlier detection}
}

@article{10.1016/j.procs.2020.03.213,
author = {Verma, Chaman and Stoffov\'{a}, Veronika and Ill\'{e}s, Zolt\'{a}n},
title = {Prediction of residence country of student towards information, communication and mobile technology for real-time: preliminary results},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.213},
doi = {10.1016/j.procs.2020.03.213},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {224–234},
numpages = {11},
keywords = {Binary Classification, Feature Selection, Machine Learning, Real-Time, Prediction of Residence Country}
}

@article{10.1016/j.ins.2019.02.039,
author = {Ordozgoiti, Bruno and Mozo, Alberto and L\'{o}pez de Lacalle, Jes\'{u}s Garc\'{\i}a},
title = {Regularized greedy column subset selection},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {486},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.039},
doi = {10.1016/j.ins.2019.02.039},
journal = {Inf. Sci.},
month = jun,
pages = {393–418},
numpages = {26},
keywords = {Unsupervised learning, Column subset selection, Feature selection}
}

@inproceedings{10.1007/978-3-030-59277-6_33,
author = {Bulbul, Abdullah Al-Mamun and Abdul Awal, Md. and Debjit, Kumar},
title = {EEG Based Sleep-Wake Classification Using JOPS Algorithm},
year = {2020},
isbn = {978-3-030-59276-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59277-6_33},
doi = {10.1007/978-3-030-59277-6_33},
abstract = {Classification of sleep-wake is necessary for the diagnosis and treatment of sleep disorders, and EEG is normally used to assess sleep quality. Manual scoring is time-consuming and requires a sleep expert. Therefore, automatic sleep classification is essential. To accomplish this, features are extracted from the time domain, frequency domain, wavelet domain, and also from non-linear dynamics. In this study, a novel Jaya Optimization based hyper-Parameter and feature Selection (JOPS) algorithm is proposed to select optimal feature subset as well as hyper-parameters of the classifier such as KNN and SVM, simultaneously. JOPS is self-adaptive that automatically adapts to the population size. The proposed JPOS yielded the accuracy of 94.99% and 94.85% using KNN and SVM, respectively. JPOS algorithm is compared with genetic algorithm and differential evaluation-based feature selection algorithm. Finally, a decision support system is created to graphically visualize the sleep-wake state which will be beneficial to clinical staffs. Furthermore, the proposed JOPS can not only be used in sleep-wake classification but could be applied in other classification problems.},
booktitle = {Brain Informatics: 13th International Conference, BI 2020, Padua, Italy, September 19, 2020, Proceedings},
pages = {361–371},
numpages = {11},
keywords = {Decision Support System, Classification, JOPS algorithm, Feature Selection, Hyper-parameter tuning, Sleep-Wake cycle},
location = {Padua, Italy}
}

@inproceedings{10.1145/3442520.3442529,
author = {Barut, Onur and Zhu, Rebecca and Luo, Yan and Zhang, Tong},
title = {TLS Encrypted Application Classification Using Machine Learning with Flow Feature Engineering},
year = {2021},
isbn = {9781450389037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442520.3442529},
doi = {10.1145/3442520.3442529},
abstract = {Network traffic classification has become increasingly important as the number of devices connected to the Internet is rapidly growing. Proportionally, the amount of encrypted traffic is also increasing, making payload based classification methods obsolete. Consequently, machine learning approaches have become crucial when user privacy is concerned. For this purpose, we propose an accurate, fast, and privacy preserved encrypted traffic classification approach with engineered flow feature extraction and appropriate feature selection. The proposed scheme achieves a 0.92899 macro-average F1 score and a 0.88313 macro-averaged mAP score for the encrypted traffic classification of Audio, Email, Chat, and Video classes derived from the non-vpn2016 dataset. Further experiments on the mixed non-encrypted and encrypted flow dataset with a data augmentation method called Synthetic Minority Over-Sampling Technique are conducted and the results are discussed for TLS-encrypted and mixed flows.},
booktitle = {Proceedings of the 2020 10th International Conference on Communication and Network Security},
pages = {32–41},
numpages = {10},
keywords = {machine learning, flow feature extraction, feature selection, encrypted traffic analysis, deep learning},
location = {Tokyo, Japan},
series = {ICCNS '20}
}

@article{10.1504/ijkedm.2019.105265,
author = {Sumathi, C.P. and Padmavathi, M.S.},
title = {An experimental approach of applying boruta and elastic net for variable selection in classifying breast cancer datasets},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {4},
issn = {1755-2087},
url = {https://doi.org/10.1504/ijkedm.2019.105265},
doi = {10.1504/ijkedm.2019.105265},
abstract = {Feature selection identifies the key aspects involved in predicting the outcome. In this study, we propose boruta and elastic net (Enet) feature selection for classifying breast cancer datasets. A comparative study of boruta, Enet along with genetic algorithms (GA) and consistency-based subset feature selections are done, where Enet selected best features for Wisconsin diagnostic breast cancer (WDBC) and breast cancer datasets. To prove the stability of Enet feature selection, variable importance of machine learning algorithms like naive Bayes (NB), multilayer perceptron (MLP) and random forest (RF) is evaluated and compared. It is proved that the features obtained by Enet contain all the common variables selected by tested machine learning algorithms. The proposed Enet feature selection along with MLP for classification yields a better receiver operating characteristic (ROC): 0.990, 0.687 and a reduced root mean squared error (RMSE): 0.159, 0.429 for WDBC and breast cancer datasets, when compared with naive Bayes and RF.},
journal = {Int. J. Knowl. Eng. Data Min.},
month = jan,
pages = {356–375},
numpages = {19},
keywords = {random forest, variable importance, naive Bayes, genetic algorithm, MLP, multilayer perceptron, elastic net, consistency-based, breast cancer, boruta}
}

@inproceedings{10.1007/978-3-030-45371-8_4,
author = {Reis, Bruno and Maia, Eva and Pra\c{c}a, Isabel},
title = {Selection and Performance Analysis of CICIDS2017 Features Importance},
year = {2019},
isbn = {978-3-030-45370-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45371-8_4},
doi = {10.1007/978-3-030-45371-8_4},
abstract = {During the last decade network infrastructures have been in a constant evolution. And, at the same time, attacks and attack vectors become increasingly sophisticated. Hence, networks contain a lot of different features that can be used to identify attacks. Machine learning are particularly useful at dealing with large and varied datasets, which are crucial to develop an accurate intrusion detection system. Thus, the huge challenge that intrusion detection represents can be supported by machine learning techniques. In this work, several feature selection and ensemble methods are applied to the recent CICIDS2017 dataset in order to develop valid models to detect intrusions as soon as they occur. Using permutation importance the original 69 features in the dataset have been reduced to only 10 features, which allows the reduction of models execution time, and leads to faster intrusion detection systems. The reduced dataset was evaluated using Random Forest algorithm, and the obtained results show that the optimized dataset maintains a high detection rate performance.},
booktitle = {Foundations and Practice of Security: 12th International Symposium, FPS 2019, Toulouse, France, November 5–7, 2019, Revised Selected Papers},
pages = {56–71},
numpages = {16},
keywords = {CICIDS2017, Feature selection, Decision trees, Random Forest, Ensemble learning, Intrusion Detection System (IDS), Machine learning},
location = {Toulouse, France}
}

@inproceedings{10.1145/3474124.3474136,
author = {Sardana, Neetu and Jadhav Bhatt, Arpita},
title = {Detection of iOS Malware apps based on Significant Services Identification using Borda count},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474136},
doi = {10.1145/3474124.3474136},
abstract = {In today's era, smartphones are used in daily lives because they are ubiquitous and provide internet connectivity everywhere. The primary reason for the increased usage of smartphones is their functional expandability by installing third-party apps, which span a wide range of categories including books, social networking, instant messaging, etc. Users are compelled to use these feature-rich apps. As a result, the menaces because of these apps, which are potentially risky for user's privacy, have increased. As the information on smartphones is perhaps, more personal than compared to data stored on desktops or computers because smartphones remain with individuals throughout the day and generate contextual data through sensors making it an easy target for intruders. Both Android and iOS follow a permission-based access control mechanism to protect the privacy of their users where an app has to specify the permissions it will use during its run-time. However, the users are unaware whether the app is breaching the user's privacy or sharing it with third-party apps. A lot of work for detecting malicious Android apps using feature selection techniques has been conducted because of the availability of a large permission set and labeled data set. However, minimal work has been conducted for the iOS platform because of the limited permission set, limited labeled data set, and closed-source platform. To combat this problem, in the paper we propose an approach to detect malicious iOS apps based on the app's category using static analysis of app permissions to identify the most significant permission. In this work, several feature ranking techniques such as Correlation, Gain ratio, Info gain, OneR, and ReliefF have been employed on a data set of 1150 iOS apps to identify the riskiest permission across 12 different app categories. To improve the permission selection process and improve the precision of classifiers, the Borda count method has been utilized. Our empirical analysis proves that the proposed approach effectively identifies the top 5 risky permissions within different categories.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {86–91},
numpages = {6},
keywords = {static analysis, malware detection, iOS apps, feature selection, borda count},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1007/978-3-030-72699-7_38,
author = {Jankovic, Anja and Eftimov, Tome and Doerr, Carola},
title = {Towards Feature-Based Performance Regression Using Trajectory Data},
year = {2021},
isbn = {978-3-030-72698-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72699-7_38},
doi = {10.1007/978-3-030-72699-7_38},
abstract = {Black-box optimization is a very active area of research, with many new algorithms being developed every year. This variety is needed, on the one hand, since different algorithms are most suitable for different types of optimization problems. But the variety also poses a meta-problem: which algorithm to choose for a given problem at hand? Past research has shown that per-instance algorithm selection based on exploratory landscape analysis (ELA) can be an efficient mean to tackle this meta-problem. Existing approaches, however, require the approximation of problem features based on a significant number of samples, which are typically selected through uniform sampling or Latin Hypercube Designs. The evaluation of these points is costly, and the benefit of an ELA-based algorithm selection over a default algorithm must therefore be significant in order to pay off. One could hope to by-pass the evaluations for the feature approximations by using the samples that a default algorithm would anyway perform, i.e., by using the points of the default algorithm’s trajectory. We analyze in this paper how well such an approach can work. Concretely, we test how accurately trajectory-based ELA approaches can predict the final solution quality of the CMA-ES after a fixed budget of function evaluations. We observe that the loss of trajectory-based predictions can be surprisingly small compared to the classical global sampling approach, if the remaining budget for which solution quality shall be predicted is not too large. Feature selection, in contrast, did not show any advantage in our experiments and rather led to worsened prediction accuracy. The inclusion of state variables of CMA-ES only has a moderate effect on the prediction accuracy.},
booktitle = {Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {601–617},
numpages = {17},
keywords = {Feature selection, Performance regression, Black-box optimization, Automated algorithm selection, Exploratory landscape analysis}
}

@article{10.3233/JIFS-201124,
author = {Zhu, Wenhua and Peng, Hu and Leng, Chaohui and Deng, Changshou and Wu, Zhijian},
title = {Surrogate-assisted firefly algorithm for breast cancer detection},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-201124},
doi = {10.3233/JIFS-201124},
abstract = {Breast cancer is a severe disease for women health, however, with expensive diagnostic cost or obsolete medical technique, many patients are hard to obtain prompt medical treatment. Thus, efficient detection result of breast cancer while lower medical cost may be a promising way to protect women health. Breast cancer detection using all features will take a lot of time and computational resources. Thus, in this paper, we proposed a novel framework with surrogate-assisted firefly algorithm (FA) for breast cancer detection (SFA-BCD). As an advanced evolutionary algorithm (EA), FA is adopted to make feature selection, and the machine learning as classifier identify the breast cancer. Moreover, the surrogate model is utilized to decrease computation cost and expensive computation, which is the approximation function built by offline data to the real object function. The comprehensive experiments have been conducted under several breast cancer dataset derived from UCI. Experimental results verified that the proposed framework with surrogate-assisted FA significantly reduced the computation cost.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {8915–8926},
numpages = {12},
keywords = {feature selection, surrogate model, machine learning, firefly algorithm, Breast cancer detection}
}

@article{10.1016/j.compbiomed.2019.103569,
author = {Hsu, Kai-Cheng and Lin, Ching-Heng and Johnson, Kory R. and Liu, Chi-Hung and Chang, Ting-Yu and Huang, Kuo-Lun and Fann, Yang-Cheng and Lee, Tsong-Hai},
title = {Autodetect extracranial and intracranial artery stenosis by machine learning using ultrasound},
year = {2020},
issue_date = {Jan 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.103569},
doi = {10.1016/j.compbiomed.2019.103569},
journal = {Comput. Biol. Med.},
month = jan,
numpages = {9},
keywords = {Intracranial artery stenosis, Machine learning, Angiography, Carotid ultrasound}
}

@article{10.1007/s10664-015-9360-1,
author = {Hunsen, Claus and Zhang, Bo and Siegmund, Janet and K\"{a}stner, Christian and Leβenich, Olaf and Becker, Martin and Apel, Sven},
title = {Preprocessor-based variability in open-source and industrial software systems: An empirical study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9360-1},
doi = {10.1007/s10664-015-9360-1},
abstract = {Almost every sufficiently complex software system today is configurable. Conditional compilation is a simple variability-implementation mechanism that is widely used in open-source projects and industry. Especially, the C preprocessor (CPP) is very popular in practice, but it is also gaining (again) interest in academia. Although there have been several attempts to understand and improve CPP, there is a lack of understanding of how it is used in open-source and industrial systems and whether different usage patterns have emerged. The background is that much research on configurable systems and product lines concentrates on open-source systems, simply because they are available for study in the first place. This leads to the potentially problematic situation that it is unclear whether the results obtained from these studies are transferable to industrial systems. We aim at lowering this gap by comparing the use of CPP in open-source projects and industry--especially from the embedded-systems domain--based on a substantial set of subject systems and well-known variability metrics, including size, scattering, and tangling metrics. A key result of our empirical study is that, regarding almost all aspects we studied, the analyzed open-source systems and the considered embedded systems from industry are similar regarding most metrics, including systems that have been developed in industry and made open source at some point. So, our study indicates that, regarding CPP as variability-implementation mechanism, insights, methods, and tools developed based on studies of open-source systems are transferable to industrial systems--at least, with respect to the metrics we considered.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {449–482},
numpages = {34},
keywords = {cppstats, Variability, Software product lines, Open-source systems, Industrial systems, Configurable systems, C preprocessor}
}

@article{10.1016/j.compbiomed.2011.03.001,
author = {\"{O}z\c{c}ift, Akin},
title = {Random forests ensemble classifier trained with data resampling strategy to improve cardiac arrhythmia diagnosis},
year = {2011},
issue_date = {May, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {5},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2011.03.001},
doi = {10.1016/j.compbiomed.2011.03.001},
abstract = {Supervised classification algorithms are commonly used in the designing of computer-aided diagnosis systems. In this study, we present a resampling strategy based Random Forests (RF) ensemble classifier to improve diagnosis of cardiac arrhythmia. Random forests is an ensemble classifier that consists of many decision trees and outputs the class that is the mode of the class's output by individual trees. In this way, an RF ensemble classifier performs better than a single tree from classification performance point of view. In general, multiclass datasets having unbalanced distribution of sample sizes are difficult to analyze in terms of class discrimination. Cardiac arrhythmia is such a dataset that has multiple classes with small sample sizes and it is therefore adequate to test our resampling based training strategy. The dataset contains 452 samples in fourteen types of arrhythmias and eleven of these classes have sample sizes less than 15. Our diagnosis strategy consists of two parts: (i) a correlation based feature selection algorithm is used to select relevant features from cardiac arrhythmia dataset. (ii) RF machine learning algorithm is used to evaluate the performance of selected features with and without simple random sampling to evaluate the efficiency of proposed training strategy. The resultant accuracy of the classifier is found to be 90.0% and this is a quite high diagnosis performance for cardiac arrhythmia. Furthermore, three case studies, i.e., thyroid, cardiotocography and audiology, are used to benchmark the effectiveness of the proposed method. The results of experiments demonstrated the efficiency of random sampling strategy in training RF ensemble classification algorithm.},
journal = {Comput. Biol. Med.},
month = may,
pages = {265–271},
numpages = {7},
keywords = {Simple random sampling, Random forests ensemble classification, Filter feature selection, Correlation based feature selection, Cardiac arrhythmia, Backward elimination}
}

@article{10.1016/j.neucom.2015.10.056,
author = {Ayech, Mohamed Walid and Ziou, Djemel},
title = {Terahertz image segmentation using k-means clustering based on weighted feature learning and random pixel sampling},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {175},
number = {PA},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.10.056},
doi = {10.1016/j.neucom.2015.10.056},
abstract = {Terahertz (THz) imaging is an innovative technology of imaging which can supply a large amount of data unavailable through other sensors. However, the higher dimension of THz images can be a hurdle to their display, their analysis and their interpretation. In this study, we propose a weighted feature space and a simple random sampling in k-means clustering for THz image segmentation. Our approach consists in estimating the expected centers, selecting the relevant features and their scores, and classifying the observed pixels of THz images. Automatic estimation of the random sample size and the selected feature number are also proposed in this paper. Our approach is more appropriate for achieving the best compactness inside clusters, the best discrimination of features, and the best tradeoff between the clustering accuracy and the low computational cost. Our approach of segmentation is evaluated by measuring performances and appraised by a comparison with some related works. HighlightsA novel approach of segmentation of Terahertz images is proposed.We propose a weighted feature space and a simple random sampling in k-means clustering.The goal of the approach is to estimate the expected centers, select the relevant features and their scores, and classify the observed pixels.We propose to estimate automatically the random sample size and the selected feature number.Our approach is more appropriate for achieving the best compactness inside clusters, the best discrimination of features, and the best tradeoff between the clustering accuracy and the low computational cost.},
journal = {Neurocomput.},
month = jan,
pages = {243–264},
numpages = {22},
keywords = {k-means, Terahertz imaging, Simple random sampling, Segmentation, Feature weighting, Dispersion}
}

@article{10.1007/s10044-017-0602-2,
author = {Lu, Xiao-Yong and Chen, Mu-Sheng and Wu, Jheng-Long and Chang, Pei-Chan and Chen, Meng-Hui},
title = {A novel ensemble decision tree based on under-sampling and clonal selection for web spam detection},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-017-0602-2},
doi = {10.1007/s10044-017-0602-2},
abstract = {Currently, web spamming is a serious problem for search engines. It not only degrades the quality of search results by intentionally boosting undesirable web pages to users, but also causes the search engine to waste a significant amount of computational and storage resources in manipulating useless information. In this paper, we present a novel ensemble classifier for web spam detection which combines the clonal selection algorithm for feature selection and under-sampling for data balancing. This web spam detection system is called USCS. The USCS ensemble classifiers can automatically sample and select sub-classifiers. First, the system will convert the imbalanced training dataset into several balanced datasets using the under-sampling method. Second, the system will automatically select several optimal feature subsets for each sub-classifier using a customized clonal selection algorithm. Third, the system will build several C4.5 decision tree sub-classifiers from these balanced datasets based on its specified features. Finally, these sub-classifiers will be used to construct an ensemble decision tree classifier which will be applied to classify the examples in the testing data. Experiments on WEBSPAM-UK2006 dataset on the web spam problem show that our proposed approach, the USCS ensemble web spam classifier, contributes significant classification performance compared to several baseline systems and state-of-the-art approaches.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {741–754},
numpages = {14},
keywords = {Web spam detection, Feature selection, Ensemble learning, Decision trees, Clonal selection algorithm}
}

@article{10.1007/s10472-020-09705-3,
author = {Frid, Alex and Manevitz, Larry M. and Nawa, Norberto Eiji},
title = {Classifying the valence of autobiographical memories from fMRI data},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {88},
number = {11–12},
issn = {1012-2443},
url = {https://doi.org/10.1007/s10472-020-09705-3},
doi = {10.1007/s10472-020-09705-3},
abstract = {We show that fMRI analysis using machine learning tools are sufficient to distinguish valence (i.e., positive or negative) of freely retrieved autobiographical memories in a cross-participant setting. Our methodology uses feature selection (ReliefF) in combination with boosting methods, both applied directly to data represented in voxel space. In previous work using the same data set, Nawa and Ando showed that whole-brain based classification could achieve above-chance classification accuracy only when both training and testing data came from the same individual. In a cross-participant setting, classification results were not statistically significant. Additionally, on average the classification accuracy obtained when using ReliefF is substantially higher than previous results - 81% for the within-participant classification, and 62% for the cross-participant classification. Furthermore, since features are defined in voxel space, it is possible to show brain maps indicating the regions of that are most relevant in determining the results of the classification. Interestingly, the voxels that were selected using the proposed computational pipeline seem to be consistent with current neurophysiological theories regarding the brain regions actively involved in autobiographical memory processes.},
journal = {Annals of Mathematics and Artificial Intelligence},
month = dec,
pages = {1261–1274},
numpages = {14},
keywords = {Feature selection, Machine learning, Classification, Autobiographical memories, Analysis of cognitive processes}
}

@inproceedings{10.1145/3340531.3412037,
author = {Chu, Zhixuan and Rathbun, Stephen L. and Li, Sheng},
title = {Matching in Selective and Balanced Representation Space for Treatment Effects Estimation},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412037},
doi = {10.1145/3340531.3412037},
abstract = {The dramatically growing availability of observational data is being witnessed in various domains of science and technology, which facilitates the study of causal inference. However, estimating treatment effects from observational data is faced with two major challenges, missing counterfactual outcomes and treatment selection bias. Matching methods are among the mostly widely used and fundamental approaches to estimating treatment effects, but existing matching methods have poor performance when facing data with high dimensional and complicated variables. We propose a feature selection representation matching (FSRM) method based on deep representation learning and matching, which maps the original covariate space into a selective, nonlinear, and balanced representation space, and then conducts matching in the learned representation space. FSRM adopts deep feature selection to minimize the influence of irrelevant variables for estimating treatment effects and incorporates a regularizer based on the Wasserstein distance to learn balanced representations. We evaluate the performance of our FSRM method on three datasets, and the results demonstrate superiority over the state-of-the-art methods.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {205–214},
numpages = {10},
keywords = {treatment effects estimation, representation learning, observational data, deep feature selection, causal inference},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1007/s007780050061,
author = {Chakrabarti, Soumen and Dom, Byron and Agrawal, Rakesh and Raghavan, Prabhakar},
title = {Scalable feature selection, classification and signature generation for organizing large text databases into hierarchical topic taxonomies},
year = {1998},
issue_date = {August 1998},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1066-8888},
url = {https://doi.org/10.1007/s007780050061},
doi = {10.1007/s007780050061},
abstract = {We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand. We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy. To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or discriminants, from thenoise words at each node of the taxonomy. Using these, we build a multilevel classifier. At each node, this classifier can ignore the large number of “noise” words in a document. Thus, the classifier has a small model size and is very fast. Owing to the use of context-sensitive features, the classifier is very accurate. As a by-product, we can compute for each document a set of terms that occur significantly more often in it than in the classes to which it belongs. We describe the design and implementation of our system, stressing how to exploit standard, efficient relational operations like sorts and joins. We report on experiences with the Reuters newswire benchmark, the US patent database, and web document samples from Yahoo!. We discuss applications where our system can improve searching and filtering capabilities.},
journal = {The VLDB Journal},
month = aug,
pages = {163–178},
numpages = {16}
}

@inproceedings{10.1145/3168776.3168787,
author = {Khan, Fazlullah and Akbar, Shahid and Basit, Abdul and Khan, Inamullah and Akhlaq, Hamza},
title = {Identification of Anticancer Peptides Using Optimal Feature Space of Chou's Split Amino Acid Composition and Support Vector Machine},
year = {2017},
isbn = {9781450354844},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168776.3168787},
doi = {10.1145/3168776.3168787},
abstract = {Cancer is a serious disease and occurs the cause of death around the world. Various traditional methods i.e. targeted therapy chemotherapy and radiation based therapies have been extensively used by the investigators but still it is considered ineffective due to its high cost, side effects and Vulnerability towards finding errors. Therefore; an automatic and efficient model highly desirable to identify anticancer peptides. In this paper, the peptides sequences are formulated using three numerical descriptors namely; Split amino acid composition, dipeptide composition and Pseudo amino acid composition. The predicted outcomes of the proposed method is evaluated using two different nature classification learners, i.e., instance based k-nearest neighbor and Support vector machine. Our proposed model achieved the an accuracy of 93.31% sensitivity of 86.23% and specificity of 98.06% and MCC of 0.86, the success rate shows the remarkable improvement in performance matrices in comparison with existing techniques in the literature. It is observed that our proposed method will be useful for the investigators in the area of drugs design and proteomics.},
booktitle = {Proceedings of the 2017 4th International Conference on Biomedical and Bioinformatics Engineering},
pages = {91–96},
numpages = {6},
keywords = {feature selection, SVM, SAAC, PseAAC, KNN, DPC, Anticancer peptides},
location = {Seoul, Republic of Korea},
series = {ICBBE '17}
}

@article{10.1016/j.ins.2019.10.045,
author = {Vyas, Krishna and Frasincar, Flavius},
title = {Determining the most representative image on a Web page},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {512},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.10.045},
doi = {10.1016/j.ins.2019.10.045},
journal = {Inf. Sci.},
month = feb,
pages = {1234–1248},
numpages = {15},
keywords = {Feature selection, Support vector machines, Representative image, Image search}
}

@article{10.1007/s00521-018-3439-8,
author = {Oliveira, Roberta B. and Pereira, Aledir S. and Tavares, Jo\~{a}o Manuel R. S.},
title = {Computational diagnosis of skin lesions from dermoscopic images using combined features},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3439-8},
doi = {10.1007/s00521-018-3439-8},
abstract = {There has been an alarming increase in the number of skin cancer cases worldwide in recent years, which has raised interest in computational systems for automatic diagnosis to assist early diagnosis and prevention. Feature extraction to describe skin lesions is a challenging research area due to the difficulty in selecting meaningful features. The main objective of this work is to find the best combination of features, based on shape properties, colour variation and texture analysis, to be extracted using various feature extraction methods. Several colour spaces are used for the extraction of both colour- and texture-related features. Different categories of classifiers were adopted to evaluate the proposed feature extraction step, and several feature selection algorithms were compared for the classification of skin lesions. The developed skin lesion computational diagnosis system was applied to a set of 1104 dermoscopic images using a cross-validation procedure. The best results were obtained by an optimum-path forest classifier with very promising results. The proposed system achieved an accuracy of 92.3%, sensitivity of 87.5% and specificity of 97.1% when the full set of features was used. Furthermore, it achieved an accuracy of 91.6%, sensitivity of 87% and specificity of 96.2%, when 50 features were selected using a correlation-based feature selection algorithm.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6091–6111},
numpages = {21},
keywords = {Co-occurrence matrix, Discrete wavelet transform, Fractal dimension analysis, Feature extraction and selection}
}

@article{10.1007/s00500-021-06013-8,
author = {Yaghobzadeh, Reyhaneh and Kamel, Seyed Reza and Asgari, Mojtaba},
title = {Enhancing the precision and accuracy of renal failure diagnosis using the modified support vector machine algorithm and dragonfly algorithm},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {16},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06013-8},
doi = {10.1007/s00500-021-06013-8},
abstract = {Various methods have been proposed to diagnose renal failure based on data mining and artificial intelligence techniques. The use of data mining approaches, renal failure could be predicted based on several features and risk factors that exacerbate the condition. At the same time, each is associated with issues such as complex computation and a long implementation period. Moreover, these methods have varied accuracies due to their dependency on algorithms, performance, and nature of data. The present study aimed to propose an approach to increasing the accuracy and efficiency of the renal failure diagnosis. To this end, we developed a feature selection method based on the dragonfly algorithm. In addition, the optimal parameters of the support vector machine algorithm were presented using the preceding algorithm to optimize data classification. The performance of the proposed algorithm has been evaluated in comparison with the latest available methods. According to other algorithms, the proposed method is improved by 3.37% and 9.17% accuracy. Accordingly, in terms of accuracy compared to the latest work done, the proposed method has a significant improvement of 34.12%. The proposed method has been tested again on the information received from 7 patients from one of the specialized dialysis clinics in Mashhad.},
journal = {Soft Comput.},
month = aug,
pages = {10647–10659},
numpages = {13},
keywords = {Dragonfly algorithm (DA), Feature selection, Support vector machine (SVM), Classifier, Data mining, Renal failure}
}

@article{10.1504/ijsn.2019.100092,
author = {Costa, Victor G. Turrisi Da and Barbon, Sylvio and Miani, Rodrigo S. and Rodrigues, Joel J.P.C. and Zarpel\~{a}o, Bruno Bogaz},
title = {Mobile botnets detection based on machine learning over system calls},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {2},
issn = {1747-8405},
url = {https://doi.org/10.1504/ijsn.2019.100092},
doi = {10.1504/ijsn.2019.100092},
abstract = {Mobile botnets are a growing threat to the internet security field. These botnets target less secure devices with lower computational power, while sometimes taking advantage of features specific to them, e.g., SMS messages. We propose a host-based approach using machine learning techniques to detect mobile botnets with features derived from system calls. Patterns created tend to be shared among applications with similar actions. Therefore, different botnets are likely to share similar system call patterns. To measure the effectiveness of our approach, a dataset containing multiple botnets and legitimate applications was created. We carried out three experiments, namely finding out the best time-window, and performing feature selection and hyperparameter tuning. A high performance (over 84%) was achieved in multiple metrics across multiple machine learning algorithms. An in-depth analysis of the features is also presented to help future work with a solid discussion about system call-based features.},
journal = {Int. J. Secur. Netw.},
month = jan,
pages = {103–118},
numpages = {15},
keywords = {host-based approach, feature selection, mobile botnet detection}
}

@inproceedings{10.1007/978-3-319-96133-0_6,
author = {Cetiner, Erkan and Gungor, Vehbi Cagri and Kocak, Taskin},
title = {Evaluation of Hybrid Classification Approaches: Case Studies on Credit Datasets},
year = {2018},
isbn = {978-3-319-96132-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-96133-0_6},
doi = {10.1007/978-3-319-96133-0_6},
abstract = {Hybrid classification approaches on credit domain are widely used to obtain valuable information about customer behaviours. Single classification algorithms such as neural networks, support vector machines and regression analysis have been used since years on related area. In this paper, we propose hybrid classification approaches, which try to combine several classifiers and ensemble learners to boost accuracy on classification results. We worked with two credit datasets, German dataset which is a public dataset and a Turkish Corporate Bank dataset. The goal of using such diverse datasets is to search for generalization ability of proposed model. Results show that feature selection plays a vital role on classification accuracy, hybrid approaches which shaped with ensemble learners outperform single classification techniques and hybrid approaches which consists SVM has better accuracy performance than other hybrid approaches.},
booktitle = {Machine Learning and Data Mining in Pattern Recognition: 14th International Conference, MLDM 2018, New York, NY, USA, July 15-19, 2018, Proceedings, Part II},
pages = {72–86},
numpages = {15},
keywords = {Credit-risk, Hybrid-classifier, Feature selection},
location = {New York, NY, USA}
}

@inproceedings{10.1007/978-3-030-04182-3_37,
author = {Braytee, Ali and Anaissi, Ali and Kennedy, Paul J.},
title = {Sparse Feature Learning Using Ensemble Model for Highly-Correlated High-Dimensional Data},
year = {2018},
isbn = {978-3-030-04181-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-04182-3_37},
doi = {10.1007/978-3-030-04182-3_37},
abstract = {High-dimensional highly correlated data exist in several domains such as genomics. Many feature selection techniques consider correlated features as redundant and therefore need to be removed. Several studies investigate the interpretation of the correlated features in domains such as genomics, but investigating the classification capabilities of the correlated feature groups is a point of interest in several domains. In this paper, a novel method is proposed by integrating the ensemble feature ranking and co-expression networks to identify the optimal features for classification. The main advantage of the proposed method lies in the fact, that it does not consider the correlated features as redundant. But, it shows the importance of the selected correlated features to improve the performance of classification. A series of experiments on five high dimensional highly correlated datasets with different levels of imbalance ratios show that the proposed method outperformed the state-of-the-art methods.},
booktitle = {Neural Information Processing: 25th International Conference, ICONIP 2018, Siem Reap, Cambodia, December 13–16, 2018, Proceedings, Part III},
pages = {423–434},
numpages = {12},
keywords = {Feature selection, High-dimensional data, Feature correlation},
location = {Siem Reap, Cambodia}
}

@article{10.1016/j.patcog.2021.108141,
author = {Sevilla-Salcedo, Carlos and G\'{o}mez-Verdejo, Vanessa and Olmos, Pablo M.},
title = {Sparse semi-supervised heterogeneous interbattery bayesian analysis},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108141},
doi = {10.1016/j.patcog.2021.108141},
journal = {Pattern Recogn.},
month = dec,
numpages = {13},
keywords = {Multi-task, Semi-supervised, Feature selection, Factor analysis, Principal component analysis, Canonical correlation analysis, Bayesian model}
}

@article{10.1016/j.procs.2021.09.123,
author = {Dehkordi, Maryam Banitalebi and Zaraki, Abolfazl and Setchi, Rossitza},
title = {Optimal Feature Set for Smartphone-based Activity Recognition},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.09.123},
doi = {10.1016/j.procs.2021.09.123},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {3497–3506},
numpages = {10},
keywords = {smartphone, inertial sensor, feature selection, feature extraction, Activity recognition}
}

@article{10.1007/s00766-015-0237-z,
author = {Oliinyk, Olesia and Petersen, Kai and Schoelzke, Manfred and Becker, Martin and Schneickert, Soeren},
title = {Structuring automotive product lines and feature models: an exploratory study at Opel},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0237-z},
doi = {10.1007/s00766-015-0237-z},
abstract = {Automotive systems are highly complex and customized systems containing a vast amount of variability. 
Feature modeling plays a key role in customization. Empirical evidence through industry application, and in particular methodological guidance of how to structure automotive product lines and their feature models is needed. The overall aim of this work is to provide guidance to practitioners how to structure automotive product lines and their feature models, understanding strengths and weaknesses of alternative structures. The research was conducted in three phases. In the first phase, the context situation was understood using interviews and workshops. In the second phase, possible structures of product lines and feature models were evaluated based on industry feedback collected in workshops. In the third phase, the structures were implemented in the tool GEARS and practitioner feedback was collected. One key challenge was the unavailability of structuring guidelines, which was the focus of this research. The structures considered most suitable for the automotive product line were multiple product lines with modular decomposition. The structures most suitable for the feature model were functional decomposition, using context variability, models corresponding to assets, and feature categories. Other structures have been discarded, and the rationales have been presented. It was possible to support the most suitable structures with the commercial tool GEARS. The implementation in GEARS and the feedback from the practitioners provide early indications for the potential usefulness of the structures and the tool implementation.},
journal = {Requir. Eng.},
month = mar,
pages = {105–135},
numpages = {31},
keywords = {Variability modeling, Product line engineering, Feature modeling, Empirical, Case study, Automotive}
}

@inproceedings{10.1145/3501409.3501651,
author = {Li, Zhanbo and Li, Xiaoyang},
title = {Intrusion Detection Method Based on Genetic Algorithm of Optimizing LightGBM},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501651},
doi = {10.1145/3501409.3501651},
abstract = {In response to the high-precision requirements of intrusion detection systems, an intrusion detection method based on genetic algorithm of optimizing LightGBM is proposed. The recursive feature elimination algorithm is used to select the optimal feature subset, and a weighted loss function is designed to solve the problem of imbalanced network traffic data. Aiming at the problem that the performance of LightGBM is greatly affected by parameters and the cumbersome parameter adjustment, the powerful global search capability of genetic algorithm is used to optimize LightGBM and automatically determine the optimal parameter combination. Using the CIC-IDS2017 data set experiment, the experimental results show that the accuracy of this method is as high as 99.88%, which has higher detection accuracy than other methods.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1366–1371},
numpages = {6},
keywords = {Data imbalance, Feature selection, Genetic algorithm, Intrusion detection, LightGBM, Parameter optimization},
location = {Xiamen, China},
series = {EITCE '21}
}

@article{10.1016/j.patcog.2020.107213,
author = {Chen, Weizhao and Yang, Zhijing and Ren, Jinchang and Cao, Jiangzhong and Cai, Nian and Zhao, Huimin and Yuen, Peter},
title = {MIMN-DPP: Maximum-information and minimum-noise determinantal point processes for unsupervised hyperspectral band selection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {102},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2020.107213},
doi = {10.1016/j.patcog.2020.107213},
journal = {Pattern Recogn.},
month = jun,
numpages = {14},
keywords = {Determinantal point processes (DPP), Maximum information and minimum noise (MIMN) criterion, Unsupervised band selection, Hyperspectral images (HSI)}
}

@article{10.1007/s10462-016-9528-0,
author = {Tommasel, Antonela and Godoy, Daniela},
title = {Short-text feature construction and selection in social media data: a survey},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {3},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-016-9528-0},
doi = {10.1007/s10462-016-9528-0},
abstract = {Social networking sites such as Facebook or Twitter attract millions of users, who everyday post an enormous amount of content in the form of tweets, comments and posts. Since social network texts are usually short, learning tasks have to deal with a very high dimensional and sparse feature space, in which most features have low frequencies. As a result, extracting useful knowledge from such noisy data is a challenging task, that converts large-scale short-text learning tasks in social environments into one of the most relevant problems in machine learning and data mining. Feature selection is one of the most known and commonly used techniques for reducing the impact of the high dimensional feature space in text learning. A wide variety of feature selection techniques can be found in the literature applied to traditional, long-texts and document collections. However, short-texts coming from the social Web pose new challenges to this well-studied problem as texts' shortness offers a limited context to extract enough statistical evidence about words relations (e.g. correlation), and instances usually arrive in continuous streams (e.g. Twitter timeline), so that the number of features and instances is unknown, among other problems. This paper surveys feature selection techniques for dealing with short texts in both offline and online settings. Then, open issues and research opportunities for performing online feature selection over social media data are discussed.},
journal = {Artif. Intell. Rev.},
month = mar,
pages = {301–338},
numpages = {38},
keywords = {Text learning, Social media data, Short-text, Feature selection}
}

@article{10.1007/s10916-019-1311-8,
author = {Vijayarajeswari, R. and Nagabhushan, M. and Parthasarathy, P.},
title = {An Enhanced Symptom Clustering with Profile Based Prescription Suggestion in Biomedical application},
year = {2019},
issue_date = {Jun 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {6},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1311-8},
doi = {10.1007/s10916-019-1311-8},
abstract = {The application of data mining has been increasing day to day whereas the data base is also enhancing simultaneously. Hence retrieving required content from a huge data base is a critical task. This paper focus on biomedical engineering field, it concentrates on initial stage of database such as data preprocessing and cleansing to deal with noise and missing data in large biomedical data sets. The database of biomedical is huge and enhancing nature retrieving of specific content will be a critical task. Suggesting prescription with respect to identified disease based on profile analysis of specific patient is not available in current system. This paper proposes a recommendation system of prescription based on disease identification is done by combining user and professional suggestion with profile based analysis. Hence this focuses on profile based suggestions and report will be generated. The retrieving of specific suggestion from a huge database is done by hybrid feature selection algorithm. This approach focuses on enabling recommendation based on user profile and implementing Hybrid feature selection algorithm to retrieve specific content from a huge database. Hence it attains better retrieval of required content from a huge database compared to other existing approaches and suggests better recommendation with respect to user profile.},
journal = {J. Med. Syst.},
month = jun,
pages = {1–6},
numpages = {6},
keywords = {Recommendation, Profile based analysis, Preprocessing and cleaning, Feature selection}
}

@inproceedings{10.1145/3373509.3373544,
author = {Liu, Hongma and Li, Yali and Wang, Shengjin},
title = {Exploiting Generalized Matched Filter for Efficient and Effective P300 Detection in Speller Paradigm},
year = {2020},
isbn = {9781450376570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373509.3373544},
doi = {10.1145/3373509.3373544},
abstract = {Brain computer interface (BCI) is aimed to produce a direct way for people to communicate with computer. P300 speller interprets brain signals so that people can spell a word without body movement. However, the conventional classification methods are time-consuming and not practical for application. In this paper, we propose a method that tackles the problem through a generalized matched filter. This work is distinguished by two key contributions. First, we investigate statistical signal processing and its feasibility in our task. A model based on hypothesis testing is built and we derive a generalized matched filter (GMF) which is a simple linear function of the signal due to the Gaussian noise assumption. Second, feature selection is introduced according to the estimated covariance matrix, which improves the performance and enhances the computing efficiency simultaneously. Experimental results have demonstrated the great effectiveness of our methods. We improve the performance significantly by 30.5%, 14% at 5, 15 epochs over the original GMF model on BCI competition III dataset II. Moreover, the results are also competitive and much more efficient in applications compared with state-of-the-art methods. Our algorithms need no time for training and cost only 0.016s per character for testing.},
booktitle = {Proceedings of the 2019 8th International Conference on Computing and Pattern Recognition},
pages = {395–399},
numpages = {5},
keywords = {generalized matched filter, feature selection, P300 speller},
location = {Beijing, China},
series = {ICCPR '19}
}

@article{10.1016/j.eswa.2020.113236,
author = {Martins, Isabelle D. and Bahiense, Laura and Infante, Carlos E.D. and Arruda, Edilson F.},
title = {Dimensionality reduction for multi-criteria problems: An application to the decommissioning of oil and gas installations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {148},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113236},
doi = {10.1016/j.eswa.2020.113236},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {11},
keywords = {Multi-criteria decision analysis, Machine learning, Feature selection, Dimensionality reduction, Decommissioning, Oil and gas}
}

@article{10.1504/ijdmb.2021.124107,
author = {Singh, Divjot and Mishra, Ashutosh},
title = {Multiple-ensemble methods for prediction of Alzheimer disease},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {26},
number = {1–2},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2021.124107},
doi = {10.1504/ijdmb.2021.124107},
abstract = {Alzheimer's Disease (AD) is a neurodegenerative disease whose permanent cure is not yet available. However, its prediction at an early stage may increase the life span of a person by many years. The main predicament is to detect AD at an early stage and select the features responsible for it. The objective of this study was to predict AD at an early stage and identify the features that facilitate early prediction using ensemble learning. First, we implemented the ADNI data set on different machine-learning and deep-learning models. The proposed multiple ensemble method overcomes the limitations of existing models by applying feature selection for the early prediction, and it is observed that the best ensemble model is having the top 6-selected features and achieves an accuracy of 96.71% with higher ROC. Our model performed well compared with other machine and deep learning models.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {99–128},
numpages = {29},
keywords = {feature selection, deep learning, DL, ensemble learning, EL, machine learning, ML, Alzheimer disease, AD}
}

@article{10.1007/s00500-019-04019-x,
author = {Chang, Jing-Rong and Chen, Mu-Yen and Chen, Long-Sheng and Chien, Wan-Ting},
title = {Recognizing important factors of influencing trust in O2O models: an example of OpenTable},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {11},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04019-x},
doi = {10.1007/s00500-019-04019-x},
abstract = {Online-to-offline/offline-to-online (O2O) business models have attracted lots of enterprisers to enter this market. In such a fast-growing competition, some studies indicated that lack of trust will bring a great damage to O2O business. Related works already confirm trust is the key factor to the success of O2O. Besides, social media has been changing the way providers communicate with consumers. Negative comments in social media will decrease the consumers’ trust to O2O companies and platforms. Available O2O studies are almost always conducted by means of questionnaires or interviews, which cannot provide immediate customer response and require a lot of manpower and time. Since online reviews are the main information source for consumers. Therefore, this study presented a text mining-based scheme which uses text mining technique to find important factors from online electronic word-of-mouth, to replace the traditional questionnaire survey method of collecting data. Two feature selection methods, Support Vector Machines Recursive Feature Elimination and Least Absolute Shrinkage and Selection Operator have employed to select important factors that affect O2O trust. We also evaluate the performance of extracted feature subsets by Support Vector Machines. The findings can be referenced for O2O market enterprises to carefully response customers’ comments to avoid hurting customers’ trust and improve service quality.},
journal = {Soft Comput.},
month = jun,
pages = {7907–7923},
numpages = {17},
keywords = {LASSO, SVM-RFE, Feature selection, Sentiment classification, Trust, O2O}
}

@article{10.1007/s11042-020-09740-6,
author = {Rezk, Nermeen Gamal and Hemdan, Ezz El-Din and Attia, Abdel-Fattah and El-Sayed, Ayman and El-Rashidy, Mohamed A.},
title = {An efficient IoT based smart farming system using machine learning algorithms},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {1},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09740-6},
doi = {10.1007/s11042-020-09740-6},
abstract = {This paper suggests an IoT based smart farming system along with an efficient prediction method called WPART based on machine learning techniques to predict crop productivity and drought for proficient decision support making in IoT based smart farming systems. The crop productivity and drought predictions is very important to the farmers and agriculture’s executives, which greatly help agriculture-affected countries around the world. Drought prediction plays a significant role in drought early warning to mitigate its impacts on crop productivity, drought prediction research aims to enhance our understanding of the physical mechanism of drought and improve predictability skill by taking full advantage of sources of predictability. In this work, an intelligent method based on the blend of a wrapper feature selection approach, and PART classification technique is proposed for crop productivity and drought predicting. Five datasets are used for estimating the proposed method. The results indicated that the projected method is robust, accurate, and precise to classify and predict crop productivity and drought in comparison with the existing techniques. From the results, the proposed method proved to be most accurate in providing drought prediction as well as the productivity of crops like Bajra, Soybean, Jowar, and Sugarcane. The WPART method attains the maximum accuracy compared to the existing supreme standard algorithms, it is obtained up to 92.51%, 96.77%, 98.04%, 96.12%, and 98.15% for the five datasets for drought classification, and crop productivity respectively. Likewise, the proposed method outperforms existing algorithms with precision, sensitivity, and F Score metrics.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {773–797},
numpages = {25},
keywords = {Feature selection, And, Crop productivity, Drought, Prediction, Smart farming, Internet of things, Machine learning}
}

@article{10.1016/j.eswa.2014.11.050,
author = {Ayech, Mohamed Walid and Ziou, Djemel},
title = {Segmentation of Terahertz imaging using k-means clustering based on ranked set sampling},
year = {2015},
issue_date = {April 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {6},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.11.050},
doi = {10.1016/j.eswa.2014.11.050},
abstract = {A novel approach of segmentation of Terahertz images is proposed.We explain how to reformulate the k-means technique under the ranked set sample.The goal of the approach is to estimate good centers and classify the data.We compare our approach to k-means based on simple random sampling technique. Terahertz imaging is a novel imaging modality that has been used with great potential in many applications. Due to its specific properties, the segmentation of this type of images makes possible the discrimination of diverse regions within a sample. Among many segmentation methods, k-means clustering is considered as one of the most popular techniques. However, it is known that k-means is especially sensitive to initial starting centers. In this paper, we propose an original version of k-means for the segmentation of Terahertz images, called ranked-k-means, which is essentially less sensitive to the initialization of the centers. We present the ranked set sampling design and explain how to reformulate the k-means technique under the ranked sample to estimate the expected centers as well as the clustering of the observed data. Our clustering approach is tested on various real Terahertz images. Experimental results show that k-means clustering based on ranked set sampling is more efficient than other clustering techniques such as the k-means based on the fundamental sampling design simple random sampling technique, the standard k-means and the k-means based on the Bradley refinement of initial centers.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2959–2974},
numpages = {16},
keywords = {k-means, Terahertz imaging, Simple random sampling, Segmentation, Ranked set sampling}
}

@article{10.1016/j.asoc.2019.01.032,
author = {Kublanov, Vladimir and Dolganov, Anton},
title = {Development of a decision support system for neuro-electrostimulation: Diagnosing disorders of the cardiovascular system and evaluation of the treatment efficiency},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.01.032},
doi = {10.1016/j.asoc.2019.01.032},
journal = {Appl. Soft Comput.},
month = apr,
pages = {329–343},
numpages = {15},
keywords = {Heart rate variability, Arterial hypertension, Feature selection, Machine learning, Decision support}
}

@article{10.1016/j.eswa.2020.114558,
author = {Popiolek, Pedro Freire and Machado, Karina dos Santos and Mendizabal, Odorico Machado},
title = {Low overhead performance monitoring for shared infrastructures},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114558},
doi = {10.1016/j.eswa.2020.114558},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {13},
keywords = {Data science, Feature selection, Virtualization, Performance optimization, Performance counters, System monitoring}
}

@inproceedings{10.5555/645991.674495,
author = {Ruiz, Roberto and Aguilar-Ruiz, Jes\'{u}s S. and Santos, Jos\'{e} Crist\'{o}bal Riquelme},
title = {SOAP: Efficient Feature Selection of Numeric Attributes},
year = {2002},
isbn = {354000131X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The attribute selection techniques for supervised learning, used in the preprocessing phase to emphasize the most relevant attributes, allow making models of classification simpler and easy to understand. Depending on the method to apply: starting point, search organization, evaluation strategy, and the stopping criterion, there is an added cost to the classification algorithm that we are going to use, that normally will be compensated, in greater or smaller extent, by the attribute reduction in the classification model. The algorithm (SOAP: Selection of Attributes by Projection) has some interesting characteristics: lower computational cost (O(mn log n) m attributes and n examples in the data set) with respect to other typical algorithms due to the absence of distance and statistical calculations; with no need for transformation. The performance of SOAP is analysed in two ways: percentage of reduction and classification. SOAP has been compared to CFS [6] and ReliefF [11]. The results are generated by C4.5 and 1NN before and after the application of the algorithms.},
booktitle = {Proceedings of the 8th Ibero-American Conference on AI: Advances in Artificial Intelligence},
pages = {233–242},
numpages = {10},
series = {IBERAMIA 2002}
}

@article{10.1145/3295690,
author = {Shekofteh, S.-Kazem and Noori, Hamid and Naghibzadeh, Mahmoud and Yazdi, Hadi Sadoghi and Fr\"{o}ning, Holger},
title = {Metric Selection for GPU Kernel Classification},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1544-3566},
url = {https://doi.org/10.1145/3295690},
doi = {10.1145/3295690},
abstract = {Graphics Processing Units (GPUs) are vastly used for running massively parallel programs. GPU kernels exhibit different behavior at runtime and can usually be classified in a simple form as either “compute-bound” or “memory-bound.” Recent GPUs are capable of concurrently running multiple kernels, which raises the question of how to most appropriately schedule kernels to achieve higher performance. In particular, co-scheduling of compute-bound and memory-bound kernels seems promising. However, its benefits as well as drawbacks must be determined along with which kernels should be selected for a concurrent execution. Classifying kernels can be performed online by instrumentation based on performance counters. This work conducts a thorough analysis of the metrics collected from various benchmarks from Rodinia and CUDA SDK. The goal is to find the minimum number of effective metrics that enables online classification of kernels with a low overhead. This study employs a wrapper-based feature selection method based on the Fisher feature selection criterion. The results of experiments show that to classify kernels with a high accuracy, only three and five metrics are sufficient on a Kepler and a Pascal GPU, respectively. The proposed method is then utilized for a runtime scheduler. The results show an average speedup of 1.18\texttimes{} and 1.1\texttimes{} compared with a serial and a random scheduler, respectively.},
journal = {ACM Trans. Archit. Code Optim.},
month = jan,
articleno = {68},
numpages = {27},
keywords = {resource utilization, kernel metrics, feature selection, concurrency, Classification}
}

@article{10.1007/s00500-021-05809-y,
author = {Tian, Qiuting and Han, Dezhi and Hsieh, Meng-Yen and Li, Kuan-Ching and Castiglione, Arcangelo},
title = {A two-stage intrusion detection approach for software-defined IoT networks},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {16},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05809-y},
doi = {10.1007/s00500-021-05809-y},
abstract = {The concept of software-defined Internet of Things (SD-IoT) is becoming even more widespread. SD-IoT enables us to realize programmable networks and business, simplifying the management of the Internet of Things (IoT) and improving the IoT flexibility and scalability. However, with the promotion of SD-IoT-based applications and services, security issues in SD-IoT networks have become increasingly prominent. Aimed to deal with such issues, in this paper, we propose a two-stage intrusion detection approach for SD-IoT networks. It can more intelligently detect attacks under SD-IoT networks. In particular, we use the differential evolution algorithm's mutation mechanism to improve the firefly algorithm to solve the existing firefly algorithm's problems, such as slow convergence speed, easy to fall into local optimum on complex problems, and low accuracy. Next, based on the wrapper feature selection method, the selected features are sent to a novel ensemble classifier, composed of the C4.5 decision tree, multilayer perceptron, and instance-based learning. Again, the proposed approach uses the weighted voting method to determine whether network traffic is abnormal. Our proposal's detection performance is evaluated in binary and multiclass classifications by adopting the NSL-KDD and UNSW-NB15 public data sets. Experimental results show that the proposed multiclass classification approach's accuracy is 99.00% and 88.46%, respectively, while the false-positive rate is 0.81% and 4.16%, respectively. Finally, experimental results show that our proposal outperforms existing methods in terms of detection performance.},
journal = {Soft Comput.},
month = aug,
pages = {10935–10951},
numpages = {17},
keywords = {Ensemble classifier, Feature selection, Intrusion detection, Firefly algorithm, Software-defined Internet of Things (SD-IoT)}
}

@inproceedings{10.1145/3410530.3414334,
author = {Rahman, Arafat and Nahid, Nazmun and Hassan, Iqbal and Ahad, M. A. R.},
title = {Nurse care activity recognition: using random forest to handle imbalanced class problem},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414334},
doi = {10.1145/3410530.3414334},
abstract = {Nurse care activity recognition is a new challenging research field in human activity recognition (HAR) because unlike other activity recognition, it has severe class imbalance problem and intra-class variability depending on both the subject and the receiver. In this paper, we applied the Random Forest-based resampling method to solve the class imbalance problem in the Heiseikai data, nurse care activity dataset. This method consists of resampling, feature selection based on Gini impurity, and model training and validation with Stratified KFold cross-validation. By implementing the Random Forest classifier, we achieved 65.9% average cross-validation accuracy in classifying 12 activities conducted by nurses in both lab and real-life settings. Our team, "Britter Baire" developed this algorithmic pipeline for "The 2nd Nurse Care Activity Recognition Challenge Using Lab and Field Data".},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {419–424},
numpages = {6},
keywords = {stratified KFold cross-validation, random forest, nurse care, feature selection, activity recognition, accelerometer},
location = {Virtual Event, Mexico},
series = {UbiComp/ISWC '20 Adjunct}
}

@article{10.1504/ijbra.2021.117930,
author = {Usharani, R. and Murali, M.},
title = {A review of dimensionality reduction methods applied on clinical data of diabetic neuropathy complaints},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {17},
number = {4},
issn = {1744-5485},
url = {https://doi.org/10.1504/ijbra.2021.117930},
doi = {10.1504/ijbra.2021.117930},
abstract = {Aim of this paper is to apply Dimensionality Reduction techniques on a large clinical data of Type II diabetes patients in identifying the causes and symptoms that they tend to develop neuropathic complications. Data preprocessing is an essential technique using Machine Learning (ML) and Data Mining are ineffective for big data analytics. An effective approach to dimensionality reduction is reduction of the number of independent/dependent variables which are necessary for our analysis. These processes help in identifying the features to be considered for selection and extraction avoiding the redundant and irrelevant features by choosing subset features which are a linear combination of the original. Both supervised and unsupervised learning are applied for prediction and further analysis. This paper primarily focuses on Supervised Learning, the variables are known beforehand. Combination of feature selection techniques and ML algorithms are used to support practitioners with the best methods for feature reduction and extraction.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {324–342},
numpages = {18},
keywords = {feature extraction, feature selection, machine learning algorithms, dimensionality reduction, Type II diabetes mellitus neuropathic complaints}
}

@article{10.1186/s13638-020-01729-x,
author = {Guo, Ni and Gui, Weifeng and Chen, Wei and Tian, Xin and Qiu, Weiguo and Tian, Zijian and Zhang, Xiangyang},
title = {Using improved support vector regression to predict the transmitted energy consumption data by distributed wireless sensor network},
year = {2020},
issue_date = {Nov 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-020-01729-x},
doi = {10.1186/s13638-020-01729-x},
abstract = {Massive energy consumption data of buildings was generated with the development of information technology, and the real-time energy consumption data was transmitted to energy consumption monitoring system by the distributed wireless sensor network (WSN). Accurately predicting the energy consumption is of importance for energy manager to make advisable decision and achieve the energy conservation. In recent years, considerable attention has been gained on predicting energy use of buildings in China. More and more predictive models appeared in recent years, but it is still a hard work to construct an accurate model to predict the energy consumption due to the complexity of the influencing factors. In this paper, 40 weather factors were considered into the research as input variables, and the electricity of supermarket which was acquired by the energy monitoring system was taken as the target variable. With the aim to seek the optimal subset, three feature selection (FS) algorithms were involved in the study, respectively: stepwise, least angle regression (Lars), and Boruta algorithms. In addition, three machine learning methods that include random forest (RF) regression, gradient boosting regression (GBR), and support vector regression (SVR) algorithms were utilized in this paper and combined with three feature selection (FS) algorithms, totally are nine hybrid models aimed to explore an improved model to get a higher prediction performance. The results indicate that the FS algorithm Boruta has relatively better performance because it could work well both on RF and SVR algorithms, the machine learning method SVR could get higher accuracy on small dataset compared with the RF and GBR algorithms, and the hybrid model called SVR-Boruta was chosen to be the proposed model in this paper. What is more, four evaluate indicators were selected to verify the model performance respectively are the mean absolute error (MAE), the mean squared error(MSE), the root mean squared error (RMSE), and the R-squared (R2), and the experiment results further verified the superiority of the recommended methodology.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = jun,
numpages = {16},
keywords = {Climate factors, Feature selection, Machine learning, Energy consumption, Distributed wireless sensor network}
}

@article{10.14778/3368289.3368302,
author = {Walenz, Brett and Sintos, Stavros and Roy, Sudeepa and Yang, Jun},
title = {Learning to sample: counting with complex queries},
year = {2019},
issue_date = {November 2019},
publisher = {VLDB Endowment},
volume = {13},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3368289.3368302},
doi = {10.14778/3368289.3368302},
abstract = {We study the problem of efficiently estimating counts for queries involving complex filters, such as user-defined functions, or predicates involving self-joins and correlated subqueries. For such queries, traditional sampling techniques may not be applicable due to the complexity of the filter preventing sampling over joins, and sampling after the join may not be feasible due to the cost of computing the full join. The other natural approach of training and using an inexpensive classifier to estimate the count instead of the expensive predicate suffers from the difficulties in training a good classifier and giving meaningful confidence intervals. In this paper we propose a new method of learning to sample where we combine the best of both worlds by using sampling in two phases. First, we use samples to learn a probabilistic classifier, and then use the classifier to design a stratified sampling method to obtain the final estimates. We theoretically analyze algorithms for obtaining an optimal stratification, and compare our approach with a suite of natural alternatives like quantification learning, weighted and stratified sampling, and other techniques from the literature. We also provide extensive experiments in diverse use cases using multiple real and synthetic datasets to evaluate the quality, efficiency, and robustness of our approach.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {390–402},
numpages = {13}
}

@article{10.4018/IJACI.2019070102,
author = {Wang, Rujuan and Wang, Gang},
title = {Web Text Categorization Based on Statistical Merging Algorithm in Big Data Environment},
year = {2019},
issue_date = {Jul 2019},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {3},
issn = {1941-6237},
url = {https://doi.org/10.4018/IJACI.2019070102},
doi = {10.4018/IJACI.2019070102},
abstract = {In the field of modern information technology, how to find information quickly, accurately and comprehensively that users really needed has become the focus of research in this field. In this article, a feature selection method based on a complex network is proposed for the structure and content characteristics of large-scale web text information. The preprocessed web text is converted into a complex network. The nodes in the network correspond to the entries in the text. The edges of the network correspond to the links between the entries in the text, and the degree of nodes and the aggregation system are used. Second, the text classification method is studied from the point of view of data sampling, and a text classification method based on density statistics is proposed. This method uses not only the density information of the text feature set in the classification process, but also the use of statistical merging criteria to get the text. The difference information of each feature has a better classification effect for large text collections.},
journal = {Int. J. Ambient Comput. Intell.},
month = jul,
pages = {17–32},
numpages = {16},
keywords = {Text Categorization, Statistical Leaders, Feature Selection, Data Set}
}

@inproceedings{10.1007/978-3-030-68796-0_34,
author = {Szalma, J\'{a}nos and Amora, Kathleen Kay and Vidny\'{a}nszky, Zolt\'{a}n and Weiss, B\'{e}la},
title = {Investigating the Effect of Inter-letter Spacing Modulation on Data-Driven Detection of Developmental Dyslexia Based on Eye-Movement Correlates of Reading: A Machine Learning Approach},
year = {2021},
isbn = {978-3-030-68795-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68796-0_34},
doi = {10.1007/978-3-030-68796-0_34},
abstract = {Developmental dyslexia is a reading disability estimated to affect between 5 to 10% of the population. However, current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Accordingly, investigating the eye-movement correlates of reading in a machine learning framework could potentially enhance the detection of poor readers. Here, the capability of eye-movement measures in classifying dyslexic and control young adults (24 dyslexic, 24 control) was assessed on eye-tracking data acquired during reading of isolated sentences presented at five inter-letter spacing levels. The set of 65 eye-movement features included properties of fixations, saccades and glissades. Classification accuracy and importance of features were assessed for all spacing levels by aggregating the results of five feature selection methods. Highest classification accuracy (73.25%) was achieved for an increased spacing level, while the worst classification performance (63%) was obtained for the minimal spacing condition. However, the classification performance did not differ significantly between these two spacing levels (p = 0.28). The most important features contributing to the best classification performance across the spacing levels were as follows: median of progressive and all saccade amplitudes, median of fixation duration and interquartile range of forward glissade duration. Selection frequency was even for the median of fixation duration, while the median amplitude of all and forward saccades measures exhibited complementary distributions across the spacing levels. The results suggest that although the importance of features may vary with the size of inter-letter spacing, the classification performance remains invariant.},
booktitle = {Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 10–15, 2021, Proceedings, Part III},
pages = {467–481},
numpages = {15},
keywords = {Feature selection, Eye-movement features, Support vector machine, Machine learning, Inter-letter spacing, Reading, Developmental dyslexia}
}

@article{10.1016/j.neucom.2019.10.016,
author = {Weng, Wei and Chen, Yan-Nan and Chen, Chin-Ling and Wu, Shun-Xiang and Liu, Jing-Hua},
title = {Non-sparse label specific features selection for multi-label classification},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {377},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.016},
doi = {10.1016/j.neucom.2019.10.016},
journal = {Neurocomput.},
month = feb,
pages = {85–94},
numpages = {10},
keywords = {Label specific features, Numerical label, Multi-label learning}
}

