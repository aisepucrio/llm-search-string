@inproceedings{10.1145/3382025.3414956,
author = {Gr\"{u}ner, Sten and Burger, Andreas and Kantonen, Tuomas and R\"{u}ckert, Julius},
title = {Incremental migration to software product line engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414956},
doi = {10.1145/3382025.3414956},
abstract = {Current market developments require organizations to produce high-quality tailored software variants while simultaneously coping with increased software complexity. Software Product Line Engineering (SPLE) is a well-known approach to manage this variability and complexity, however no step-wise migration process is available allowing a co-existence of SPLE along with established development processes. In this paper, we introduce an incremental SPLE migration strategy and process starting from using the feature model as synchronized product and variant documentation. They can be applied as a first step of SPLE migration along with the continuous software development cycle. We performed initial steps of the process on industrial low voltage drive embedded firmware spanning around few millions lines of code using a commercial SPLE tool and validated short-term benefits by means of stakeholder feedback.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {5},
numpages = {11},
keywords = {software product line engineering, non-invasive migration, migration process, incremental migration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461002.3473951,
author = {Morais Ferreira, David and Tenev, Vasil L. and Becker, Martin},
title = {Product-line analysis cookbook: a classification system for complex analysis toolchains},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473951},
doi = {10.1145/3461002.3473951},
abstract = {Adopting Product Line Engineering (PLE) approaches in the context of software-intensive systems reduces overall development and maintenance costs, reduces time to market and leads to an overall improvement in product quality. The Software and System Product Line (SPL) community has provided a large number of different analysis approaches and tools, which were developed in different contexts, answer different questions, and can contribute to the fulfillment of different analysis goals. Typically, these analysis tools are initially developed as part of a research study, where they serve a specific purpose, e. g. for investigating the use of a new technology, or to demonstrate the transfer of methods from other fields. Generally, such purpose is aligned with a specific, but not explicitly stated, high-level goal. The pursuit of these goals requires holistic approaches, i. e. integrated toolchains and classification of analyses, which are documented as a centralized collection of wisdom. Therefore, we propose a classification system which describes existing analyses and reveals possible combinations, i. e. integrated toolchains, and provide first examples. This method supports the search for toolchains which address complex industrial needs. With the support of the SPL community, we hope to collaboratively document existing analyses and corresponding goals on an open platform.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {99–104},
numpages = {6},
keywords = {reverse engineering, product-line aware analyses, product line engineering, holistic toolchain},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3336294.3336321,
author = {Ghofrani, Javad and Kozegar, Ehsan and Fehlhaber, Anna Lena and Soorati, Mohammad Divband},
title = {Applying Product Line Engineering Concepts to Deep Neural Networks},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336321},
doi = {10.1145/3336294.3336321},
abstract = {Deep Neural Networks (DNNs) are increasingly being used as a machine learning solution thanks to the complexity of their architecture and hyperparameters-weights. A drawback is the excessive demand for massive computational power during the training process. Not only as a whole but parts of neural networks can also be in charge of certain functionalities. We present a novel challenge in an intersection between machine learning and variability management communities to reuse modules of DNNs without further training. Let us assume that we are given a DNN for image processing that recognizes cats and dogs. By extracting a part of the network, without additional training a new DNN should be divisible with the functionality of recognizing only cats. Existing research in variability management can offer a foundation for a product line of DNNs composing the reusable functionalities. An ideal solution can be evaluated based on its speed, granularity of determined functionalities, and the support for adding variability to the network. The challenge is decomposed in three subchallenges: feature extraction, feature abstraction, and the implementation of a product line of DNNs.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {72–77},
numpages = {6},
keywords = {variability, transfer learning, software product lines, machine learning, deep neural networks},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3425774,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {A multi-company empirical evaluation of a framework that evaluates the convenience of adopting product line engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425774},
doi = {10.1145/3382026.3425774},
abstract = {Companies considering adopting a product line engineering approach should ideally analyze the pros and cons to determine the sound reasons for this decision. In order to support this analysis, in previous work we proposed the APPLIES evaluation framework. This framework provides information to evaluate the convenience of adopting a product line engineering approach.This paper presents an empirical evaluation of APPLIES. This experience includes 18 potential practitioners that used the framework to evaluate the convenience of adopting product line engineering in 19 different companies. The collected evidence was used to evaluate the perceived usefulness, intention to use and ease of use of APPLIES. The results presented increase confidence that APPLIESis a useful tool, but also identify some possibilities for improvement. In addition, four categories for classifying potential adopters of product line engineering emerged during the analysis of the results: unprepared adopter, potential adopter, ready adopter, and unmotivated adopter. These categories could be useful to classify companies that are considering adopting product line engineering.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {13–20},
numpages = {8},
keywords = {product line engineering adoption, perceived usefulness, intention to use, ease of use, Empirical evaluation},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@article{10.1016/j.infsof.2020.106389,
author = {Chac\'{o}n-Luna, Ana Eva and Guti\'{e}rrez, Antonio Manuel and Galindo, Jos\'{e} A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Systematic literature review, Experiment, Case study, Empirical strategies, Software product lines}
}

@inproceedings{10.1145/2934466.2934491,
author = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
title = {Ten years of product line engineering at Danfoss: lessons learned and way ahead},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934491},
doi = {10.1145/2934466.2934491},
abstract = {Software and systems product line engineering (PLE) has been an established approach for reducing time to market as well as cost and increasing quality in a set of related products for two decades now. Although there is a huge body of knowledge on PLE, adopting a concrete PLE approach is still not a trivial endeavor for interested companies. With the increasing importance of development speed, the advent of agile engineering approaches, and decreasing management interest in improvements that require large organizational transformations and only show benefits after several years, companies are facing challenges in successfully adopting this approach. They often hesitate as there is no clear adoption path, nor any certainty, that the intended improvement steps will also provide added value in the short- and mid-term perspective. In consequence, a considerable amount of PLE potential still remains unexploited.To help such companies with the adoption of PLE, the goal of this paper is to provide inspiration and evidence that PLE is a sound approach and its successful introduction is possible even in settings that differ substantially from those of pioneer product lines.To this end, this paper presents the following main contributions with the PLE adoption case at Danfoss Drives: an overview of the key change drivers and the motivation for adopting a PLE approach, a discussion of incremental PLE introduction in an agile engineering context, a presentation of the current PLE setting with a focus on key concepts, and finally a presentation of motivators and directions for future improvements.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {252–261},
numpages = {10},
keywords = {product line evaluation, product line adoption, industrial experiences},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2791060.2791067,
author = {Yue, Tao and Ali, Shaukat and Selic, Bran},
title = {Cyber-physical system product line engineering: comprehensive domain analysis and experience report},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791067},
doi = {10.1145/2791060.2791067},
abstract = {Cyber-Physical Systems (CPSs) are the future generation of highly connected embedded systems having applications in diverse domains including Oil and Gas. Employing Product Line Engineering (PLE) is believed to bring potential benefits with respect to reduced cost, higher productivity, higher quality, and faster time-to-market. However, relatively few industrial field studies are reported regarding the application of PLE to develop large-scale systems, and more specifically CPSs. In this paper, we report about our experiences and insights gained from investigating the application of model-based PLE at a large international organization developing subsea production systems (typical CPSs) to manage the exploitation of oil and gas production fields. We report in this paper 1) how two systematic domain analyses (on requirements engineering and product configuration/derivation) were conducted to elicit CPS PLE requirements and challenges, 2) key results of the domain analysis (commonly observed in other domains), and 3) our initial experience of developing and applying two Model Based System Engineering (MBSE) PLE solution to address some of the requirements and challenges elicited during the domain analyses.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {338–347},
numpages = {10},
keywords = {requirements engineering, product line engineering (PLE), model based system engineering, domain analysis, cyber physical system (CPS)},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {variant management, variability realization, variability mechanism, system and software product line engineering, model-based systems engineering, decision support, UML, SysML},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-64148-1_6,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A Portfolio-Driven Development Model and Its Management Method of Agile Product Line Engineering Applied to Automotive Software Development},
year = {2020},
isbn = {978-3-030-64147-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64148-1_6},
doi = {10.1007/978-3-030-64148-1_6},
abstract = {In recent automotive systems development, realizing both variability and agility is the key competitiveness to meet the diverse requirements in global markets and rapidly increasing intelligent functions. This article proposes a portfolio-driven development method and its management method of APLE (Agile Product Line Engineering). The proposed method is intended to manage agile evolution of multiple product lines while increasing variability of products. To establish a portfolio management of development resources, it is necessary for an organization to manage multiple product lines on APLE in an entire development. We propose a portfolio-driven development method of three layers on APLE and its management method based on a concept of portfolio management life cycle. We applied the proposed management model and method to the multiple product lines of automotive software systems, and demonstrated an improvement of manageability with better predictability of both productivity and development size. This article contributes to provide an entire development management method for APLE, and its practical experience in the automotive multiple product lines.},
booktitle = {Product-Focused Software Process Improvement: 21st International Conference, PROFES 2020, Turin, Italy, November 25–27, 2020, Proceedings},
pages = {88–105},
numpages = {18},
keywords = {Automotive software development, Portfolio management, Agile product line engineering, Agile software development, Software product line},
location = {Turin, Italy}
}

@inproceedings{10.1145/3425269.3425278,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Supporting user preferences in search-based product line architecture design using Machine Learning},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425278},
doi = {10.1145/3425269.3425278},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA design requires intensive human effort as it involves several conflicting factors. In order to support this task, an interactive search-based approach, automated by a tool named OPLA-Tool, was proposed in a previous work. Through this tool the software architect evaluates the generated solutions during the optimization process. Considering that evaluating PLA is a complex task and search-based algorithms demand a high number of generations, the evaluation of all solutions in all generations cause human fatigue. In this work, we incorporated in OPLA-Tool a Machine Learning (ML) model to represent the architect in some moments during the optimization process aiming to decrease the architect's effort. Through the execution of a quantiqualitative exploratory study it was possible to demonstrate the reduction of the fatigue problem and that the solutions produced at the end of the process, in most cases, met the architect's needs.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {11–20},
numpages = {10},
keywords = {Product Line Architecture, Machine Learning, Human-computer interaction},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {variability modeling, variability management, requirements engineering, quality assurance, design, Software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/3382025.3414953,
author = {Abbas, Muhammad and Jongeling, Robbert and Lindskog, Claes and Enoiu, Eduard Paul and Saadatmand, Mehrdad and Sundmark, Daniel},
title = {Product line adoption in industry: an experience report from the railway domain},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414953},
doi = {10.1145/3382025.3414953},
abstract = {The software system controlling a train is typically deployed on various hardware architectures and must process various signals across those deployments. The increase of such customization scenarios and the needed adherence of the software to various safety standards in different application domains has led to the adoption of product line engineering within the railway domain. This paper explores the current state-of-practice of software product line development within a team developing industrial embedded software for a train propulsion control system. Evidence is collected using a focus group session with several engineers and through inspection of archival data. We report several benefits and challenges experienced during product line adoption and deployment. Furthermore, we identify and discuss improvement opportunities, focusing mainly on product line evolution and test automation.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {3},
numpages = {11},
keywords = {software product-line engineering, overloaded assets, challenges and opportunities},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/2814058.2814112,
author = {Lobato, Luanna Lopes and Bittar, Thiago Jabur},
title = {A Risk Management Approach for Software Product Line Engineering},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {TSoftware Product Line (SPL) Engineering is a software development paradigm that fosters systematic reuse. It is focused on improving software practices, leading companies to experience benefits, such as reduced time-to-market and effort, and higher quality for the products delivered to customers. However, establishing a SPL is neither a simple nor a cheap task, and may affect several aspects of a software company. Besides, it involves a range of risks that may hinder project success. These have to be managed accordingly, so as to minimize the likelihood of project failure. Despite the importance of Risk Management (RM) for SPL Engineering, little has been published in terms of suitable and structured practices to cope with that. This present paper reports an approach for RM in SPL Engineering, named RiPLERM (Rise Product Line Engineering and Risk Management). The approach presents activities to structure RM in SPL projects, The design of the RiPLE-RM approach elaborated on results from empirical investigations, and was proposed to facilitate the management and provide significant insights that can be used to avoid and solve risks.},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
pages = {331–338},
numpages = {8},
keywords = {Software Product Line Engineering, Software Process, Risk Management, Project management},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@inproceedings{10.1145/3336294.3336316,
author = {Tolvanen, Juha-Pekka and Kelly, Steven},
title = {How Domain-Specific Modeling Languages Address Variability in Product Line Development: Investigation of 23 Cases},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336316},
doi = {10.1145/3336294.3336316},
abstract = {Domain-Specific Modeling raises the level of abstraction beyond programming by specifying the solution directly with domain concepts. Within product lines domain-specific approaches are applied to specify variability and then generate final products together with commonality. Such automated product derivation is possible because both the modeling language and generator are made for a particular product line --- often inside a single company. In this paper we examine which kinds of reuse and product line approaches are applied in industry with domain-specific modeling. Our work is based on empirical analysis of 23 cases and the languages and models created there. The analysis reveals a wide variety and some commonalities in the size of languages and in the ways they apply reuse and product line approaches.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {155–163},
numpages = {9},
keywords = {product line variability, product derivation, domain-specific modeling, domain-specific language, code generation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3233027.3233048,
author = {Hayashi, Kengo and Aoyama, Mikio},
title = {A multiple product line development method based on variability structure analysis},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233048},
doi = {10.1145/3233027.3233048},
abstract = {This article proposes a multiple product line development method based on variability structure analysis. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires to meet both agility and extreme diversity, which is a big challenge. We developed a structural analysis method of variability for multiple product lines using an extended model of OVM (Orthogonal Variability Model). Together with the variability analysis method, we propose an agile application development method to refine development items according to variability dependency based on the analysis, and develop them incrementally. We applied the proposed method to the development of the multiple product lines of automotive software systems, and demonstrated to reduce the volatility of the test efforts and usage of the test environment, and higher velocity and better manageability of the value stream.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {160–169},
numpages = {10},
keywords = {variability analysis, software product line, multiple product lines, automotive software, agile development},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233038,
author = {Martinez, Jabier and T\"{e}rnava, Xhevahire and Ziadi, Tewfik},
title = {Software product line extraction from variability-rich systems: the robocode case study},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233038},
doi = {10.1145/3233027.3233038},
abstract = {The engineering of a Software Product Line (SPL), either by creating it from scratch or through the re-engineering of existing variants, it uses to be a project that spans several years with a high investment. It is often hard to analyse and quantify this investment, especially in the context of extractive SPL adoption when the related software variants are independently created by different developers following different system architectures and implementation conventions. This paper reports an experience on the creation of an SPL by re-engineering system variants implemented around an educational game called Robocode. The objective of this game is to program a bot (a battle tank) that battles against the bots of other developers. The world-wide Robocode community creates and maintains a large base of knowledge and implementations that are mainly organized in terms of features, although not presented as an SPL. Therefore, a group of master students analysed this variability-rich domain and extracted a Robocode SPL. We present the results of such extraction augmented with an analysis and a quantification regarding the spent time and effort. We believe that the results and the a-posteriori analysis can provide insights on global challenges on SPL adoption. We also provide all the elements to SPL educators to reproduce the teaching activity, and we make available this SPL to be used for any research purpose.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {132–142},
numpages = {11},
keywords = {software product lines, robocode, reverse-engineering, extractive software product line adoption, education},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3233027.3233032,
author = {Kr\"{o}her, Christian and Gerling, Lea and Schmid, Klaus},
title = {Identifying the intensity of variability changes in software product line evolution},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233032},
doi = {10.1145/3233027.3233032},
abstract = {The evolution of a Software Product Line (SPL) typically affects a variety of artifact types. The intensity (the frequency and the amount) in which developers change variability information in these different types of artifacts is currently unknown. In this paper, we present a fine-grained approach for the variability-centric extraction and analysis of changes to code, build, and variability model artifacts introduced by commits. This approach complements existing work that is typically based on a feature-perspective and, thus, abstracts from this level of detail. Further, it provides a detailed understanding of the intensity of changes affecting variability information in these types of artifacts. We apply our approach to the Linux kernel revealing that changes to variability information occur infrequently and only affect small parts of the analyzed artifacts. Further, we outline how these results may improve certain analysis and verification tasks during SPL evolution.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {54–64},
numpages = {11},
keywords = {variability changes, software product line evolution, intensity, evolution analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2811681.2811703,
author = {Tan, Lei and Lin, Yuqing},
title = {An Aspect-Oriented Feature Modelling Framework for Software Product Line Engineering},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811703},
doi = {10.1145/2811681.2811703},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that focusing on systematic software assets reuse. SPLE treats software products in the same application domains as a product family and developing various of assets could be reused in the product family. Feature modelling is a critical activity of SPLE, which developing the requirement model for product families and providing guidance for individual product implementation. In this paper, we discuss several drawbacks of current feature modelling and propose a solution which adopting aspect-oriented development ideas and approaches. The proposed framework is intended to better manage complex feature relationships, and enhance quality-aware feature modelling. We include a case study of a real-life experience to demonstrate the proposed approach.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {111–115},
numpages = {5},
keywords = {software product line engineering, feature modelling, aspectoriented},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/3233027.3233029,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Extracting software product line feature models from natural language specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233029},
doi = {10.1145/3233027.3233029},
abstract = {The specification of a family of software products may include documents written in natural language. Automatically extracting knowledge from these documents is a challenging problem that requires using Natural Language Processing (NLP) techniques. This knowledge can be formalized as a Feature Model (FM), a diagram capturing the key features and the relationships among them.In this paper, we first review previous works that have presented tools for extracting FMs from textual specifications and compare their strengths and limitations. Then, we propose a framework for feature and relationship extraction, which overcomes the identified limitations and is built upon state-of-the-art open-source NLP tools. This framework is evaluated against previous works using several case studies, showing improved results.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {43–53},
numpages = {11},
keywords = {software product line, requirements engineering, natural language processing, feature model extraction, NLTK},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.scico.2012.06.006,
author = {Ubayashi, Naoyasu and Nakajima, Shin and Hirayama, Masayuki},
title = {Context-dependent product line engineering with lightweight formal approaches},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.006},
doi = {10.1016/j.scico.2012.06.006},
abstract = {This paper proposes a new style of product line engineering methods. It focuses on constructing embedded systems that take into account the contexts such as the external physical environments. In current product line development projects, Feature Analysis is mainly conducted from the viewpoint of system configurations: how hardware and software components are configured to constitute a system. In most cases, contexts are not considered explicitly. As a result, unexpected and unfavorable behavior might emerge in a system if a developer does not recognize any possible conflicting combinations between the system and contexts. To deal with this problem, this paper provides the notion of a context-dependent product line, which is composed of the system and context lines. The former is obtained by analyzing a family of systems. The latter is obtained by analyzing features of contexts associated to the systems. The system and context lines contain reusable core assets. The configuration of selected system components and contexts can be formally checked at the specification level. In this paper, we show a development process that includes the creation of both product line assets as well as context assets.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2331–2346},
numpages = {16},
keywords = {Product line engineering, Formal methods, Context analysis}
}

@article{10.1007/s11219-020-09522-1,
author = {Bhushan, Megha and Negi, Arun and Samant, Piyush and Goel, Shivani and Kumar, Ajay},
title = {A classification and systematic review of product line feature model defects},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09522-1},
doi = {10.1007/s11219-020-09522-1},
abstract = {Product line (PL)-based development is a thriving research area to develop software-intensive systems. Feature models (FMs) facilitate derivation of valid products from a PL by managing commonalities and variabilities among software products. However, the researchers in academia as well as in the industries experience difficulties in quality assessment of FMs. The increasing complexity and size of FMs may lead to defects, which outweigh the benefits of PL. This paper provides a systematic literature review and key research issues related to the FM defects in PL. We derive a typology of FM defects according to their level of importance. The information on defects’ identification and explanations are provided with formalization. Further, corrective explanations are presented which incorporates various techniques used to fix defects with their implementation. This information would help software engineering community by enabling developers or modelers to find the types of defects and their causes and to choose an appropriate technique to fix defects in order to produce defect-free products from FMs, thereby enhancing the overall quality of PL-based development.},
journal = {Software Quality Journal},
month = dec,
pages = {1507–1550},
numpages = {44},
keywords = {Quality, Product line model, Defect, Software product line, Feature model}
}

@inproceedings{10.1007/978-3-030-29983-5_9,
author = {W\"{a}gemann, Tobias and Tavakoli Kolagari, Ramin and Schmid, Klaus},
title = {ADOOPLA - Combining Product-Line- and Product-Level Criteria in Multi-objective Optimization of Product Line Architectures},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29983-5_9},
doi = {10.1007/978-3-030-29983-5_9},
abstract = {Product lines of software-intensive systems have a great diversity of features and products, which leads to vast design spaces that are difficult to explore. In addition, finding optimal product line system architectures usually requires a consideration of several quality trade-offs at once, involving both product-level as well as product-line-wide criteria. This challenge cannot be solved manually for all but the smallest problems, and can therefore benefit from automated support. In this paper we propose ADOOPLA, a tool-supported approach for the optimization of product line system architectures. In contrast to existing approaches where product-level approaches only support product-level criteria and product-line oriented approaches only support product-line-wide criteria, our approach integrates criteria from both levels in the optimization of product line architectures. Further, the approach can handle multiple objectives at once, supporting the architect in exploring the multi-dimensional Pareto-front of a given problem. We describe the theoretical principles of the ADOOPLA approach and demonstrate its application to a simplified case study from the automotive domain.},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {126–142},
numpages = {17},
keywords = {Automotive, Variability modeling, Multiobjective, Architecture optimization, Design space exploration, Product line architectures},
location = {Paris, France}
}

@inproceedings{10.1007/978-3-030-61362-4_5,
author = {Damiani, Ferruccio and Lienhardt, Michael and Paolini, Luca},
title = {On Slicing Software Product Line Signatures},
year = {2020},
isbn = {978-3-030-61361-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61362-4_5},
doi = {10.1007/978-3-030-61362-4_5},
abstract = {A Software Product Line (SPL) is a family of similar programs (called variants) generated from a common artifact base. Variability in an SPL can be documented in terms of abstract description of functionalities (called features): a feature model (FM) identifies each variant by a set of features (called a product). Delta-orientation is a flexible approach to implement SPLs. An SPL Signature (SPLS) is a variability-aware Application Programming Interface (API), i.e., an SPL where each variant is the API of a program. In this paper we introduce and formalize, by abstracting from SPL implementation approaches, the notion of slice of an SPLS K for a set of features F (i.e., an SPLS obtained from by K by hiding the features that are not in F). Moreover, we formulate the challenge of defining an efficient algorithm that, given a delta-oriented SPLS K and a set of features F, sreturns a delta-oriented SPLS that is an slice of K for F. Thus paving the way for further research on devising such an algorithm. The proposed notions are formalized for SPLs of programs written in an imperative version of Featherweight Java.},
booktitle = {Leveraging Applications of Formal Methods, Verification and Validation: Verification Principles: 9th International Symposium on Leveraging Applications of Formal Methods, ISoLA 2020, Rhodes, Greece, October 20–30, 2020, Proceedings, Part I},
pages = {81–102},
numpages = {22},
location = {Rhodes, Greece}
}

@inproceedings{10.1145/3483899.3483909,
author = {Furtado, Viviane and OliveiraJr, Edson and Kalinowski, Marcos},
title = {Guidelines for Promoting Software Product Line Experiments},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483909},
doi = {10.1145/3483899.3483909},
abstract = {The importance of experimentation for Software Engineering research has been notably established in the last years. The software engineering community has discussed how to proper report and evaluate experiments using different approaches, such as quality criteria, scales, and checklists. Nevertheless, there are no guidelines to support researchers and practitioners active in straightforward software engineering research areas, as in Software Product Lines (SPL), at conducting experiments. We hypothesize that experimentation guidelines may aid such a specific area by providing advice and actual excerpts reflecting good practices of SPL experimentation, thus experimentally evolving this area. Therefore, the goal of this paper is to provide guidelines for properly reporting and promoting SPL experiments. We defined such guidelines based on well-known software engineering experiment reports, quality evaluation checklists, and data extracted from 211 SPL experiments identified in a systematic mapping study. We evaluated the guidelines with a qualitative study with SPL and experimentation experts applying open and axial coding procedures. The evaluation enabled us to improve the guidelines. The resulting guidelines contain specific advice to researchers active in SPL and provide examples taken from published SPL experiments. The experts’ positive points indicate that the proposed guidelines can aid SPL researchers and practitioners. Sharing the resulting guidelines could support conducting SPL experiments and allow further area evolution based on prospective experiment replications and reproductions from well-designed and reported experiments.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {31–40},
numpages = {10},
keywords = {SPL Experiments, Qualitative Study, Guidelines, Experiment Reporting and Sharing},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/3382026.3425769,
author = {Nieke, Michael and Sampaio, Gabriela and Th\"{u}m, Thomas and Seidl, Christoph and Teixeira, Leopoldo and Schaefer, Ina},
title = {GuyDance: Guiding Configuration Updates for Product-Line Evolution},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425769},
doi = {10.1145/3382026.3425769},
abstract = {A product line is an approach for systematically managing configuration options of customizable systems, usually by means of features. Products are generated by utilizing configurations consisting of selected features. Product-line evolution can lead to unintended changes to product behavior. We illustrate that updating configurations after product-line evolution requires decisions of both, domain engineers responsible for product-line evolution as well as application engineers responsible for configurations. The challenge is that domain and application engineers might not be able to talk to each other. We propose a formal foundation and a methodology that enables domain engineers to guide application engineers through configuration evolution by sharing knowledge on product-line evolution and by defining configuration update operations. As an effect, we enable knowledge transfer between those engineers without the need to talk to each other. We evaluate our method by providing formal proofs that show product behavior of configurations can be preserved for typical evolution scenarios.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {56–64},
numpages = {9},
keywords = {software product line, evolution, configuration},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461702.3462585,
author = {Belitz, Clara and Jiang, Lan and Bosch, Nigel},
title = {Automating Procedurally Fair Feature Selection in Machine Learning},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462585},
doi = {10.1145/3461702.3462585},
abstract = {In recent years, machine learning has become more common in everyday applications. Consequently, numerous studies have explored issues of unfairness against specific groups or individuals in the context of these applications. Much of the previous work on unfairness in machine learning has focused on the fairness of outcomes rather than process. We propose a feature selection method inspired by fair process (procedural fairness) in addition to fair outcome. Specifically, we introduce the notion of unfairness weight, which indicates how heavily to weight unfairness versus accuracy when measuring the marginal benefit of adding a new feature to a model. Our goal is to maintain accuracy while reducing unfairness, as defined by six common statistical definitions. We show that this approach demonstrably decreases unfairness as the unfairness weight is increased, for most combinations of metrics and classifiers used. A small subset of all the combinations of datasets (4), unfairness metrics (6), and classifiers (3), however, demonstrated relatively low unfairness initially. For these specific combinations, neither unfairness nor accuracy were affected as unfairness weight changed, demonstrating that this method does not reduce accuracy unless there is also an equivalent decrease in unfairness. We also show that this approach selects unfair features and sensitive features for the model less frequently as the unfairness weight increases. As such, this procedure is an effective approach to constructing classifiers that both reduce unfairness and are less likely to include unfair features in the modeling process.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {379–389},
numpages = {11},
keywords = {machine learning, feature selection, fairness, bias},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Software Product Line, Requirements Engineering, Natural Language Processing, Machine Learning, Feature Model Validation},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/2934466.2934471,
author = {Th\"{u}m, Thomas and Ribeiro, M\'{a}rcio and Schr\"{o}ter, Reimar and Siegmund, Janet and Dalton, Francisco},
title = {Product-line maintenance with emergent contract interfaces},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934471},
doi = {10.1145/2934466.2934471},
abstract = {A software product line evolves whenever one of its products need to evolve. Maintenance of preprocessor-based product lines is a difficult task, as changes to the code base may unintentionally influence the behavior of uninvolved products. Hence, developers should be supported during maintenance. We present emergent contract interfaces to make product-line development more efficient and less error-prone. The key idea is that for a given maintenance point (i.e., an assignment), we calculate (a) features in the source code that may be affected and (b) assertions based on contracts defined in the code base. By means of a controlled experiment, we provide empirical evidence regarding efficiency and error-avoidance with emergent contract interfaces.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {134–143},
numpages = {10},
keywords = {weakest precondition, software product lines, preprocessor variability, maintenance, evolution, design by contract},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3071178.3071261,
author = {Safdar, Safdar Aqeel and Lu, Hong and Yue, Tao and Ali, Shaukat},
title = {Mining cross product line rules with multi-objective search and machine learning},
year = {2017},
isbn = {9781450349208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3071178.3071261},
doi = {10.1145/3071178.3071261},
abstract = {Nowadays, an increasing number of systems are being developed by integrating products (belonging to different product lines) that communicate with each other through information networks. Cost-effectively supporting Product Line Engineering (PLE) and in particular enabling automation of configuration in PLE is a challenge. Capturing rules is the key for enabling automation of configuration. Product configuration has a direct impact on runtime interactions of communicating products. Such products might be within or across product lines and there usually don't exist explicitly specified rules constraining configurable parameter values of such products. Manually specifying such rules is tedious, time-consuming, and requires expert's knowledge of the domain and the product lines. To address this challenge, we propose an approach named as SBRM that combines multi-objective search with machine learning to mine rules. To evaluate the proposed approach, we performed a real case study of two communicating Video Conferencing Systems belonging to two different product lines. Results show that SBRM performed significantly better than Random Search in terms of fitness values, Hyper-Volume, and machine learning quality measurements. When comparing with rules mined with real data, SBRM performed significantly better in terms of Failed Precision (18%), Failed Recall (72%), and Failed F-measure (59%).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1319–1326},
numpages = {8},
keywords = {rule mining, product line, multi-objective search, machine learning, configuration},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.jss.2019.06.002,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Towards complex product line variability modelling: Mining relationships from non-boolean descriptions},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.002},
doi = {10.1016/j.jss.2019.06.002},
journal = {J. Syst. Softw.},
month = oct,
pages = {341–360},
numpages = {20},
keywords = {Pattern structures, Formal concept analysis, Extended feature models, Variability modelling, Reverse engineering, Complex software product line}
}

@inproceedings{10.1145/3461001.3471152,
author = {Silva, Publio and Bezerra, Carla I. M. and Machado, Ivan},
title = {A machine learning model to classify the feature model maintainability},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471152},
doi = {10.1145/3461001.3471152},
abstract = {Software Product Lines (SPL) are generally specified using a Feature Model (FM), an artifact designed in the early stages of the SPL development life cycle. This artifact can quickly become too complex, which makes it challenging to maintain an SPL. Therefore, it is essential to evaluate the artifact's maintainability continuously. The literature brings some approaches that evaluate FM maintainability through the aggregation of maintainability measures. Machine Learning (ML) models can be used to create these approaches. They can aggregate the values of independent variables into a single target data, also called a dependent variable. Besides, when using white-box ML models, it is possible to interpret and explain the ML model results. This work proposes white-box ML models intending to classify the FM maintainability based on 15 measures. To build the models, we performed the following steps: (i) we compared two approaches to evaluate the FM maintainability through a human-based oracle of FM maintainability classifications; (ii) we used the best approach to pre-classify the ML training dataset; (iii) we generated three ML models and compared them against classification accuracy, precision, recall, F1 and AUC-ROC; and, (iv) we used the best model to create a mechanism capable of providing improvement indicators to domain engineers. The best model used the decision tree algorithm that obtained accuracy, precision, and recall of 0.81, F1-Score of 0.79, and AUC-ROC of 0.91. Using this model, we could reduce the number of measures needed to evaluate the FM maintainability from 15 to 9 measures.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {35–45},
numpages = {11},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2934483,
author = {Richenhagen, Johannes and Rumpe, Bernhard and Schlo\ss{}er, Axel and Schulze, Christoph and Thissen, Kevin and von Wenckstern, Michael},
title = {Test-driven semantical similarity analysis for software product line extraction},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934483},
doi = {10.1145/2934466.2934483},
abstract = {Software product line engineering rests upon the assumption that a set of products share a common base of similar functionality. The correct identification of similarities between different products can be a time-intensive task. Hence, this paper proposes an automated semantical similarity analysis supporting software product line extraction and maintenance. Under the assumption of an already identified compatible interface, the degree of semantical similarity is identified based on provided test cases. Therefore, the analysis can also be applied in a test-driven development. This is done by translating available test sequences for both components into two I/O extended finite automata and performing an abstraction of the defined behavior until a simulation relation is established. The test-based approach avoids complexity issues regarding the state space explosion problem, a common issue in model checking. The proposed approach is applied on different variants and versions of industrially used software components provided by an automotive supplier to demonstrate the method's applicability.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {174–183},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3448016.3457295,
author = {Neutatz, Felix and Biessmann, Felix and Abedjan, Ziawasch},
title = {Enforcing Constraints for Machine Learning Systems via Declarative Feature Selection: An Experimental Study},
year = {2021},
isbn = {9781450383431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448016.3457295},
doi = {10.1145/3448016.3457295},
abstract = {Responsible usage of Machine Learning (ML) systems in practice does not only require enforcing high prediction quality, but also accounting for other constraints, such as fairness, privacy, or execution time. One way to address multiple user-specified constraints on ML systems is feature selection. Yet, optimizing feature selection strategies for multiple metrics is difficult to implement and has been underrepresented in previous experimental studies. Here, we propose Declarative Feature Selection (DFS) to simplify the design and validation of ML systems satisfying diverse user-specified constraints. We benchmark and evaluate a representative series of feature selection algorithms. From our extensive experimental results, we derive concrete suggestions on when to use which strategy and show that a meta-learning-driven optimizer can accurately predict the right strategy for an ML task at hand. These results demonstrate that feature selection can help to build ML systems that meet combinations of user-specified constraints, independent of the ML methods used.},
booktitle = {Proceedings of the 2021 International Conference on Management of Data},
pages = {1345–1358},
numpages = {14},
keywords = {robustness, privacy, meta learning, machine learning, feature selection, fairness, declarative ml, declarative machine learning, declarative feature selection, bias, DFS},
location = {Virtual Event, China},
series = {SIGMOD '21}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {software product line, reusability, performance, knowledge, interoperability, industrial analytics, extensibility, asset health},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/1629716.1629720,
author = {Chae, Wonseok and Blume, Matthias},
title = {Language support for feature-oriented product line engineering},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629720},
doi = {10.1145/1629716.1629720},
abstract = {Product line engineering is an emerging paradigm of developing a family of products. While product line analysis and design mainly focus on reasoning about commonality and variability of family members, product line implementation gives its attention to mechanisms of managing variability. In many cases, however, product line methods do not impose any specific synthesis mechanisms on product line implementation, so implementation details are left to developers. In our previous work, we adopted feature-oriented product line engineering to build a family of compilers and managed variations using the Standard ML module system. We demonstrated the applicability of this module system to product line implementation. Although we have benefited from the product line engineering paradigm, it mostly served us as a design paradigm to change the way we think about a set of closely related compilers, not to change the way we build them. The problem was that Standard ML did not fully realize this paradigm at the code level, which caused some difficulties when we were developing a set of compilers.In this paper, we address such issues with a language-based solution. MLPolyR is our choice of an implementation language. It supports three different programming styles. First, its first-class cases facilitate composable extensions at the expression levels. Second, its module language provides extensible and parameterized modules, which make large-scale extensible programming possible. Third, its macro system simplifies specification and composition of feature related code. We will show how the combination of these language features work together to facilitate the product line engineering paradigm.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {3–10},
numpages = {8},
keywords = {feature-oriented programming, product line engineering},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@inproceedings{10.1145/2362536.2362545,
author = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
title = {A survey on software product line testing},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362545},
doi = {10.1145/2362536.2362545},
abstract = {Software product line (SPL) testing consists of two separate but closely related test engineering activities: domain testing and application testing. Various software product line testing approaches have been developed over the last decade, and surveys have been conducted on them. However, thus far none of them deeply addressed the questions of what researches have been conducted in order to overcome the challenges posed by the two separate testing activities and their relationships. Thus, this paper surveys the current software product line testing approaches by defining a reference SPL testing processes and identifying, based on them, key research perspectives that are important in SPL testing. Through this survey, we identify the researches that addressed the challenges and also derive open research opportunities from each perspective.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {31–40},
numpages = {10},
keywords = {software testing, software product line testing, software product line engineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1007/978-3-642-34032-1_22,
author = {Ferrari, Alessio and Spagnolo, Giorgio Oronzo and Martelli, Giacomo and Menabeni, Simone},
title = {Product line engineering applied to CBTC systems development},
year = {2012},
isbn = {9783642340314},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34032-1_22},
doi = {10.1007/978-3-642-34032-1_22},
abstract = {Communications-based Train Control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology.This paper reports intermediate results in an effort aimed at defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The adopted methodology is discussed and a preliminary model is presented.},
booktitle = {Proceedings of the 5th International Conference on Leveraging Applications of Formal Methods, Verification and Validation: Applications and Case Studies - Volume Part II},
pages = {216–230},
numpages = {15},
location = {Heraklion, Crete, Greece},
series = {ISoLA'12}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Service-oriented architecture}
}

@article{10.1007/s10796-010-9230-8,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {A business maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-010-9230-8},
doi = {10.1007/s10796-010-9230-8},
abstract = {In the recent past, software product line engineering has become one of the most promising practices in software industry with the potential to substantially increase the software development productivity. Software product line engineering approach spans the dimensions of business, architecture, software engineering process and organization. The increasing popularity of software product line engineering in the software industry necessitates a process maturity evaluation methodology. Accordingly, this paper presents a business maturity model of software product line, which is a methodology to evaluate the current maturity of the business dimension of a software product line in an organization. This model examines the coordination between product line engineering and the business aspects of software product line. It evaluates the maturity of the business dimension of software product line as a function of how a set of business practices are aligned with product line engineering in an organization. Using the model presented in this paper, we conducted two case studies and reported the assessment results. This research contributes towards establishing a comprehensive and unified strategy for a process maturity evaluation of software product lines.},
journal = {Information Systems Frontiers},
month = sep,
pages = {543–560},
numpages = {18},
keywords = {Software product line, Software process model, Software process assessment, Organizational management, Maturity evaluation, Business process}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Software product line, Predictive analytics, Literature review, Industrial SPL tools, Inconsistencies, Automated feature selection, Artificial intelligence}
}

@article{10.1007/s11334-011-0159-y,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An architecture process maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-011-0159-y},
doi = {10.1007/s11334-011-0159-y},
abstract = {Software architecture has been a key research area in the software engineering community due to its significant role in creating high-quality software. The trend of developing product lines rather than single products has made the software product line a viable option in the industry. Software product line architecture (SPLA) is regarded as one of the crucial components in the product lines, since all of the resulting products share this common architecture. The increased popularity of software product lines demands a process maturity evaluation methodology. Consequently, this paper presents an architecture process maturity model for software product line engineering to evaluate the current maturity of the product line architecture development process in an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective of the questionnaires is to collect information about the SPLA development process. Thus, in general this work contributes towards the establishment of a comprehensive and unified strategy for the process maturity evaluation of software product line engineering. Furthermore, we conducted two case studies and reported the assessment results, which show the maturity of the architecture development process in two organizations.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {191–207},
numpages = {17},
keywords = {Software product line, Software architecture, Process assessment, Domain engineering, Application engineering}
}

@inproceedings{10.1145/2647908.2655967,
author = {Assun\c{c}\~{a}o, Wesley Klewerton Guez and Vergilio, Silvia Regina},
title = {Feature location for software product line migration: a mapping study},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655967},
doi = {10.1145/2647908.2655967},
abstract = {Developing software from scratch is a high cost and error-prone activity. A possible solution to reduce time-to-market and produce high quality software is the reuse of existing software. But when the number of features in the system grows, the maintenance becomes more complex. In such cases, to adopt a systematic approach, such as Software Product Line Engineering, is necessary. Existing systems are generally migrated to a product line, allowing systematic reuse of artefacts and easing maintenance. To this end, some approaches have been proposed in the literature in the last years. A mapping of works on this subject and the identification of some research gaps can lead to an improvement of such approaches. This paper describes the main outcomes of a systematic mapping study on the evolution and migration of systems to SPL. The main works found are presented and classified according to adopted strategy, artefacts used, and evaluation conducted. Analysis of the evolution along the past years are also presented. At the end, we summarize some trends and open issues to serve as reference to new researches.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {52–59},
numpages = {8},
keywords = {software product line, reuse, reengineering, evolution},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1016/j.infsof.2012.06.014,
author = {Andersson, Henric and Herzog, Erik and \"{O}Lvander, Johan},
title = {Experience from model and software reuse in aircraft simulator product line engineering},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.06.014},
doi = {10.1016/j.infsof.2012.06.014},
abstract = {Context: ''Reuse'' and ''Model Based Development'' are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft. Objective: The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method: The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results: A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics. Conclusion: The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {595–606},
numpages = {12},
keywords = {Software Product Line, SPL, PDM, Model Based Development, Knowledge Based Engineering, Configurator}
}

@article{10.1007/s11219-009-9088-5,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An organizational maturity model of software product line engineering},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-009-9088-5},
doi = {10.1007/s11219-009-9088-5},
abstract = {Software product line engineering is an inter-disciplinary concept. It spans the dimensions of business, architecture, process, and the organization. Some of the potential benefits of this approach include cost reduction, improvements in product quality and a decrease in product development time. The increasing popularity of software product line engineering in the software industry necessitates a process maturity evaluation methodology. Accordingly, this paper presents an organizational maturity model of software product line engineering for evaluating the maturity of organizational dimension. The model assumes that organizational theories, behavior, and management play a critical role in the institutionalization of software product line engineering within an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective and design of the questionnaires are to collect information about the software product line engineering process from the dual perspectives of organizational behavior and management. Furthermore, we conducted two case studies and reported the assessment results using the organizational maturity model presented in this paper.},
journal = {Software Quality Journal},
month = jun,
pages = {195–225},
numpages = {31},
keywords = {Software process maturity, Software process improvement, Software process assessment, Software engineering, Software, Product Line}
}

@inproceedings{10.5555/2666064.2666079,
author = {Saratxaga, C. L. and Alonso-Montes, C. and Haugen, O. and Ekelin, C. and Mitschke, A.},
title = {Product line tool-chain: variability in critical systems},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Competitiveness has thrown industries towards adding more features to existent products increasing their inherent complexity. One of the main challenges is to define mechanisms and tools to control the propagation of the dependencies through the different engineering phases, keeping consistency among requirements and the final system design. SPL provide mechanisms to control the evolution and design of product families, based on an exhaustive variant analysis. However, the critical system industry does not adopt them due to the lack of tool support for the complete life-cycle. In this paper, a product line tool chain is presented based on the analysis of current SPL tools and approaches in order to fit the specific needs within industry partners in the CESAR project. The main goal is to show the benefits of a combination of SPL tools in an industrial scenario.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {57–60},
numpages = {4},
keywords = {variability management, product line tool chain, critical systems, SPL, PLUM, CVL, CESAR},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/3336294.3336303,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Galindo, Jos\'{e} A. and Ramos-Guti\'{e}rrez, Bel\'{e}n and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa and Benavides, David},
title = {Process Mining to Unleash Variability Management: Discovering Configuration Workflows Using Logs},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336303},
doi = {10.1145/3336294.3336303},
abstract = {Variability models are used to build configurators. Configurators are programs that guide users through the configuration process to reach a desired configuration that fulfils user requirements. The same variability model can be used to design different configurators employing different techniques. One of the elements that can change in a configurator is the configuration workflow, i.e., the order and sequence in which the different configuration elements are presented to the configuration stakeholders. When developing a configurator, a challenge is to decide the configuration workflow that better suites stakeholders according to previous configurations. For example, when configuring a Linux distribution, the configuration process start by choosing the network or the graphic card, and then other packages with respect to a given sequence. In this paper, we present COnfiguration workfLOw proceSS mIning (COLOSSI), an automated technique that given a set of logs of previous configurations and a variability model can automatically assist to determine the configuration workflow that better fits the configuration logs generated by user activities. The technique is based on process discovery, commonly used in the process mining area, with an adaptation to configuration contexts. Our proposal is validated using existing data from an ERP configuration environment showing its feasibility. Furthermore, we open the door to new applications of process mining techniques in different areas of software product line engineering.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {265–276},
numpages = {12},
keywords = {variability, process mining, process discovery, configuration workflow, clustering},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2362536.2362573,
author = {Bartholdt, J\"{o}rg and Becker, Detlef},
title = {Scope extension of an existing product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362573},
doi = {10.1145/2362536.2362573},
abstract = {At the beginning, creating a product line needs a well defined and narrow scope to meet short time to market demands. When established, there is a tendency to broaden the scope and to cover more domains and products.We have undergone a scope extension of our medical diagnostic platform that was implemented while the platform and (existing) products were evolving. In this paper, we list best practices for the migration process and how to come to a sustainable solution without cannibalizing the existing platform and products.In particular, we describe our way of identification beneficial sub-domains using C/V analysis and give an example scenario with alignments in order to increase commonality. We explain the maturity considerations for deciding on reuse of existing implementations and a carve-out strategy to split existing assets into common modules and product-line specific extensions. Furthermore, we describe our best practices for making the scope extension sustainable in a long term, using various types of governance means. We briefly complement these experiences with further insights gained during execution of this endeavor.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {scope extension, hierarchical product-line, governance, C/V analysis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.5555/3546258.3546374,
author = {Hamer, Victor and Dupont, Pierre},
title = {An importance weighted feature selection stability measure},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Current feature selection methods, especially applied to high dimensional data, tend to suffer from instability since marginal modifications in the data may result in largely distinct selected feature sets. Such instability strongly limits a sound interpretation of the selected variables by domain experts. Defining an adequate stability measure is also a research question. In this work, we propose to incorporate into the stability measure the importances of the selected features in predictive models. Such feature importances are directly proportional to feature weights in a linear model. We also consider the generalization to a non-linear setting.We illustrate, theoretically and experimentally, that current stability measures are subject to undesirable behaviors, for example, when they are jointly optimized with predictive accuracy. Results on micro-array and mass-spectrometric data show that our novel stability measure corrects for overly optimistic stability estimates in such a bi-objective context, which leads to improved decision-making. It is also shown to be less prone to the underor over-estimation of the stability value in feature spaces with groups of highly correlated variables.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {116},
numpages = {57},
keywords = {feature importance, bioinformatics, bi-objective optimization, selection stability, feature selection}
}

@article{10.1016/j.jss.2018.02.021,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Change impact analysis for evolving configuration decisions in product line use case models},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.02.021},
doi = {10.1016/j.jss.2018.02.021},
journal = {J. Syst. Softw.},
month = may,
pages = {211–237},
numpages = {27},
keywords = {Incremental reconfiguration, Evolving decisions, Use case configurator, Use case driven development, Product line engineering, Change impact analysis}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3382025.3414976,
author = {Pereira, Juliana Alves and Martin, Hugo and Temple, Paul and Acher, Mathieu},
title = {Machine learning and configurable systems: a gentle introduction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414976},
doi = {10.1145/3382025.3414976},
abstract = {The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {40},
numpages = {1},
keywords = {software product lines, machine learning, configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2362536.2362569,
author = {Patzke, Thomas and Becker, Martin and Steffens, Michaela and Sierszecki, Krzysztof and Savolainen, Juha Erik and Fogdal, Thomas},
title = {Identifying improvement potential in evolving product line infrastructures: 3 case studies},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362569},
doi = {10.1145/2362536.2362569},
abstract = {Successful software products evolve continuously to meet the changing stakeholder requirements. For software product lines, an additional challenge is that variabilities, characteristics that vary among products, change as well over time. That challenge must be carefully tackled during the evolution of the product line infrastructure. This is a significant problem for many software development organizations, as practical guidelines on how to evolve core assets, and especially source code, are missing.This paper investigates how to achieve "good enough" variability management during the evolution of variation in software design and implementation assets. As a first contribution, we present a customizable goal-based approach which helps to identify improvement potential in existing core assets to ease evolution. To find concrete ways to improve the product line infrastructure, we list the typical symptoms of variability "code smells" and show how to refine them to root causes, questions, and finally to metrics that can be extracted from large code bases.As a second main contribution, we show how this method was applied to evaluate the reuse quality of three industrial embedded systems. These systems are implemented in C or C++ and use Conditional Compilation as the main variability mechanism. We also introduce the analysis and refactoring tool set that was used in the case studies and discuss the lessons learnt.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {239–248},
numpages = {10},
keywords = {variability code smells, product line code evolution, industrial case study, goal-based product line measurement, PuLSE-E},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3289402.3289504,
author = {Sebbaq, Hanane and Retbi, Asmaa and Idrissi, Mohammed Khalidi and Bennani, Samir},
title = {Software Product Line to overcome the variability issue in E-Learning: Systematic literature review},
year = {2018},
isbn = {9781450364621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3289402.3289504},
doi = {10.1145/3289402.3289504},
abstract = {The disparity of educational technologies, pedagogies and learning styles implies a problem of variability when modeling E-learning systems. Furthermore, the current learning context, which has become very open and heterogeneous, raises the problem of automating the modeling, development and maintenance of personalized E-learning systems based on various pedagogies. For its part, the "Software Product Line" is a paradigm that aims to produce product families based on the principles of reuse, configuration and derivation. The main purpose of this literature review is to explore the different potential applications of "SPL" in the E-learning domain to figure out the problem of variability. We will adopt a protocol for a systematic review of literature, after which we will draw up an analysis report.},
booktitle = {Proceedings of the 12th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {8},
keywords = {variety, systematic literature review, scale, heterogeneity, Variability, Software Product line, E-learning},
location = {Rabat, Morocco},
series = {SITA'18}
}

@inproceedings{10.1007/978-3-030-74251-5_16,
author = {da Silva, Luan V. M. and Cerri, Ricardo},
title = {Feature Selection for Hierarchical Multi-label Classification},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_16},
doi = {10.1007/978-3-030-74251-5_16},
abstract = {In this work we study how conventional feature selection methods can be applied to Hierarchical Multi-label Classification Problems. In Hierarchical Multi-label Classification, instances can belong to two or more classes (labels) simultaneously, where such classes are hierarchically structured. Feature selection plays an important role in Machine Learning classification tasks, once it can effectively reduce the dataset dimensionality by removing irrelevant and/or redundant features, improving classification accuracy. Although many relevant real-world problems are from the hierarchical and multi-label domains, the majority of the related researches address the feature selection task focusing on single-label problems. In many works, even when the proposal deals with multi-label problems, the classes are not associated with a hierarchical structure. Therefore, in this work we study how feature selection can be applied in the Hierarchical Multi-label Classification context. For this, we propose four hierarchical strategies combining the Binary Relevance (BR) and Label Powerset (LP) multi-label transformations with the attribute evaluators ReliefF (RF) and Information Gain (IG). We tested our strategies on 10 real-world datasets from the functional genomic field, commonly used in Hierarchical Multi-label Classification works. As main results, three of the four proposed strategies produced some relevant subsets of features, while keeping predictive performances in comparison to the use of the complete set of features.},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {196–208},
numpages = {13},
keywords = {Machine Learning, Hierarchical Multi-label Classification, Feature selection},
location = {Porto, Portugal}
}

@inproceedings{10.1145/2364412.2364451,
author = {Machado, Ivan do Carmo},
title = {Towards a reasoning framework for software product line testing},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364451},
doi = {10.1145/2364412.2364451},
abstract = {Testing can still be considered a bottleneck for software product line engineering. The variability implemented in the source artifacts increases its complexity. Due to its key role for product line quality, testing requires cost-effective practices, such as techniques for test selection should be produced to enable companies to experience the substantial production cost savings. In this paper, we present the outline of a Ph.D. research aimed at developing a reasoning framework to improve SPL testing practices. Based on multiple sources of evidence, the framework intends to provide testers with an automated reasoner for determining which techniques may be suitable for a given variability implementation mechanism, and how these should be employed in order to makes testing in a SPL a more effective and efficient practice. We plan to perform empirical evaluations in order to assess the proposal effectiveness.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {229–232},
numpages = {4},
keywords = {variability management, software testing, software product lines, fault models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3350768.3351993,
author = {Bindewald, Carlos Vinicius and Freire, Willian M. and Amaral, Aline M. M. Miotto and Colanzi, Thelma Elita},
title = {Towards the support of user preferences in search-based product line architecture design: an exploratory study},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351993},
doi = {10.1145/3350768.3351993},
abstract = {Software Product Lines (SPLs) is a reuse approach in which a family of products is generalized in a common architecture that can be adapted to different clients. The Product Line Architecture (PLA) is one of the most important artifacts of a SPL. PLA design requires great human effort as it involves several factors that are usually in conflict. To ease this task, PLA design can be formulated as an optimization problem with many factors, i.e, as a multi-objective optimization problem. In this context, the MOA4PLA approach was proposed to optimize PLA design using search algorithms and metrics specific to the context. This approach supported by OPLA-Tool has already been used in several works demonstrating its applicability. However, MOA4PLA does not take into account aspects that are subjective, such as the preferences of a particular Decision Maker (DM). To do so, this paper presents a proposal to incorporate the user preferences in the optimization process performed by MOA4PLA, through an interactive process in which the DM subjectively evaluates the solutions in processing time. Thus, the solutions generated can be better suited to the DM's needs or preferences. In order to allow the user interaction, modifications were made in MOA4PLA and implemented in the OPLA-Tool. Aiming at an initial validation of the proposal, an exploratory study was carried out, composed of two experiments: a qualitative and a quantitative. These experiments were realized with the participation of a software architect. Empirical results pointed out that the proposed interactive process enables the generation of PLAs that are in accordance with the architect's preferences. Another significant contribution are the lessons learned on how to improve the interactive process.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {387–396},
numpages = {10},
keywords = {Product Line Architecture, Multi-Objective Optimization, Human-computer interaction},
location = {Salvador, Brazil},
series = {SBES '19}
}

@inproceedings{10.1007/978-3-030-46150-8_21,
author = {Doquet, Guillaume and Sebag, Mich\`{e}le},
title = {Agnostic Feature Selection},
year = {2019},
isbn = {978-3-030-46149-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-46150-8_21},
doi = {10.1007/978-3-030-46150-8_21},
abstract = {Unsupervised feature selection is mostly assessed along a supervised learning setting, depending on whether the selected features efficiently permit to predict the (unknown) target variable. Another setting is proposed in this paper: the selected features aim to efficiently recover the whole dataset. The proposed algorithm, called AgnoS, combines an AutoEncoder with structural regularizations to sidestep the combinatorial optimization problem at the core of feature selection. The extensive experimental validation of AgnoS on the scikit-feature benchmark suite demonstrates its ability compared to the state of the art, both in terms of supervised learning and data compression.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W\"{u}rzburg, Germany, September 16–20, 2019, Proceedings, Part I},
pages = {343–358},
numpages = {16},
keywords = {Interpretable models, Feature selection, Clustering and unsupervised learning},
location = {W\"{u}rzburg, Germany}
}

@article{10.1007/s10270-017-0614-9,
author = {Guizzo, Giovani and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Applying design patterns in the search-based optimization of software product line architectures},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0614-9},
doi = {10.1007/s10270-017-0614-9},
abstract = {The design of the product line architecture (PLA) is a difficult activity that can benefit from the application of design patterns and from the use of a search-based optimization approach, which is generally guided by different objectives related, for instance, to cohesion, coupling and PLA extensibility. The use of design patterns for PLAs is a recent research field, not completely explored yet. Some works apply the patterns manually and for a specific domain. Approaches to search-based PLA design do not consider the usage of these patterns. To allow such use, this paper introduces a mutation operator named “Pattern-Driven Mutation Operator” that includes methods to automatically identify suitable scopes and apply the patterns Strategy, Bridge and Mediator with the search-based approach multi-objective optimization approach for PLA. A metamodel is proposed to represent and identify suitable scopes to receive each one of the patterns, avoiding the introduction of architectural anomalies. Empirical results are also presented, showing evidences that the use of the proposed operator produces a greater diversity of solutions and improves the quality of the PLAs obtained in the search-based optimization process, regarding the values of software metrics.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1487–1512},
numpages = {26},
keywords = {Software product line architecture, Search-based software engineering, Design pattern}
}

@article{10.1007/s10664-012-9234-8,
author = {Reinhartz-Berger, Iris and Sturm, Arnon},
title = {Comprehensibility of UML-based software product line specifications},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9234-8},
doi = {10.1007/s10664-012-9234-8},
abstract = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {678–713},
numpages = {36},
keywords = {Variability management, UML, Software product line engineering, Empirical evaluation, Domain models}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {software certification, safety-critical embedded systems, development process},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@inproceedings{10.1145/2384716.2384733,
author = {Asaithambi, Suriya Priya R. and Jarzabek, Stan},
title = {Generic adaptable test cases for software product line testing: software product line},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384733},
doi = {10.1145/2384716.2384733},
abstract = {This research study is about constructing "generic adaptable test cases" to counter test case libraries explosion problem. Our work focuses on effort reduction via systematic reuse of generic test assets by taking advantage of common aspects and predicted variability in test cases. We envision that the proposed approach to organizing test case libraries will be particularly useful in the context of Software Product Line Testing (SPLT). By exploring strategies for generic test cases, I hope to address problems of domain-level testing. Our work will investigate existing testing (SPLT) practices in variability management context by conducting empirical studies. We plan to synthesize principles for "generic test case" design, identify gaps between required and exiting techniques, and finally propose new approach for generic adaptive test case construction.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {33–36},
numpages = {4},
keywords = {software product line testing},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@inproceedings{10.1145/3432291.3432308,
author = {Portier, Willy K. and Li, Yujian and Kouassi, Bonzou A.},
title = {Feature Selection using Machine Learning Techniques Based on Search Engine Parameters},
year = {2020},
isbn = {9781450375733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3432291.3432308},
doi = {10.1145/3432291.3432308},
abstract = {In the last two decades, Internet visibility became mandatory for any companies wishing to get exposure and get revenues. Among many ways to be visible on the Internet, one of the most important is to be on top of search engines' results for keywords relative to companies' business. It is the art of Search Engine Optimization (SEO), which is a collection of techniques to get more traffic from a search engine. More a website is SEO optimized, thus more search engines give it a high ranking on results' pages for a maximal exposure. So, Google, with 90% market share worldwide, is the main search engine outside of China (Baidu) and Russia (Yandex), and its algorithm is like a black box all marketers want to discover. Google claims to have more than 200 features in his algorithm made to rank results for queries among billions of pages. This article tries different machine learning methods to determine the most important parameters using a selection of 30 features in a dataset made with around 28,000 observations. A binary classification approach was done to detect if a keyword can be found or not in Top10 search engine result. During the simulation, the importance of features was determined to find the most important parameters used for building related search results. According to the research result, it leads that there are three kinds of parameters which influence the process of ranking the results on search engine Google for web pages: editorial features, notoriety features and technical features. Moreover, few features with minimum importance were found, for example, the low importance of using "https" protocol in a web resource.},
booktitle = {Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning},
pages = {28–34},
numpages = {7},
keywords = {machine learning, features selection, Site ranking, Search Engine Optimization},
location = {Beijing, China},
series = {SPML '20}
}

@article{10.4018/ijkss.2014100101,
author = {Tekinerdogan, Bedir and Erdo\u{g}an, \"{O}zg\"{u} \"{O}zk\"{o}se and Aktu\u{g}, Onur},
title = {Supporting Incremental Product Development using Multiple Product Line Architecture},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100101},
doi = {10.4018/ijkss.2014100101},
abstract = {Software product line engineering SPLE has been successfully applied in various application domains to support systematic reuse. Besides of its benefits it is also acknowledged that the SPLE process can in practice be considered too time consuming and heavyweight due to the required planning and development of the asset base. For this reason more lightweight SPLE processes are required that can be integrated in the ongoing product development of the organization. In this context, the authors share their experiences in adopting a multiple product line architecture to support the incremental product development of Aselsan REHIS, a leading high technology company in Turkey. The authors first discuss the important business needs for defining a more lightweight multiple product line engineering MPLE process. Then they discuss the multiple product line architecture and how it has been used to guide the incremental product development.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {1–16},
numpages = {16},
keywords = {Software architecture design, Multiple Product Line Engineering, Introduction, Industrial Case, Incremental Process}
}

@inproceedings{10.1145/3336294.3342383,
author = {Martin, Hugo and Pereira, Juliana Alves and Acher, Mathieu and Temple, Paul},
title = {Machine Learning and Configurable Systems: A Gentle Introduction},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3342383},
doi = {10.1145/3336294.3342383},
abstract = {The goal of this tutorial is to give an introduction to how machine learning can be used to support activities related to the engineering of configurable systems and software product lines. To the best of our knowledge, this is the first practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance prediction) on real-world systems (VaryLaTeX, x264).},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {325–326},
numpages = {2},
keywords = {software product lines, machine learning, configurable systems},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11042-021-10567-y,
author = {Rahman, Md Arafatur and Asyhari, A. Taufiq and Wen, Ong Wei and Ajra, Husnul and Ahmed, Yussuf and Anwar, Farhat},
title = {Effective combining of feature selection techniques for machine learning-enabled IoT intrusion detection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10567-y},
doi = {10.1007/s11042-021-10567-y},
abstract = {The rapid advancement of technologies has enabled businesses to carryout their activities seamlessly and revolutionised communications across the globe. There is a significant growth in the amount and complexity of Internet of Things devices that are deployed in a wider range of environments. These devices mostly communicate through Wi-Fi networks and particularly in smart environments. Besides the benefits, these devices also introduce security challenges. In this paper, we investigate and leverage effective feature selection techniques to improve intrusion detection using machine learning methods. The proposed approach is based on a centralised intrusion detection system, which uses the deep feature abstraction, feature selection and classification to train the model for detecting the malicious and anomalous actions in the traffic. The deep feature abstraction uses deep learning techniques of artificial neural network in the form of unsupervised autoencoder to construct more features for the traffic. Based on the availability of cumulative features, the system then employs a variety of wrapper-based feature selection techniques ranging from SVM and decision tree to Naive Bayes for selecting high-ranked features, which are then combined and fed into an artificial neural network classifier for distinguishing attack and normal behaviors. The experimental results reveal the effectiveness of the proposed method on Aegean Wi-Fi Intrusion Dataset, which achieves high detection accuracy of up to 99.95%, relatively competitive to the existing machine learning works for the same dataset.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {31381–31399},
numpages = {19},
keywords = {Wi-Fi, Internet of things, Impersonation attack, Feature selection, Deep learning, Centralized intrusion detection, Attack classification}
}

@inproceedings{10.1007/978-3-319-91253-0_58,
author = {Ghosh, Indrajit},
title = {Probabilistic Feature Selection in Machine Learning},
year = {2018},
isbn = {978-3-319-91252-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-91253-0_58},
doi = {10.1007/978-3-319-91253-0_58},
abstract = {In machine learning, Case Based Reasoning is a prominent technique for harvesting knowledge from past experiences. The past experiences are represented in the form of a repository of cases having a set of features. But each feature may not have the equal relevancy in describing a case. Measuring the relevancy of each feature is always a prime issue. A subset of relevant features describes a case with adequate accuracy. An appropriate subset of relevant features should be selected for improving the performance of the system and to reduce dimensionality. In case based domain, feature selection is a process of selecting an appropriate subset of relevant features. There are various real domains which are inherently case based and features are expressed in terms of linguistic variables. To assign a numerical weight to each linguistic feature, a lot of feature subset selection algorithms have been proposed. But the weighting values are usually determined using subjective judgement or a trial and error basis.This work presents an alternative concept in this direction. It can be efficiently applied to select the relevant linguistic features by measuring the probability in term of numerical values. It can also rule out irrelevant and noisy features. Applications of this approach in various real world domain show an excellent performance.},
booktitle = {Artificial Intelligence and Soft Computing: 17th International Conference, ICAISC 2018, Zakopane, Poland, June 3-7, 2018, Proceedings, Part I},
pages = {623–632},
numpages = {10},
keywords = {Probabilistic feature selection, Machine learning, Case Based Reasoning},
location = {Zakopane, Poland}
}

@inproceedings{10.1007/978-3-030-89847-2_7,
author = {Zhang, Winston and Turkestani, Najla Al and Bianchi, Jonas and Le, Celia and Deleat-Besson, Romain and Ruellas, Antonio and Cevidanes, Lucia and Yatabe, Marilia and Gon\c{c}alves, Joao and Benavides, Erika and Soki, Fabiana and Prieto, Juan and Paniagua, Beatriz and Gryak, Jonathan and Najarian, Kayvan and Soroushmehr, Reza},
title = {Feature Selection for Privileged Modalities in Disease Classification},
year = {2021},
isbn = {978-3-030-89846-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89847-2_7},
doi = {10.1007/978-3-030-89847-2_7},
abstract = {Multimodal data allows supervised learning while considering multiple complementary views of a problem, improving final diagnostic performance of trained models. Data modalities that are missing or difficult to obtain in clinical situations can still be incorporated into model training using the learning using privileged information (LUPI) framework. However, noisy or redundant features in the privileged modality space can limit the amount of knowledge transferred to the diagnostic model during the LUPI learning process. We consider the problem of selecting desirable features from both standard features which are available during both model training and testing, and privileged features which are only available during model training. A novel filter feature selection method named NMIFS+ is introduced that considers redundancy between standard and privileged feature spaces. The algorithm is evaluated on two disease classification datasets with privileged modalities. Results demonstrate an improvement in diagnostic performance over comparable filter selection algorithms.},
booktitle = {Multimodal Learning for Clinical Decision Support: 11th International Workshop, ML-CDS 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings},
pages = {69–80},
numpages = {12},
keywords = {Clinical decision support, Multimodal data, Feature selection, Knowledge transfer, Mutual information, Privileged learning},
location = {Strasbourg, France}
}

@article{10.1007/s10270-015-0479-8,
author = {Devroey, Xavier and Perrouin, Gilles and Cordy, Maxime and Samih, Hamza and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Statistical prioritization for software product line testing: an experience report},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0479-8},
doi = {10.1007/s10270-015-0479-8},
abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {153–171},
numpages = {19},
keywords = {Statistical testing, Software product line testing, Prioritization, D.2.7, D.2.5}
}

@inproceedings{10.1145/2791060.2791105,
author = {Teixeira, Leopoldo and Alves, Vander and Borba, Paulo and Gheyi, Rohit},
title = {A product line of theories for reasoning about safe evolution of product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791105},
doi = {10.1145/2791060.2791105},
abstract = {A product line refinement theory formalizes safe evolution in terms of a refinement notion, which does not rely on particular languages for the elements that constitute a product line. Based on this theory, we can derive refinement templates to support safe evolution scenarios. To do so, we need to provide formalizations for particular languages, to specify and prove the templates. Without a systematic approach, this leads to many similar templates and thus repetitive verification tasks. We investigate and explore similarities between these concrete languages, which ultimately results in a product line of theories, where different languages correspond to features, and products correspond to theory instantiations. This also leads to specifying refinement templates at a higher abstraction level, which, in the long run, reduces the specification and proof effort, and also provides the benefits of reusing such templates for additional languages plugged into the theory. We use the Prototype Verification System to encode and prove soundness of the theories and their instantiations. Moreover, we also use the refinement theory to reason about safe evolution of the proposed product line of theories.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {161–170},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.dsp.2021.103175,
author = {Sheng, Chao and Song, Peng and Zhang, Weijian and Chen, Dongliang},
title = {Dual-graph regularized subspace learning based feature selection},
year = {2021},
issue_date = {Oct 2021},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {1051-2004},
url = {https://doi.org/10.1016/j.dsp.2021.103175},
doi = {10.1016/j.dsp.2021.103175},
journal = {Digit. Signal Process.},
month = oct,
numpages = {12},
keywords = {Unsupervised learning, Feature selection, Graph learning, Subspace learning, Dimension reduction}
}

@article{10.1016/j.eswa.2021.115191,
author = {Kamalov, Firuz},
title = {Orthogonal variance decomposition based feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115191},
doi = {10.1016/j.eswa.2021.115191},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {11},
keywords = {Data mining, Wrapper methods, Total sensitivity index, Sensitivity index, Sobol decomposition, Variance decomposition, Feature selection}
}

@inproceedings{10.1145/3318396.3318448,
author = {Babaagba, Kehinde Oluwatoyin and Adesanya, Samuel Olumide},
title = {A Study on the Effect of Feature Selection on Malware Analysis using Machine Learning},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318448},
doi = {10.1145/3318396.3318448},
abstract = {In this paper, the effect of feature selection in malware detection using machine learning techniques is studied. We employ supervised and unsupervised machine learning algorithms with and without feature selection. These include both classification and clustering algorithms. The algorithms are compared for effectiveness and efficiency using their predictive accuracy, among others, as performance metric. From the studies, we observe that the best detection rate was attained for supervised learning with feature selection. The supervised learning algorithm used was Multilayer Perceptron (MLP) algorithm. The analysis also reveals that our system can detect viruses from varying sources.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {51–55},
numpages = {5},
keywords = {Malware Detection, Feature Selection and Machine Learning},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@article{10.1016/j.jss.2019.110419,
author = {Jung, Pilsu and Kang, Sungwon and Lee, Jihyun},
title = {Automated code-based test selection for software product line regression testing},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110419},
doi = {10.1016/j.jss.2019.110419},
journal = {J. Syst. Softw.},
month = dec,
numpages = {19},
keywords = {Software evolution, Software maintenance, Regression test selection, Product lines testing}
}

@inproceedings{10.5555/1753235.1753257,
author = {Carbon, Ralf and Adam, Sebastian and Uchida, Takayuki},
title = {Towards a product line approach for office devices: facilitating customization of office devices at Ricoh Co. Ltd.},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Like in many other domains, customization is one of the major challenges for providers of office devices like Ricoh Co. Ltd. The specific challenge is the integration of office devices with the workflows and existing office infrastructures of customers. Hence, an approach to support customization of office devices has been developed by Ricoh and Fraunhofer IESE. The approach builds upon product line engineering and focuses on improving application engineering to support customization based on the workflows and office infrastructures of specific customers. The key ideas of the approach are flexibility concepts on architecture level that support recurring types of customizations and a requirements engineering process that is driven by the workflows of individual customers. In this paper, the approach developed in cooperation with Ricoh is presented. A case study illustrates the applicability of the concepts and the overall approach.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {151–160},
numpages = {10},
keywords = {application engineering, flexibility, product line architecture, product line engineering, service orientation},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1504/ijhpcn.2019.102355,
author = {Patil, Dharmaraj R. and Patil, Jayantrao B.},
title = {Malicious web pages detection using feature selection techniques and machine learning},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {4},
issn = {1740-0562},
url = {https://doi.org/10.1504/ijhpcn.2019.102355},
doi = {10.1504/ijhpcn.2019.102355},
abstract = {In recent years, researchers have provided significant solutions to detect malicious web pages, still there are many open issues. This paper proposes a methodology for the effective detection of malicious web pages using feature selection methods and machine learning. Our methodology consists of three modules: feature selection, training and classification. To evaluate our methodology, six feature selection methods and eight supervised machine learning classifiers are used. Experiments are performed on the balanced binary dataset. It is found that by using feature selection methods, the classifiers achieved significant detection accuracy of 94-99% and above, error-rate of 0.19-5.55%, FPR of 0.006-0.094, FNR of 0.000-0.013 with minimum system overhead. Our multi-model system using majority voting classifier and wrapper+Naive Bayes feature selection method with GreedyStepwise search technique using only 15 features achieved a highest accuracy of 99.15%, FPR of 0.017 and FNR of 0.000.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jan,
pages = {473–488},
numpages = {15},
keywords = {cyber security, web security, multi-model system, supervised learning, machine learning, feature selection, malicious web pages}
}

@article{10.1007/s10489-021-02257-x,
author = {Xu, Ruohao and Li, Mengmeng and Yang, Zhongliang and Yang, Lifang and Qiao, Kangjia and Shang, Zhigang},
title = {Dynamic feature selection algorithm based on Q-learning mechanism},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02257-x},
doi = {10.1007/s10489-021-02257-x},
abstract = {Feature selection is a technique to improve the classification accuracy of classifiers and a convenient data visualization method. As an incremental, task oriented, and model-free learning algorithm, Q-learning is suitable for feature selection, this study proposes a dynamic feature selection algorithm, which combines feature selection and Q-learning into a framework. First, the Q-learning is used to construct the discriminant functions for each class of the data. Next, the feature ranking is achieved according to the all discrimination functions vectors for each class of the data comprehensively, and the feature ranking is doing during the process of updating discriminant function vectors. Finally, experiments are designed to compare the performance of the proposed algorithm with four feature selection algorithms, the experimental results on the benchmark data set verify the effectiveness of the proposed algorithm, the classification performance of the proposed algorithm is better than the other feature selection algorithms, meanwhile the proposed algorithm also has good performance in removing the redundant features, and the experiments of the effect of learning rates on the our algorithm demonstrate that the selection of parameters in our algorithm is very simple.},
journal = {Applied Intelligence},
month = oct,
pages = {7233–7244},
numpages = {12},
keywords = {Markov decision process, Dynamic, Q-learning, Feature selection}
}

@article{10.1016/j.asoc.2021.107745,
author = {Moldovan, Dorin and Slowik, Adam},
title = {Energy consumption prediction of appliances using machine learning and multi-objective binary grey wolf optimization for feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107745},
doi = {10.1016/j.asoc.2021.107745},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {23},
keywords = {MOORA, Energy consumption prediction of appliances, Feature selection, Multi-objective, Grey wolf optimizer}
}

@inproceedings{10.1145/3266237.3266275,
author = {Filho, Helson Luiz Jakubovski and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Multiple objective test set selection for software product line testing: evaluating different preference-based algorithms},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266275},
doi = {10.1145/3266237.3266275},
abstract = {The selection of optimal test sets for Software Product Lines (SPLs) is a complex task impacted by many factors and that needs to consider the tester's preferences. To help in this task, Preference-based Evolutionary Multi-objective Algorithms (PEMOAs) have been explored. They use a Reference Point (RP), which represents the user preference and guides the search, resulting in a greater number of solutions in the ROI (Region of Interest). This region contains solutions that are more interesting from the tester's point of view. However, the explored PEMOAs have not been compared yet and the results reported in the literature do not consider many-objective formulations. Such an evaluation is important because in the presence of more than three objectives the performance of the algorithms may change and the number of solutions increases. Considering this fact, this work presents evaluation results of four PEMOAs for selection of products in the SPL testing considering cost, testing criteria coverage, products similarity, and the number of revealed faults, given by the mutation score. The PEMOAs present better performance than traditional algorithms, avoiding uninteresting solutions. We introduce a hyper-heuristic version of the PEMOA R-NSGA-II that presents the best results in a general case.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {162–171},
numpages = {10},
keywords = {software product line testing, search-based software engineering, preference-based multi-objective algorithms},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Software product line, Refactoring, Reengineering, Legacy system, Evolution}
}

@inproceedings{10.1145/3357765.3359515,
author = {Hinterreiter, Daniel and Nieke, Michael and Linsbauer, Lukas and Seidl, Christoph and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {Harmonized temporal feature modeling to uniformly perform, track, analyze, and replay software product line evolution},
year = {2019},
isbn = {9781450369800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357765.3359515},
doi = {10.1145/3357765.3359515},
abstract = {A feature model (FM) describes commonalities and variability within a software product line (SPL) and represents the configuration options at one point in time. A temporal feature model (TFM) additionally represents FM evolution, e.g., the change history or the planning of future releases. The increasing number of different TFM notations hampers research collaborations due to a lack of interoperability regarding notations, editors, and analyses. We present a common API for TFMs, which provides the core of a TFM ecosystem, to harmonize notations. We identified the requirements for the API based on systematically classifying and comparing the capabilities of existing TFM approaches. Our approach allows to work seamlessly with different TFM notations to perform, track, analyze and replay evolution. Our evaluation investigates two research questions on the expressiveness (RQ1) and utility (RQ2) of our approach by presenting implementations for several existing FM and TFM notations and replaying evolution histories from two case study systems.},
booktitle = {Proceedings of the 18th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {115–128},
numpages = {14},
keywords = {software product lines, evolution},
location = {Athens, Greece},
series = {GPCE 2019}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Software Product Lines, Recommenders, Product-Line Configuration, Personalized Recommendations},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@article{10.1016/j.eswa.2021.115365,
author = {Wang, Lianxi and Jiang, Shengyi and Jiang, Siyu},
title = {A feature selection method via analysis of relevance, redundancy, and interaction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115365},
doi = {10.1016/j.eswa.2021.115365},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {11},
keywords = {Feature relevance, Classification, Interaction information, Feature redundancy, Feature selection}
}

@article{10.1016/j.knosys.2019.105462,
author = {Liu, Yanfang and Ye, Dongyi and Li, Wenbin and Wang, Huihui and Gao, Yang},
title = {Robust neighborhood embedding for unsupervised feature selection},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105462},
doi = {10.1016/j.knosys.2019.105462},
journal = {Know.-Based Syst.},
month = apr,
numpages = {11},
keywords = {Manifold structure, Neighborhood embedding, Feature selection, Unsupervised learning, Machine learning}
}

@inproceedings{10.1145/3178298.3178300,
author = {Elmoniem, Mohamed A. Abd and Nasr, Eman S. and Gheith, Mervat H.},
title = {A Requirements Elicitation Tool for Cloud-Based ERP Software Product Line},
year = {2017},
isbn = {9781450355124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178298.3178300},
doi = {10.1145/3178298.3178300},
abstract = {Software Product Line (SPL) 1 is a very promising trend of software reusability. It could be applied in different fields in order to enhance and facilitate the software development process. In the last years, SPLs have broken into Enterprise Resource Planning (ERP) systems. An increasing need showed up for techniques, approaches and tools that combine SPL with ERP. Cloud ERP systems offer many benefits for Small and Medium Enterprises (SME). Managing the requirements elicitation process for Cloud ERP SPLs is a challenging process that faces many difficulties, such as the indirect interaction and the larger context of the target consumers. Facilitating the requirements elicitation process for cloud-based ERP SPLs by using automated tools will help to solve the faced difficulties. To the best of our knowledge, in the context of ERP SPLs, there are no tools for eliciting the requirements of cloud--based ERP SPLs nor even for ERP SPL. This paper exploits the advantages of the Form-Based Model (FBM) as a conceptual model to integrate it with cloud based ERP SPL. In addition, based on this integration, the paper presents a tool for eliciting the requirements of cloud-based ERP SPLs},
booktitle = {Proceedings of the 3rd Africa and Middle East Conference on Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {software product line, requirements engineering tool, requirements engineering, mapping functional requirements, form-based model, feature model, extended feature model, SaaS SPLs, SaaS ERP tool, SaaS ERP, ERP, Cloud SPLs},
location = {Cairo, Egypt},
series = {AMECSE '17}
}

@article{10.5555/3288339.3288348,
author = {Shafiq, Muhammad and Yu, Xiangzhan and Bashir, Ali Kashif and Chaudhry, Hassan Nazeer and Wang, Dawei},
title = {A machine learning approach for feature selection traffic classification using security analysis},
year = {2018},
issue_date = {Oct 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {10},
issn = {0920-8542},
abstract = {Class imbalance has become a big problem that leads to inaccurate traffic classification. Accurate traffic classification of traffic flows helps us in security monitoring, IP management, intrusion detection, etc. To address the traffic classification problem, in literature, machine learning (ML) approaches are widely used. Therefore, in this paper, we also proposed an ML-based hybrid feature selection algorithm named WMI_AUC that make use of two metrics: weighted mutual information (WMI) metric and area under ROC curve (AUC). These metrics select effective features from a traffic flow. However, in order to select robust features from the selected features, we proposed robust features selection algorithm. The proposed approach increases the accuracy of ML classifiers and helps in detecting malicious traffic. We evaluate our work using 11 well-known ML classifiers on the different network environment traces datasets. Experimental results showed that our algorithms achieve more than 95% flow accuracy results.},
journal = {J. Supercomput.},
month = oct,
pages = {4867–4892},
numpages = {26},
keywords = {Security, Network traffic classification, Machine learning, Feature selection, Class imbalance}
}

@article{10.1016/j.eswa.2021.114765,
author = {Odhiambo Omuya, Erick and Onyango Okeyo, George and Waema Kimwele, Michael},
title = {Feature Selection for Classification using Principal Component Analysis and Information Gain},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114765},
doi = {10.1016/j.eswa.2021.114765},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Principal component analysis, Information gain, Filter model, Dimensionality reduction, Classification, Feature selection}
}

@article{10.1007/s10664-014-9358-0,
author = {Koziolek, Heiko and Goldschmidt, Thomas and Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus},
title = {Assessing software product line potential: an exploratory industrial case study},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9358-0},
doi = {10.1007/s10664-014-9358-0},
abstract = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {411–448},
numpages = {38},
keywords = {Software product lines, Domain analysis, Business case}
}

@article{10.1016/j.infsof.2006.08.008,
author = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
title = {A framework for evaluating reusability of core asset in product line engineering},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.008},
doi = {10.1016/j.infsof.2006.08.008},
abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {740–760},
numpages = {21},
keywords = {Reusability, Quality model, Product line engineering, Metric, Core asset}
}

@article{10.1007/s10044-020-00916-2,
author = {Li, Zhang},
title = {A new feature selection using dynamic interaction},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-020-00916-2},
doi = {10.1007/s10044-020-00916-2},
abstract = {With the continuous development of Internet technology, data gradually present a complicated and high-dimensional trend. These high-dimensional data have a large number of redundant features and irrelevant features, which bring great challenges to the existing machine learning algorithms. Feature selection is one of the important research topics in the fields of machine learning, pattern recognition and data mining, and it is also an important means in the data preprocessing stage. Feature selection is to look for the optimal feature subset from the original feature set, which would improve the classification accuracy and reduce the machine learning time. The traditional feature selection algorithm tends to ignore the kind of feature which has a weak distinguishing capacity as a monomer, whereas the feature group’s distinguishing capacity is strong. Therefore, a new dynamic interaction feature selection (DIFS) algorithm is proposed in this paper. Initially, under the theoretical framework of interactive information, it redefines the relevance, irrelevance and redundancy of the features. Secondly, it offers the computational formulas for calculating interactive information. Finally, under the eleven data sets of UCI and three different classifiers, namely, KNN, SVM and C4.5, the DIFS algorithm increases the classification accuracy of the FullSet by 3.2848% and averagely decreases the number of features selected by 15.137. Hence, the DIFS algorithm can not only identify the relevance feature effectively, but also identify the irrelevant and redundant features. Moreover, it can effectively improve the classification accuracy of the data sets and reduce the feature dimensions of the data sets.},
journal = {Pattern Anal. Appl.},
month = feb,
pages = {203–215},
numpages = {13},
keywords = {Filter method, Feature redundancy, Feature relevance, Feature interaction, Feature selection}
}

@inproceedings{10.1145/2362536.2362570,
author = {Braga, Rosana T. V. and Trindade, Onofre and Branco, Kalinka R. L. J. Castelo and Lee, Jaejoon},
title = {Incorporating certification in feature modelling of an unmanned aerial vehicle product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362570},
doi = {10.1145/2362536.2362570},
abstract = {Safety critical systems (e.g., an avionics control system for safe flight) are often required to achieve certification under pre-established standards (e.g., DO-178B for software considerations in airborne systems and equipment certification). We have been working with our industrial partner for the last three years to develop product line assets for their avionics software product line (SPL) and, recently, we encountered two major challenges regarding certification. Firstly, an individual product must be certified, but each may require a different certification level: there might be variations in the certification requirements according to specific system usage contexts. Secondly, certification involves not only product but also process, as standards such as DO-178B also assess the quality of the development process. In this paper, we propose to include a certification view during feature modelling to provide a better understanding of the relationships between features and a certification level required for each product. The experience of introducing certification into the design model of an Unmanned Aerial Vehicle (UAV) SPL is presented to illustrate some key ideas. We also describe the lessons we have learned from this experience.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {249–258},
numpages = {10},
keywords = {software product lines, software certification, feature modelling, critical software development},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/1753235.1753250,
author = {Jepsen, Hans Peter and Beuche, Danilo},
title = {Running a software product line: standing still is going backwards},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Danfoss Drives - one of the largest producers of frequency converters in the world - has been doing Software Product Line development for its frequency converter products for about 3 years. This paper describes the approach used and the experiences with it. It discusses processes, ways to convince the unconvinced and arising tool issues when doing product line development.This paper is a follow-up on a previous article which described the product line migration process in detail.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {101–110},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2016.07.039,
author = {Shatnawi, Anas and Seriai, Abdelhak-Djamel and Sahraoui, Houari},
title = {Recovering software product line architecture of a family of object-oriented product variants},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {131},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.039},
doi = {10.1016/j.jss.2016.07.039},
abstract = {Automatic architecture recovery from a set of software product variants.Recovering variability concerning component, configuration, and dependencies.Experimented on two families of product variants. Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively.},
journal = {J. Syst. Softw.},
month = sep,
pages = {325–346},
numpages = {22},
keywords = {Software reuse, Software product line, Software component, Software architecture recovery, Object-oriented product variants, Formal concept analysis}
}

@article{10.1007/s00766-014-0203-1,
author = {D\'{\i}az, Jessica and P\'{e}rez, Jennifer and Garbajosa, Juan},
title = {A model for tracing variability from features to product-line architectures: a case study in smart grids},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0203-1},
doi = {10.1007/s00766-014-0203-1},
abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
journal = {Requir. Eng.},
month = sep,
pages = {323–343},
numpages = {21},
keywords = {Variability, Traceability modeling, Software product line engineering, Product-line architecture}
}

@inproceedings{10.1145/3422392.3422498,
author = {Freire, Willian Marques and Massago, Mamoru and Zavadski, Arthur Cattaneo and Malachini, Aline Maria and Amaral, Miotto and Colanzi, Thelma Elita},
title = {OPLA-Tool v2.0: a Tool for Product Line Architecture Design Optimization},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422498},
doi = {10.1145/3422392.3422498},
abstract = {The Multi-objective Optimization Approach for Product Line Architecture Design (MOA4PLA) is the seminal approach that successfully optimizes Product Line Architecture (PLA) design using search algorithms. The tool named OPLA-Tool was developed in order to automate the use of MOA4PLA. Over time, the customization of the tool to suit the needs of new research and application scenarios led to several problems. The main problems identified in the original version of OPLA-Tool are environment configuration, maintainability and usability problems, and PLA design modeling and visualization. Such problems motivated the development of a new version of this tool: OPLA-Tool v2.0, presented in this work. In this version, those problems were solved by the source code refactoring, migration to a web-based graphical user interface (GUI) and inclusion of a new support tool for PLA modeling and visualization. Furthermore, OPLA-Tool v2.0 has new functionalities, such as new objective functions, new search operators, intelligent interaction with users during the optimization process, multi-user authentication and simultaneous execution of several experiments to PLA optimization. Such a new version of OPLA-Tool is an important achievement to PLA design optimization as it provides an easier and more complete way to automate this task.},
booktitle = {Proceedings of the XXXIV Brazilian Symposium on Software Engineering},
pages = {818–823},
numpages = {6},
keywords = {Software product line, multi-objective evolutionary algorithms, product line architecture},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {software product line engineering, security, safety, mapping study, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s11042-020-10367-w,
author = {Mahindru, Arvind and Sangal, A.L.},
title = {FSDroid:- A feature selection technique to detect malware from Android using Machine Learning Techniques: FSDroid},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {9},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10367-w},
doi = {10.1007/s11042-020-10367-w},
abstract = {With the recognition of free apps, Android has become the most widely used smartphone operating system these days and it naturally invited cyber-criminals to build malware-infected apps that can steal vital information from these devices. The most critical problem is to detect malware-infected apps and keep them out of Google play store. The vulnerability lies in the underlying permission model of Android apps. Consequently, it has become the responsibility of the app developers to precisely specify the permissions which are going to be demanded by the apps during their installation and execution time. In this study, we examine the permission-induced risk which begins by giving unnecessary permissions to these Android apps. The experimental work done in this research paper includes the development of an effective malware detection system which helps to determine and investigate the detective influence of numerous well-known and broadly used set of features for malware detection. To select best features from our collected features data set we implement ten distinct feature selection approaches. Further, we developed the malware detection model by utilizing LSSVM (Least Square Support Vector Machine) learning approach connected through three distinct kernel functions i.e., linear, radial basis and polynomial. Experiments were performed by using 2,00,000 distinct Android apps. Empirical result reveals that the model build by utilizing LSSVM with RBF (i.e., radial basis kernel function) named as FSdroid is able to detect 98.8% of malware when compared to distinct anti-virus scanners and also achieved 3% higher detection rate when compared to different frameworks or approaches proposed in the literature.},
journal = {Multimedia Tools Appl.},
month = apr,
pages = {13271–13323},
numpages = {53},
keywords = {Cyber-security, Machine learning, Dynamic-analysis, Feature selection, Permissions based analysis, Intrusion-detection}
}

@article{10.1007/s11063-020-10216-9,
author = {Liu, Tong and Martin, Gaven},
title = {Joint Feature Selection with Dynamic Spectral Clustering},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10216-9},
doi = {10.1007/s11063-020-10216-9},
abstract = {Current clustering algorithms solved a few of the issues around clustering such as similarity measure learning, or the cluster number estimation. For instance, some clustering algorithms can learn the data similarity matrix, but to do so they need to know the cluster number beforehand. On the other hand, some clustering algorithms estimate the cluster number, but to do so they need the similarity matrix as an input. Real-world data often contains redundant features and outliers, which many algorithms are susceptive to. None of the current clustering algorithms are able to learn the data similarity measure and the cluster number simultaneously, and at the same time reduce the influence of outliers and redundant features. Here we propose a joint feature selection with dynamic spectral clustering (FSDS) algorithm that not only learns the cluster number k and data similarity measure simultaneously, but also employs the L2,1-norm to reduce the influence of outliers and redundant features. The optimal performance could be reached when all the separated stages are combined in a unified way. Experimental results on eight real-world benchmark datasets show that our FSDS clustering algorithm outperformed the comparison clustering algorithms in terms of two evaluation metrics for clustering algorithms including ACC and Purity.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1745–1763},
numpages = {19},
keywords = {Unsupervised learning, Similarity measure, Outlier reduction, Feature selection, Spectral clustering, K-means, Clustering}
}

@inproceedings{10.1007/978-3-030-61401-0_50,
author = {Lucas, Thiago Jos\'{e} and Tojeiro, Carlos Alexandre Carvalho and Pires, Rafael Gon\c{c}alves and da Costa, Kelton Augusto Pontara and Papa, Jo\~{a}o Paulo},
title = {Machine Learning for Web Intrusion Detection: A Comparative Analysis of Feature Selection Methods mRMR and PFI},
year = {2020},
isbn = {978-3-030-61400-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61401-0_50},
doi = {10.1007/978-3-030-61401-0_50},
abstract = {Select from the best features in a complex dataset that is a critical task for machine learning algorithms. This work presents a comparative analysis between two resource selection techniques: Minimum Redundancy Maximum Relevance (mRMR) and Permutation Feature Important (PFI). The application of PFI to the dataset in issue is unusual. The dataset used in the experiments is HTTP CSIC 2010, which shows great results with the mRMR observed in a related work
[22]. Our PFI tests resulted in a selection of features best suited for machine learning methods and the best results for an accuracy of 97% with logistic regression and Bayes Point Machine, 98% with Support Vector Machine, and 99.9% using an artificial neural network.},
booktitle = {Artificial Intelligence and Soft Computing: 19th International Conference, ICAISC 2020, Zakopane, Poland, October 12-14, 2020, Proceedings, Part I},
pages = {535–546},
numpages = {12},
keywords = {Feature selection, Machine learning, Intrusion detection},
location = {Zakopane, Poland}
}

@article{10.1016/j.procs.2016.09.369,
author = {Cai, Fuyu and Wang, Hao and Tang, Xiaoqin and Emmerich, Michael and Verbeek, Fons J.},
title = {Fuzzy Criteria in Multi-objective Feature Selection for Unsupervised Learning},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2016.09.369},
doi = {10.1016/j.procs.2016.09.369},
abstract = {Feature selection in which most informative variables are selected for model generation is an important step in pattern recognition. Here, one often tries to optimize multiple criteria such as discriminating power of the descriptor, performance of model and cardinality of a subset. In this paper we propose a fuzzy criterion in multi-objective unsupervised feature selection by applying the hybridized filter-wrapper approach (FC-MOFS). These formulations allow for an efficient way to pick features from a pool and to avoid misunderstanding of overlapping features via crisp clustered learning in a conventional multi-objective optimization procedure. Moreover, the optimization problem is solved by using non-dominated sorting genetic algorithm, type two (NSGA-II). The performance of the proposed approach is then examined on six benchmark datasets from multiple disciplines and different numbers of features. Systematic comparisons of the proposed method and representative non-fuzzified approaches are illustrated in this work. The experimental studies show a superior performance of the proposed approach in terms of accuracy and feasibility.},
journal = {Procedia Comput. Sci.},
month = dec,
pages = {51–58},
numpages = {8},
keywords = {unsupervised learning, multi-objective optimization, fuzzy criteria, feature selection}
}

@inproceedings{10.1145/2993412.3003392,
author = {Boss, Birgit and Tischer, Christian and Krishnan, Sreejith and Nutakki, Arun and Gopinath, Vinod},
title = {Setting up architectural SW health builds in a new product line generation},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003392},
doi = {10.1145/2993412.3003392},
abstract = {Setting up a new product line generation in a mature domain, typically does not start from scratch but takes into consideration the architecture and assets of the former product line generation. Being able to accommodate legacy and 3rd party code is one of the major product line qualities to be met. On the other side, product line qualities like reusability, maintainability and alterability, i.e. being able to cope up with a large amount of variability, with configurability and fast integratability are major drivers.While setting up a new product line generation and thus a new corresponding architecture, we this time focused on architectural software (SW) health and tracking of architectural metrics from the very beginning. Taking the definition of "architecture being a set of design decisions" [18] literally, we attempt to implement an architectural check for every design decision taken. Architectural design decisions in our understanding do not only - and even not mainly - deal with the definition of components and their interaction but with patterns and rules or anti-patterns. The rules and anti-patterns, "what not to do" or more often also "what not to do &lt;u&gt;any more&lt;/u&gt;", is even more important in setting up a new product line generation because developers are not only used to the old style of developing and the old architecture, but also still have to develop assets for both generations.In this article we describe selected architectural checks that we have implemented, the layered architecture check and the check for usage of obsolete services. Additionally we discuss selected architectural metrics: the coupling coefficient metrics and the instability metrics. In the summary and outlook we describe our experiences and still open topics in setting up architectural SW health checks for a large-scale product line.The real-world examples are taken from the domain of Engine Control Unit development at Robert Bosch GmbH.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {16},
numpages = {7},
keywords = {technical debt, software erosion, software architecture, product line development, embedded software, architectural technical debt, architectural checks},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1007/s11042-021-10725-2,
author = {Shrestha, Ujjwol and Alsadoon, Abeer and Prasad, P. W. C. and Al Aloussi, Sarmad and Alsadoon, Omar Hisham},
title = {Supervised machine learning for early predicting the sepsis patient: modified mean imputation and modified chi-square feature selection},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {13},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10725-2},
doi = {10.1007/s11042-021-10725-2},
abstract = {Sepsis is a typical and significant emergency in medical clinics comprehensively. A creative and possible instrument for identifying sepsis stays elusive. Supervised models can identify potential clinical factors and give a more accurate prediction than the existing benchmark rule-based tools. This research aims to increase the sensitivity to accurately predict the sepsis patient. The proposed system consists of the mean imputation and chi-square technique to replace the missing features and feature selection, respectively. All datasets are fed into the chi-square technique for feature selection by measuring how expectations compare to actual observed data. The essential missing data are then replaced using the mean-imputation method by calculating the mean value of the available data. Finally, the selected features are used as an input to the supervised machine learning model for the classification of sepsis patient. The results of accuracy and processing time are obtained by using different datasets. The results show that the proposed solution achieves better classification performance in different data scenarios and different review types. The proposed solution provides a classification accuracy of 97.67% against the current accuracy of 91.12% on average. It also provides a processing time of 29.1 milliseconds against the current processing time of 32.8 milliseconds on average. The proposed system is focused on the feature selection process that is involved in the machine learning model. Finally, this study solves the issue of model overfitting with supervised machine learning.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {20477–20500},
numpages = {24},
keywords = {Electronic health records, Sensitivity, Specificity, Supervised learning, Machine learning, Prediction, Sepsis}
}

@inproceedings{10.1145/2648511.2648541,
author = {Gregg, Susan P. and Scharadin, Rick and LeGore, Eric and Clements, Paul},
title = {Lessons from AEGIS: organizational and governance aspects of a major product line in a multi-program environment},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648541},
doi = {10.1145/2648511.2648541},
abstract = {This paper tells the story of the AEGIS Weapon System product line and how it evolved from a series of standalone software programs with no sharing into a true systems and software product line. The paper focuses on the strong internal and external governance of the product line. The need for strong governance is brought about by the strong role that the AEGIS customer community plays in oversight of design, development, and procurement. The paper recounts the product line's beginnings, and describes how the product line is operated today. Organizational issues, measurement issues, and governance issues are covered, along with a summary of important lessons learned about operating a product line in an environment of strong competing interests.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {264–273},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line governance, product line engineering, product derivation, product configurator, product baselines, product audit, hierarchical product lines, feature profiles, feature modeling, command and control, combat systems, bill-of-features, Navy, AEGIS},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2934466.2934472,
author = {Temple, Paul and Galindo, Jos\'{e} A. and Acher, Mathieu and J\'{e}z\'{e}quel, Jean-Marc},
title = {Using machine learning to infer constraints for product lines},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934472},
doi = {10.1145/2934466.2934472},
abstract = {Variability intensive systems may include several thousand features allowing for an enormous number of possible configurations, including wrong ones (e.g. the derived product does not compile). For years, engineers have been using constraints to a priori restrict the space of possible configurations, i.e. to exclude configurations that would violate these constraints. The challenge is to find the set of constraints that would be both precise (allow all correct configurations) and complete (never allow a wrong configuration with respect to some oracle). In this paper, we propose the use of a machine learning approach to infer such product-line constraints from an oracle that is able to assess whether a given product is correct. We propose to randomly generate products from the product line, keeping for each of them its resolution model. Then we classify these products according to the oracle, and use their resolution models to infer cross-tree constraints over the product-line. We validate our approach on a product-line video generator, using a simple computer vision algorithm as an oracle. We show that an interesting set of cross-tree constraint can be generated, with reasonable precision and recall.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {209–218},
numpages = {10},
keywords = {variability modeling, software testing, software product lines, machine learning, constraints and variability mining},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2019136.2019184,
author = {Holl, Gerald},
title = {Product line bundles to support product derivation in multi product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019184},
doi = {10.1145/2019136.2019184},
abstract = {A multi product line comprises several heterogeneous product lines that are part of a large-scale system. Typically, multi product lines (MPLs) cannot be managed centrally as the involved product lines are developed and evolved by multiple teams that often work independently. Support for modularity and the management of dependencies are thus essential in MPLs. We aim at developing a tool-supported approach for modularizing MPLs to facilitate product derivation in MPLs. We elicit requirements for MPL tool support based on a literature survey and by involving experts from our industry partners. Our approach is based on product line bundles which allow to organize and deploy product line models and specific tool support. We also support the distributed configuration of a MPL based on different types of dependencies between product lines. We will evaluate our research in the laboratory and industrial case studies. We will also conduct experiments to assess the usefulness of the developed tool features.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {41},
numpages = {6},
keywords = {tool support, product derivation, multi product lines},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.ins.2021.08.032,
author = {Sun, Lin and Wang, Tianxiang and Ding, Weiping and Xu, Jiucheng and Lin, Yaojin},
title = {Feature selection using Fisher score and multilabel neighborhood rough sets for multilabel classification},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {578},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.08.032},
doi = {10.1016/j.ins.2021.08.032},
journal = {Inf. Sci.},
month = nov,
pages = {887–912},
numpages = {26},
keywords = {Multilabel classification, Fisher Score, Neighborhood rough sets, Feature selection}
}

@article{10.1007/s10489-020-02141-0,
author = {Gan, Min and Zhang, Li},
title = {Iteratively local fisher score for feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02141-0},
doi = {10.1007/s10489-020-02141-0},
abstract = {In machine learning, feature selection is a kind of important dimension reduction techniques, which aims to choose features with the best discriminant ability to avoid the issue of curse of dimensionality for subsequent processing. As a supervised feature selection method, Fisher score (FS) provides a feature evaluation criterion and has been widely used. However, FS ignores the association between features by assessing all features independently and loses the local information for fully connecting within-class samples. In order to solve these issues, this paper proposes a novel feature evaluation criterion based on FS, named iteratively local Fisher score (ILFS). Compared with FS, the new criterion pays more attention to the local structure of data by using K nearest neighbours instead of all samples when calculating the scatters of within-class and between-class. In order to consider the relationship between features, we calculate local Fisher scores of feature subsets instead of scores of single features, and iteratively select the current optimal feature to achieve this idea like sequential forward selection (SFS). Experimental results on UCI and TEP data sets show that the improved algorithm performs well in classification activities compared with some other state-of-the-art methods.},
journal = {Applied Intelligence},
month = aug,
pages = {6167–6181},
numpages = {15},
keywords = {Iterative, Neighbourhood, Fisher score, Feature selection}
}

@inproceedings{10.1145/2430502.2430531,
author = {Wulf-Hadash, Ora and Reinhartz-Berger, Iris},
title = {Cross product line analysis},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430531},
doi = {10.1145/2430502.2430531},
abstract = {Due to increase in market competition and merger and acquisition of companies, different software product lines (SPLs) may exist under the same roof. These SPLs may be developed applying different domain analysis processes, but are likely not disjoint. Cross product line analysis aims to examine the common and variable aspects of different SPLs for improving maintenance and future development of related SPLs. Currently different SPL artifacts, or more accurately feature models, are compared, matched, and merged for supporting scalability, increasing modularity and reuse, synchronizing feature model versions, and modeling multiple SPLs for software supply chains. However, in all these cases the focus is on creating valid merged models from the input feature models. Furthermore, the terminology used in all the input feature models is assumed to be the same, namely similar features are named the same. As a result these methods cannot be simply applied to feature models that represent different SPLs. In this work we offer adapting similarity metrics and text clustering techniques in order to enable cross product line analysis. This way analysis of feature models that use different terminologies in the same domain can be done in order to improve the management of the involved SPLs. Preliminary results reveal that the suggested method helps systematically analyze the commonality and variability between related SPLs, potentially suggesting improvements to existing SPLs and to the maintenance of sets of SPLs.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {21},
numpages = {8},
keywords = {feature similarity, feature diagram merging, feature diagram matching, feature clustering, empirical evaluation},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.knosys.2021.107377,
author = {Wang, Zhenyu and Wang, Chenchen and Wei, Jinmao and Liu, Jian},
title = {Multi-class feature selection by exploring reliable class correlation▪},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {230},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107377},
doi = {10.1016/j.knosys.2021.107377},
journal = {Know.-Based Syst.},
month = oct,
numpages = {11},
keywords = {Dimension reduction feature space, Reliable class correlation, Feature selection, Multi-class learning}
}

@article{10.1016/j.patcog.2021.108058,
author = {Cui, Lixin and Bai, Lu and Wang, Yue and Yu, Philip S. and Hancock, Edwin R.},
title = {Fused lasso for feature selection using structural information},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108058},
doi = {10.1016/j.patcog.2021.108058},
journal = {Pattern Recogn.},
month = nov,
numpages = {14},
keywords = {Correlated feature group, Sparse learning, Graph-based feature selection, Fused lasso, Structural relationship, Feature selection}
}

@article{10.1016/j.asoc.2021.108006,
author = {Khoder, A. and Dornaika, F.},
title = {Ensemble learning via feature selection and multiple transformed subsets: Application to image classification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.108006},
doi = {10.1016/j.asoc.2021.108006},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {14},
keywords = {Class sparsity least square regression, Image classification, Feature selection, Machine learning, Multi-models, Feature subsets, Ensemble learning}
}

@article{10.1007/s10515-015-0185-3,
author = {B\"{u}rdek, Johannes and Kehrer, Timo and Lochau, Malte and Reuling, Dennis and Kelter, Udo and Sch\"{u}rr, Andy},
title = {Reasoning about product-line evolution using complex feature model differences},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0185-3},
doi = {10.1007/s10515-015-0185-3},
abstract = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets.},
journal = {Automated Software Engg.},
month = dec,
pages = {687–733},
numpages = {47},
keywords = {Software product lines, Software evolution, Model-driven engineering, Feature models}
}

@article{10.1016/j.patcog.2021.108149,
author = {Huang, Rui and Wu, Zhejun},
title = {Multi-label feature selection via manifold regularization and dependence maximization},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108149},
doi = {10.1016/j.patcog.2021.108149},
journal = {Pattern Recogn.},
month = dec,
numpages = {12},
keywords = {Dependence maximization, Manifold regularization, Sparse regression, Feature selection, Multi-label learning}
}

@article{10.1007/s10462-021-09970-6,
author = {Pintas, Julliano Trindade and Fernandes, Leandro A. F. and Garcia, Ana Cristina Bicharra},
title = {Feature selection methods for text classification: a systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {8},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-021-09970-6},
doi = {10.1007/s10462-021-09970-6},
abstract = {Feature Selection (FS) methods alleviate key problems in classification procedures as they are used to improve classification accuracy, reduce data dimensionality, and remove irrelevant data. FS methods have received a great deal of attention from the text classification community. However, only a few literature surveys include them focusing on text classification, and the ones available are either a superficial analysis or present a very small set of work in the subject. For this reason, we conducted a Systematic Literature Review (SLR) that asses 1376 unique papers from journals and conferences published in the past eight years (2013–2020). After abstract screening and full-text eligibility analysis, 175 studies were included in our SLR. Our contribution is twofold. We have considered several aspects of each proposed method and mapped them into a new categorization schema. Additionally, we mapped the main characteristics of the experiments, identifying which datasets, languages, machine learning algorithms, and validation methods have been used to evaluate new and existing techniques. By following the SLR protocol, we allow the replication of our revision process and minimize the chances of bias while classifying the included studies. By mapping issues and experiment settings, our SLR helps researchers to develop and position new studies with respect to the existing literature.},
journal = {Artif. Intell. Rev.},
month = dec,
pages = {6149–6200},
numpages = {52},
keywords = {Systematic literature review, Text classification, Dimensionality reduction, Feature selection}
}

@inproceedings{10.1007/978-3-030-86230-5_30,
author = {Silva, Lucia Emilia Soares and Machado, Vinicius Ponte and Araujo, Sidiney Souza and de Lima, Bruno Vicente Alves and Veras, Rodrigo de Melo Souza},
title = {Using Regression Error Analysis and Feature Selection to Automatic Cluster Labeling},
year = {2021},
isbn = {978-3-030-86229-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86230-5_30},
doi = {10.1007/978-3-030-86230-5_30},
abstract = {Cluster Labeling Models apply Artificial Intelligence techniques to extract the key features of clustered data to provide a tool for clustering interpretation. For this purpose, we applied different techniques such as Classification, Regression, Fuzzy Logic, and Data Discretization to identify essential attributes for cluster formation and the ranges of values associated with them. This paper presents an improvement to the Regression-based Cluster Labeling Model that integrates to the model an attribute selection step based on the coefficient of determination obtained by regression models in order to make its application possible in large datasets. The model was tested on the literature datasets Iris, Breast Cancer, and Parkinson’s Disease, evaluating the labeling performance of different dimensionality. The results obtained from the experiments showed that the model is sound, providing specific labels for each cluster representing between 99% and 100% of the elements of the clusters for the datasets used.},
booktitle = {Progress in Artificial Intelligence: 20th EPIA Conference on Artificial Intelligence, EPIA 2021, Virtual Event, September 7–9, 2021, Proceedings},
pages = {376–388},
numpages = {13},
keywords = {Dimensionality reduction, Feature selection, Unsupervised learning, Labeling, Cluster interpretation, Clustering}
}

@article{10.1016/j.asoc.2016.08.024,
author = {dos Santos Neto, Pedro de Alcntara and Britto, Ricardo and Rablo, Ricardo de Andrade Lira and Cruz, Jonathas Jivago de Almeida and Lira, Werney Ayala Luz},
title = {A hybrid approach to suggest software product line portfolios},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.024},
doi = {10.1016/j.asoc.2016.08.024},
abstract = {Graphical abstractDisplay Omitted HighlightsThe work proposes a hybrid approach to deal with the Product Portfolio Scope Problem.The approach is composed by a solution to deploy the feature relevance indicated by the customers into code assets of a SPL, based on a systematic method (SQFD).The approach includes a method to estimate the cost of an asset based on common and relevant measures related to source code, together with a fuzzy system to deal with the imprecision to set reference values.The work presents an application of an NSGA-II to search for products minimizing the cost and maximizing the relevance of the candidate products.The approach was evaluated using different scenarios, exploring the mains aspects related to method in the practice: size, granularity of features and products search space.The previous version of our hybrid approach was dependent on the employed technologies and algorithms. Herein we reformulate our approach, detaching it from any particular technique/algorithm.The data collection process associated with our approach was improved to facilitate the hybrid approach's usage and mitigate associated construct validity threats.A more comprehensive evaluation, which focused on show the real word usefulness and scalability of our hybrid approach. To validate the usefulness of our approach, it was used the SPL associated with a tool broadly employed in both industrial and academic contexts (ArgoUML-SPL). The scalability of our approach was evaluated using a synthetic SPL.All the experiments were based on the guidelines defined by Arcuri and Briand in order to evaluate the statistical significance of this kind of work. Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1243–1255},
numpages = {13},
keywords = {Software product lines, Search based software engineering, Search based feature model selection, Product portfolio scoping, NSGA-II, Fuzzy inference systems, Feature model selection problem}
}

@article{10.1287/deca.2013.0273,
author = {Xiong, Hui and Chen, Ying-Ju},
title = {Product Line Design with Deliberation Costs: A Two-Stage Process},
year = {2013},
issue_date = {September 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {10},
number = {3},
issn = {1545-8490},
url = {https://doi.org/10.1287/deca.2013.0273},
doi = {10.1287/deca.2013.0273},
abstract = {Motivated by the current practice of some service industries, we study a two-stage product line design problem in the presence of private consumer deliberation costs. In this problem, consumers pay an upfront payment before accessing the products. A consumer may incur a cost to introspect about her preferences, and the product line design enables the seller to encourage or discourage introspection. Getting consumers to deliberate about their preferences benefits them and the seller, because it facilitates a tailored product line. However, in so doing, the seller is forced to compensate the consumers for their cognitive cost of deliberation. We show that this compensation causes the seller an additional expense to avoid cannibalization across product lines. If the heterogeneity in deliberation costs is high, the seller is better off by offering a compromise product to consumers with high deliberation costs to discourage them from deliberation. The seller does not intentionally downward distort the qualities and optimally designs the product line that is ex-post socially efficient instead. This is because the seller is able to counterbalance the loss of second-stage information asymmetry by charging the upfront payment. Nonetheless, heterogeneity on consumer deliberation costs leads to an unambiguous reduction of consumer deliberation.},
journal = {Decision Analysis},
month = sep,
pages = {225–244},
numpages = {20},
keywords = {product line design, dynamic mechanism design, deliberation cost, adverse selection}
}

@article{10.1016/j.asoc.2021.107956,
author = {Liu, Wei and Wang, Jianyu},
title = {Recursive elimination–election algorithms for wrapper feature selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107956},
doi = {10.1016/j.asoc.2021.107956},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {13},
keywords = {High dimensionality, Recursion technique, Classification, Wrapper feature selection}
}

@article{10.3233/JIFS-211348,
author = {Shu, Lei and Huang, Kun and Jiang, Wenhao and Wu, Wenming and Liu, Hongling},
title = {Feature selection using autoencoders with Bayesian methods to high-dimensional data},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-211348},
doi = {10.3233/JIFS-211348},
abstract = {It is easy to lead to poor generalization in machine learning tasks using real-world data directly, since such data is usually high-dimensional dimensionality and limited. Through learning the low dimensional representations of high-dimensional data, feature selection can retain useful features for machine learning tasks. Using these useful features effectively trains machine learning models. Hence, it is a challenge for feature selection from high-dimensional data. To address this issue, in this paper, a hybrid approach consisted of an autoencoder and Bayesian methods is proposed for a novel feature selection. Firstly, Bayesian methods are embedded in the proposed autoencoder as a special hidden layer. This of doing is to increase the precision during selecting non-redundant features. Then, the other hidden layers of the autoencoder are used for non-redundant feature selection. Finally, compared with the mainstream approaches for feature selection, the proposed method outperforms them. We find that the way consisted of autoencoders and probabilistic correction methods is more meaningful than that of stacking architectures or adding constraints to autoencoders as regards feature selection. We also demonstrate that stacked autoencoders are more suitable for large-scale feature selection, however, sparse autoencoders are beneficial for a smaller number of feature selection. We indicate that the value of the proposed method provides a theoretical reference to analyze the optimality of feature selection.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7397–7406},
numpages = {10},
keywords = {high-dimensional data, feature selection, Bayesian method, Autoencoder}
}

@article{10.1007/s00521-020-05529-8,
author = {Ma, Zhengjing and Mei, Gang and Piccialli, Francesco},
title = {Machine learning for landslides prevention: a survey},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05529-8},
doi = {10.1007/s00521-020-05529-8},
abstract = {Landslides are one of the most critical categories of natural disasters worldwide and induce severely destructive outcomes to human life and the overall economic system. To reduce its negative effects, landslides prevention has become an urgent task, which includes investigating landslide-related information and predicting potential landslides. Machine learning is a state-of-the-art analytics tool that has been widely used in landslides prevention. This paper presents a comprehensive survey of relevant research on machine learning applied in landslides prevention, mainly focusing on (1) landslides detection based on images, (2) landslides susceptibility assessment, and (3) the development of landslide warning systems. Moreover, this paper discusses the current challenges and potential opportunities in the application of machine learning algorithms for landslides prevention.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {10881–10907},
numpages = {27},
keywords = {Deep learning, Unsupervised learning, Supervised learning, Machine learning, Landslides prevention, Natural disasters}
}

@article{10.1016/j.ins.2019.01.064,
author = {Chiew, Kang Leng and Tan, Choon Lin and Wong, KokSheik and Yong, Kelvin S.C. and Tiong, Wei King},
title = {A new hybrid ensemble feature selection framework for machine learning-based phishing detection system},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {484},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.064},
doi = {10.1016/j.ins.2019.01.064},
journal = {Inf. Sci.},
month = may,
pages = {153–166},
numpages = {14},
keywords = {Phishing dataset, Classification, Ensemble-based, Machine learning, Feature selection, Phishing detection}
}

@inproceedings{10.1145/2556624.2556643,
author = {Fenske, Wolfram and Th\"{u}m, Thomas and Saake, Gunter},
title = {A taxonomy of software product line reengineering},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556643},
doi = {10.1145/2556624.2556643},
abstract = {In the context of single software systems, refactoring is commonly accepted to be the process of restructuring an existing body of code in order to improve its internal structure without changing its external behavior. This process is vital to the maintenance and evolution of software systems.Software product line engineering is a paradigm for the construction and customization of large-scale software systems. As systems grow in complexity and size, maintaining a clean structure becomes arguably more important. However, product line literature uses the term "refactoring" for such a wide range of reengineering activities that it has become difficult to see how these activities pertain to maintenance and evolution and how they are related.We improve this situation in the following way: i) We identify the dimensions along which product line reengineering occurs. ii) We derive a taxonomy that distinguishes and relates these reengineering activities. iii) We propose definitions for the three main branches of this taxonomy. iv) We classify a corpus of existing work.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {4},
numpages = {8},
keywords = {taxonomy, software product lines, refactoring, reengineering},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {machine learning, feature, Software vulnerability prediction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1016/j.knosys.2021.107450,
author = {Salesi, Sadegh and Cosma, Georgina},
title = {Generalisation Power Analysis for finding a stable set of features using evolutionary algorithms for feature selection▪},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {231},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107450},
doi = {10.1016/j.knosys.2021.107450},
journal = {Know.-Based Syst.},
month = nov,
numpages = {12},
keywords = {Feature selection stability, Evolutionary computation, Machine learning, Generalisation Power Index, Generalisation Power Analysis, Feature selection}
}

@article{10.1287/mksc.1120.0736,
author = {Guo, Liang and Zhang, Juanjuan},
title = {Consumer Deliberation and Product Line Design},
year = {2012},
issue_date = {11-12 2012},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {31},
number = {6},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.1120.0736},
doi = {10.1287/mksc.1120.0736},
abstract = {This paper studies optimal product line design when consumers need to incur costly deliberation to uncover their valuations for quality. To induce deliberation, a firm must maintain quality dispersion and cut the price of the high-end product so that consumers are motivated to deliberate in the hope that high-end consumption fits their needs. To prevent deliberation, the firm may have to offer downgraded quality at a low price so that an impulsive purchase will not appear too wasteful. Whether the firm should induce deliberation depends on how much surplus it creates by aligning the supply of quality with heterogeneous demand for quality and how much surplus it captures during this process. Interestingly, equilibrium firm profit, consumer surplus, and social welfare can all increase with the cost of deliberation. We extend the model to accommodate consumers' heterogeneous prior beliefs of their valuations for quality. We also discuss how market research could benefit from taking into account the endogeneity of consumer deliberation.},
journal = {Marketing Science},
month = nov,
pages = {995–1007},
numpages = {13},
keywords = {product line design, price discrimination, preference construction, information acquisition, consumer deliberation, agency theory}
}

@article{10.1016/j.knosys.2021.107595,
author = {Xu, Yuanyuan and Yin, Yu and Wang, Jun and Wei, Jinmao and Liu, Jian and Yao, Lina and Zhang, Wenjie},
title = {Unsupervised Cross-View Feature Selection on incomplete data▪},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {234},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107595},
doi = {10.1016/j.knosys.2021.107595},
journal = {Know.-Based Syst.},
month = dec,
numpages = {13},
keywords = {View diversity, Feature redundancy, Incomplete views, Unsupervised learning, Multi-view feature selection}
}

@phdthesis{10.5555/AAI28263301,
author = {Al-Dhaheri, Sami and Sakas, William and Li, Xiang Dong and Alhajj, Reda},
advisor = {Uz, Tansel, Abdullah},
title = {A New Feature Selection Method Based on Class Association Rule},
year = {2021},
isbn = {9798569924738},
publisher = {City University of New York},
address = {USA},
abstract = {Feature selection is a key process for supervised learning algorithms. It involves discarding irrelevant attributes from the training dataset from which the models are derived. One of the vital feature selection approaches is Filtering, which often uses mathematical models to compute the relevance for each feature in the training dataset and then sorts the features into descending order based on their computed scores. However, most Filtering methods face several challenges including, but not limited to, merely considering feature-class correlation when defining a feature's relevance; additionally, not recommending which subset of features to retain. Leaving this decision to the end-user may be impractical for multiple reasons such as the experience required in the application domain, care, accuracy, and time. In this research, we propose a new hybrid Filtering method called Class Association Rule Filter (CARF) that deals with the aforementioned issues by identifying relevant features through the Class Association Rule Mining approach and then using these rules to define weights for the available features in the training dataset. More crucially, we propose a new procedure based on mutual information within the CARF method which suggests the subset of features to be retained by the end-user, hence reducing time and effort. Empirical evaluation using small, medium, and large datasets that belong to various dissimilar domains reveals that CARF was able to reduce the dimensionality of the search space when contrasted with other common Filtering methods. More importantly, the classification models devised by the different machine learning algorithms against the subsets of features selected by CARF were highly competitive in terms of various performance measures. These results indeed reflect the quality of the subsets of features selected by CARF and show the impact of the new cut-off procedure proposed.},
note = {AAI28263301}
}

@article{10.1007/s10115-017-1121-6,
author = {Pecli, Antonio and Cavalcanti, Maria Claudia and Goldschmidt, Ronaldo},
title = {Automatic feature selection for supervised learning in link prediction applications: a comparative study},
year = {2018},
issue_date = {July      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {56},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1121-6},
doi = {10.1007/s10115-017-1121-6},
abstract = {For the last years, a considerable amount of attention has been devoted to the research about the link prediction (LP) problem in complex networks. This problem tries to predict the likelihood of an association between two not interconnected nodes in a network to appear in the future. One of the most important approaches to the LP problem is based on supervised machine learning (ML) techniques for classification. Although many works have presented promising results with this approach, choosing the set of features (variables) to train the classifiers is still a major challenge. In this article, we report on the effects of three different automatic variable selection strategies (Forward, Backward and Evolutionary) applied to the feature-based supervised learning approach in LP applications. The results of the experiments show that the use of these strategies does lead to better classification models than classifiers built with the complete set of variables. Such experiments were performed over three datasets (Microsoft Academic Network, Amazon and Flickr) that contained more than twenty different features each, including topological and domain-specific ones. We also describe the specification and implementation of the process used to support the experiments. It combines the use of the feature selection strategies, six different classification algorithms (SVM, K-NN, na\"{\i}ve Bayes, CART, random forest and multilayer perceptron) and three evaluation metrics (Precision, F-Measure and Area Under the Curve). Moreover, this process includes a novel ML voting committee inspired approach that suggests sets of features to represent data in LP applications. It mines the log of the experiments in order to identify sets of features frequently selected to produce classification models with high performance. The experiments showed interesting correlations between frequently selected features and datasets.},
journal = {Knowl. Inf. Syst.},
month = jul,
pages = {85–121},
numpages = {37},
keywords = {Link prediction, Feature selection, Complex network analysis, Binary classification}
}

@article{10.1016/j.ins.2019.07.091,
author = {Tsai, Chih-Fong and Chen, Yu-Chi},
title = {The optimal combination of feature selection and data discretization: An empirical study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {505},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.07.091},
doi = {10.1016/j.ins.2019.07.091},
journal = {Inf. Sci.},
month = dec,
pages = {282–293},
numpages = {12},
keywords = {Machine learning, Feature selection, Discretization, Data mining}
}

@inproceedings{10.1145/3459637.3482106,
author = {Xing, Xiaoying and Liu, Hongfu and Chen, Chen and Li, Jundong},
title = {Fairness-Aware Unsupervised Feature Selection},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482106},
doi = {10.1145/3459637.3482106},
abstract = {Feature selection is a prevalent data preprocessing paradigm for various learning tasks. Due to the expensive cost of acquiring supervision information, unsupervised feature selection sparks great interests recently. However, existing unsupervised feature selection algorithms do not have fairness considerations and suffer from a high risk of amplifying discrimination by selecting features that are over associated with protected attributes such as gender, race, and ethnicity. In this paper, we make an initial investigation of the fairness-aware unsupervised feature selection problem and develop a principled framework, which leverages kernel alignment to find a subset of high-quality features that can best preserve the information in the original feature space while being minimally correlated with protected attributes. Specifically, different from the mainstream in-processing debiasing methods, our proposed framework can be regarded as a model-agnostic debiasing strategy that eliminates biases and discrimination before downstream learning algorithms are involved. Experimental results on real-world datasets demonstrate that our framework achieves a good trade-off between feature utility and promoting feature fairness.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3548–3552},
numpages = {5},
keywords = {unsupervised learning, feature selection, fairness},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.3233/JIFS-189876,
author = {Tripathi, Diwakar and Ramachandra Reddy, B. and Padmanabha Reddy, Y.C.A. and Shukla, Alok Kumar and Kumar, Ravi Kant and Sharma, Neeraj Kumar and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Trajkovic, Ljiljana},
title = {BAT algorithm based feature selection: Application in credit scoring},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189876},
doi = {10.3233/JIFS-189876},
abstract = {Credit scoring plays a vital role for financial institutions to estimate the risk associated with a credit applicant applied for credit product. It is estimated based on applicants’ credentials and directly affects to viability of issuing institutions. However, there may be a large number of irrelevant features in the credit scoring dataset. Due to irrelevant features, the credit scoring models may lead to poorer classification performances and higher complexity. So, by removing redundant and irrelevant features may overcome the problem with large number of features. In this work, we emphasized on the role of feature selection to enhance the predictive performance of credit scoring model. Towards to feature selection, Binary BAT optimization technique is utilized with a novel fitness function. Further, proposed approach aggregated with “Radial Basis Function Neural Network (RBFN)”, “Support Vector Machine (SVM)” and “Random Forest (RF)” for classification. Proposed approach is validated on four bench-marked credit scoring datasets obtained from UCI repository. Further, the comprehensive investigational results analysis are directed to show the comparative performance of the classification tasks with features selected by various approaches and other state-of-the-art approaches for credit scoring.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {5561–5570},
numpages = {10},
keywords = {BAT algorithm, credit score, feature selection}
}

@inproceedings{10.1145/2816839.2816850,
author = {Lahiani, Nesrine and Bennouar, Djamal},
title = {A Model Driven Approach to Derive e-Learning Applications in Software Product Line},
year = {2015},
isbn = {9781450334587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2816839.2816850},
doi = {10.1145/2816839.2816850},
abstract = {Platforms such as Moodle aims to ease and improve the teaching-learning process by means of taking advantage of internet technologies. All existing e-learning platforms are pretty similar the concepts of activity, assignment, deliverable or grade. But also a wide range of differences among them exists. Software Product Line (SPL) has as goal the effective production of similar software systems.. Product derivation represents a fundamental aspect in SPL. It is also the main challenge that SPL faces. Despite its importance, there is only a little research on product derivation compared to the large work on developing product lines. In addition, the few available research reports guidance about how to derive a product from a product line. In this paper we describe a combination of SPL and MDA which both fit perfectly together in order to build applications in cost effective way. We proposed an approach for product derivation that adopts MDA with its organized layers of models to achieve SPL goals.},
booktitle = {Proceedings of the International Conference on Intelligent Information Processing, Security and Advanced Communication},
articleno = {78},
numpages = {6},
keywords = {e-learning, Software Product Line, Product Derivation, Model Driven Architecture},
location = {Batna, Algeria},
series = {IPAC '15}
}

@inproceedings{10.1145/2791060.2791074,
author = {Reuling, Dennis and B\"{u}rdek, Johannes and Rot\"{a}rmel, Serge and Lochau, Malte and Kelter, Udo},
title = {Fault-based product-line testing: effective sample generation based on feature-diagram mutation},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791074},
doi = {10.1145/2791060.2791074},
abstract = {Testing every member of a product line individually is often impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sample generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line testing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equivalent mutants. We further introduce similarity-based mutant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {131–140},
numpages = {10},
keywords = {mutation testing, combinatorial interaction testing},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3424978.3425085,
author = {Hou, Jiaqi and Sang, Yongsheng and Liu, Yuping and Lu, Li},
title = {Feature Selection and Prediction Model for Type 2 Diabetes in the Chinese Population with Machine Learning},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425085},
doi = {10.1145/3424978.3425085},
abstract = {Diabetes is a chronic disease characterized by hyperglycemia. Based on the rising incidence of the disease in recent years, diabetes is affecting more and more families. In 2017 alone, it caused 5 million deaths and cost $850 billion in global healthcare. In this paper, we proposed a method to predict the prevalence of diabetes based on a selected set of features from physical examination data. We used Fisher's score, RFE and decision tree to select features. Random forest, logistic regression, SVM and MLP were used to predict the prevalence of diabetes. EA and Fisher' s score helped us to reduce dimensions. We used random forest to classify diabetes accurately. Our results show that the highest accuracy (0.987) can be achieved by using random forest with 85 features. The prediction accuracy using Fisher's Score with 19 features also reached 0.986. We finally selected 5 features based on our method to form a new dataset for diabetes prediction. The 5 features are fasting plasma glucose, HbA1c, HDL, total cholesterol level and hypertension. The values of accuracy, precision, sensitivity, F1 score, MCC and AUC were 0.977, 0.968, 0.812, 0.883, 0.875, and 0.905, respectively. Results show that our method can be successfully used to select features for diabetes classifier and improve its performance, which will provide support for clinicians to quickly identify diabetes.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {103},
numpages = {7},
keywords = {Random forest, Machine learning, Feature selection, Diabetes mellitus, Classification},
location = {Sanya, China},
series = {CSAE '20}
}

@article{10.1007/s00500-020-04734-w,
author = {Coelho, Frederico and Costa, Marcelo and Verleysen, Michel and Braga, Ant\^{o}nio P.},
title = {LASSO multi-objective learning algorithm for feature selection},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {17},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-04734-w},
doi = {10.1007/s00500-020-04734-w},
abstract = {This work proposes a new algorithm for training neural networks to solve the problems of feature selection and function approximation. The algorithm applies different weight constraint functions for the hidden and the output layers of a multilayer perceptron neural network. The LASSO operator is applied to the hidden layer; therefore, the training provides automatic selection of relevant features and the standard norm regularization function is applied to the output layer. Therefore, we propose a multi-objective training algorithm that is able to select the important features while solving the approximation problem.},
journal = {Soft Comput.},
month = sep,
pages = {13209–13217},
numpages = {9},
keywords = {LASSO, Multi-objective, Feature selection, Supervised learning}
}

@article{10.1287/opre.2016.1546,
author = {Bertsimas, Dimitris and Mi\v{s}i\'{c}, Velibor V.},
title = {Robust Product Line Design},
year = {2017},
issue_date = {January-February 2017},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {65},
number = {1},
issn = {0030-364X},
url = {https://doi.org/10.1287/opre.2016.1546},
doi = {10.1287/opre.2016.1546},
abstract = {The majority of approaches to product line design that have been proposed by marketing scientists assume that the underlying choice model that describes how the customer population will respond to a new product line is known precisely. In reality, however, marketers do not precisely know how the customer population will respond and can only obtain an estimate of the choice model from limited conjoint data. In this paper, we propose a new type of optimization approach for product line design under uncertainty. Our approach is based on the paradigm of robust optimization where, rather than optimizing the expected revenue with respect to a single model, one optimizes the worst-case expected revenue with respect to an uncertainty set of models. This framework allows us to account for parameter uncertainty, when we may be confident about the type of model structure but not about the values of the parameters, and structural uncertainty, when we may not even be confident about the right model structure to use to describe the customer population. Through computational experiments with a real conjoint data set, we demonstrate the benefits of our approach in addressing parameter and structural uncertainty. With regard to parameter uncertainty, we show that product lines designed without accounting for parameter uncertainty are fragile and can experience worst-case revenue losses as high as 23%, and that the robust product line can significantly outperform the nominal product line in the worst case, with relative improvements of up to 14%. With regard to structural uncertainty, we similarly show that product lines that are designed for a single model structure can be highly suboptimal under other structures (worst-case losses of up to 37%), while a product line that optimizes against the worst of a set of structurally distinct models can outperform single model product lines by as much as 55% in the worst case and can guarantee good aggregate performance over structurally distinct models.},
journal = {Oper. Res.},
month = feb,
pages = {19–37},
numpages = {19},
keywords = {model uncertainty, structural uncertainty, parameter uncertainty, robust optimization, product line design}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1007/s11634-020-00397-5,
author = {Lausser, Ludwig and Szekely, Robin and Kestler, Hans A.},
title = {Chained correlations for feature selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-020-00397-5},
doi = {10.1007/s11634-020-00397-5},
abstract = {Data-driven algorithms stand and fall with the availability and quality of existing data sources. Both can be limited in high-dimensional settings (n≫m). For example, supervised learning algorithms designed for molecular pheno- or genotyping are restricted to samples of the corresponding diagnostic classes. Samples of other related entities, such as arise in differential diagnosis, are usually not utilized in this learning scheme. Nevertheless, they might provide domain knowledge on the background or context of the original diagnostic task. In this work, we discuss the possibility of incorporating samples of foreign classes in the training of diagnostic classification models that can be related to the task of differential diagnosis. Especially in heterogeneous data collections comprising multiple diagnostic categories, the foreign ones can change the magnitude of available samples. More precisely, we utilize this information for the internal feature selection process of diagnostic models. We propose the use of chained correlations of original and foreign diagnostic classes. This method allows the detection of intermediate foreign classes by evaluating the correlation between class labels and features for each pair of original and foreign categories. Interestingly, this criterion does not require direct comparisons of the initial diagnostic groups and therefore, might be suitable for settings with restricted data access.},
journal = {Adv. Data Anal. Classif.},
month = dec,
pages = {871–884},
numpages = {14},
keywords = {62P10 Applications to biology and medical sciences, 68T10 Pattern recognition, etc.), canonical correlation, 62H20 Measures of association (correlation, 62H30 Classification and discrimination, Differential diagnosis, High-dimensional data, Feature selection, Classification}
}

@inproceedings{10.1007/978-3-030-47426-3_62,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {Group Based Unsupervised Feature Selection},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_62},
doi = {10.1007/978-3-030-47426-3_62},
abstract = {Unsupervised feature selection is an important task in machine learning applications, yet challenging due to the unavailability of class labels. Although a few unsupervised methods take advantage of external sources of correlations within feature groups in feature selection, they are limited to genomic data, and suffer poor accuracy because they ignore input data or encourage features from the same group. We propose a framework which facilitates unsupervised filter feature selection methods to exploit input data and feature group information simultaneously, encouraging features from different groups. We use this framework to incorporate feature group information into Laplace Score algorithm. Our method achieves high accuracy compared to other popular unsupervised feature selection methods (30% maximum improvement of Normalized Mutual Information (NMI)) with low computational costs (50 times lower than embedded methods on average). It has many real world applications, particularly the ones that use image, text and genomic data, whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {805–817},
numpages = {13},
keywords = {[inline-graphic not available: see fulltext] norm minimisation., Feature groups, Unsupervised feature selection},
location = {Singapore, Singapore}
}

@article{10.1007/s10270-015-0471-3,
author = {Bonif\'{a}cio, Rodrigo and Borba, Paulo and Ferraz, Cristiano and Accioly, Paola},
title = {Empirical assessment of two approaches for specifying software product line use case scenarios},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0471-3},
doi = {10.1007/s10270-015-0471-3},
abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {97–123},
numpages = {27},
keywords = {Usage scenarios, Software product lines, Software modularity, Requirements engineering, Experimentation in software engineering}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.inffus.2018.11.008,
author = {Bol\'{o}n-Canedo, Ver\'{o}nica and Alonso-Betanzos, Amparo},
title = {Ensembles for feature selection: A review and future trends},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.11.008},
doi = {10.1016/j.inffus.2018.11.008},
journal = {Inf. Fusion},
month = dec,
pages = {1–12},
numpages = {12},
keywords = {Feature selection, Ensemble learning}
}

@article{10.1016/j.jss.2016.01.039,
author = {Mariani, Thain\'{a} and Elita Colanzi, Thelma and Regina Vergilio, Silvia},
title = {Preserving architectural styles in the search based design of software product line architectures},
year = {2016},
issue_date = {May 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {115},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.01.039},
doi = {10.1016/j.jss.2016.01.039},
abstract = {This paper presents search operators for layered and client/server architectures.The goal is to preserve the style of a PLA in a search based optimization approach.A representation for layered and client/server PLAs in a class diagram is presented.Rules for each style based on their representation are defined.The operators preserved the styles and contributed to improve quantitative results. Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values.},
journal = {J. Syst. Softw.},
month = may,
pages = {157–173},
numpages = {17},
keywords = {Software product line, Search based design, Architectural style}
}

@article{10.1007/s11063-020-10307-7,
author = {Wang, Xiujuan and Yan, Yixuan and Ma, Xiaoyue},
title = {Feature Selection Method Based on Differential Correlation Information Entropy},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {2},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10307-7},
doi = {10.1007/s11063-020-10307-7},
abstract = {Feature selection is one of the major aspects of pattern classification systems. In previous studies, Ding and Peng recognized the importance of feature selection and proposed a minimum redundancy feature selection method to minimize redundant features for sequential selection in microarray gene expression data. However, since the minimum redundancy feature selection method is used mainly to measure the dependency between random variables of mutual information, the results cannot be optimal without consideration of global feature selection. Therefore, based on the framework of minimum redundancy-maximum correlation, this paper introduces entropy to measure global feature selection and proposes a new feature subset evaluation method, differential correlation information entropy. In our function, different bivariate correlation metrics are selected. Then, the feature selection is completed through sequence forward search. Two different classification models are used on eleven standard data sets of the UCI machine learning knowledge base to compare various comparison algorithms, such as mRMR, reliefF and feature selection method with joint maximal information entropy, with our method. The experimental results show that feature selection based on our proposed method is obviously superior to that of other models.},
journal = {Neural Process. Lett.},
month = oct,
pages = {1339–1358},
numpages = {20},
keywords = {Feature selection, Classification, mRMR, Differential correlation information entropy}
}

@article{10.1016/j.ins.2021.10.026,
author = {Wan, Jihong and Chen, Hongmei and Li, Tianrui and Yang, Xiaoling and Sang, Binbin},
title = {Dynamic interaction feature selection based on fuzzy rough set},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {581},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.10.026},
doi = {10.1016/j.ins.2021.10.026},
journal = {Inf. Sci.},
month = dec,
pages = {891–911},
numpages = {21},
keywords = {Mixed data, Information measures, Dynamic feature weight, Feature interaction, Feature selection, Fuzzy rough set}
}

@article{10.1016/j.knosys.2021.107538,
author = {Joodaki, Mehdi and Dowlatshahi, Mohammad Bagher and Joodaki, Nazanin Zahra},
title = {An ensemble feature selection algorithm based on PageRank centrality and fuzzy logic},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {233},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107538},
doi = {10.1016/j.knosys.2021.107538},
journal = {Know.-Based Syst.},
month = dec,
numpages = {13},
keywords = {Fuzzy Type-I, High-dimensional data, PageRank, Ensemble feature selection, Feature selection}
}

@article{10.1016/j.ijar.2021.06.005,
author = {Campagner, Andrea and Ciucci, Davide and H\"{u}llermeier, Eyke},
title = {Rough set-based feature selection for weakly labeled data},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2021.06.005},
doi = {10.1016/j.ijar.2021.06.005},
journal = {Int. J. Approx. Reasoning},
month = sep,
pages = {150–167},
numpages = {18},
keywords = {Entropy, Evidence Theory, Feature Selection, Rough Sets, Superset Learning}
}

@article{10.1007/s11063-020-10192-0,
author = {Shanthi, S. and Rajkumar, N.},
title = {Lung Cancer Prediction Using Stochastic Diffusion Search (SDS) Based Feature Selection and Machine Learning Methods},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {4},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10192-0},
doi = {10.1007/s11063-020-10192-0},
abstract = {The symptoms of cancer normally appear only in the advanced stages, so it is very hard to detect resulting in a high mortality rate among the other types of cancers. Thus, there is a need for early prediction of lung cancer for the purpose of diagnosing and this can result in better chances of it being able to be treated successfully. Histopathology images of lung scan can be used for classification of lung cancer using image processing methods. The features from lung images are extracted and employed in the system for prediction. Grey level co-occurrence matrix along with the methods of Gabor filter feature extraction are employed in this investigation. Another important step in enhancing the classification is feature selection that tends to provide significant features that helps differentiating between various classes in an accurate and efficient manner. Thus, optimal feature subsets can significantly improve the performance of the classifiers. In this work, a novel algorithm of feature selection that is wrapper-based is proposed by employing the modified stochastic diffusion search (SDS) algorithm. The SDS, will benefit from the direct communication of agents in order to identify optimal feature subsets. The neural network, Na\"{\i}ve Bayes and the decision tree have been used for classification. The results of the experiment prove that the proposed method is capable of achieving better levels of performance compared to existing methods like minimum redundancy maximum relevance, and correlation-based feature selection.},
journal = {Neural Process. Lett.},
month = aug,
pages = {2617–2630},
numpages = {14},
keywords = {Naive Bayes and decision tree, Neural network (NN), Stochastic diffusion search (SDS), Gabor filter, Gray level co-occurrence matrix (GLCM), Radiomic features, Non-small cell lung cancer (NSCLC), Small cell lung cancer (SCLC), Lung cancer}
}

@article{10.1007/s11634-020-00408-5,
author = {Hijazi, Samah and Hamad, Denis and Kalakech, Mariam and Kalakech, Ali},
title = {Active learning of constraints for weighted feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {2},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-020-00408-5},
doi = {10.1007/s11634-020-00408-5},
abstract = {Pairwise constraints, a cheaper kind of supervision information that does not need to reveal the class labels of data points, were initially suggested to enhance the performance of clustering algorithms. Recently, researchers were interested in using them for feature selection. However, in most current methods, pairwise constraints are provided passively and generated randomly over multiple algorithmic runs by which the results are averaged. This leads to the need of a large number of constraints that might be redundant, unnecessary, and under some circumstances even inimical to the algorithm’s performance. It also masks the individual effect of each constraint set and introduces a human labor-cost burden. Therefore, in this paper, we suggest a framework for actively selecting and then propagating constraints for feature selection. For that, we benefit from the graph Laplacian that is defined on the similarity matrix. We assume that when a small perturbation of the similarity value between a data couple leads to a more well-separated cluster indicator based on the second eigenvector of the graph Laplacian, this couple is definitely expected to be a pairwise query of higher and more significant impact. Constraints propagation on the other side ensures increasing supervision information while decreasing the cost of human-labor. Finally, experimental results validated our proposal in comparison to other known feature selection methods and proved to be prominent.},
journal = {Adv. Data Anal. Classif.},
month = jun,
pages = {337–377},
numpages = {41},
keywords = {47N10, 68T10, 62H30, 65F15, 15A18, Matrix perturbation, Uncertainty reduction, Graph Laplacian, Constraint propagation, Pairwise constraint selection, Active learning, Feature selection}
}

@article{10.1007/s10489-020-02009-3,
author = {Khoder, A. and Dornaika, F.},
title = {A hybrid discriminant embedding with feature selection: application to image categorization},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02009-3},
doi = {10.1007/s10489-020-02009-3},
abstract = {In recent times, feature extraction was the focus of many researches due to its usefulness in the machine learning and pattern recognition fields. Feature extraction mainly aims to extract informative representations from the original set of features. This can be carried out using various ways. The proposed method is targeting a hybrid linear feature extraction scheme for supervised multi-class classification problems. Inspired by recent robust sparse LDA and Inter-class sparsity frameworks, we will propose a unifying criterion that is able to retain these two powerful linear discriminant method’s advantages. Thus, the obtained transformation encapsulates two different types of discrimination, the inter-class sparsity and robust Linear Discriminant Analysis with feature selection. We will introduce an iterative alternating minimization scheme in order to estimate the linear transform and the orthogonal matrix. The linear transform is efficiently updated via the steepest descent gradient technique. We will also introduce two initialization schemes for the linear transform. The proposed framework is generic in the sense that it allows the combination and tuning of other linear discriminant embedding methods. According to the experiments which have been carried out on several datasets including faces, objects and digits, the proposed method was able to outperform the competing methods in most cases.},
journal = {Applied Intelligence},
month = jun,
pages = {3142–3158},
numpages = {17},
keywords = {image classification, dimensionality reduction, class sparsity, linear embedding, feature extraction, discriminant analysis, Supervised learning}
}

@article{10.1007/s10270-015-0462-4,
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
title = {Automated product line test case selection: industrial case study and controlled experiment},
year = {2017},
issue_date = {May       2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0462-4},
doi = {10.1007/s10270-015-0462-4},
abstract = {Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection (e.g., selection time), while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a feature model for testing (FM_T) to capture commonalities and variabilities of a product line and a component family model for testing (CFM_T) to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We evaluated our methodology via three different ways: (1) We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco, and the results show that our methodology can reduce the selection effort significantly; (2) we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models (FM_T and CFM_T) in their current practice; (3) we conducted a controlled experiment with 20 graduate students to assess the performance (i.e., cost, effectiveness and efficiency) of our automated methodology as compared to the manual approach. The results showed that our methodology is cost-effective as compared to the manual approach, and at the same time, its efficiency is not affected by the increased complexity of products.},
journal = {Softw. Syst. Model.},
month = may,
pages = {417–441},
numpages = {25},
keywords = {Test case selection, Product line, Feature model, Component family model}
}

@article{10.1016/j.cie.2021.107481,
author = {Kiziloz, Hakan Ezgi and Deniz, Ay\c{c}a},
title = {An evolutionary parallel multiobjective feature selection framework},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2021.107481},
doi = {10.1016/j.cie.2021.107481},
journal = {Comput. Ind. Eng.},
month = sep,
numpages = {13},
keywords = {Evolutionary computation, Parallel processing, Multiobjective optimization, Feature selection}
}

@article{10.1145/3409382,
author = {Yu, Kui and Guo, Xianjie and Liu, Lin and Li, Jiuyong and Wang, Hao and Ling, Zhaolong and Wu, Xindong},
title = {Causality-based Feature Selection: Methods and Evaluations},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3409382},
doi = {10.1145/3409382},
abstract = {Feature selection is a crucial preprocessing step in data analytics and machine learning. Classical feature selection algorithms select features based on the correlations between predictive features and the class variable and do not attempt to capture causal relationships between them. It has been shown that the knowledge about the causal relationships between features and the class variable has potential benefits for building interpretable and robust prediction models, since causal relationships imply the underlying mechanism of a system. Consequently, causality-based feature selection has gradually attracted greater attentions and many algorithms have been proposed. In this article, we present a comprehensive review of recent advances in causality-based feature selection. To facilitate the development of new algorithms in the research area and make it easy for the comparisons between new methods and existing ones, we develop the first open-source package, called CausalFS, which consists of most of the representative causality-based feature selection algorithms (available at https://github.com/kuiy/CausalFS). Using CausalFS, we conduct extensive experiments to compare the representative algorithms with both synthetic and real-world datasets. Finally, we discuss some challenging problems to be tackled in future research.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {111},
numpages = {36},
keywords = {Markov boundary, Feature selection, Bayesian network}
}

@inproceedings{10.1007/978-3-030-55130-8_36,
author = {Pang, Qing-Qing and Zhang, Li},
title = {Fast Backward Iterative Laplacian Score for Unsupervised Feature Selection},
year = {2020},
isbn = {978-3-030-55129-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55130-8_36},
doi = {10.1007/978-3-030-55130-8_36},
abstract = {Iterative Laplacian Score (IterativeLS), an extension of Laplacian score (LS) for unsupervised feature selection, iteratively updates the nearest neighborhood graph for evaluating the importance of a feature by its local preserving ability. However, LS and IterativeLS separately measure the importance of each feature and do not consider the association of features. To remedy it, this paper proposes an enhanced version of IterativeLS, called fast backward iterative Laplacian score (FBILS). The goal of FBILS is to fast remove some unimportant features by taking into account the association of features. The proposed FBILS evaluates the feature importance according to the joint local preserving ability that reflects the association of features. In addition, FBILS deletes more than one feature in an iteration, which would speed up the process of feature selection. Extensive experiments are conducted on UCI and microarray gene datasets. Experimental results confirm that FBILS can achieve a good performance.},
booktitle = {Knowledge Science, Engineering and Management: 13th International Conference, KSEM 2020, Hangzhou, China, August 28–30, 2020, Proceedings, Part I},
pages = {409–420},
numpages = {12},
keywords = {Unsupervised learning, Feature selection, Laplacian score, Local preserving, Iteration algorithm},
location = {Hangzhou, China}
}

@inproceedings{10.1145/3001867.3001872,
author = {Lity, Sascha and Kowal, Matthias and Schaefer, Ina},
title = {Higher-order delta modeling for software product line evolution},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3001867.3001872},
doi = {10.1145/3001867.3001872},
abstract = {In software product lines (SPL), i.e., a family of similar software systems sharing common and variable artifacts, modeling evolution and reasoning about it is challenging, as not only a single system, but rather a set of system variants as well as their interdependencies change. An integrated modeling formalism for variability and evolution is required to allow the capturing of evolution operations that are applied to SPL artifacts, and to facilitate the impact analysis of evolution on the artifact level. Delta modeling is a flexible transformational variability modeling approach, where the variability and commonality between variants are explicitly documented and analyzable by means of transformations modeled as deltas. In this paper, we lift the notion of delta modeling to capture both, variability and evolution, by deltas. We evolve a delta model specifying a set of variants by applying higher-order deltas. A higher-order delta encapsulates evolution operations, i.e., additions, removals, or modifications of deltas, and transforms a delta model in its new version. In this way, we capture the complete evolution history of delta-oriented SPLs by higher-order delta models. By analyzing each higher-order delta application, we are further able to reason about the impact and, thus, the changes to the specified set of variants. We prototypically implement our formalism and show its applicability using a system from the automation engineering domain.},
booktitle = {Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
pages = {39–48},
numpages = {10},
keywords = {Software Product Lines, Software Evolution, Delta Modeling},
location = {Amsterdam, Netherlands},
series = {FOSD 2016}
}

@article{10.1016/j.cose.2021.102448,
author = {Halim, Zahid and Yousaf, Muhammad Nadeem and Waqas, Muhammad and Sulaiman, Muhammad and Abbas, Ghulam and Hussain, Masroor and Ahmad, Iftekhar and Hanif, Muhammad},
title = {An effective genetic algorithm-based feature selection method for intrusion detection systems},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {110},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102448},
doi = {10.1016/j.cose.2021.102448},
journal = {Comput. Secur.},
month = nov,
numpages = {20},
keywords = {Data analysis, Machine learning, Intrusion detection, Genetic algorithm, Feature selection}
}

@article{10.1016/j.compbiomed.2021.104664,
author = {Prabha, Anju and Yadav, Jyoti and Rani, Asha and Singh, Vijander},
title = {Design of intelligent diabetes mellitus detection system using hybrid feature selection based XGBoost classifier},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104664},
doi = {10.1016/j.compbiomed.2021.104664},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {9},
keywords = {XGBoost, Feature selection, MFCC, PPG, Diabetes detection}
}

@article{10.4018/IJEACH.2019010106,
author = {Fong, Simon and Li, Tengyue},
title = {A Fast Feature Selection Method Based on Coefficient of Variation for Diabetics Prediction Using Machine Learning},
year = {2019},
issue_date = {Jan 2019},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {1},
issn = {2577-4794},
url = {https://doi.org/10.4018/IJEACH.2019010106},
doi = {10.4018/IJEACH.2019010106},
abstract = {Diabetes has become a prevalent metabolic disease nowadays, affecting patients of all age groups and large populations around the world. Early detection would facilitate early treatment that helps the prognosis. In the literature of computational intelligence and medical care communities, different techniques have been proposed in predicting diabetes based on the historical records of related symptoms. The researchers share a common goal of improving the accuracy of a diabetes prediction model. In addition to the model induction algorithms, feature selection is a significant approach in retaining only the relevant attributes for the sake of building a quality prediction model later. In this article, a novel and simple feature selection criterion called Coefficient of Variation (CV) is proposed as a filter-based feature selection scheme. By following the CV method, attributes that have a data dispersion too low are disqualified from the model construction process. Thereby the attributes which are factors leading to poor model accuracy are discarded. The computation of CV is simple, hence enabling an efficient feature selection process. Computer simulation experiments by using the Prima Indian diabetes dataset is used to compare the performance of CV with other traditional feature selection methods. Superior results by CV are observed.},
journal = {International Journal of Extreme Automation and Connectivity in Healthcare},
month = jan,
pages = {55–65},
numpages = {11},
keywords = {Classification, Coefficient of Variation, Data Mining, Diabetes Prediction, Feature Selection, Pre-Processing}
}

@article{10.1016/j.engappai.2021.104504,
author = {Vaish, Rachna and Dwivedi, U.D. and Tewari, Saurabh and Tripathi, S.M.},
title = {Machine learning applications in power system fault diagnosis: Research advancements and perspectives},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104504},
doi = {10.1016/j.engappai.2021.104504},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {33},
keywords = {Unsupervised learning, Transfer learning, Supervised learning, Reinforcement learning, Machine learning (ML)}
}

@inproceedings{10.5555/3106050.3106053,
author = {H\"{a}nsel, Joachim and Giese, Holger},
title = {Towards collective online and offline testing for dynamic software product line systems},
year = {2017},
isbn = {9781538628034},
publisher = {IEEE Press},
abstract = {Dynamic Software Product Line (DSPLs) based Systems are capable of adapting in response to changes concerning their observations at runtime in order to exhibit appropriate behavior. The observation space and the variability in the configuration space is usually known at design time. However, running a set of tests with all combinations of configuration and observation from these spaces is likely to be infeasible. We propose to make use of monitoring results from multiple instances of systems derived from a DSPL at runtime collecting their observations and the employed configurations. The collective of systems is enabled to profit from an operational profile with regard to proper coverage by systematic tests. The systematic tests are carried out offline. Additional online testing further improves the confidence in the system.},
booktitle = {Proceedings of the 2nd International Workshop on Variability and Complexity in Software Design},
pages = {9–12},
numpages = {4},
keywords = {self-adaptive software systems, online testing, offline testing, dynamic software product lines},
location = {Buenos Aires, Argentina},
series = {VACE '17}
}

@article{10.1016/j.neucom.2021.08.003,
author = {Gonz\'{a}lez, Jes\'{u}s and Ortega, Julio and Escobar, Juan Jos\'{e} and Damas, Miguel},
title = {A lexicographic cooperative co-evolutionary approach for feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {463},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.08.003},
doi = {10.1016/j.neucom.2021.08.003},
journal = {Neurocomput.},
month = nov,
pages = {59–76},
numpages = {18},
keywords = {Classification, Feature selection, Lexicographic optimization, Multi-objective optimization, Cooperative co-evolution}
}

@article{10.1016/j.ins.2018.12.034,
author = {Wang, Hongjun and Zhang, Yinghui and Zhang, Ji and Li, Tianrui and Peng, Lingxi},
title = {A factor graph model for unsupervised feature selection},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {480},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.12.034},
doi = {10.1016/j.ins.2018.12.034},
journal = {Inf. Sci.},
month = apr,
pages = {144–159},
numpages = {16},
keywords = {Unsupervised learning, Message-passing algorithm, Factor graph, Feature selection}
}

@article{10.1016/j.patcog.2021.108169,
author = {Fan, Yuling and Liu, Jinghua and Liu, Peizhong and Du, Yongzhao and Lan, Weiyao and Wu, Shunxiang},
title = {Manifold learning with structured subspace for multi-label feature selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108169},
doi = {10.1016/j.patcog.2021.108169},
journal = {Pattern Recogn.},
month = dec,
numpages = {16},
keywords = {Label correlations, Instance correlations, Structured subspace, Manifold learning, Feature selection, Multi-label learning}
}

@article{10.1016/j.asoc.2019.105956,
author = {Noorie, Zahir and Afsari, Fatemeh},
title = {Sparse feature selection: Relevance, redundancy and locality structure preserving guided by pairwise constraints},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {87},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105956},
doi = {10.1016/j.asoc.2019.105956},
journal = {Appl. Soft Comput.},
month = feb,
numpages = {18},
keywords = {Pairwise constraints, Locality structure preserving, Graph Laplacian, Pairwise redundancy, ℓ 1-norm, Sparse feature selection}
}

@article{10.4018/ijkss.2014100104,
author = {Ripon, Shamim H and Hossain, Sk. Jahir and Piash, Moshiur Mahamud},
title = {Logic-Based Analysis and Verification of Software Product Line Variant Requirement Model},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100104},
doi = {10.4018/ijkss.2014100104},
abstract = {Software Product Line SPL provides the facility to systematically reuse of software improving the efficiency of software development regarding time, cost and quality. The main idea of SPL is to identify the common core functionality that can be implemented once and reused afterwards. A variant model has also to be developed to manage the variants of the SPL. Usually, a domain model consisting of the common and variant requirements is developed during domain engineering phase to alleviate the reuse opportunity. The authors present a product line model comprising of a variant part for the management of variant and a decision table to depict the customization of decision regarding each variant. Feature diagrams are widely used to model SPL variants. Both feature diagram and our variant model, which is based on tabular method, lacks logically sound formal representation and hence, not amenable to formal verification. Formal representation and verification of SPL has gained much interest in recent years. This chapter presents a logical representation of the variant model by using first order logic. With this representation, the table based variant model as well as the graphical feature diagram can now be verified logically. Besides applying first-order-logic to model the features, the authors also present an approach to model and analyze SPL model by using semantic web approach using OWL-DL. The OWL-DL representation also facilitates the search and maintenance of feature models and support knowledge sharing within a reusable engineering context. Reasoning tools are used to verify the consistency of the feature configuration for both logic-based and semantic web-based approaches.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {52–76},
numpages = {25},
keywords = {Web-Based Approaches, Software Product Line SPL, OWL-DL, Feature Diagrams, Domain Model}
}

@article{10.1007/s10489-019-01563-9,
author = {Dornaika, F.},
title = {Multi-layer manifold learning with feature selection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01563-9},
doi = {10.1007/s10489-019-01563-9},
abstract = {Many fundamental problems in machine learning require some form of dimensionality reduction. To this end, two different strategies were used: Manifold Learning and Feature Selection. Manifold learning (or data embedding) attempts to compute a subspace from original data by feature recombination/transformation. Feature selection aims to select the most relevant features in the original space. In this paper, we propose a novel cooperative Manifold learning-Feature selection that goes beyond the simple concatenation of these two modules. Our basic idea is to transform a given shallow embedding to a deep variant by computing a cascade of embeddings in which each embedding undergoes feature selection and elimination. We use filter approaches in order to efficiently select irrelevant features at any stage of the process. For a case study, our proposed framework was used with two typical linear embedding algorithms: Local Discriminant Embedding (LDE) (a supervised technique) and Locality Preserving Projections (LPP) (unsupervised technique) on four challenging face databases and it has been conveniently compared with other cooperative schemes. Moreover, a comparison with several state-of-the-art manifold learning methods is provided. As it is exhibited by our experimental study, the proposed framework can achieve superior learning performance with respect to classic cooperative schemes and to many competing manifold learning methods.},
journal = {Applied Intelligence},
month = jun,
pages = {1859–1871},
numpages = {13},
keywords = {Pattern classification, Face recognition, Manifold learning, Feature extraction, Feature selection, Data embedding}
}

@inproceedings{10.1007/978-3-031-08147-7_7,
author = {Avila, Ricardo and Khoury, Rapha\"{e}l and Pere, Christophe and Khanmohammadi, Kobra},
title = {Employing Feature Selection to&nbsp;Improve the&nbsp;Performance of&nbsp;Intrusion Detection Systems},
year = {2021},
isbn = {978-3-031-08146-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08147-7_7},
doi = {10.1007/978-3-031-08147-7_7},
abstract = {Intrusion detection systems use datasets with various features to detect attacks and protect computers and network systems from these attacks. However, some of these features are irrelevant and may reduce the intrusion detection system’s speed and accuracy. In this study, we use feature selection methods to eliminate non-relevant features. We compare the performance of fourteen feature-selection methods, on three ML techniques using the UNSW-NB15, Kyoto 2006+ and DoHBrw-2020 datasets. The most relevant features of each dataset are identified, which show that feature selection methods can increase the accuracy of anomaly detection and classification.},
booktitle = {Foundations and Practice of Security: 14th International Symposium, FPS 2021, Paris, France, December 7–10, 2021, Revised Selected Papers},
pages = {93–112},
numpages = {20},
keywords = {Intrusion detection systems, Machine learning, Feature selection},
location = {Paris, France}
}

@article{10.1007/s00521-018-3467-4,
author = {Chen, Si-Bao and Ding, Chris H. Q. and Zhou, Zhi-Li and Luo, Bin},
title = {Feature selection based on correlation deflation},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3467-4},
doi = {10.1007/s00521-018-3467-4},
abstract = {Feature selection is very important in many machine learning and data mining applications. In this paper, a simple and effective correlation-deflation-based feature selection method is proposed. The objective function of residual minimization constrained by $$L_{2,0}$$-norm is proved to be equivalent to maximizing sum of square of correlations between class labels and features. Then the whole procedure of correlation-deflation-based feature selection turns into selecting features out one-by-one by deflating correlations. Experiments on several public benchmark data sets show that the proposed method has better residual reduction and classification performance than many state-of-the-art feature selection methods.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {6383–6392},
numpages = {10},
keywords = {Pattern classification, Residual reduction, Correlation deflation, Feature selection}
}

@article{10.1287/mksc.2019.1160,
author = {Xu, Zibin and Dukes, Anthony},
title = {Product Line Design Under Preference Uncertainty Using Aggregate Consumer Data},
year = {2019},
issue_date = {July-August 2019},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {38},
number = {4},
issn = {1526-548X},
url = {https://doi.org/10.1287/mksc.2019.1160},
doi = {10.1287/mksc.2019.1160},
abstract = {This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences.This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences. If perceptual errors are driven by common variables, then a firm may use aggregate consumer data (e.g., conjoint studies or anonymous usage data) to deduce the errors and infer the consumer preferences. In this way, we develop microfoundations necessary to show when and how the firm can understand consumer preferences better than consumers themselves, a situation we call superior knowledge. But is superior knowledge ever unprofitable? How should the firm with superior knowledge design its product line? Do consumers receive more-relevant products or simply have more surplus extracted? Can data collection help consumers make better choices? Our results suggest that consumers’ rational suspicions may prevent the firm from exploiting its superior knowledge. In addition, the burden of signaling may force the firm to offer efficient quality for its products. Therefore, allowing the firm to collect aggregate consumer data may be strictly Pareto improving.},
journal = {Marketing Science},
month = jul,
pages = {669–689},
numpages = {21},
keywords = {signaling model, perceptual error, uninformed preference, superior knowledge, product line design, consumer data collection}
}

@article{10.1007/s13748-021-00238-2,
author = {Tripathi, Diwakar and Edla, Damodar Reddy and Bablani, Annushree and Shukla, Alok Kumar and Reddy, B. Ramachandra},
title = {Experimental analysis of machine learning methods for credit score classification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {3},
url = {https://doi.org/10.1007/s13748-021-00238-2},
doi = {10.1007/s13748-021-00238-2},
abstract = {Credit scoring concerns with emerging empirical model to assist the financial institutions for financial decision-making process. Credit risk analysis plays a vital role for decision-making process; statistical and machine learning approaches are utilized to estimate the risk associated with a credit applicant. Enhancing the performance of credit scoring model, particularly toward non-trustworthy “or non-creditworthy” group, may result incredible effect for financial institution. However, credit scoring data may have excess and unimportant data and features which degrades the performance of model. So, selection of important features (or reduction in irrelevant and redundant features) may play the key role for improving the effectiveness and reducing the complexity of the model. This study presents a experimental results analysis of various combinations of feature selection approaches with various classification approaches and impact of feature selection approaches. For experimental results analysis, nine feature selection and sixteen classification state-of-the-art approaches have been applied on seven benched marked credit scoring datasets.},
journal = {Prog. in Artif. Intell.},
month = sep,
pages = {217–243},
numpages = {27},
keywords = {Feature selection, Credit scoring, Classification}
}

@article{10.1016/j.eswa.2019.05.011,
author = {Mao, Xiangke and Yang, Hui and Huang, Shaobin and Liu, Ye and Li, Rongsheng},
title = {Extractive summarization using supervised and unsupervised learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {133},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.05.011},
doi = {10.1016/j.eswa.2019.05.011},
journal = {Expert Syst. Appl.},
month = nov,
pages = {173–181},
numpages = {9},
keywords = {Biased-LexRank, Summarization, Combination, Unsupervised learning, Supervised learning}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Variability management, Software architecture, Sensitivity points, Quality evaluation, Model driven - software product lines, Domain specific modeling}
}

@article{10.1016/j.cor.2021.105334,
author = {Aksakalli, Vural and D. Yenice, Zeren and Malekipirbazari, Milad and Kargar, Kamyar},
title = {Feature selection using stochastic approximation with Barzilai and Borwein non-monotone gains},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {132},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2021.105334},
doi = {10.1016/j.cor.2021.105334},
journal = {Comput. Oper. Res.},
month = aug,
numpages = {14},
keywords = {Explainable artificial intelligence, Feature selection, Stochastic approximation, Gradient descent, Barzilai and Borwein method, Genetic algorithm}
}

@inproceedings{10.5555/1885639.1885657,
author = {Elsner, Christoph and Ulbrich, Peter and Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang},
title = {Consistent product line configuration across file type and product line boundaries},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Creating a valid software configuration of a product line can require laborious customizations involving multiple configuration file types, such as feature models, domain-specific languages, or preprocessor defines in C header files. Using configurable off-the-shelf components causes additional complexity. Without checking of constraints across file types boundaries already at configuration time, intricate inconsistencies are likely to be introduced--resulting in product defects, which are costly to discover and resolve later on.Up to now, at best ad-hoc solutions have been applied. To tackle this problem in a general way, we have developed an approach and a corresponding plug-in infrastructure. It allows for convenient definition and checking of constraints across configuration file types and product line boundaries. Internally, all configuration files are converted to models, facilitating the use of model-based constraint languages (e.g., OCL). Converter plug-ins for arbitrary configuration file types may be integrated and hide a large amount of complexity usually associated with modeling. We have validated our approach using a quadrotor helicopter product line comprising three sub-product-lines and four different configuration file formats. The results give evidence that our approach is practically applicable, reduces time and effort for product derivation (by avoiding repeated compiling, testing, and reconfiguration cycles), and prevents faulty software deployment.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {181–195},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1007/s10489-020-01683-7,
author = {Men, Min and Zhong, Ping and Wang, Zhi and Lin, Qiang},
title = {Distributed learning for supervised multiview feature selection},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {9},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01683-7},
doi = {10.1007/s10489-020-01683-7},
abstract = {Multiview feature selection technique is specifically designed to reduce the dimensionality of multiview data and has received much attention. Most proposed multiview supervised feature selection methods suffer from the problem of efficiently handling the large-scale and high-dimensional data. To address this, this paper designs an efficient supervised multiview feature selection method for multiclass problems by combining the distributed optimization method in the Alternating Direction Method of Multipliers (ADMM). Specifically, the distributed strategy is reflected in two aspects. On the one hand, a sample-partition based distributed strategy is adopted, which calculates the loss term of each category individually. On the other hand, a view-partition based distributed strategy is used to explore the consistent and characteristic information of views. We adopt the individual regularization on each view and the common loss term which is obtained by fusing different views to jointly share the label matrix. Benefited from the distributed framework, the model can realize a distributed solution for the transformation matrix and reduce the complexity for multiview feature selection. Extensive experiments have demonstrated that the proposed method achieves a great improvement on training time, and the comparable or better performance compared to several state-of-the-art supervised feature selection algorithms.},
journal = {Applied Intelligence},
month = sep,
pages = {2749–2769},
numpages = {21},
keywords = {ADMM, Distributed strategy, Multiview learning, Feature selection}
}

@inproceedings{10.1007/978-3-030-63833-7_30,
author = {Zheng, Xiaohan and Zhang, Li and Yan, Leilei},
title = {Feature Selection Using Sparse Twin Bounded Support Vector Machine},
year = {2020},
isbn = {978-3-030-63832-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63833-7_30},
doi = {10.1007/978-3-030-63833-7_30},
abstract = {Although twin bounded support machine (TBSVM) has a lower time complexity than support vector machine (SVM), TBSVM has a poor ability to select features. To overcome the shortcoming of TBSVM, we propose a sparse twin bounded support machine (STBSVM) inspired by the sparsity of the ℓ1-norm. The objective function of STBSVM contains the hinge loss and the ℓ1-norm terms, both which can induce sparsity. We find solutions in the primal space instead of the dual space and avoid the operation of matrix inversion. All of these can assure the sparsity of STBSVM, or the ability to select features. Experiments carried out on synthetic and UCI datasets show that STBSVM has a good ability to select features and simultaneously enhances the classification performance.},
booktitle = {Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 23–27, 2020, Proceedings, Part II},
pages = {357–369},
numpages = {13},
keywords = {Supervised learning, Twin support vector machine, ℓ1-norm regularization, Sparsity, Feature selection},
location = {Bangkok, Thailand}
}

@article{10.1007/s42979-021-00687-5,
author = {Chantar, Hamouda and Tubishat, Mohammad and Essgaer, Mansour and Mirjalili, Seyedali},
title = {Hybrid Binary Dragonfly Algorithm with Simulated Annealing for Feature Selection},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {4},
url = {https://doi.org/10.1007/s42979-021-00687-5},
doi = {10.1007/s42979-021-00687-5},
abstract = {There are various fields are affected by the growth of data dimensionality. The major problems which are resulted from high dimensionality of data including high memory requirements, high computational cost, and low machine learning classifier performance. Therefore, proper selection of relevant features from the set of available features and the removal of irrelevant features will solve these problems. Therefore, to solve the feature selection problem, an improved version of Dragonfly Algorithm (DA) is proposed by combining it with Simulated Annealing (SA), where the improved algorithm named BDA-SA. To solve the local optima problem of DA and enhance its ability in selecting the best subset of features for classification problems, Simulated Annealing (SA) was applied to the best solution found by Binary Dragonfly algorithm in attempt to improve its accuracy. A set of frequently used data sets from UCI repository was utilized to evaluate the performance of the proposed FS approach. Results show that the proposed hybrid approach, named BDA-SA, has superior performance when compared to wrapper-based FS methods including a feature selection method based on the basic version of Binary Dragonfly Algorithm.},
journal = {SN Comput. Sci.},
month = may,
numpages = {11},
keywords = {Feature selection, Dragonfly algorithm, Simulated annealing algorithm, Optimization}
}

@article{10.1016/j.eswa.2021.114643,
author = {Miao, Jianyu and Ping, Yuan and Chen, Zhensong and Jin, Xiao-Bo and Li, Peijia and Niu, Lingfeng},
title = {Unsupervised feature selection by non-convex regularized self-representation},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {173},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114643},
doi = {10.1016/j.eswa.2021.114643},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {11},
keywords = {ADMM, CCCP, Non-convex regularization, Self-representation, Unsupervised feature selection}
}

@inproceedings{10.5555/2662572.2662582,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {Representation of software product line architectures for search-based design},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). Search-based approaches can provide automated discovery of near-optimal PLAs and make its design less dependent on human architects. To do this, it is necessary to adopt a suitable PLA representation to apply the search operators. In this sense, we review existing architecture representations proposed by related work, but all of them need to be extended to encompass specific characteristics of SPL. Then, the use of such representations for PLA is discussed and, based on the performed analysis, we introduce a novel direct PLA representation for search-based optimization. Some implementation aspects are discussed involving implementation details about the proposed PLA representation, constraints and impact on specific search operators. Ongoing work addresses the application of specific search operators for the proposed representation and the definition of a fitness function to be applied in a multi-objective search-based approach for the PLA design.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {28–33},
numpages = {6},
keywords = {software product line, multi-objective search-based approach, architecture modelling},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@article{10.1016/j.ins.2019.07.018,
author = {Chamakura, Lily and Saha, Goutam},
title = {An instance voting approach to feature selection},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {504},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.07.018},
doi = {10.1016/j.ins.2019.07.018},
journal = {Inf. Sci.},
month = dec,
pages = {449–469},
numpages = {21},
keywords = {Priority coverage, Graph modularity, Instance voting, Set-covering problem, Filter-based method, Feature selection}
}

@article{10.1016/j.patcog.2019.02.016,
author = {Sharmin, Sadia and Shoyaib, Mohammad and Ali, Amin Ahsan and Khan, Muhammad Asif Hossain and Chae, Oksam},
title = {Simultaneous feature selection and discretization based on mutual information},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.02.016},
doi = {10.1016/j.patcog.2019.02.016},
journal = {Pattern Recogn.},
month = jul,
pages = {162–174},
numpages = {13},
keywords = {Dynamic discretization, Bias, Mutual information, Feature selection}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {variability models, multiobjective optimization, evolutionary algorithms, automated configuration, SMT solvers},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1016/j.knosys.2021.107264,
author = {Herrera-Semenets, Vitali and Bustio-Mart\'{\i}nez, L\'{a}zaro and Hern\'{a}ndez-Le\'{o}n, Raudel and van den Berg, Jan},
title = {A multi-measure feature selection algorithm for efficacious intrusion detection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107264},
doi = {10.1016/j.knosys.2021.107264},
journal = {Know.-Based Syst.},
month = sep,
numpages = {11},
keywords = {Intrusion detection algorithms, Data reduction, Feature selection}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Stacking, Hybrid feature selection, Ensemble machine learning, Machine learning, Code smell}
}

@article{10.1016/j.knosys.2021.107331,
author = {Lin, Qiang and Yang, Liran and Zhong, Ping and Zou, Hui},
title = {Robust supervised multi-view feature selection with weighted shared loss and maximum margin criterion},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {229},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107331},
doi = {10.1016/j.knosys.2021.107331},
journal = {Know.-Based Syst.},
month = oct,
numpages = {17},
keywords = {Feature selection, Robust, Maximum margin criterion, Weighted shared loss, Multi-view learning}
}

@inproceedings{10.1145/3457682.3457693,
author = {Lu, Jingjing and Yi, Shuangyan and Zhao, Jiaoyan and Liang, Yongsheng and Liu, Wei},
title = {Interpretable Robust Feature Selection via Joint -Norms Minimization},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457693},
doi = {10.1145/3457682.3457693},
abstract = {Dimension reduction is a hot topic in data processing field. The challenge lies in how to find a suitable feature subset in low-dimensional space to accurately summarize the important information in high-dimensional space, rather than redundant information or noise. This requires the proposed model to reasonably explain the importance of features and be robust to noise. In order to solve this problem, this paper proposes an interpretable robust feature selection method, in which both the reconstruction error term and the regularization term are constrained by -norm. The reconstruction error term can capture samples corroded by noise, while the regular term automatically finds a group of discriminative features on relatively clean samples. Experimental results show the effectiveness of the proposed method, especially on noise data sets.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {72–78},
numpages = {7},
keywords = {reconstruction, feature selection, ℓ2,p-norm},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@inproceedings{10.1007/978-3-030-90235-3_37,
author = {Khan, Ijaz and Al-Mamari, Aysha and Al-Abdulsalam, Bashayer and Al-Abdulsalam, Fatma and Al-Khansuri, Maryam and Iqbal Malik, Sohail and Ahmad, Abdul Rahim},
title = {A Machine Learning Classification Application to Identify Inefficient Novice Programmers},
year = {2021},
isbn = {978-3-030-90234-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90235-3_37},
doi = {10.1007/978-3-030-90235-3_37},
abstract = {To preserve their reputation and prestige, the educational institutes are required to provide evidences of their students’ academic performance to the governmental bureaus and accreditation agencies. As a consequence, the monitoring individual student academic performance is emerging as a vital task for the educational institutes. The indispensability of this prediction amplifies when it comes to programming language course; which emerges as backbone for Computer Science students. Machine Learning classifiers are considered as productive tools to develop models which can identify the students with inefficient academic performance. The early identification of inefficient students will provide an opportunity to instructor to take appropriate precautionary measures. This paper proposes a prediction model with an added application layer with graphical user interface. The experimental part of paper compares the performance of several machine learning algorithms and comes up with k-NN as appropriate classifier in the addressed context. Further, the application layer of the proposed architecture facilitates instructor with a Graphical User Interface to execute a wide range of operations.},
booktitle = {Advances in Visual Informatics: 7th International Visual Informatics Conference, IVIC 2021, Kajang, Malaysia, November 23–25, 2021, Proceedings},
pages = {423–434},
numpages = {12},
keywords = {Supervised learning, K-Nearest neighbor, Machine learning, Educational data mining},
location = {Kajang, Malaysia}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1016/j.neucom.2018.12.030,
author = {Shi, Xiaoshuang and Xing, Fuyong and Guo, Zhenhua and Su, Hai and Liu, Fujun and Yang, Lin},
title = {Structured orthogonal matching pursuit for feature selection},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {349},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.12.030},
doi = {10.1016/j.neucom.2018.12.030},
journal = {Neurocomput.},
month = jul,
pages = {164–172},
numpages = {9},
keywords = {Correlations, Feature structure, OMP, Feature selection}
}

@inproceedings{10.1145/3461002.3473947,
author = {Pinnecke, Marcus},
title = {Product-lining the elinvar wealthtech microservice platform},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473947},
doi = {10.1145/3461002.3473947},
abstract = {Software product lining is the act of providing different but related software products under the same brand, known as a software product line (SPL). As engineering, management and validation of SPLs is far from trivial, special solutions for software product line engineering (SPLE) have a continuous momentum in both academic and industry. In general, it is hard to judge when to reasonably favor SPLE over alternative solutions that are more common in the industry. In this paper, we illustrate how we as Elinvar manage variability within our WealthTech Platform as a Service (PaaS) at different granularity levels, and discuss methods for SPLE in this context. More in detail, we share our techniques and concepts to address configuration management, and show how we manage a single microservice SPL including inter-service communication. Finally, we provide insights into platform solutions by means of packages for our clients. We end with a discussion on SPLE techniques in context of service SPLs and our packaging strategy. We conclude that while we are good to go with industry-standard approaches for microservice SPLs, the variability modeling and analysis advantages within SPLE is promising for our packaging strategy.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {60–68},
numpages = {9},
keywords = {variability management, technologies and concepts, product families, microservice platforms, configuration management},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-47426-3_61,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {A Framework for Feature Selection to Exploit Feature Group Structures},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_61},
doi = {10.1007/978-3-030-47426-3_61},
abstract = {Filter feature selection methods play an important role in machine learning tasks when low computational costs, classifier independence or simplicity is important. Existing filter methods predominantly focus only on the input data and do not take advantage of the external sources of correlations within feature groups to improve the classification accuracy. We propose a framework which facilitates supervised filter feature selection methods to exploit feature group information from external sources of knowledge and use this framework to incorporate feature group information into minimum Redundancy Maximum Relevance (mRMR) algorithm, resulting in GroupMRMR algorithm. We show that GroupMRMR achieves high accuracy gains over mRMR (up&nbsp;to 35%) and other popular filter methods (up&nbsp;to 50%). GroupMRMR has same computational complexity as that of mRMR, therefore, does not incur additional computational costs. Proposed method has many real world applications, particularly the ones that use genomic, text and image data whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {792–804},
numpages = {13},
keywords = {Squared [inline-graphic not available: see fulltext] norm minimisation, Feature groups, Filter feature selection},
location = {Singapore, Singapore}
}

@article{10.1016/j.asoc.2021.107887,
author = {Hu, Xiao-Min and Zhang, Shou-Rong and Li, Min and Deng, Jeremiah D.},
title = {Multimodal particle swarm optimization for feature selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107887},
doi = {10.1016/j.asoc.2021.107887},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {18},
keywords = {Niching techniques, Multimodal, Particle swarm optimization (PSO), Feature selection}
}

@inproceedings{10.5555/3524938.3525925,
author = {Yamada, Yutaro and Lindenbaum, Ofir and Negahban, Sahand and Kluger, Yuval},
title = {Feature selection using stochastic gates},
year = {2020},
publisher = {JMLR.org},
abstract = {Feature selection problems have been extensively studied in the setting of linear estimation (e.g. LASSO), but less emphasis has been placed on feature selection for non-linear functions. In this study, we propose a method for feature selection in neural network estimation problems. The new procedure is based on probabilistic relaxation of the l0 norm of features, or the count of the number of selected features. Our l0-based regularization relies on a continuous relaxation of the Bernoulli distribution; such relaxation allows our model to learn the parameters of the approximate Bernoulli distributions via gradient descent. The proposed framework simultaneously learns either a nonlinear regression or classification function while selecting a small subset of features. We provide an information-theoretic justification for incorporating Bernoulli distribution into feature selection. Furthermore, we evaluate our method using synthetic and real-life data to demonstrate that our approach outperforms other commonly used methods in both predictive performance and feature selection.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {987},
numpages = {12},
series = {ICML'20}
}

@article{10.1016/j.sigpro.2019.107332,
author = {Shi, Caijuan and Gu, Zhibin and Duan, Changyu and Tian, Qi},
title = {Multi-view adaptive semi-supervised feature selection with the self-paced learning},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {168},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2019.107332},
doi = {10.1016/j.sigpro.2019.107332},
journal = {Signal Process.},
month = mar,
numpages = {11},
keywords = {Multi-view learning, Semi-supervised feature selection, Self-paced learning, Graph-based semi-supervised learning}
}

@article{10.1145/3136625,
author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
title = {Feature Selection: A Data Perspective},
year = {2017},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3136625},
doi = {10.1145/3136625},
abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing data (especially high-dimensional data) for various data-mining and machine-learning problems. The objectives of feature selection include building simpler and more comprehensible models, improving data-mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities to feature selection. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the era of big data, we revisit feature selection research from a data perspective and review representative feature selection algorithms for conventional data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for conventional data, we categorize them into four main groups: similarity-based, information-theoretical-based, sparse-learning-based, and statistical-based methods. To facilitate and promote the research in this community, we also present an open source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). Also, we use it as an example to show how to evaluate feature selection algorithms. At the end of the survey, we present a discussion about some open problems and challenges that require more attention in future research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {94},
numpages = {45},
keywords = {Feature selection}
}

@article{10.1016/j.inffus.2018.11.019,
author = {Zhang, Rui and Nie, Feiping and Li, Xuelong and Wei, Xian},
title = {Feature selection with multi-view data: A survey},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {50},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2018.11.019},
doi = {10.1016/j.inffus.2018.11.019},
journal = {Inf. Fusion},
month = oct,
pages = {158–167},
numpages = {10},
keywords = {Multi-view, Information fusion, Feature selection}
}

@inproceedings{10.5555/3540261.3541780,
author = {Wu, Xinxing and Cheng, Qiang},
title = {Algorithmic stability and generalization of an unsupervised feature selection algorithm},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically confirmed.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1519},
numpages = {16},
series = {NIPS '21}
}

@inproceedings{10.1109/SEiA.2019.00009,
author = {Omar, Sinayobye Janvier and Fred, Kiwanuka N. and Swaib, Kaawaase Kyanda and Richard, Musabe},
title = {Hybrid model of correlation based filter feature selection and machine learning classifiers applied on smart meter data set},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEiA.2019.00009},
doi = {10.1109/SEiA.2019.00009},
abstract = {Feature selection is referred to the process of obtaining a subset from an original feature set according to certain feature selection criterion, which selects the relevant features of the dataset. It plays a role in compressing the data processing scale, where the redundant and irrelevant features are removed. Feature selection techniques show that more information is not always good in machine learning applications. Apply different algorithms for the data at hand and with baseline classification performance values we can select a final feature selection algorithm. In this paper, we propose a hybrid classification model, which has correlation based filter feature selection algorithm and Machine learning as classifiers. The objective of this study is to select relevant features and analyze the outperform machine learning algorithms in order to train our model, predict and compare their classification performance. In this method, features are ordered according to their Absolute correlation value with respect to the class attribute. Then top K Features are selected from ordered list of features to form a reduced dataset. This proposed classifier model is applied to our smart meter datasets. To measure the performance of these selected features; seven benchmark classifier are used; Random Forest (RF), Logistic Regression (LR), k-Nearest Neighbor (kNN), Na\"{\i}ve Bayes (NB), Decision Tree (DT), Linear Discriminant Analysis (LDA) and Support Vector Machine (SVM). This paper then analyzes the performance of all classifiers with feature selection in term of accuracy, sensitivity, F-Measure, Specificity, Precision, and MCC. From our experiment, we found that Random Forest classifier performed higher than other used classifiers.},
booktitle = {Proceedings of the 2nd Symposium on Software Engineering in Africa},
pages = {1–10},
numpages = {10},
keywords = {smart meter data sets, machine learning, feature selection, feature extraction},
location = {Montreal, Quebec, Canada},
series = {SEiA '19}
}

@article{10.1007/s11042-021-11474-y,
author = {Mahajan, Shubham and Pandit, Amit Kant},
title = {Hybrid method to supervise feature selection using signal processing and complex algebra techniques},
year = {2021},
issue_date = {Mar 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {82},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-11474-y},
doi = {10.1007/s11042-021-11474-y},
abstract = {Research in AI has proved to be revolutionarily beneficial to humankind from the past few decades. Many supporting techniques have been developed that indirectly evolved AI and directly enhanced various machine learning models; one of them being the feature engineering. It can also be considered as applied-ML. Another is the so-called feature selection which is method in which most contributing feature to final decision making, out of the entire feature space are selected for processing into an ML model. It is not an easy task to precisely calculate the dependency of the output variable onto the candidate features, particularly when the data is high in dimensions. In this regard, this study proposes a novel method named the cisoidal analysis based feature selection (CAFS) which uses both pre-established algorithms as well as a new approach of relating members of feature space to a complex sinusoid (cisoid) mathematically, then using signal processing techniques to eliminate certain elements in the entire feature space for enhanced feature selection and hence to obtain higher classification accuracy. Derived from experiments with five high dimensional datasets, CAFS displays significantly competitive performance than some of the pre-existing algorithms. CAFS is highly advantageous in reducing dimension of feature space in most of the applications.},
journal = {Multimedia Tools Appl.},
month = oct,
pages = {8213–8234},
numpages = {22},
keywords = {Image, Fourier transform, Signal processing, Hybrid feature selection, Feature engineering, Complex algebra, Feature selection}
}

@article{10.1016/j.neucom.2021.04.073,
author = {Min, Yan and Ye, Mao and Tian, Liang and Jian, Yulin and Zhu, Ce and Yang, Shangming},
title = {Unsupervised feature selection via multi-step markov probability relationship},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {453},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.04.073},
doi = {10.1016/j.neucom.2021.04.073},
journal = {Neurocomput.},
month = sep,
pages = {241–253},
numpages = {13},
keywords = {Machine learning, Multi-step Markov transition probability, Data structure preserving, Unsupervised feature selection}
}

@article{10.1145/3383685,
author = {Goldstein, Orpaz and Kachuee, Mohammad and Karkkainen, Kimmo and Sarrafzadeh, Majid},
title = {Target-Focused Feature Selection Using Uncertainty Measurements in Healthcare Data},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3383685},
doi = {10.1145/3383685},
abstract = {Healthcare big data remains under-utilized due to various incompatibility issues between the domains of data analytics and healthcare. The lack of generalizable iterative feature acquisition methods under budget and machine learning models that allow reasoning with a model’s uncertainty are two examples. Meanwhile, a boost to the available data is currently under way with the rapid growth in the Internet of Things applications and personalized healthcare. For the healthcare domain to be able to adopt models that take advantage of this big data, machine learning models should be coupled with more informative, germane feature acquisition methods, consequently adding robustness to the model’s results. We introduce an approach to feature selection that is based on Bayesian learning, allowing us to report the level of uncertainty in the model, combined with false-positive and false-negative rates. In addition, measuring target-specific uncertainty lifts the restriction on feature selection being target agnostic, allowing for feature acquisition based on a target of focus. We show that acquiring features for a specific target is at least as good as deep learning feature selection methods and common linear feature selection approaches for small non-sparse datasets, and surpasses these when faced with real-world data that is larger in scale and sparseness.},
journal = {ACM Trans. Comput. Healthcare},
month = may,
articleno = {15},
numpages = {17},
keywords = {machine learning for health, machine learning, healthcare big data, health informatics, Healthcare feature selection, Bayesian learning}
}

@article{10.1016/j.eswa.2021.115219,
author = {Chen, Jianting and Yuan, Shuhan and Lv, Dongdong and Xiang, Yang},
title = {A novel self-learning feature selection approach based on feature attributions},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115219},
doi = {10.1016/j.eswa.2021.115219},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {23},
keywords = {Self-learning, Feature attribution, Local search, Feature selection}
}

@article{10.1145/2180921.2180941,
author = {Ripon, Shamim H.},
title = {A unified tabular method for modeling variants of software product line},
year = {2012},
issue_date = {May 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/2180921.2180941},
doi = {10.1145/2180921.2180941},
abstract = {Reuse of software is a promising approach to improving the efficiency of software development regarding time, cost and quality. Reuse requires a systematic approach. The best results are achieved if we focus on systems in a specific domain, so-called product line. The key difference between the conventional software engineering and software product line engineering is variant management. The main idea of software product line is to identify the common core functionality which can be implemented once and reused afterwards for all members of the product line. To facilitate this reuse opportunity the domain engineering phase makes the domain model comprising the common as well as variant requirements. In principle, common requirements among systems in a family are easy to handle. However, problem arises during handling variants. Different variants have dependencies on each other; a single variant can affect several variants of the domain model. These problems become complex when the volume of information grows in a domain and there are a lot of variants with several interdependencies. Hence, a separate model is required for handling the variants. This paper presents a mechanism, which we call, Unified Tabular Method to facilitate the management of variant dependencies in product lines. The tabular method consists of a variant part to model the variants and their dependencies, and a decision table to depict the customization decision regarding each variant while deriving customized products. Tabular method alleviates the problem of possible explosion of variant combinations and facilitates the tracing of variant information in the domain model},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–7},
numpages = {7},
keywords = {unified tabular method, software product line, modeling variants}
}

@article{10.1016/j.neucom.2019.07.020,
author = {Zhang, Han and Zhang, Rui and Nie, Feiping and Li, Xuelong},
title = {An efficient framework for unsupervised feature selection},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {366},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.07.020},
doi = {10.1016/j.neucom.2019.07.020},
journal = {Neurocomput.},
month = nov,
pages = {194–207},
numpages = {14},
keywords = {Uncorrelated regression model, Discrete indicator matrix, Bipartite graph, Efficient unsupervised feature selection}
}

@inproceedings{10.1007/978-3-030-86523-8_25,
author = {Don\`{a}, J\'{e}r\'{e}mie and Gallinari, Patrick},
title = {Differentiable Feature Selection, A Reparameterization Approach},
year = {2021},
isbn = {978-3-030-86522-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86523-8_25},
doi = {10.1007/978-3-030-86523-8_25},
abstract = {We consider the task of feature selection for reconstruction which consists in choosing a small subset of features from which whole data instances can be reconstructed. This is of particular importance in several contexts involving for example costly physical measurements, sensor placement or information compression. To break the intrinsic combinatorial nature of this problem, we formulate the task as optimizing a binary mask distribution enabling an accurate reconstruction. We then face two main challenges. One concerns differentiability issues due to the binary distribution. The second one corresponds to the elimination of redundant information by selecting variables in a correlated fashion which requires modeling the covariance of the binary distribution. We address both issues by introducing a relaxation of the problem via a novel reparameterization of the logitNormal distribution. We demonstrate that the proposed method provides an effective exploration scheme and leads to efficient feature selection for reconstruction through evaluation on several high dimensional image benchmarks. We show that the method leverages the intrinsic geometry of the data, facilitating reconstruction. (We refer to  for a complete version of the article including proofs, thorough experimental details and results.)},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part III},
pages = {414–429},
numpages = {16},
keywords = {Representation learning, Sparse methods},
location = {Bilbao, Spain}
}

@article{10.1016/j.knosys.2019.05.030,
author = {Zeng, Zhiqiang and Wang, Xiaodong and Yan, Fei and Chen, Yuming},
title = {Local adaptive learning for semi-supervised feature selection with group sparsity},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {181},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.05.030},
doi = {10.1016/j.knosys.2019.05.030},
journal = {Know.-Based Syst.},
month = oct,
numpages = {11},
keywords = {Feature selection, Manifold structure, Adaptive learning, l 2 , p-norm regularization}
}

@article{10.1016/j.neunet.2021.04.038,
author = {Wang, Yadi and Wang, Jun and Che, Hangjun},
title = {Two-timescale neurodynamic approaches to supervised feature selection based on alternative problem formulations},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {142},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2021.04.038},
doi = {10.1016/j.neunet.2021.04.038},
journal = {Neural Netw.},
month = oct,
pages = {180–191},
numpages = {12},
keywords = {Two-timescale neurodynamics, Neurodynamic optimization, Recurrent neural networks, Feature selection}
}

@article{10.1016/j.neucom.2019.05.048,
author = {Mielniczuk, Jan and Teisseyre, Pawe\l{}},
title = {Stopping rules for mutual information-based feature selection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {358},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.05.048},
doi = {10.1016/j.neucom.2019.05.048},
journal = {Neurocomput.},
month = sep,
pages = {255–274},
numpages = {20},
keywords = {Stopping rules, Multiple hypothesis testing, Feature selection, Interaction information, Mutual information, Entropy}
}

@inproceedings{10.1145/3391812.3396270,
author = {Bang, Jiwoo and Kim, Chungyong and Wu, Kesheng and Sim, Alex and Byna, Suren and Kim, Sunggon and Eom, Hyeonsang},
title = {HPC Workload Characterization Using Feature Selection and Clustering},
year = {2020},
isbn = {9781450379809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391812.3396270},
doi = {10.1145/3391812.3396270},
abstract = {Large high-performance computers (HPC) are expensive tools responsible for supporting thousands of scientific applications. However, it is not easy to determine the best set of configurations for workloads to best utilize the storage and I/O systems. Users typically use the default configurations provided by the system administrators, which typically results in poor performance. In an effort to identify application characteristics more important to I/O performance, we applied several machine learning techniques to characterize these applications. To identify the features that are most relevant to the I/O performance, we evaluate a number of different feature selection methods, e.g., Mutual information regression and F regression, and develop a novel feature selection method based on Min-max mutual information. These feature selection methods allow us to sift through a large set of the real-world workloads collected from NERSC's Cori supercomputer system, and identify the most important features. We employ a number of different clustering algorithms, including KMeans, Gaussian Mixture Model (GMM) and Ward linkage, and measure the cluster quality with Davies Boulder Index (DBI), Silhouette and a new Combined Score developed for this work. The cluster evaluation result shows that the test dataset could be best divided into three clusters, where cluster 1 contains mostly small jobs with operations on standard I/O units, cluster 2 consists of middle size parallel jobs dominated by read operations, and cluster 3 include large parallel jobs with heavy write operations. The cluster characteristics suggest that using parallel I/O library MPI IO and a large number of parallel cores are important to achieve high I/O throughput.},
booktitle = {Proceedings of the 3rd International Workshop on Systems and Network Telemetry and Analytics},
pages = {33–40},
numpages = {8},
keywords = {workload characterization, supercomputer, high performance computing, feature selection, clustering},
location = {Stockholm, Sweden},
series = {SNTA '20}
}

@article{10.1016/j.patcog.2021.107933,
author = {Ma, Wenping and Zhou, Xiaobo and Zhu, Hao and Li, Longwei and Jiao, Licheng},
title = {A two-stage hybrid ant colony optimization for high-dimensional feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.107933},
doi = {10.1016/j.patcog.2021.107933},
journal = {Pattern Recogn.},
month = aug,
numpages = {13},
keywords = {Optimal feature subset size, Classification, High-dimensional data, Ant colony optimization, Feature selection}
}

@article{10.1007/s10115-018-1281-z,
author = {Wang, Heyong and Hong, Ming and Lau, Raymond Yiu Keung},
title = {Utility-based feature selection for text classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {61},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1281-z},
doi = {10.1007/s10115-018-1281-z},
abstract = {Feature selection is a significant step before a classification task used to reduce excessive computational costs and enhance classification performance. This paper illustrates a novel feature selection method based on the concept of utility that is grounded in economics theory. In particular, we focus on a utility-based feature selection method for enhancing text classification. Different from existing feature selection methods, the proposed method selects discriminative semantic terms according to how authors utilize terms to express the main ideas in textual documents, i.e., the “utility of terms,” a criteria that can be used to measure the usefulness of terms on expressing authors’ main ideas. To our best knowledge, our work represents the successful research on the leveraging economics theory for developing a semantically rich feature selection method to improve text classification. Our empirical tests based on six UCI benchmark datasets confirm that the proposed method often outperforms other state-of-the-art feature selection methods in text classification. Moreover, our method provides an economics explanation of term weighting for information retrieval and semantic information acquisition in textual documents.},
journal = {Knowl. Inf. Syst.},
month = oct,
pages = {197–226},
numpages = {30},
keywords = {Economics theory, Utility theory, Text mining, Text classification, Feature selection}
}

@inproceedings{10.1145/3230905.3230959,
author = {Nouinou, Sa\^{a}d and El Afia, Abdellatif and El Fkihi, Sanaa},
title = {Overview on last advances of feature selection},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230959},
doi = {10.1145/3230905.3230959},
abstract = {In the context of learning from high-dimensional datasets, the performance of machine learning algorithms decreases and can even deteriorate in the presence of data noisiness. Feature selection is a dimension reduction process for encountering the curse of dimensionality, which can also solve the noise issue. This paper aims to describe the latest advances in feature selection. The contributions are investigated as attempts for overcoming challenges related to the feasibility, computational complexity, accuracy and reliability. This paper may allow the researcher to take a broad view of developments in feature selection.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {58},
numpages = {6},
keywords = {Machine learning, Feature selection},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1007/s00521-021-06224-y,
author = {Too, Jingwei and Mafarja, Majdi and Mirjalili, Seyedali},
title = {Spatial bound whale optimization algorithm: an efficient high-dimensional feature selection approach},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {23},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06224-y},
doi = {10.1007/s00521-021-06224-y},
abstract = {Selecting a subset of candidate features is one of the important steps in the data mining process. The ultimate goal of feature selection is to select an optimal number of high-quality features that can maximize the performance of the learning algorithm. However, this problem becomes challenging when the number of features increases in a dataset. Hence, advanced optimization techniques are used these days to search for the optimal feature combinations. Whale Optimization Algorithm (WOA) is a recent metaheuristic that has successfully applied to different optimization problems. In this work, we propose a new variant of WOA (SBWOA) based on spatial bounding strategy to play the role of finding the potential features from the high-dimensional feature space. Also, a simplified version of SBWOA is introduced in an attempt to maintain a low computational complexity. The effectiveness of the proposed approach was validated on 16 high-dimensional datasets gathered from Arizona State University, and the results are compared with the other eight state-of-the-art feature selection methods. Among the competitors, SBWOA has achieved the highest accuracy for most datasets such as TOX_171, Colon, and Prostate_GE. The results obtained demonstrate the supremacy of the proposed approaches over the comparison methods.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {16229–16250},
numpages = {22},
keywords = {Evolutionary, Swarm intelligence, WOA, Benchmark, Optimization, High Dimensional Data, Classification, Data mining, Feature selection, Whale optimization algorithm}
}

@inproceedings{10.1145/3267183.3267188,
author = {Barbosa, Jefferson and Andrade, Rossana M. C. and Filho, Jo\~{a}o Bosco F. and Bezerra, Carla I. M. and Barreto, Isaac and Capilla, Rafael},
title = {Cloning in Customization Classes: A Case of a Worldwide Software Product Line},
year = {2018},
isbn = {9781450365543},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267183.3267188},
doi = {10.1145/3267183.3267188},
abstract = {Cloning-and-owning, in the long run, can severely affect evolution, as changes in cloned fragments may require modifications in various parts of the system. This problem scales if cloning is used in classes that derive products in a Software Product Line, because these classes can impact in several features and products. However, it is hard to know to which extent cloning in customization classes can impact in a project. We conduct a study, within an SPL that generates mobile software for over 150 countries, to analyze cloning practices and how cloned parts relate to the maintainability of customization classes. We collect and identify clones inside customization classes during a period of 13 months, involving 70 customization classes and 5 branches. In parallel, we collect the respective issues from the issue tracking tool of the SPL project, obtaining over 140 issues related to customization classes. We then confront the time spent to solve each issue with its nature (i.e., if it relates to cloned code or not). As first result, we verify that issues related to cloning take in average 136% more time to be solved. Our study helps to understand how cloning relates to maintainability in the context of mass customization, giving insights about cloned code evolution and its impacts in a software product line project.},
booktitle = {Proceedings of the VII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {43–52},
numpages = {10},
keywords = {Software Product Line, Customization, Clone},
location = {Sao Carlos, Brazil},
series = {SBCARS '18}
}

@article{10.1016/j.cageo.2021.104696,
author = {Charifi, Rajaa and Es-sbai, Najia and Zennayi, Yahya and Hosni, Taha and Bourzeix, Fran\c{c}ois and Mansouri, Anass},
title = {Sedimentary phosphate classification based on spectral analysis and machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2021.104696},
doi = {10.1016/j.cageo.2021.104696},
journal = {Comput. Geosci.},
month = may,
numpages = {16},
keywords = {Machine learning, Bhattacharyya distance, Feature selection, Spectral analysis, Classification, Phosphate}
}

@inproceedings{10.1007/978-3-030-30484-3_2,
author = {Ma, Rui and Wang, Yijie and Cheng, Li},
title = {Unsupervised Feature Selection via Local Total-Order Preservation},
year = {2019},
isbn = {978-3-030-30483-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30484-3_2},
doi = {10.1007/978-3-030-30484-3_2},
abstract = {Without class label, unsupervised feature selection methods choose a subset of features that faithfully maintain the intrinsic structure of original data. Conventional methods assume that the exact value of pairwise samples distance used in structure regularization is effective. However, this assumption imposes strict restrictions to feature selection, and it causes more features to be kept for data representation. Motivated by this, we propose Unsupervised Feature Selection via Local Total-order Preservation, called UFSLTP. In particular, we characterize a local structure by a novel total-order relation, which applies the comparison of pairwise samples distance. To achieve a desirable features subset, we map total-order relation into probability space and attempt to preserve the relation by minimizing the differences of the probability distributions calculated before and after feature selection. Due to the inherent nature of machine learning and total-order relation, less features are needed to represent data without adverse effecting on performance. Moreover, we propose two efficient methods, namely Adaptive Neighbors Selection(ANS) and Uniform Neighbors Serialization(UNS), to reduce the computational complexity and improve the method performance. The results of experiments on benchmark datasets demonstrate that the proposed method significantly outperforms the state-of-the-art methods. Compared to the competitors by clustering performance, it averagely achieves 31.01% improvement in terms of NMI and 14.44% in terms of Silhouette Coefficient.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part II},
pages = {16–28},
numpages = {13},
keywords = {Local manifold structure, Total-order relation, Unsupervised feature selection},
location = {Munich, Germany}
}

@article{10.1007/s11042-018-6083-5,
author = {Deng, Xuelian and Li, Yuqing and Weng, Jian and Zhang, Jilian},
title = {Feature selection for text classification: A review},
year = {2019},
issue_date = {Feb 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {3},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6083-5},
doi = {10.1007/s11042-018-6083-5},
abstract = {Big multimedia data is heterogeneous in essence, that is, the data may be a mixture of video, audio, text, and images. This is due to the prevalence of novel applications in recent years, such as social media, video sharing, and location based services (LBS), etc. In many multimedia applications, for example, video/image tagging and multimedia recommendation, text classification techniques have been used extensively to facilitate multimedia data processing. In this paper, we give a comprehensive review on feature selection techniques for text classification. We begin by introducing some popular representation schemes for documents, and similarity measures used in text classification. Then, we review the most popular text classifiers, including Nearest Neighbor (NN) method, Na\"{\i}ve Bayes (NB), Support Vector Machine (SVM), Decision Tree (DT), and Neural Networks. Next, we survey four feature selection models, namely the filter, wrapper, embedded and hybrid, discussing pros and cons of the state-of-the-art feature selection approaches. Finally, we conclude the paper and give a brief introduction to some interesting feature selection work that does not belong to the four models.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {3797–3816},
numpages = {20},
keywords = {Text classifiers, Text classification, Multimedia, Feature Selection}
}

@inproceedings{10.1007/978-3-030-30484-3_4,
author = {Fan, Yang and Dai, Jianhua and Zhang, Qilai and Liu, Shuai},
title = {Joint Dictionary Learning for Unsupervised Feature Selection},
year = {2019},
isbn = {978-3-030-30483-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30484-3_4},
doi = {10.1007/978-3-030-30484-3_4},
abstract = {Unsupervised feature selection (UFS) as an effective method to reduce time complexity and storage burden has been widely applied to various machine learning tasks. The selected features should model data distribution, preserve data reconstruction and maintain manifold structure. However, most UFS methods don’t consider these three factors simultaneously. Motivated by this, we propose a novel joint dictionary learning method, which handles these three key factors simultaneously. In joint dictionary learning, an intrinsic space shared by feature space and pseudo label space is introduced, which can model cluster structure and reveal data reconstruction. To ensure the sparseness of intrinsic space, the ℓ1-norm regularization is imposed on the representation coefficients matrix. The joint learning of robust sparse regression model and spectral clustering can select features that maintain data distribution and manifold structure. An efficient algorithm is designed to solve the proposed optimization problem. Experimental results on various types of benchmark datasets validate the effectiveness of our method.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part II},
pages = {46–58},
numpages = {13},
keywords = {Joint dictionary learning, Manifold structure, Data reconstruction, Data distribution, Feature selection},
location = {Munich, Germany}
}

@article{10.1145/3319616,
author = {Wu, Tongshuang and Weld, Daniel S. and Heer, Jeffrey},
title = {Local Decision Pitfalls in Interactive Machine Learning: An Investigation into Feature Selection in Sentiment Analysis},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3319616},
doi = {10.1145/3319616},
abstract = {Tools for Interactive Machine Learning (IML) enable end users to update models in a “rapid, focused, and incremental”—yet local—manner. In this work, we study the question of local decision making in an IML context around feature selection for a sentiment classification task. Specifically, we characterize the utility of interactive feature selection through a combination of human-subjects experiments and computational simulations. We find that, in expectation, interactive modification fails to improve model performance and may hamper generalization due to overfitting. We examine how these trends are affected by the dataset, learning algorithm, and the training set size. Across these factors we observe consistent generalization issues. Our results suggest that rapid iterations with IML systems can be dangerous if they encourage local actions divorced from global context, degrading overall model performance. We conclude by discussing the implications of our feature selection results to the broader area of IML systems and research.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
articleno = {24},
numpages = {27},
keywords = {text classification, performance analysis, Machine learning}
}

@article{10.1016/j.eswa.2019.112878,
author = {Wang, Shiping and Chen, Jiawei and Guo, Wenzhong and Liu, Genggeng},
title = {Structured learning for unsupervised feature selection with high-order matrix factorization},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {140},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112878},
doi = {10.1016/j.eswa.2019.112878},
journal = {Expert Syst. Appl.},
month = feb,
numpages = {11},
keywords = {High-order matrix factorization, Graph Laplacian, Local learning, Data fusion, Feature selection, Machine learning}
}

@article{10.1007/s11227-019-02975-7,
author = {Munirathinam, Deepak Raj and Ranganadhan, Mohanasundaram},
title = {A new improved filter-based feature selection model for high-dimensional data},
year = {2020},
issue_date = {Aug 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-02975-7},
doi = {10.1007/s11227-019-02975-7},
abstract = {Preprocessing of data is ubiquitous, and choosing significant attributes has been one of the important steps in the prior processing of data. Feature selection is used to create a subset of relevant feature for effective classification of data. In a classification of high-dimensional data, the classifier usually depends on the feature subset that has been used for classification. The Relief algorithm is a popular heuristic approach to select significant feature subsets. The Relief algorithm estimates feature individually and selects top-scored feature for subset generation. Many extensions of the Relief algorithm have been developed. However, an important defect in the Relief-based algorithms has been ignored for years. Because of the uncertainty and noise of the instances used for measuring the feature score in the Relief algorithm, the outcome results will vacillate with the instances, which lead to poor classification accuracy. To fix this problem, a novel feature selection algorithm based on Chebyshev distance-outlier detection model is proposed called noisy feature removal-Relief, NFR-ReliefF in short. To demonstrate the performance of NFR-ReliefF algorithm, an extensive experiment, including classification tests, has been carried out on nine benchmarking high-dimensional datasets by uniting the proposed model with standard classifiers, including the na\"{\i}ve Bayes, C4.5 and KNN. The results prove that NFR-ReliefF outperforms the other models on most tested datasets.},
journal = {J. Supercomput.},
month = aug,
pages = {5745–5762},
numpages = {18},
keywords = {Noisy feature, Bioinformatics, Relief, Feature selection, Data mining, Classification}
}

@article{10.5555/2747015.2747184,
author = {da Silva, Ivonei Freitas and da Mota Silveira Neto, Paulo Anselmo and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Software product line scoping and requirements engineering in a small and medium-sized enterprise},
year = {2014},
issue_date = {February 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0164-1212},
abstract = {HighlightsWe described a detailed qualitative study on software product line scoping and requirements engineering.We examine weaknesses regarding the iterativeness, adaptability, and communication.Agile methods can mitigate the iterativeness, adaptability, and communication weaknesses. Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
journal = {J. Syst. Softw.},
month = feb,
pages = {189–206},
numpages = {18},
keywords = {Software product line scoping, Requirements engineering, Agile methods}
}

@article{10.1016/j.patrec.2021.06.035,
author = {Wang, Zheng and Nie, Feiping and Zhang, Canyu and Wang, Rong and Li, Xuelong},
title = {Joint nonlinear feature selection and continuous values regression network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {150},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.06.035},
doi = {10.1016/j.patrec.2021.06.035},
journal = {Pattern Recogn. Lett.},
month = oct,
pages = {197–206},
numpages = {10},
keywords = {Re-weighted back propagation optimization algorithm, ℓ 2 , 1-Norm regularized hidden layer, Continuous values regression, Nonlinear feature selection}
}

@article{10.1016/j.envsoft.2017.12.001,
author = {Meyer, Hanna and Reudenbach, Christoph and Hengl, Tomislav and Katurji, Marwan and Nauss, Thomas},
title = {Improving performance of spatio-temporal machine learning models using forward feature selection and target-oriented validation},
year = {2018},
issue_date = {Mar 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {1364-8152},
url = {https://doi.org/10.1016/j.envsoft.2017.12.001},
doi = {10.1016/j.envsoft.2017.12.001},
journal = {Environ. Model. Softw.},
month = mar,
pages = {1–9},
numpages = {9},
keywords = {Target-oriented validation, Spatio-temporal, Random forest, Over-fitting, Feature selection, Cross-validation}
}

@article{10.3233/JIFS-189090,
author = {Algin, Ramazan and Alkaya, Ali Fuat and Agaoglu, Mustafa and Kahraman, Cengiz},
title = {Feature selection via computational intelligence techniques},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189090},
doi = {10.3233/JIFS-189090},
abstract = {Feature selection (FS) has become an essential task in overcoming high dimensional and complex machine learning problems. FS is a process used for reducing the size of the dataset by separating or extracting unnecessary and unrelated properties from it. This process improves the performance of classification algorithms and reduces the evaluation time by enabling the use of small sized datasets with useful features during the classification process. FS aims to gain a minimal feature subset in a problem domain while retaining the accuracy of the original data. In this study, four computational intelligence techniques, namely, migrating birds optimization (MBO), simulated annealing (SA), differential evolution (DE) and particle swarm optimization (PSO) are implemented for the FS problem as search algorithms and compared on the 17 well-known datasets taken from UCI machine learning repository where the dimension of the tackled datasets vary from 4 to 500. This is the first time that MBO is applied for solving the FS problem. In order to judge the quality of the subsets generated by the search algorithms, two different subset evaluation methods are implemented in this study. These methods are probabilistic consistency-based FS (PCFS) and correlation-based FS (CFS). Performance comparison of the algorithms is done by using three well-known classifiers; k-nearest neighbor, naive bayes and decision tree (C4.5). As a benchmark, the accuracy values found by classifiers using the datasets with all features are used. Results of the experiments show that our MBO-based filter approach outperforms the other three approaches in terms of accuracy values. In the experiments, it is also observed that as a subset evaluator CFS outperforms PCFS and as a classifier C4.5 gets better results when compared to k-nearest neighbor and naive bayes.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6205–6216},
numpages = {12},
keywords = {subset evaluators, classification algorithms, meta-heuristics, dimensionality reduction, computational intelligence, Feature selection}
}

@inproceedings{10.1007/978-3-030-77967-2_8,
author = {Grzyb, Joanna and Topolski, Mariusz and Wo\'{z}niak, Micha\l{}},
title = {Application of Multi-objective Optimization to Feature Selection for a Difficult Data Classification Task},
year = {2021},
isbn = {978-3-030-77966-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77967-2_8},
doi = {10.1007/978-3-030-77967-2_8},
abstract = {Many different decision problems require taking a compromise between the various goals we want to achieve into account. A specific group of features often decides the state of a given object. An example of such a task is the feature selection that allows increasing the decision’s quality while minimizing the cost of features or the total budget. The work’s main purpose is to compare feature selection methods such as the classical approach, the one-objective optimization, and the multi-objective optimization. The article proposes a feature selection algorithm using the Genetic Algorithm with various criteria, i.e., the cost and accuracy. In this way, the optimal Pareto points for the nonlinear problem of multi-criteria optimization were obtained. These points constitute a compromise between two conflicting objectives. By carrying out various experiments on various base classifiers, it has been shown that the proposed approach can be used in the task of optimizing difficult data.},
booktitle = {Computational Science – ICCS 2021: 21st International Conference, Krakow, Poland, June 16–18, 2021, Proceedings, Part III},
pages = {81–94},
numpages = {14},
keywords = {Classification, Cost-sensitive, Feature selection, Multi-objective optimization},
location = {Krakow, Poland}
}

@inproceedings{10.1145/3461598.3461606,
author = {Wong, Hiu-Man and Chen, Xingjian and Tam, Hiu-Hin and Lin, Jiecong and Zhang, Shixiong and Yan, Shankai and Li, Xiangtao and Wong, Ka-Chun},
title = {Feature Selection and Feature Extraction: Highlights},
year = {2021},
isbn = {9781450389679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461598.3461606},
doi = {10.1145/3461598.3461606},
abstract = {In recent years, big data deluges have resulted in exciting data science opportunities. In particular, there is always a desire to extract the most from different data sources. To address it, a promising and recurring task is to perform feature selection and feature extraction. Specifically, the objective is to obtain the non-redundant and informative set of input features (also known as attributes or predictor variables) for downstream data science tasks. In this study, we highlight the existing approaches in both feature selection and feature extraction. In particular, benchmark comparisons are conducted for independent evaluations.},
booktitle = {Proceedings of the 2021 5th International Conference on Intelligent Systems, Metaheuristics &amp; Swarm Intelligence},
pages = {49–53},
numpages = {5},
keywords = {Survey, Information Science, Feature Selection, Feature Extraction, Data Science, Data Mining, Comparison, Benchmark},
location = {Victoria, Seychelles},
series = {ISMSI '21}
}

@article{10.1016/j.neucom.2019.12.017,
author = {Mirzaei, Ali and Pourahmadi, Vahid and Soltani, Mehran and Sheikhzadeh, Hamid},
title = {Deep feature selection using a teacher-student network},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {383},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.12.017},
doi = {10.1016/j.neucom.2019.12.017},
journal = {Neurocomput.},
month = mar,
pages = {396–408},
numpages = {13},
keywords = {Teacher-student, Deep learning, Feature-selection}
}

@article{10.1016/j.jksuci.2018.06.004,
author = {Larabi Marie-Sainte, Souad and Alalyani, Nada},
title = {Firefly Algorithm based Feature Selection for Arabic Text Classification},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {3},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.06.004},
doi = {10.1016/j.jksuci.2018.06.004},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = mar,
pages = {320–328},
numpages = {9},
keywords = {Text Classification, Firefly optimization method, Feature Selection, Arabic Natural Language Processing}
}

@article{10.1007/s11227-018-02737-x,
author = {Lee, Hyeonseo and Lee, Nakyeong and Seo, Harim and Song, Min},
title = {Developing a supervised learning-based social media business sentiment index: Developing a supervised learning-based social media…},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {5},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-02737-x},
doi = {10.1007/s11227-018-02737-x},
abstract = {The fast-growing digital data generation leads to the emergence of the era of big data, which become particularly more valuable because approximately 70% of the collected data in the world comes from social media. Thus, the investigation of online social network services is of paramount importance. In this paper, we use the sentiment analysis, which detects attitudes and emotions toward issues of society posted in social media, to understand the actual economic situation. To this end, two steps are suggested. In the first step, after training the sentiment classifiers with several big data sources of social media datasets, we consider three types of feature sets: feature vector, sequence vector and a combination of dictionary-based feature and sequence vectors. Then, the performance of six classifiers is assessed: MaxEnt-L1, C4.5 decision tree, SVM-kernel, Ada-boost, Na\"{\i}ve Bayes and MaxEnt. In the second step, we collect datasets that are relevant to several economic words that the public use to explicitly express their opinions. Finally, we use a vector auto-regression analysis to confirm our hypothesis. The results show the statistically significant relationship between public sentiment and economic performance. That is, “depression” and “unemployment” lead to KOSPI. Also, it shows that the extracted keywords from the sentiment analysis, such as “price,” “year-end-tax” and “budget deficit,” cause the exchange rates.},
journal = {J. Supercomput.},
month = may,
pages = {3882–3897},
numpages = {16},
keywords = {Sentiment analysis, Social media, Machine learning, Supervised learning}
}

@article{10.1016/j.knosys.2021.107218,
author = {Xue, Yu and Zhu, Haokai and Liang, Jiayu and S\l{}owik, Adam},
title = {Adaptive crossover operator based multi-objective binary genetic algorithm for feature selection in classification▪},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107218},
doi = {10.1016/j.knosys.2021.107218},
journal = {Know.-Based Syst.},
month = sep,
numpages = {9},
keywords = {Classification, Adaptive operator selection, Multi-objective optimization, Binary genetic algorithm, Feature selection}
}

@article{10.1007/s10515-019-00266-2,
author = {Safdar, Safdar Aqeel and Yue, Tao and Ali, Shaukat and Lu, Hong},
title = {Using multi-objective search and machine learning to infer rules constraining product configurations},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-019-00266-2},
doi = {10.1007/s10515-019-00266-2},
abstract = {Modern systems are being developed by integrating multiple products within/across product lines that communicate with each other through information networks. Runtime behaviors of such systems are related to product configurations and information networks. Cost-effectively supporting Product Line Engineering (PLE) of such systems is challenging mainly because of lacking the support of automation of the configuration process. Capturing rules is the key for automating the configuration process in PLE. However, there does not exist explicitly-specified rules constraining configurable parameter values of such products and product lines. Manually specifying such rules is tedious and time-consuming. To address this challenge, in this paper, we present an improved version (named as SBRM+) of our previously proposed Search-based Rule Mining (SBRM) approach. SBRM+ incorporates two machine learning algorithms (i.e., C4.5 and PART) and two multi-objective search algorithms (i.e., NSGA-II and NSGA-III), employs a clustering algorithm (i.e., k means) for classifying rules as high or low confidence rules, which are used for defining three objectives to guide the search. To evaluate SBRM+ (i.e., SBRMNSGA-II+-C45, SBRMNSGA-III+-C45, SBRMNSGA-II+-PART, and SBRMNSGA-III+-PART), we performed two case studies (Cisco and Jitsi) and conducted three types of analyses of results: difference analysis, correlation analysis, and trend analysis. Results of the analyses show that all the SBRM+ approaches performed significantly better than two Random Search-based approaches (RBRM+-C45 and RBRM+-PART) in terms of fitness values, six quality indicators, and 17 machine learning quality measurements (MLQMs). As compared to RBRM+ approaches, SBRM+ approaches have improved the quality of rules based on MLQMs up to 27% for the Cisco case study and 28% for the Jitsi case study.},
journal = {Automated Software Engg.},
month = jun,
pages = {1–62},
numpages = {62},
keywords = {Interacting products, Machine learning, Multi-objective search, Rule mining, Configuration, Product line}
}

@inproceedings{10.1007/978-3-642-34487-9_47,
author = {Adhikary, Jyoti Ranjan and Narasimha Murty, M.},
title = {Feature selection for unsupervised learning},
year = {2012},
isbn = {9783642344862},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34487-9_47},
doi = {10.1007/978-3-642-34487-9_47},
abstract = {In this paper, we present a methodology for identifying best features from a large feature space. In high dimensional feature space nearest neighbor search is meaningless. In this feature space we see quality and performance issue with nearest neighbor search. Many data mining algorithms use nearest neighbor search. So instead of doing nearest neighbor search using all the features we need to select relevant features. We propose feature selection using Non-negative Matrix Factorization(NMF) and its application to nearest neighbor search.Recent clustering algorithm based on Locally Consistent Concept Factorization(LCCF) shows better quality of document clustering by using local geometrical and discriminating structure of the data. By using our feature selection method we have shown further improvement of performance in the clustering.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing - Volume Part III},
pages = {382–389},
numpages = {8},
keywords = {non-negative matrix factorization (NMF), locally consistent concept factorization (LCCF), feature selection},
location = {Doha, Qatar},
series = {ICONIP'12}
}

@inproceedings{10.1145/3394486.3403173,
author = {Bai, Zilong and Nguyen, Hoa and Davidson, Ian},
title = {Block Model Guided Unsupervised Feature Selection},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403173},
doi = {10.1145/3394486.3403173},
abstract = {Feature selection is a core area of data mining with a recent innovation of graph-driven unsupervised feature selection for linked data. In this setting we have a dataset Y consisting of n instances each with m features and a corresponding n node graph (whose adjacency matrix is A) with an edge indicating that the two instances are similar. Existing efforts for unsupervised feature selection on attributed networks have explored either directly regenerating the links by solving for f such that f(yi,yj) ~ Ai,j or finding community structure in A and using the features in Y to predict these communities. However, graph-driven unsupervised feature selection remains an understudied area with respect to exploring more complex guidance. Here we take the novel approach of first building a block model on the graph and then using the block model for feature selection. That is, we discover FMFT ~ A and then find a subset of features S that induces another graph to preserve both F and M. We call our approach Block Model Guided Unsupervised Feature Selection (BMGUFS). Experimental results show that our method outperforms the state of the art on several real-world public datasets in finding high-quality features for clustering.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1201–1211},
numpages = {11},
keywords = {unsupervised feature selection, block model, attributed networks},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3425269.3425276,
author = {Silva, Publio and Bezerra, Carla I. M. and Lima, Rafael and Machado, Ivan},
title = {Classifying Feature Models Maintainability based on Machine Learning Algorithms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425276},
doi = {10.1145/3425269.3425276},
abstract = {Maintenance in the context of SPLs is a topic of interest, and that still needs further investigation. There are several ways to evaluate the maintainability of a feature model (FM), one of which is a manual or automated analysis of quality measures. However, the use of measures does not allow to evaluate the FM quality as a whole, as each measure considers a specific characteristic of FM. In general, the measures have wide ranges of values and do not have a clear definition of what is appropriate and inappropriate. In this context, the goal of this work is to investigate the use of machine learning techniques to classify the feature model maintainability. The research questions investigated in the study were: (i) how could machine learning techniques aid to classify FMs maintainability; and, (ii) which FM classification model has the best accuracy and precision. In this work, we proposed an approach for FM maintainability classification using machine learning technics. For that, we used a dataset of 15 FM maintainability measures calculated for 326 FMs, and we used machine learning algorithms to clustering. After this, we used thresholds to evaluate the general maintainability of each cluster. With this, we built 5 maintainability classification models that have been evaluated with the accuracy and precision metrics.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {software product line, quality evaluation, machine learning, feature model},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@article{10.1016/j.asoc.2020.107026,
author = {Albashish, Dheeb and Hammouri, Abdelaziz I. and Braik, Malik and Atwan, Jaffar and Sahran, Shahnorbanun},
title = {Binary biogeography-based optimization based SVM-RFE for feature selection},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {101},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.107026},
doi = {10.1016/j.asoc.2020.107026},
journal = {Appl. Soft Comput.},
month = mar,
numpages = {19},
keywords = {Curse of dimensionality, Underfitting, Overfitting, SVM-RFE, BBO, Feature selection}
}

@article{10.1016/j.jbi.2020.103591,
author = {Khandezamin, Ziba and Naderan, Marjan and Rashti, Mohammad Javad},
title = {Detection and classification of breast cancer using logistic regression feature selection and GMDH classifier},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {111},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2020.103591},
doi = {10.1016/j.jbi.2020.103591},
journal = {J. of Biomedical Informatics},
month = nov,
numpages = {16},
keywords = {Group method data handling, Machine learning, Logistic regression, Feature selection, Breast cancer}
}

@article{10.1016/j.compbiomed.2020.103991,
author = {Sreejith, S. and Khanna Nehemiah, H. and Kannan, A.},
title = {Clinical data classification using an enhanced SMOTE and chaotic evolutionary feature selection},
year = {2020},
issue_date = {Nov 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {126},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.103991},
doi = {10.1016/j.compbiomed.2020.103991},
journal = {Comput. Biol. Med.},
month = nov,
numpages = {14},
keywords = {Multi Verse Optimisation, SMOTE, Classification, Chaotic maps, Feature selection, Class imbalance, Clinical decision support system}
}

@inproceedings{10.1007/978-3-030-58799-4_57,
author = {Nakao, Eduardo K. and Levada, Alexandre L. M.},
title = {Unsupervised Learning and Feature Extraction in Hyperspectral Imagery},
year = {2020},
isbn = {978-3-030-58798-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58799-4_57},
doi = {10.1007/978-3-030-58799-4_57},
abstract = {Remotely sensed hyperspectral scenes are typically defined by large area coverage and hundreds of spectral bands. Those characteristics imply smooth transitions in the spectral-spatio domains. As consequence, subtle differences in the scene are evidenced, benefiting precision applications, but values in neighboring locations and wavelengths are highly correlated. Nondiagonal covariance matrices and wide autocorrelation functions can be observed this way, implying increased intraclass and decreased interclass variation, in both spectral and spatial domains. This leads to lower interpretation accuracies and makes it reasonable to investigate if hyperspectral imagery suffer from Curse of Dimensionality. Moreover, as this Curse can compromise linear method’s Euclidean behavior assumption, it is relevant to compare linear and nonlinear dimensionality reduction performance. So, in this work we verify these two aspects empirically using multiple nonparametric statistical comparisons of Gaussian Mixture Model clustering performances in the cases of: absence, linear and nonlinear unsupervised feature extraction. Experimental results indicate Curse of Dimensionality presence and nonlinear adequacy.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part I},
pages = {792–806},
numpages = {15},
keywords = {Unsupervised learning, Nonlinear dimensionality reduction, Unsupervised feature extraction, Curse of dimensionality, Hyperspectral imagery},
location = {Cagliari, Italy}
}

@article{10.1007/s00521-021-05871-5,
author = {Mehanovi\'{c}, D\v{z}elila and Ke\v{c}o, Dino and Kevri\'{c}, Jasmin and Juki\'{c}, Samed and Miljkovi\'{c}, Adnan and Ma\v{s}eti\'{c}, Zerina},
title = {Feature selection using cloud-based parallel genetic algorithm for intrusion detection data classification},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {18},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05871-5},
doi = {10.1007/s00521-021-05871-5},
abstract = {With the exponential growth of the amount of data being generated, stored and processed on a daily basis in the machine learning, data analytics and decision-making systems, the data preprocessing established itself as the key factor for building reliable high-performance machine learning models. One of the roles in preprocessing is variable reduction using feature selection methods; however, the processing time needed for these methods is a major drawback. This study aims at mitigating this problem by migrating the algorithm to a MapReduce implementation suitable for parallelization on a high number of commodity hardware units. The genetic algorithm-based methods were put in the focus of this study. Hadoop, an open-source MapReduce library, was used as a framework for implementing parallel genetic algorithms within our research. The representative machine learning methods, SVM (support vector machine), ANN (artificial neural network), RT (random tree), logistic regression and Naive Bayes, were embedded into implementation for feature selection. The feature selection methods were applied to four NSL-KDD data sets, and the number of features is reduced from cca 40 to cca 10 data sets with the accuracy of 90.45%. These results have both significant practical and theoretical impact. On the one hand, the genetic algorithm has been parallelized in the MapReduce manner, which has been considered unachievable in a strict sense. Furthermore, the genetic algorithm allows randomness-enhanced feature selection and its parallelization reduces overall data preprocessing and allows larger population count which in turn leads to better feature selection. On the practical side, it has been shown that this implementation outperforms the existing feature selection methods.},
journal = {Neural Comput. Appl.},
month = sep,
pages = {11861–11873},
numpages = {13},
keywords = {Intrusion detection systems, Feature selection, Machine learning, Parallel genetic algorithm}
}

@inproceedings{10.1145/3453800.3453820,
author = {Lv, Xiaolin and Du, Liang},
title = {Graph-based Kullback-Leibler Divergence Minimization for Unsupervised Feature Selection},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453820},
doi = {10.1145/3453800.3453820},
abstract = {We live in an era of big data, in which feature selection technology is getting more and more attention. Feature selection technology is one of the important methods to reduce the dimension of data. It can select some useful features for learning tasks. The traditional feature selection methods mainly select the useful features by the scores of the features under a certain standard. However, the performance of these methods are less satisfactory in many cases because they ignore the correlation between features. For this article, we present a new unsupervised method by minimizing the Kullback-Leibler(KL) divergence based on graph matching. Firstly, we extract manifold structures from all features of the original data space by using non-negative Local Linear Embedding(NNLLE). Then, we extract manifold structure of each feature by using non-negative local linear embedding (NNLLE). We assess the importance of every feature by minimizing the KL-divergence between the graphs using all features and weighted linear combination of base graphs on each individual feature. At the same time, a global optimization algorithm based on proximal gradient descent framework is proposed. Experiments show that the proposed method is better than many existing unsupervised methods.},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {109–114},
numpages = {6},
keywords = {Unsupervised Feature Selection, KL-divergence Minimization, Feature Graph},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@article{10.1016/j.neucom.2021.09.007,
author = {Tan, Anhui and Liang, Jiye and Wu, Wei-Zhi and Zhang, Jia and Sun, Lin and Chen, Chao},
title = {Fuzzy rough discrimination and label weighting for multi-label feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {465},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.09.007},
doi = {10.1016/j.neucom.2021.09.007},
journal = {Neurocomput.},
month = nov,
pages = {128–140},
numpages = {13},
keywords = {Multi-label data, Fuzzy relation, Feature weighting, Feature selection, Discernibility matrix, Fuzzy rough set}
}

@article{10.1145/3433180,
author = {Mohammed, Mazin Abed and Elhoseny, Mohamed and Abdulkareem, Karrar Hameed and Mostafa, Salama A. and Maashi, Mashael S.},
title = {A Multi-agent Feature Selection and Hybrid Classification Model for Parkinson's Disease Diagnosis},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2s},
issn = {1551-6857},
url = {https://doi.org/10.1145/3433180},
doi = {10.1145/3433180},
abstract = {Parkinson's disease (PD) diagnostics includes numerous analyses related to the neurological, physical, and psychical status of the patient. Medical teams analyze multiple symptoms and patient history considering verified genetic influences. The proposed method investigates the voice symptoms of this disease. The voice files are processed, and the feature extraction is conducted. Several machine learning techniques are used to recognize Parkinson's and healthy patients. This study focuses on examining PD diagnosis through voice data features. A new multi-agent feature filter (MAFT) algorithm is proposed to select the best features from the voice dataset. The MAFT algorithm is designed to select a set of features to improve the overall performance of prediction models and prevent over-fitting possibly due to extreme reduction to the features. Moreover, this algorithm aims to reduce the complexity of the prediction, accelerate the training phase, and build a robust training model. Ten different machine learning methods are then integrated with the MAFT algorithm to form a powerful voice-based PD diagnosis model. Recorded test results of the PD prediction model using the actual and filtered features yielded 86.38% and 86.67% accuracies on average, respectively. With the aid of the MAFT feature selection, the test results are improved by 3.2% considering the hybrid model (HM) and 3.1% considering the Na\"{\i}ve Bayesian and random forest. Subsequently, an HM, which comprises a binary convolutional neural network and three feature selection algorithms (namely, genetic algorithm, Adam optimizer, and mini-batch gradient descent), is proposed to improve the classification accuracy of the PD. The results reveal that PD achieves an overall accuracy of 93.7%. The HM is integrated with the MAFT, and the combination realizes an overall accuracy of 96.9%. These results demonstrate that the combination of the MAFT algorithm and the HM model significantly enhances the PD diagnosis outcomes.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {74},
numpages = {22},
keywords = {Convolutional Neural Network, Hybrid Classification Model, multi-agent feature filter, voice feature, multi-agent system, feature evaluation, machine learning, Parkinson's disease}
}

@inproceedings{10.1145/3410530.3414848,
author = {Kalabakov, Stefan and Stankoski, Simon and Re\v{s}\v{c}i\v{c}, Nina and Kiprijanovska, Ivana and Andova, Andrejaana and Picard, Clement and Janko, Vito and Gjoreski, Martin and Lu\v{s}trek, Mitja},
title = {Tackling the SHL challenge 2020 with person-specific classifiers and semi-supervised learning},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414848},
doi = {10.1145/3410530.3414848},
abstract = {The SHL recognition challenge 2020 was an open competition in activity recognition where the participants were tasked with recognizing eight different modes of locomotion and transportation with smartphone sensors. The main challenges were that the training data was recorded by a different person than the validation and test data, and that the smartphone location in the test data was unknown to the participants. We, team "Third time's a charm", tackled the first challenge by attempting to identify the persons with clustering, and then performed cluster/person-specific feature selection to build a separate classifier for each person. The smartphone location appears not to make much difference. We also used semi-supervised learning to classify the test data. Internal tests using this methodology yielded an accuracy of 81.01%.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {323–328},
numpages = {6},
keywords = {smartphone, semi-supervised learning, machine learning, feature extraction, competition, clustering, activity recognition},
location = {Virtual Event, Mexico},
series = {UbiComp/ISWC '20 Adjunct}
}

@article{10.1016/j.jss.2019.05.001,
author = {Kicsi, Andr\'{a}s and Csuvik, Viktor and Vid\'{a}cs, L\'{a}szl\'{o} and Horv\'{a}th, Ferenc and Besz\'{e}des, \'{A}rp\'{a}d and Gyim\'{o}thy, Tibor and Kocsis, Ferenc},
title = {Feature analysis using information retrieval, community detection and structural analysis methods in product line adoption},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {155},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.05.001},
doi = {10.1016/j.jss.2019.05.001},
journal = {J. Syst. Softw.},
month = sep,
pages = {70–90},
numpages = {21},
keywords = {Community detection, Information retrieval, Feature extraction, Software product line}
}

@inproceedings{10.1145/3487923.3487938,
author = {Chindove, Hatitye and Brown, Dane},
title = {Adaptive Machine Learning Based Network Intrusion Detection},
year = {2021},
isbn = {9781450385756},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487923.3487938},
doi = {10.1145/3487923.3487938},
abstract = {Network intrusion detection system (NIDS) adoption is essential for mitigating computer network attacks in various scenarios. However, the increasing complexity of computer networks and attacks make it challenging to classify network traffic. Machine learning (ML) techniques in a NIDS can be affected by different scenarios, and thus the recency, size and applicability of datasets are vital factors to consider when selecting and tuning a machine learning classifier. The proposed approach evaluates relatively new datasets constructed such that they depict real-world scenarios. It includes analyses of dataset balancing and sampling, feature engineering and systematic ML-based NIDS model tuning focused on the adaptive improvement of intrusion detection. A comparison between machine learning classifiers forms part of the evaluation process. Results on the proposed approach model effectiveness for NIDS are discussed. Recurrent neural networks and random forests models consistently achieved high f1-score results with macro f1-scores of 0.73 and 0.87 for the CICIDS 2017 dataset; and 0.73 and 0.72 against the CICIDS 2018 dataset, respectively.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence and Its Applications},
articleno = {15},
numpages = {6},
location = {Virtual Event, Mauritius},
series = {icARTi '21}
}

@article{10.1016/j.knosys.2021.107156,
author = {Huang, Yanyong and Shen, Zongxin and Cai, Fuxu and Li, Tianrui and Lv, Fengmao},
title = {Adaptive graph-based generalized regression model for unsupervised feature selection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107156},
doi = {10.1016/j.knosys.2021.107156},
journal = {Know.-Based Syst.},
month = sep,
numpages = {14},
keywords = {Adaptive graph learning, Generalized regression model, Unsupervised feature selection}
}

@article{10.1007/s10489-021-02234-4,
author = {Cui, Zhiyan and Lu, Na},
title = {Feature selection accelerated convolutional neural networks for visual tracking},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02234-4},
doi = {10.1007/s10489-021-02234-4},
abstract = {Most of the existing tracking methods based on convolutional neural network (CNN) models are too slow for use in real-time applications despite their excellent tracking accuracy in comparison with traditional methods. Meanwhile, CNN tracking solutions are memory intensive and require considerable computational resources. In this paper, we propose a time-efficient and accurate tracking scheme, a feature selection accelerated CNN (FSNet) tracking solution based on MDNet (Multi-Domain Network). The large number of convolutional operations is a major contributor to the high computational cost of MDNet. To reduce the computational complexity, we incorporated an efficient mutual information-based feature selection over the convolutional layer that reduces the feature redundancy in feature maps. Considering that tracking is a typical binary classification problem, redundant feature maps can simply be pruned, which results in an insignificant influence on the tracking performance. To further accelerate the CNN tracking solution, a RoIAlign layer is added that can apply convolution to the entire image instead of just to each RoI (Region of Interest). The bilinear interpolation of RoIAlign could well reduce misalignment errors of the tracked target. In addition, a new fine-tuning strategy is used in the fully-connected layers to accelerate the online updating process. By combining the above strategies, the accelerated CNN achieves a speedup to 60 FPS (Frame Per Second) on the GPU compared with the original MDNet, which functioned at 1 FPS with a very low impact on tracking accuracy. We evaluated the proposed solution on four benchmarks: OTB50, OTB100 ,VOT2016 and UAV123. The extensive comparison results verify the superior performance of FSNet.},
journal = {Applied Intelligence},
month = nov,
pages = {8230–8244},
numpages = {15},
keywords = {RoIAlign, Feature selection, Mutual information, Visual tracking}
}

@article{10.1016/j.procs.2019.12.004,
author = {Niu, Yuting and Shang, Yuan and Tian, Yingjie},
title = {Multi-view SVM Classification with Feature Selection},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {162},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.12.004},
doi = {10.1016/j.procs.2019.12.004},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {405–412},
numpages = {8},
keywords = {linear programming, sparsity regularization, feature selection, classification, multi-view learning}
}

@article{10.3233/IDA-205154,
author = {Garc\'{\i}a, Maximiliano and Maldonado, Sebasti\'{a}n and Vairetti, Carla},
title = {Efficient n-gram construction for text categorization using feature selection techniques},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {3},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-205154},
doi = {10.3233/IDA-205154},
abstract = {In this paper, we present a novel approach for n-gram generation in text classification. The a-priori algorithm is adapted to prune word sequences by combining three feature selection techniques. Unlike the traditional two-step approach for text classification in which feature selection is performed after the n-gram construction process, our proposal performs an embedded feature elimination during the application of the a-priori algorithm. The proposed strategy reduces the number of branches to be explored, speeding up the process and making the construction of all the word sequences tractable. Our proposal has the additional advantage of constructing a low-dimensional dataset with only the features that are relevant for classification, that can be used directly without the need for a feature selection step. Experiments on text classification datasets for sentiment analysis demonstrate that our approach yields the best predictive performance when compared with other feature selection approaches, while also facilitating a better understanding of the words and phrases that explain a given task; in our case online reviews and ratings in various domains.},
journal = {Intell. Data Anal.},
month = jan,
pages = {509–525},
numpages = {17},
keywords = {sentiment analysis, text classification, n-gram construction, text categorization, Feature selection}
}

@article{10.3233/JIFS-200075,
author = {Ul Haq, Amin and Li, Jianping and Memon, Muhammad Hammad and khan, Jalaluddin and Ali, Zafar and Abbas, Syed Zaheer and Nazir, Shah},
title = {Recognition of the parkinson’s disease using a hybrid feature selection approach},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200075},
doi = {10.3233/JIFS-200075},
abstract = {Accurate and efficient recognition of Parkinson’s disease is one of the prominent issues in the field of healthcare. To address this problem, different methods have been proposed in the literature. However, existing methods are lacking in accurately recognizing the Parkinson’s disease and suffer from efficiency problems. To overcome these problems faced by existing models, this paper presents a machine-learning-based model for Parkinson’s disease recognition. Specifically, a hybrid feature selection algorithm has been designed by integrating the Relief and ant-colony optimization algorithms to select relevant features for training the model. Moreover, the support vector machine has been trained and tested on the selected features to achieve optimal classification accuracy. Additionally, the K-fold cross-validation technique has been employed for the optimal hyper-parameters value evaluation of the model.The experimental results on a real-world dataset, i.e., Parkinson’s disease dataset is revealed that the proposed system outperforms baseline competitors by accurately recognizing the Parkinson’s disease and achieving 99.50% accuracy on the selected features. Due to high performance is achieved our proposed method, we are highly recommended for the recognition of PD.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1319–1339},
numpages = {21},
keywords = {machine learning, classification, feature selection algorithm, parkinson’s disease recognition, ant colony optimization, Relief}
}

@article{10.1007/s00766-014-0201-3,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasingham and Gopalasundaram, Umatharan},
title = {On building architecture-centric product line architecture},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0201-3},
doi = {10.1007/s00766-014-0201-3},
abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives.},
journal = {Requir. Eng.},
month = sep,
pages = {301–321},
numpages = {21},
keywords = {Software product line, Software performance, Requirements validation, Patterns, Architecture evaluation, Architectural prototyping}
}

@article{10.1016/j.eswa.2018.04.037,
author = {Bi, Ning and Tan, Jun and Lai, Jian-Huang and Suen, Ching Y.},
title = {High-dimensional supervised feature selection via optimized kernel mutual information},
year = {2018},
issue_date = {Oct 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.04.037},
doi = {10.1016/j.eswa.2018.04.037},
journal = {Expert Syst. Appl.},
month = oct,
pages = {81–95},
numpages = {15},
keywords = {Feature selection, Kernel method, Mutual information, Classification, Optimize function, Machine learning}
}

@article{10.1016/j.ins.2019.05.038,
author = {Taradeh, Mohammad and Mafarja, Majdi and Heidari, Ali Asghar and Faris, Hossam and Aljarah, Ibrahim and Mirjalili, Seyedali and Fujita, Hamido},
title = {An evolutionary gravitational search-based feature selection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {497},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.05.038},
doi = {10.1016/j.ins.2019.05.038},
journal = {Inf. Sci.},
month = sep,
pages = {219–239},
numpages = {21},
keywords = {Optimization, Classification, Supervised learning, Feature selection, Genetic algorithm, Gravitational search algorithm}
}

@inproceedings{10.1145/3411408.3411438,
author = {Tzanetos, Alexandros and Dounias, Georgios},
title = {Sonar Inspired Optimization based Feature Selection},
year = {2020},
isbn = {9781450388788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411408.3411438},
doi = {10.1145/3411408.3411438},
abstract = {One of the problems that Machine Learning (ML) algorithms face in classification tasks is the Curse of Dimensionality, which refers to the sensitivity of their performance to the data dimensionality. The solution to this problem is to select the features that are of higher importance for the model produced. To improve the performance of a classifier, various meta-heuristic algorithms have been implemented due to their ability to provide optimal solutions in problems that there are multiple candidate solutions, such as in Feature Selection (FS). In this study, Sonar Inspired Optimization (SIO) algorithm is used to perform FS in order to improve the performance of a state-of-the-art classifier, i.e. k-NN. SIO’s performance is compared with other nature-inspired meta-heuristic algorithms that have been used for the same task.},
booktitle = {11th Hellenic Conference on Artificial Intelligence},
pages = {195–201},
numpages = {7},
keywords = {sonar inspired optimization, nature-inspired algorithms, feature selection},
location = {Athens, Greece},
series = {SETN 2020}
}

@article{10.1016/j.asoc.2021.107876,
author = {Zhang, Jie and Zhang, Gangqiang and Li, Zhaowen and Qu, Liangdong and Wen, Ching-Feng},
title = {Feature selection in a neighborhood decision information system with application to single cell RNA data classification},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107876},
doi = {10.1016/j.asoc.2021.107876},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {17},
keywords = {δ-conditional information entropy, δ-information entropy, Single cell, Feature selection, NDIS}
}

@article{10.1504/ijdmb.2020.110156,
author = {Lu, Yao and Gao, Ying-Lian and Li, Pei-Yong and Liu, Jin-Xing},
title = {A multi-view classification and feature selection method via sparse low-rank regression analysis},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {2},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2020.110156},
doi = {10.1504/ijdmb.2020.110156},
abstract = {In recent years, multi-view classification and feature selection methods have received close attention in many fields. However, in many practical classification problems, the data in each view may contain a lot of noises. In addition, when data are of high dimensions and small sample attributes, it is difficult to remove redundant features in feature selection experiments. To deal with these problems well, the sparse multi-view low-rank regression method is proposed in this paper. The method based on sparse and low-rank theory introduces the penalty factors in the matrix transformation process to decompose the matrix into sparse and low-rank results. The model is constructed by imposing L2-norm and L2,1-norm constraints on the objective function. Experimental results on sequencing data show that the proposed method has superior performance over several state-of-the-art methods in multi-view classification and feature selection.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {140–159},
numpages = {19},
keywords = {row-sparsity, multi-view data, low-rank regression, L2,1-norm, feature selection, classification}
}

@article{10.1016/j.asoc.2021.107993,
author = {Zhou, Peng and Li, Peipei and Zhao, Shu and Zhang, Yanping},
title = {Online early terminated streaming feature selection based on Rough Set theory▪},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PB},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107993},
doi = {10.1016/j.asoc.2021.107993},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {12},
keywords = {Dependency degree, Rough Set, Early terminated, Streaming features, Online feature selection}
}

@article{10.1016/j.patcog.2019.03.026,
author = {Shang, Ronghua and Meng, Yang and Wang, Wenbing and Shang, Fanhua and Jiao, Licheng},
title = {Local discriminative based sparse subspace learning for feature selection},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {92},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.03.026},
doi = {10.1016/j.patcog.2019.03.026},
journal = {Pattern Recogn.},
month = aug,
pages = {219–230},
numpages = {12},
keywords = {Feature selection, Sparse constraint, Subspace learning, Local discriminant model}
}

@inproceedings{10.1145/3448218.3448241,
author = {Wiratsin, In-On and Narupiyakul, Lalita},
title = {Feature Selection Technique for Autism Spectrum Disorder},
year = {2021},
isbn = {9781450388870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448218.3448241},
doi = {10.1145/3448218.3448241},
abstract = {Autism Spectrum Disorder (ASD) is a developmental disorder that restricts the development of behaviors, communication, and learning skills. People with ASD have difficulties in communicating and engaging with other people. Presently, a questionnaire called Autism Spectrum Quotient 10 (AQ-10) has been used to diagnose ASD symptoms for individuals [1]. However, using AQ-10 for ASD diagnosis is based on the observation from an individual's behavior and evaluate the result. It cannot show the outstanding attribute or relationship between ASD symptoms in different age-groups. Thus, this research proposed the novel feature selection technique for identifying the important attributes in AQ-10 results from three different age-groups, which are children at age of 7--12, adolescents at age of 13--20, and adults at age over 20. The results will represent the outstanding features and relationships between ASD symptoms in different age-groups, and lead to the proper design of ASD treatment programs for individuals.},
booktitle = {Proceedings of the 5th International Conference on Control Engineering and Artificial Intelligence},
pages = {53–56},
numpages = {4},
keywords = {Feature selection, Autism Spectrum Disorder screening, Apriori algorithm},
location = {Sanya, China},
series = {CCEAI '21}
}

@inproceedings{10.1007/978-3-030-87615-9_12,
author = {Akbar, Md Navid and Ruf, Sebastian and La Rocca, Marianna and Garner, Rachael and Barisano, Giuseppe and Cua, Ruskin and Vespa, Paul and Erdo\u{g}mu\c{s}, Deniz and Duncan, Dominique},
title = {Lesion Normalization and Supervised Learning in Post-traumatic Seizure Classification with Diffusion MRI},
year = {2021},
isbn = {978-3-030-87614-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87615-9_12},
doi = {10.1007/978-3-030-87615-9_12},
abstract = {Traumatic brain injury (TBI) is a serious condition, potentially causing seizures and other lifelong disabilities. Patients who experience at least one seizure one week after TBI (late seizure) are at high risk for lifelong complications of TBI, such as post-traumatic epilepsy (PTE). Identifying which TBI patients are at risk of developing seizures remains a challenge. Although magnetic resonance imaging (MRI) methods that probe structural and functional alterations after TBI are promising for biomarker detection, physical deformations following moderate-severe TBI present problems for standard processing of neuroimaging data, complicating the search for biomarkers. In this work, we consider a prediction task to identify which TBI patients will develop late seizures, using fractional anisotropy (FA) features from white matter tracts in diffusion-weighted MRI (dMRI). To understand how best to account for brain lesions and deformations, four preprocessing strategies are applied to dMRI, including the novel application of a lesion normalization technique to dMRI. The pipeline involving the lesion normalization technique provides the best prediction performance, with a mean accuracy of 0.819 and a mean area under the curve of 0.785. Finally, following statistical analyses of selected features, we recommend the dMRI alterations of a certain white matter tract as a potential biomarker.},
booktitle = {Computational Diffusion MRI: 12th International Workshop, CDMRI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings},
pages = {133–143},
numpages = {11},
keywords = {Biomarker, Classification, Feature selection, Lesion normalization, Diffusion MRI, Post-traumatic epilepsy},
location = {Strasbourg, France}
}

@inproceedings{10.1007/978-3-030-30484-3_1,
author = {Niu, Sijia and Zhu, Pengfei and Hu, Qinghua and Shi, Hong},
title = {Adaptive Graph Fusion for Unsupervised Feature Selection},
year = {2019},
isbn = {978-3-030-30483-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30484-3_1},
doi = {10.1007/978-3-030-30484-3_1},
abstract = {The massive high-dimensional data brings about great time complexity, high storage burden and poor generalization ability of learning models. Feature selection can alleviate curse of dimensionality by selecting a subset of features. Unsupervised feature selection is much challenging due to lack of label information. Most methods rely on spectral clustering to generate pseudo labels to guide feature selection in unsupervised setting. Graphs for spectral clustering can be constructed in different ways, e.g., kernel similarity, or self-representation. The construction of adjacency graphs could be affected by the parameters of kernel functions, the number of nearest neighbors or the size of the neighborhood. However, it is difficult to evaluate the effectiveness of different graphs in unsupervised feature selection. Most existing algorithms only select one graph by experience. In this paper, we propose a novel adaptive multi-graph fusion based unsupervised feature selection model (GFFS). The proposed model is free of graph selection and can combine the complementary information of different graphs. Experiments on benchmark datasets show that GFFS outperforms the state-of-the-art unsupervised feature selection algorithms.},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning: 28th International Conference on Artificial Neural Networks, Munich, Germany, September 17–19, 2019, Proceedings, Part II},
pages = {3–15},
numpages = {13},
keywords = {Self-representation, Unsupervised feature selection, Graph fusion},
location = {Munich, Germany}
}

@article{10.1007/s10994-018-5765-6,
author = {Shang, Ronghua and Meng, Yang and Liu, Chiyang and Jiao, Licheng and Esfahani, Amir M. Ghalamzan and Stolkin, Rustam},
title = {Unsupervised feature selection based on kernel fisher discriminant analysis and regression learning},
year = {2019},
issue_date = {Apr 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {4},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-018-5765-6},
doi = {10.1007/s10994-018-5765-6},
abstract = {In this paper, we propose a new feature selection method called kernel fisher discriminant analysis and regression learning based algorithm for unsupervised feature selection. The existing feature selection methods are based on either manifold learning or discriminative techniques, each of which has some shortcomings. Although some studies show the advantages of two-steps method benefiting from both manifold learning and discriminative techniques, a joint formulation has been shown to be more efficient. To do so, we construct a global discriminant objective term of a clustering framework based on the kernel method. We add another term of regression learning into the objective function, which can impose the optimization to select a low-dimensional representation of the original dataset. We use L
2,1-norm of the features to impose a sparse structure upon features, which can result in more discriminative features. We propose an algorithm to solve the optimization problem introduced in this paper. We further discuss convergence, parameter sensitivity, computational complexity, as well as the clustering and classification accuracy of the proposed algorithm. In order to demonstrate the effectiveness of the proposed algorithm, we perform a set of experiments with different available datasets. The results obtained by the proposed algorithm are compared against the state-of-the-art algorithms. These results show that our method outperforms the existing state-of-the-art methods in many cases on different datasets, but the improved performance comes with the cost of increased time complexity.},
journal = {Mach. Learn.},
month = apr,
pages = {659–686},
numpages = {28},
keywords = {Feature selection, Sparse constraint, Regression learning, Manifold learning, Kernel fisher discriminant analysis}
}

@inproceedings{10.1007/978-3-642-33176-3_7,
author = {ter Beek, Maurice H. and Muccini, Henry and Pelliccione, Patrizio},
title = {Assume-guarantee testing of evolving software product line architectures},
year = {2012},
isbn = {9783642331756},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33176-3_7},
doi = {10.1007/978-3-642-33176-3_7},
abstract = {Despite some work on testing software product lines, maintaining the quality of products when a software product line evolves is still an open problem. In this paper, we propose a novel assume-guarantee testing approach as a solution to the following research question: how can we verify the correct functioning of products of an software product line when core components evolve? The underlying idea is to retest only some of the products that conform to the software product line architecture and to infer, using assume-guarantee reasoning, the correctness of the other products. Assume-guarantee reasoning moreover permits the retesting of only those components that are affected by the changes.},
booktitle = {Proceedings of the 4th International Conference on Software Engineering for Resilient Systems},
pages = {91–105},
numpages = {15},
keywords = {software testing, evolving software product lines, compositional verification, assume-guarantee testing},
location = {Pisa, Italy},
series = {SERENE'12}
}

@article{10.1145/3467477,
author = {Telikani, Akbar and Tahmassebi, Amirhessam and Banzhaf, Wolfgang and Gandomi, Amir H.},
title = {Evolutionary Machine Learning: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3467477},
doi = {10.1145/3467477},
abstract = {Evolutionary Computation (EC) approaches are inspired by nature and solve optimization problems in a stochastic manner. They can offer a reliable and effective approach to address complex problems in real-world applications. EC algorithms have recently been used to improve the performance of Machine Learning (ML) models and the quality of their results. Evolutionary approaches can be used in all three parts of ML: preprocessing (e.g., feature selection and resampling), learning (e.g., parameter setting, membership functions, and neural network topology), and postprocessing (e.g., rule optimization, decision tree/support vectors pruning, and ensemble learning). This article investigates the role of EC algorithms in solving different ML challenges. We do not provide a comprehensive review of evolutionary ML approaches here; instead, we discuss how EC algorithms can contribute to ML by addressing conventional challenges of the artificial intelligence and ML communities. We look at the contributions of EC to ML in nine sub-fields: feature selection, resampling, classifiers, neural networks, reinforcement learning, clustering, association rule mining, and ensemble methods. For each category, we discuss evolutionary machine learning in terms of three aspects: problem formulation, search mechanisms, and fitness value computation. We also consider open issues and challenges that should be addressed in future work.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {161},
numpages = {35},
keywords = {swarm intelligence, learning optimization, Evolutionary computation}
}

@article{10.1504/ijista.2021.114648,
author = {Vishnoi, E. Susheela and Jain, Ajit Kumar},
title = {An improved Henry gas solubility optimisation-based feature selection approach for histological image taxonomy},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {1},
issn = {1740-8865},
url = {https://doi.org/10.1504/ijista.2021.114648},
doi = {10.1504/ijista.2021.114648},
abstract = {Classification of histopathological images is one of the important areas of research in the field of medical imaging. However, the complexities available in histopathological images make the classification process difficult. For such complex images, selection of prominent features for image classification is also a challenging task and is still an open research area for computer vision researchers. Therefore, an effective method for the selection of prominent features of images has been introduced in this work. For the same, an improved Henry gas solubility optimisation has been introduced in which a new position update equation has been used to balance the global and local search. The selected features are then input to classifiers to identify histopathological images. For the performance analysis of improved Henry gas solubility optimisation, 23 benchmark functions are used. The proposed feature selection method has been analysed over two datasets, namely breast cancer cell dataset and ICIAR grand challenge dataset. The proposed feature selection method eliminates the maximum 60% average features from both the datasets. To validate usefulness of selected features, results of different classifiers are compared. Experimental results show that the presented method outperforms other methods.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {58–78},
numpages = {20},
keywords = {image classification, histology images, Henry gas solubility optimisation algorithm, feature selection}
}

@inproceedings{10.1007/978-3-030-93944-1_8,
author = {Goldsteen, Abigail and Ezov, Gilad and Shmelkin, Ron and Moffie, Micha and Farkash, Ariel},
title = {Anonymizing Machine Learning Models},
year = {2021},
isbn = {978-3-030-93943-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-93944-1_8},
doi = {10.1007/978-3-030-93944-1_8},
abstract = {There is a known tension between the need to analyze personal data to drive business and the need to preserve the privacy of data subjects. Many data protection regulations, including the EU General Data Protection Regulation (GDPR) and the California Consumer Protection Act (CCPA), set out strict restrictions and obligations on the collection and processing of personal data. Moreover, machine learning models themselves can be used to derive personal information, as demonstrated by recent membership and attribute inference attacks. Anonymized data, however, is exempt from the obligations set out in these regulations. It is therefore desirable to be able to create models that are anonymized, thus also exempting them from those obligations, in addition to providing better protection against attacks.Learning on anonymized data typically results in significant degradation in accuracy. In this work, we propose a method that is able to achieve better model accuracy by using the knowledge encoded within the trained model, and guiding our anonymization process to minimize the impact on the model’s accuracy, a process we call accuracy-guided anonymization. We demonstrate that by focusing on the model’s accuracy rather than generic information loss measures, our method outperforms state of the art k-anonymity methods in terms of the achieved utility, in particular with high values of k and large numbers of quasi-identifiers.We also demonstrate that our approach has a similar, and sometimes even better ability to prevent membership inference attacks as approaches based on differential privacy, while averting some of their drawbacks such as complexity, performance overhead and model-specific implementations. In addition, since our approach does not rely on making modifications to the training algorithm, it can even work with “black-box” models where the data owner does not have full control over the training process, or within complex machine learning pipelines where it may be difficult to replace existing learning algorithms with new ones. This makes model-guided anonymization a legitimate substitute for such methods and a practical approach to creating privacy-preserving models.},
booktitle = {Data Privacy Management, Cryptocurrencies and Blockchain Technology: ESORICS 2021 International Workshops, DPM 2021 and CBT 2021, Darmstadt, Germany, October 8, 2021, Revised Selected Papers},
pages = {121–136},
numpages = {16},
keywords = {Machine learning, Privacy, Compliance, k-anonymity, Anonymization, GDPR},
location = {Darmstadt, Germany}
}

@article{10.1016/j.ins.2019.01.050,
author = {Gonz\'{a}lez-Dom\'{\i}nguez, Jorge and Bol\'{o}n-Canedo, Ver\'{o}nica and Freire, Borja and Touri\~{n}o, Juan},
title = {Parallel feature selection for distributed-memory clusters},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {496},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.050},
doi = {10.1016/j.ins.2019.01.050},
journal = {Inf. Sci.},
month = sep,
pages = {399–409},
numpages = {11},
keywords = {Parallel computing, High performance computing, Feature selection, Machine learning}
}

@article{10.4018/IJEHMC.20211101.oa5,
author = {Al-Hashem, Munder Abdulatef and Alqudah, Ali Mohammad and Qananwah, Qasem},
title = {Performance Evaluation of Different Machine Learning Classification Algorithms for Disease Diagnosis},
year = {2021},
issue_date = {Sep 2021},
publisher = {IGI Global},
address = {USA},
volume = {12},
number = {6},
issn = {1947-315X},
url = {https://doi.org/10.4018/IJEHMC.20211101.oa5},
doi = {10.4018/IJEHMC.20211101.oa5},
abstract = {Knowledge extraction within a healthcare field is a very challenging task since we are having many problems such as noise and imbalanced datasets. They are obtained from clinical studies where uncertainty and variability are popular. Lately, a wide number of machine learning algorithms are considered and evaluated to check their validity of being used in the medical field. Usually, the classification algorithms are compared against medical experts who are specialized in certain disease diagnoses and provide an effective methodological evaluation of classifiers by applying performance metrics. The performance metrics contain four criteria: accuracy, sensitivity, and specificity forming the confusion matrix of each used algorithm. We have utilized eight different well-known machine learning algorithms to evaluate their performances in six different medical datasets. Based on the experimental results we conclude that the XGBoost and K-Nearest Neighbor classifiers were the best overall among the used datasets and signs can be used for diagnosing various diseases.},
journal = {Int. J. E-Health Med. Commun.},
month = nov,
pages = {1–28},
numpages = {28},
keywords = {Performance, Medical Diseases, Machine Learning, Artificial Intelligence}
}

@article{10.1145/3309541,
author = {Jiang, Bingbing and Li, Chang and Rijke, Maarten De and Yao, Xin and Chen, Huanhuan},
title = {Probabilistic Feature Selection and Classification Vector Machine},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3309541},
doi = {10.1145/3309541},
abstract = {Sparse Bayesian learning is a state-of-the-art supervised learning algorithm that can choose a subset of relevant samples from the input data and make reliable probabilistic predictions. However, in the presence of high-dimensional data with irrelevant features, traditional sparse Bayesian classifiers suffer from performance degradation and low efficiency due to the incapability of eliminating irrelevant features. To tackle this problem, we propose a novel sparse Bayesian embedded feature selection algorithm that adopts truncated Gaussian distributions as both sample and feature priors. The proposed algorithm, called probabilistic feature selection and classification vector machine (PFCVMLP) is able to simultaneously select relevant features and samples for classification tasks. In order to derive the analytical solutions, Laplace approximation is applied to compute approximate posteriors and marginal likelihoods. Finally, parameters and hyperparameters are optimized by the type-II maximum likelihood method. Experiments on three datasets validate the performance of PFCVMLP along two dimensions: classification performance and effectiveness for feature selection. Finally, we analyze the generalization performance and derive a generalization error bound for PFCVMLP. By tightening the bound, the importance of feature selection is demonstrated.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {21},
numpages = {27},
keywords = {supervised learning, sparse Bayesian learning, probabilistic classification model, Feature selection, EEG emotion recognition}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Software product line, Software engineering, Software architecture, Empirical study, Domain engineering}
}

@article{10.1007/s10489-021-02288-4,
author = {Wu, Xinping and Chen, Hongmei and Li, Tianrui and Wan, Jihong},
title = {Semi-supervised feature selection with minimal redundancy based on local adaptive},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02288-4},
doi = {10.1007/s10489-021-02288-4},
abstract = {With the speedy development of network technology, diverse data increase by hundreds of millions per hour, causing increasing pressure on the acquisition of data labels. Semi-supervised feature selection has been among the forefront of dimensionality reduction research due to the outstanding achievements of “small labels” and “high efficiency”. Especially, the graph-based methods use data with missing labels completely and effectively, prompting it to become a research hotspots in semi-supervised feature selection. However, the existing graph-based methods do not take into account the effects of outliers, noise, and redundancy of selected features simultaneously. To solve those problems, a novel semi-supervised feature selection method based on local adaptive and minimal redundancy is proposed. The local structure is flexibly assigned weights according to the data conditions, thereby reducing the impact of outliers and noise; moreover, a high similarity penalty mechanism is introduced in the feature mapping matrix to promote discrimination and low redundancy of the selected feature subset. In addition, an iterative method is designed and its convergence is proved theoretically and experimentally. Finally, the proposed algorithm is verified to be stable and effective through experiments from five aspects on sixteen public datasets.},
journal = {Applied Intelligence},
month = nov,
pages = {8542–8563},
numpages = {22},
keywords = {Redundancy minimization, Local adaptive least squares regression, Feature selection, Semi-supervised learning}
}

@article{10.1007/s10044-021-00985-x,
author = {Jia, Heming and Sun, Kangjian},
title = {Improved barnacles mating optimizer algorithm for feature selection and support vector machine optimization},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-021-00985-x},
doi = {10.1007/s10044-021-00985-x},
abstract = {With the rapid development of computer technology, data collection becomes easier, and data object presents more complex. Data analysis method based on machine learning is an important, active, and multi-disciplinarily research field. Support vector machine (SVM) is one of the most powerful and fast classification models. The main challenges SVM faces are the selection of feature subset and the setting of kernel parameters. To improve the performance of SVM, a metaheuristic algorithm is used to optimize them simultaneously. This paper first proposes a novel classification model called IBMO-SVM, which hybridizes an improved barnacle mating optimizer (IBMO) with SVM. Three strategies, including Gaussian mutation, logistic model, and refraction-learning, are used to improve the performance of BMO from different perspectives. Through 23 classical benchmark functions, the impact of control parameters and the effectiveness of introduced strategies are analyzed. The convergence accuracy and stability are the main gains, and exploration and exploitation phases are more properly balanced. We apply IBMO-SVM to 20 real-world datasets, including 4 extremely high-dimensional datasets. Experimental results are compared with 6 state-of-the-art methods in the literature. The final statistical results show that the proposed IBMO-SVM achieves a better performance than the standard BMO-SVM and other compared methods, especially on high-dimensional datasets. In addition, the proposed model also shows significant superiority compared with 4 other classifiers.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {1249–1274},
numpages = {26},
keywords = {Barnacles mating optimizer, Feature selection, Support vector machine, Gaussian mutation, Logistic model, Refraction-learning}
}

@article{10.1561/2200000081,
author = {Holden, Sean B.},
title = {Machine Learning for Automated Theorem Proving: Learning to Solve SAT and QSAT},
year = {2021},
issue_date = {Nov 2021},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {14},
number = {6},
issn = {1935-8237},
url = {https://doi.org/10.1561/2200000081},
doi = {10.1561/2200000081},
abstract = {The decision problem for Boolean satisfiability, generally
    referred to as SAT, is the archetypal NP-complete problem,
    and encodings of many problems of practical interest exist
    allowing them to be treated as SAT problems. Its generalization
    to quantified SAT (QSAT) is PSPACE-complete, and
    is useful for the same reason. Despite the computational
    complexity of SAT and QSAT, methods have been developed
    allowing large instances to be solved within reasonable
    resource constraints. These techniques have largely exploited
    algorithmic developments; however machine learning also
    exerts a significant influence in the development of state-ofthe-
    art solvers. Here, the application of machine learning
    is delicate, as in many cases, even if a relevant learning
    problem can be solved, it may be that incorporating the
    result into a SAT or QSAT solver is counterproductive, because
    the run-time of such solvers can be sensitive to small
    implementation changes. The application of better machine
    learning methods in this area is thus an ongoing challenge,
    with characteristics unique to the field. This work provides
    a comprehensive review of the research to date on incorporating
    machine learning into SAT and QSAT solvers, as a
    resource for those interested in further advancing the field.},
journal = {Found. Trends Mach. Learn.},
month = nov,
pages = {807–989},
numpages = {187}
}

@article{10.1016/j.knosys.2019.04.024,
author = {Zhong, Jing and Wang, Nan and Lin, Qiang and Zhong, Ping},
title = {Weighted feature selection via discriminative sparse multi-view learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {178},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.04.024},
doi = {10.1016/j.knosys.2019.04.024},
journal = {Know.-Based Syst.},
month = aug,
pages = {132–148},
numpages = {17},
keywords = {Separable penalty strategy, Weighted loss, Multi-view, Supervised structured sparsity-inducing feature selection}
}

@article{10.1145/3478088,
author = {Islam, Md. Rabiul and Sakamoto, Shuji and Yamada, Yoshihiro and Vargo, Andrew W. and Iwata, Motoi and Iwamura, Masakazu and Kise, Koichi},
title = {Self-supervised Learning for Reading Activity Classification},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478088},
doi = {10.1145/3478088},
abstract = {Reading analysis can relay information about user's confidence and habits and can be used to construct useful feedback. A lack of labeled data inhibits the effective application of fully-supervised Deep Learning (DL) for automatic reading analysis. We propose a Self-supervised Learning (SSL) method for reading analysis. Previously, SSL has been effective in physical human activity recognition (HAR) tasks, but it has not been applied to cognitive HAR tasks like reading. We first evaluate the proposed method on a four-class classification task on reading detection using electrooculography datasets, followed by an evaluation of a two-class classification task of confidence estimation on multiple-choice questions using eye-tracking datasets. Fully-supervised DL and support vector machines (SVMs) are used as comparisons for the proposed SSL method. The results show that the proposed SSL method is superior to the fully-supervised DL and SVM for both tasks, especially when training data is scarce. This result indicates the proposed method is the superior choice for reading analysis tasks. These results are important for informing the design of automatic reading analysis platforms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {105},
numpages = {22},
keywords = {reading detection, reading analysis, fully-supervised deep learning, confidence estimation, Self-supervised learning}
}

@article{10.1007/s00521-017-3062-0,
author = {Coelho, Frederico and Castro, Cristiano and Braga, Ant\^{o}nio P. and Verleysen, Michel},
title = {Semi-supervised relevance index for feature selection},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-3062-0},
doi = {10.1007/s00521-017-3062-0},
abstract = {This paper presents a new relevance index based on mutual information that is based on labeled and unlabeled data. The proposed index, which is based in Mutual Information, takes into account the similarity between features and their joint influence on the output variable. Based on this principle, a method to select features is developed to eliminate redundant and irrelevant features when the relevance index value is less then a threshold value. A strategy to set the threshold is also proposed in this work. Experiments show that the new method is capable of capturing important joint relations between input and output variables, which are incorporated into a new feature selection clustering approach.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {989–997},
numpages = {9},
keywords = {Similarity criterion, Semi-supervised, Mutual information, Feature selection}
}

@article{10.1007/s10462-019-09682-y,
author = {Solorio-Fern\'{a}ndez, Sa\'{u}l and Carrasco-Ochoa, J. Ariel and Mart\'{\i}nez-Trinidad, Jos\'{e} Fco.},
title = {A review of unsupervised feature selection methods},
year = {2020},
issue_date = {Feb 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-019-09682-y},
doi = {10.1007/s10462-019-09682-y},
abstract = {In recent years, unsupervised feature selection methods have raised considerable interest in many research areas; this is mainly due to their ability to identify and select relevant features without needing class label information. In this paper, we provide a comprehensive and structured review of the most relevant and recent unsupervised feature selection methods reported in the literature. We present a taxonomy of these methods and describe the main characteristics and the fundamental ideas they are based on. Additionally, we summarized the advantages and disadvantages of the general lines in which we have categorized the methods analyzed in this review. Moreover, an experimental comparison among the most representative methods of each approach is also presented. Finally, we discuss some important open challenges in this research area.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {907–948},
numpages = {42},
keywords = {Feature selection for clustering, Unsupervised feature selection, Dimensionality reduction, Unsupervised learning}
}

@article{10.1016/j.asoc.2021.107897,
author = {Mehedi, Ibrahim Mustafa and Ahmadipour, Masoud and Salam, Zainal and Ridha, Hussein Mohammed and Bassi, Hussein and Rawa, Muhyaddin Jamal Hosin and Ajour, Mohammad and Abusorrah, Abdullah and Abdullah, Md. Pauzi},
title = {Optimal feature selection using modified cuckoo search for classification of power quality disturbances},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107897},
doi = {10.1016/j.asoc.2021.107897},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {15},
keywords = {Optimal feature selection, Multiclass support vector machine, Modified cuckoo search, Wavelet packet transform, Power quality disturbances}
}

@article{10.1016/j.neucom.2018.02.032,
author = {Xie, Ting and Ren, Pengfei and Zhang, Taiping and Tang, Yuan Yan},
title = {Distribution preserving learning for unsupervised feature selection},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {289},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.02.032},
doi = {10.1016/j.neucom.2018.02.032},
abstract = {Selection of most relevant features from high-dimensional data is difficult especially in unsupervised learning scenario, this is because there is an absence of class labels that would guide the search for relevant features. In this work, we propose a distribution preserving feature selection (DPFS) method for unsupervised feature selection. Specifically, we select those features such that the distribution of the data can be preserved. Theoretical analysis show that our proposed DPFS method share some excellent properties of kernel method. Moreover, traditional wrapper and filter feature selection methods often involve an exhaustive search optimization, feature selection problem is treated as variable of optimization problem in our proposed method, the optimization is tractable. Extensive experimental results over various real-life data sets have demonstrated the effectiveness of the proposed algorithm.},
journal = {Neurocomput.},
month = may,
pages = {231–240},
numpages = {10},
keywords = {Kernel density estimation, Feature selection, Dimensionality reduction, Density preserving, Data mining}
}

@inproceedings{10.1007/978-3-031-08147-7_13,
author = {Vitorino, Jo\~{a}o and Andrade, Rui and Pra\c{c}a, Isabel and Sousa, Orlando and Maia, Eva},
title = {A Comparative Analysis of Machine Learning Techniques for IoT Intrusion Detection},
year = {2021},
isbn = {978-3-031-08146-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08147-7_13},
doi = {10.1007/978-3-031-08147-7_13},
abstract = {The digital transformation faces tremendous security challenges. In particular, the growing number of cyber-attacks targeting Internet of Things (IoT) systems restates the need for a reliable detection of malicious network activity. This paper presents a comparative analysis of supervised, unsupervised and reinforcement learning techniques on nine malware captures of the IoT-23 dataset, considering both binary and multi-class classification scenarios. The developed models consisted of Support Vector Machine (SVM), Extreme Gradient Boosting (XGBoost), Light Gradient Boosting Machine (LightGBM), Isolation Forest (iForest), Local Outlier Factor (LOF) and a Deep Reinforcement Learning (DRL) model based on a Double Deep Q-Network (DDQIN), adapted to the intrusion detection context. The most reliable performance was achieved by LightGBM. Nonetheless, iForest displayed good anomaly detection results and the DRL model demonstrated the possible benefits of employing this methodology to continuously improve the detection. Overall, the obtained results indicate that the analyzed techniques are well suited for IoT intrusion detection.},
booktitle = {Foundations and Practice of Security: 14th International Symposium, FPS 2021, Paris, France, December 7–10, 2021, Revised Selected Papers},
pages = {191–207},
numpages = {17},
keywords = {Reinforcement learning, Unsupervised learning, Supervised learning, Intrusion detection, Internet of Things},
location = {Paris, France}
}

@article{10.1007/s00521-021-05745-w,
author = {Mohammad, Rami Mustafa A. and Alsmadi, Mutasem K.},
title = {Intrusion detection using Highest Wins feature selection algorithm},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05745-w},
doi = {10.1007/s00521-021-05745-w},
abstract = {The rapid advancement of Internet stimulates building intelligent data mining systems for detecting intrusion attacks. The performance of such systems might be negatively affected due to the big datasets employed in the learning phase. Determining the appropriate group of features within training datasets is an essential phase when building data mining classification models. Nevertheless, the resulted minimized set of features should maintain or even improve the performance of the classification models. Throughout this article, an innovative feature selection algorithm is proposed and is called “the Highest Wins” (HW). To evaluate the generalization ability of HW, it has been applied for creating classification models using na\"{\i}ve Bayes technique from 10 benchmark datasets. The obtained results were compared against two well-known strategies, namely chi-square and information gain. The experimental results confirmed the competitiveness ability of the suggested strategy in terms of various evaluation measurements such as recall, precision, and error rate while significantly decreasing the number of selected features in datasets. Further, the HW is used for building a na\"{\i}ve Bayes and decision tree intrusion detection classifiers using the well-known dataset from Network Security Laboratory-Knowledge Discovery in Databases (NSL-KDD). The results were promising not just in terms of overall performance, but also in terms of the time needed to build the classification model.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {9805–9816},
numpages = {12},
keywords = {Feature selection, Data mining, Na\"{\i}ve Bayes, Intrusion detection}
}

@article{10.1016/j.engappai.2019.103283,
author = {Han, Xiaohong and Liu, Ping and Wang, Li and Li, Dengao},
title = {Unsupervised feature selection via graph matrix learning and the low-dimensional space learning for classification},
year = {2020},
issue_date = {Jan 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {87},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2019.103283},
doi = {10.1016/j.engappai.2019.103283},
journal = {Eng. Appl. Artif. Intell.},
month = jan,
numpages = {13},
keywords = {Manifold learning, Dimensionality reduction, Graph matrix, Feature selection}
}

@inproceedings{10.1145/3473258.3473275,
author = {Mungloo-Dilmohamud, Zahra and Jaufeerally-Fakim, Yasmina and Pena-Reyes, Carlos},
title = {Ensemble Feature Selection: Are Stability Metrics a Proxy or a Complement to Predictive Performance?},
year = {2021},
isbn = {9781450389655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473258.3473275},
doi = {10.1145/3473258.3473275},
abstract = {Proper identification of biomarkers, used in the development of drugs, is critical as has been shown with the race to find a vaccine for the Covid19. Gene-expression based marker discovery often entails that feature selection be performed. However, a plethora of feature selection methods exist and they do not result in the selection of the same feature subsets for the same dataset. Often, users are faced with having to select which subset to use. To help in this conundrum, several approaches have been proposed to guide feature subset selection, among which the use of ensemble methods (i.e., combining subsets from multiple methods) has gained attention recently. In an ensemble approach there are two issues that deserve attention: the stability of the feature subsets being combined and the classification performance of the combined feature subsets. Hence the interest in exploring how stability and performance relate, which is the central topic investigated in this paper. First 5/6 different feature selection methods are used to create feature subsets for 3 different transcriptomics datasets. Then, the stability and performance of these feature subsets under a given merging strategy are computed using 5 stability metrics and 3 performance metrics for 3 different classifiers. Our results suggest that performance and stability criteria are complementary and conflicting and that both must be considered to decide on the final selected feature subsets. We use two reference metrics to illustrate such selection.},
booktitle = {Proceedings of the 2021 13th International Conference on Bioinformatics and Biomedical Technology},
pages = {108–115},
numpages = {8},
keywords = {stability metrics, performance, feature subsets merging, feature selection, ensemble methods},
location = {Xi'an, China},
series = {ICBBT '21}
}

@article{10.1016/j.eswa.2019.113133,
author = {Cura, Tunchan},
title = {Use of support vector machines with a parallel local search algorithm for data classification and feature selection},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113133},
doi = {10.1016/j.eswa.2019.113133},
journal = {Expert Syst. Appl.},
month = may,
numpages = {17},
keywords = {Machine learning, Heuristic, Classification, Feature selection, Support vector machines}
}

@article{10.1016/j.asoc.2021.107112,
author = {Almaghrabi, Fatima and Xu, Dong-Ling and Yang, Jian-Bo},
title = {An evidential reasoning rule based feature selection for improving trauma outcome prediction},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {103},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107112},
doi = {10.1016/j.asoc.2021.107112},
journal = {Appl. Soft Comput.},
month = may,
numpages = {15},
keywords = {Imbalance classes, ReliefF, Random forest, Evidential reasoning rule, Trauma, Feature selection}
}

@article{10.1016/j.eswa.2021.115312,
author = {Got, Adel and Moussaoui, Abdelouahab and Zouache, Djaafar},
title = {Hybrid filter-wrapper feature selection using whale optimization algorithm: A multi-objective approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115312},
doi = {10.1016/j.eswa.2021.115312},
journal = {Expert Syst. Appl.},
month = nov,
numpages = {10},
keywords = {Whale optimization algorithm (WOA), Multi-objective optimization, Filter and wrapper approaches, Feature selection}
}

@article{10.1007/s00521-019-04117-9,
author = {Chen, Tao and Guo, Yanrong and Hao, Shijie},
title = {Unsupervised feature selection based on joint spectral learning and general sparse regression},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04117-9},
doi = {10.1007/s00521-019-04117-9},
abstract = {Unsupervised feature selection is an important machine learning task since the manual annotated data are dramatically expensive to obtain and therefore very limited. However, due to the existence of noise and outliers in different data samples, feature selection without the help of discriminant information embedded in the annotated data is quite challenging. To relieve these limitations, we investigate the embedding of spectral learning into a general sparse regression framework for unsupervised feature selection. Generally, the proposed general spectral sparse regression (GSSR) method handles the outlier features by learning the joint sparsity and the noisy features by preserving the local structures of data, jointly. Specifically, GSSR is conducted in two stages. First, the classic sparse dictionary learning method is used to build the bases of original data. After that, the original data are project to the basis space by learning a new representation via GSSR. In GSSR, robust loss function ℓ2,r-norm(0&lt;r≤2) and ℓ2,p-norm(0&lt;p≤1) instead of the traditional F norm and least square loss function are simultaneously considered as the reconstruction term and sparse regularization term for sparse regression. Furthermore, the local topological structures of the new representations are preserved by spectral learning based on the Laplacian term. The overall objective function in GSSR is optimized and proved to be converging. Experimental results on several publicly datasets have demonstrated the validity of our algorithm, which outperformed the state-of-the-art feature selections in terms of classification performance.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6581–6589},
numpages = {9},
keywords = {Unsupervised feature selection, General sparse regression, Spectral selection}
}

@article{10.3233/JIFS-169022,
author = {Jiang, Shengyi and Wang, Lianxi and Xiao, Zheng and Li, Kenli},
title = {A clustering-based feature selection via feature separability},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {31},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169022},
doi = {10.3233/JIFS-169022},
abstract = {With the extensive increase of the amount of data, such as text categorization, genomic microarray data, bio-informatics and digital images, there are more and more challenges in feature selection. Recently, feature selection has been widely studied in supervised learning, but there is significantly less work in unsupervised learning because of the absence of class information and explicit search criteria. In this work, we introduce a new measure to assess the importance of features in terms of feature separability. A clustering-based feature selection algorithm is then introduced to conduct the feature selection. The proposed algorithm with nearly linear time complexity selects final feature subset through a ranking procedure based on the separabilities of features and it is applicable to datasets of mixed nature. Experimental results on UCI datasets show that our method, by retaining relevant features, can obtain similar or even better results of classification and clustering for most datasets, and it outperforms other traditional supervised and unsupervised feature selection methods in terms of dimensionality reduction and classification accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {927–937},
numpages = {11},
keywords = {unsupervised learning, clustering, feature separability, Feature selection}
}

@article{10.1007/s10586-020-03222-y,
author = {Krishnaveni, S. and Sivamohan, S. and Sridhar, S. S. and Prabakaran, S.},
title = {Efficient feature selection and classification through ensemble method for network intrusion detection on cloud computing},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-020-03222-y},
doi = {10.1007/s10586-020-03222-y},
abstract = {Cloud computing is a preferred option for organizations around the globe, it offers scalable and internet-based computing resources as a flexible service. Security is a key concern factor in any cloud solution due to its distributed nature. Security and privacy are huge obstacles faced in its success of the on-demand service as it is easily vulnerable to intruders for any kind of attack. A huge upsurge in network traffic 
has paved the way to security breaches which are more complicated and widespread. Tackling these attacks has become an inefficient application of traditional intrusion detection systems (IDS) environment. In this research, we developed an efficient Intrusion Detection System (IDS) for the cloud environment using ensemble feature selection and classification techniques. This proposed method was relying on the univariate ensemble feature selection technique, which is used for the selection of valuable reduced feature sets from the given intrusion datasets. While the ensemble classifiers that can competently fuse the single classifiers to produce a robust classifier using the voting technique. An ensemble based proposed method effectively classifies whether the network traffic behavior is normal or attack. The implementation of the proposed method was measured by applying various performance evaluation metrics and ROC-AUC (“area under the receiver operating characteristic curves”) across various classifiers. The results of the proposed methodology achieved a strong considerable amount of performance enhancement compared with other existing methods. Moreover, we performed a pairwise t test and proved that the performance of the proposed method was statistically significantly different from other existing approaches. Finally, the outcome of this investigation was obtained with the best accuracy and lowest false alarm rate (FAR).},
journal = {Cluster Computing},
month = sep,
pages = {1761–1779},
numpages = {19},
keywords = {Ensemble learning, Feature selection, Honeynet, Intrusion detection systems, Cloud security}
}

@inproceedings{10.5555/2022115.2022131,
author = {Wu, Yijian and Peng, Xin and Zhao, Wenyun},
title = {Architecture evolution in software product line: an industrial case study},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A software product line (SPL) usually involves a shared set of core assets and a series of application products. To ensure consistency, the evolution of the core assets and all the application products should be coordinated and synchronized under a unified evolution process. Therefore, SPL evolution often involves cross-product propagation and synchronization besides application derivation based on core assets, presenting quite different characteristic from the evolution of individual software products. As software architectures, including the product line architecture (PLA) and application architectures, play a central role in SPL engineering and evolution, architecture-based evolution analysis is a natural way for analyzing and managing SPL evolution. In this paper, we explore common practices of architecture evolution and the rationale behind in industrial SPL development. To this end, we conduct a case study with Wingsoft examination system product line (WES-PL), an industrial product line with an evolution history of eight years and more than 10 application products. In the case study, we reviewed the evolution history of WES-PL architecture and analyzed several typical evolution cases. Based on the historical analysis, we identify some special problems in industrial SPL practice from the aspect of architecture evolution and summarize some useful experiences about SPL evolution decisions to complement classical SPL methodology. On the other hand, we also propose some possible improvements for the evolution management in WES-PL.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {135–150},
numpages = {16},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@article{10.1016/j.infsof.2011.01.001,
author = {Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
year = {2011},
issue_date = {July, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.01.001},
doi = {10.1016/j.infsof.2011.01.001},
abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {707–721},
numpages = {15},
keywords = {Context, Variability, Software product line, Requirements, Feature, Evolution}
}

@inproceedings{10.1007/978-3-031-12700-7_33,
author = {Yelleti, Vivek and Sai Prasad, P. S. V. S.},
title = {Stateful MapReduce Framework for&nbsp;mRMR Feature Selection Using Horizontal Partitioning},
year = {2021},
isbn = {978-3-031-12699-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12700-7_33},
doi = {10.1007/978-3-031-12700-7_33},
abstract = {Feature selection (FS) is an important pre-processing step in building machine learning models. minimum Redundancy and Maximum Relevance (mRMR) approach has emerged as one of the successful algorithms in obtaining irredundant feature subset involving only bi-variate computations. In the current digital age, owing to the prevalence of very large scale datasets, an imminent need has arisen for scalable solutions using distributed/parallel algorithms. MapReduce solutions are proven to be one of the best approaches to design fault-tolerant and scalable solutions. This work analyses the existing Horizontal MapReduce approaches for mRMR feature selection and identifies the limitations thereof. It is observed that existing approaches involve redundant and repetitive computations and lacks a metadata framework to diminish them. This motivated us to propose Horizontal partitioning based MapReduce solutions namely HMR_mRMR, is an Iterative MapReduce algorithms and is designed under Apache Spark. Appropriate usage of metadata framework and solution formulation optimizes the computations in the proposed approaches. The comparative experimental study is conducted with existing approaches to establish the importance of HMR_mRMR.},
booktitle = {Pattern Recognition and Machine Intelligence: 9th International Conference, PReMI 2021, Kolkata, India, December 15–18, 2021, Proceedings},
pages = {317–327},
numpages = {11},
keywords = {Feature Selection, mRMR, Big data, Horizontal partitioning, MapReduce, Iterative MapReduce},
location = {Kolkata, India}
}

@inproceedings{10.1145/3474198.3478158,
author = {Yang, Ying and Cheng, LiangLun and Huang, Guoheng},
title = {Conditional Independence-based Causal Discovery Algorithm for Few-Shot Feature Selection},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478158},
doi = {10.1145/3474198.3478158},
abstract = {Feature selection can select the optimal subset of features by eliminating the irrelevant or redundant features, which has been an important challenge in machine learning tasks. Existing neural network methods for data processing often requires large number of features to be filtered, extracted, and transformed. However, the characteristics of few-shot are usually accompanied by high dimensionality and sparse sample size, which making it arduous for neural network methods to locate the truly relevant features. Consequently, a causal feature selection layer based on conditional independence is proposed to embed the traditional neural network. In particular, effective features can be screened out by mining the causal relationships through conditional independence, and it makes the data modeling with certain interpretability. Experimental results show the feasibility of our theory on both simulated and real dataset.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {72},
numpages = {7},
keywords = {Feature selection, Few-shot learning, Neural networks, Conditional independence},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@article{10.1016/j.cam.2019.03.041,
author = {Winkler, Joab R. and Mitrouli, Marilena},
title = {Condition estimation for regression and feature selection},
year = {2020},
issue_date = {Aug 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {373},
number = {C},
issn = {0377-0427},
url = {https://doi.org/10.1016/j.cam.2019.03.041},
doi = {10.1016/j.cam.2019.03.041},
journal = {J. Comput. Appl. Math.},
month = aug,
numpages = {15},
keywords = {Condition estimation, Regression, Tikhonov regularisation, Feature selection}
}

@article{10.1016/j.eswa.2021.115589,
author = {Parmezan, Antonio Rafael Sabino and Lee, Huei Diana and Spola\^{o}r, Newton and Wu, Feng Chung},
title = {Automatic recommendation of feature selection algorithms based on dataset characteristics},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115589},
doi = {10.1016/j.eswa.2021.115589},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {30},
keywords = {Feature engineering, Characterization measures, Algorithm selection, Recommendation system, Filter, Wrapper}
}

@article{10.1016/j.eswa.2019.112988,
author = {Maldonado, Sebasti\'{a}n and L\'{o}pez, Julio and Jimenez-Molina, Angel and Lira, Hern\'{a}n},
title = {Simultaneous feature selection and heterogeneity control for SVM classification: An application to mental workload assessment},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {143},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112988},
doi = {10.1016/j.eswa.2019.112988},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {16},
keywords = {Support vector machines, Feature selection, Heterogeneity control, Mental workload, Group penalty functions}
}

@article{10.1007/s10515-011-0099-7,
author = {Bagheri, Ebrahim and Ensan, Faezeh and Gasevic, Dragan},
title = {Decision support for the software product line domain engineering lifecycle},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0099-7},
doi = {10.1007/s10515-011-0099-7},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of the target domain and through the development of comprehensive and variability-covering feature models. The feature models developed within the software product line development process need to cover the relevant features and aspects of the target domain. In other words, the feature models should be elaborate representations of the feature space of that domain. Given that feature models, i.e., software product line feature models, are developed mostly by domain analysts by sifting through domain documentation, corporate records and transcribed interviews, the process is a cumbersome and error-prone one. In this paper, we propose a decision support platform that assists domain analysts throughout the domain engineering lifecycle by: (1) automatically performing natural language processing tasks over domain documents and identifying important information for the domain analysts such as the features and integrity constraints that exist in the domain documents; (2) providing a collaboration platform around the domain documents such that multiple domain analysts can collaborate with each other during the process using a Wiki; (3) formulating semantic links between domain terminology with external widely used ontologies such as WordNet in order to disambiguate the terms used in domain documents; and (4) developing traceability links between the unstructured information available in the domain documents and their formal counterparts within the formal feature model representations. Results obtained from our controlled experimentations show that the decision support platform is effective in increasing the performance of the domain analysts during the domain engineering lifecycle in terms of both the coverage and accuracy measures.},
journal = {Automated Software Engg.},
month = sep,
pages = {335–377},
numpages = {43},
keywords = {Domain engineering, Feature models, NLP model inference, Software product lines}
}

@article{10.1007/s10618-020-00731-7,
author = {Borboudakis, Giorgos and Tsamardinos, Ioannis},
title = {Extending greedy feature selection algorithms to multiple solutions},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {4},
issn = {1384-5810},
url = {https://doi.org/10.1007/s10618-020-00731-7},
doi = {10.1007/s10618-020-00731-7},
abstract = {Most feature selection methods identify only a single solution. This is acceptable for predictive purposes, but is not sufficient for knowledge discovery if multiple solutions exist. We propose a strategy to extend a class of greedy methods to efficiently identify multiple solutions, and show under which conditions it identifies all solutions. We also introduce a taxonomy of features that takes the existence of multiple solutions into account. Furthermore, we explore different definitions of statistical equivalence of solutions, as well as methods for testing equivalence. A novel algorithm for compactly representing and visualizing multiple solutions is also introduced. In experiments we show that (a) the proposed algorithm is significantly more computationally efficient than the TIE* algorithm, the only alternative approach with similar theoretical guarantees, while identifying similar solutions to it, and (b) that the identified solutions have similar predictive performance.},
journal = {Data Min. Knowl. Discov.},
month = jul,
pages = {1393–1434},
numpages = {42},
keywords = {Stepwise selection, Multiple feature selection, Multiple solutions, Feature selection}
}

@article{10.1016/j.patcog.2019.04.020,
author = {Zhang, Yong and Wang, Qi and Gong, Dun-wei and Song, Xian-fang},
title = {Nonnegative Laplacian embedding guided subspace learning for unsupervised feature selection},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.04.020},
doi = {10.1016/j.patcog.2019.04.020},
journal = {Pattern Recogn.},
month = sep,
pages = {337–352},
numpages = {16},
keywords = {Class labels, Subspace learning, Nonnegative Laplacian embedding, Unsupervised feature selection}
}

@article{10.1007/s10994-019-05809-y,
author = {Brankovic, Aida and Piroddi, Luigi},
title = {A distributed feature selection scheme with partial information sharing},
year = {2019},
issue_date = {Nov 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {11},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-019-05809-y},
doi = {10.1007/s10994-019-05809-y},
abstract = {This paper introduces a novel feature selection and classification method, based on vertical data partitioning and a distributed searching architecture. The features are divided into subsets, each of which is associated to a dedicated processor that performs a local search. When all local selection processes are completed, each processor shares the features of its locally selected model with all other processors, and the local searches are repeated until convergence. Thanks to the vertical partitioning and the distributed selection scheme, the presented method is capable of addressing relatively large scale examples. The procedure is efficient since the local processors perform the selection tasks in parallel and on much smaller search spaces. Another important feature of the proposed method is its tendency to produce simple model structures, which is generally advantageous for the interpretability and robustness of the classifier. The proposed approach is evaluated and compared to other well-known feature selection and classification approaches proposed in the literature on several benchmark datasets. The obtained results demonstrate the effectiveness of the proposed approach, both in terms of classification accuracy and computational time.},
journal = {Mach. Learn.},
month = nov,
pages = {2009–2034},
numpages = {26},
keywords = {Parallel processing, Distributed optimization, Model selection, Classification, Feature selection}
}

@article{10.1134/S0361768821050066,
author = {Slezkin, A. O. and Hodashinsky, I. A. and Shelupanov, A. A.},
title = {Binarization of the Swallow Swarm Optimization for Feature Selection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Plenum Press},
address = {USA},
volume = {47},
number = {5},
issn = {0361-7688},
url = {https://doi.org/10.1134/S0361768821050066},
doi = {10.1134/S0361768821050066},
journal = {Program. Comput. Softw.},
month = sep,
pages = {374–388},
numpages = {15}
}

@article{10.1016/j.imavis.2017.09.004,
author = {Lee, Pui Yi and Loh, Wei Ping and Chin, Jeng Feng},
title = {Feature selection in multimedia},
year = {2017},
issue_date = {November 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0262-8856},
url = {https://doi.org/10.1016/j.imavis.2017.09.004},
doi = {10.1016/j.imavis.2017.09.004},
abstract = {Multimedia data mining, particularly feature selection (FS), has been successfully applied in recent classification and recognition works. However, only a few studies in the contemporary literature have reviewed FS (e.g., analyses of data pre-processing prior to classification and clustering). This study aimed to fill this research gap by presenting an extensive survey on the current development of FS in multimedia. A total of 70 related papers published from 2001 to 2017 were collected from multiple databases. Breakdowns and analyses were performed on data types, methods, search strategies, performance measures, and challenges. The development trend of FS presages the increased prominence of heuristic search strategies and hybrid FS in the latest multimedia data mining. Reviews on 70 relevant literatures from 2001 to 2017 from multiple databasesStudy data types, methods, search strategies, performance measures, and challengesThe trend of FS shows the increased prominence of heuristic search strategies.},
journal = {Image Vision Comput.},
month = nov,
pages = {29–42},
numpages = {14},
keywords = {Data mining, Feature selection, Multimedia, Search strategies}
}

@inproceedings{10.5555/3540261.3541451,
author = {Li, Yazhe and Pogodin, Roman and Sutherland, Danica J. and Gretton, Arthur},
title = {Self-supervised learning with kernel dependence maximization},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We approach self-supervised learning of image representations from a statistical dependence perspective, proposing Self-Supervised Learning with the Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes dependence between representations of transformations of an image and the image identity, while minimizing the kernelized variance of those representations. This framework yields a new understanding of InfoNCE, a variational lower bound on the mutual information (MI) between different transformations. While the MI itself is known to have pathologies which can result in learning meaningless representations, its bound is much better behaved: we show that it implicitly approximates SSL-HSIC (with a slightly different regularizer). Our approach also gives us insight into BYOL, a negative-free SSL method, since SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to directly optimize statistical dependence in time linear in the batch size, without restrictive data assumptions or indirect mutual information estimators. Trained with or without a target network, SSL-HSIC matches the current state-of-the-art for standard linear evaluation on ImageNet [1], semi-supervised learning and transfer to other classification and vision tasks such as semantic segmentation, depth estimation and object recognition.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1190},
numpages = {14},
series = {NIPS '21}
}

@inproceedings{10.5555/3351736.3351778,
author = {Hajri, Ines and Goknil, Arda and Briand, Lionel C. and Stephany, Thierry},
title = {Applying product line use case modeling in an industrial automotive embedded system: lessons learned and a refined approach},
year = {2015},
isbn = {9781467369084},
publisher = {IEEE Press},
abstract = {In this paper, we propose, apply, and assess Product line Use case modeling Method (PUM), an approach that supports modeling variability at different levels of granularity in use cases and domain models. Our motivation is that, in many software development environments, use case modeling drives interactions among stakeholders and, therefore, use cases and domain models are common practice for requirements elicitation and analysis. In PUM, we integrate and adapt existing product line extensions for use cases and introduce some template extensions for use case specifications. Variability is captured in use case diagrams while it is reflected at a greater level of detail in use case specifications. Variability in domain concepts is captured in domain models. PUM is supported by a tool relying on Natural Language Processing (NLP). We applied PUM to an industrial automotive embedded system and report lessons learned and results from structured interviews with experienced engineers.},
booktitle = {Proceedings of the 18th International Conference on Model Driven Engineering Languages and Systems},
pages = {338–347},
numpages = {10},
location = {Ottawa, Ontario, Canada},
series = {MODELS '15}
}

@inproceedings{10.1145/3394486.3403200,
author = {Haug, Johannes and Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji},
title = {Leveraging Model Inherent Variable Importance for Stable Online Feature Selection},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403200},
doi = {10.1145/3394486.3403200},
abstract = {Feature selection can be a crucial factor in obtaining robust and accurate predictions. Online feature selection models, however, operate under considerable restrictions; they need to efficiently extract salient input features based on a bounded set of observations, while enabling robust and accurate predictions. In this work, we introduce FIRES, a novel framework for online feature selection. The proposed feature weighting mechanism leverages the importance information inherent in the parameters of a predictive model. By treating model parameters as random variables, we can penalize features with high uncertainty and thus generate more stable feature sets. Our framework is generic in that it leaves the choice of the underlying model to the user. Strikingly, experiments suggest that the model complexity has only a minor effect on the discriminative power and stability of the selected feature sets. In fact, using a simple linear model, FIRES obtains feature sets that compete with state-of-the-art methods, while dramatically reducing computation time. In addition, experiments show that the proposed framework is clearly superior in terms of feature selection stability.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {1478–1502},
numpages = {25},
keywords = {data streams, feature selection, stability, uncertainty},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{10.1287/mnsc.1090.1133,
author = {Amaldoss, Wilfred and Jain, Sanjay},
title = {Reference Groups and Product Line Decisions: An Experimental Investigation of Limited Editions and Product Proliferation},
year = {2010},
issue_date = {April 2010},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {56},
number = {4},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1133},
doi = {10.1287/mnsc.1090.1133},
abstract = {Some luxury goods manufacturers offer limited editions of their products, whereas some others market multiple product lines. Researchers have found that reference groups shape consumer evaluations of these product categories. Yet little empirical research has examined how reference groups affect the product line decisions of firms. Indeed, in a field setting it is quite a challenge to isolate reference group effects from contextual effects and correlated effects. In this paper, we propose a parsimonious model that allows us to study how reference groups influence firm behavior and that lends itself to experimental analysis. With the aid of the model, we investigate the behavior of consumers in a laboratory setting where we can focus on the reference group effects after controlling for the contextual and correlated effects. The experimental results show that in the presence of strong reference group effects, limited editions and multiple products can help improve firms' profits. Furthermore, the trends in the purchase decisions of our participants point to the possibility that they are capable of introspecting close to two steps of thinking at the outset of the game and then learning through reinforcement mechanisms.},
journal = {Manage. Sci.},
month = apr,
pages = {621–644},
numpages = {24},
keywords = {experimental economics, game theory, product line, reference groups}
}

@article{10.1016/j.ins.2019.01.041,
author = {Chen, Hongmei and Li, Tianrui and Fan, Xin and Luo, Chuan},
title = {Feature selection for imbalanced data based on neighborhood rough sets},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {483},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.041},
doi = {10.1016/j.ins.2019.01.041},
journal = {Inf. Sci.},
month = may,
pages = {1–20},
numpages = {20},
keywords = {Discernibility matrix, Imbalanced data, Feature selection, Rough set theory}
}

@article{10.1016/j.patcog.2019.06.003,
author = {Zhang, Jia and Luo, Zhiming and Li, Candong and Zhou, Changen and Li, Shaozi},
title = {Manifold regularized discriminative feature selection for multi-label learning},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {95},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.06.003},
doi = {10.1016/j.patcog.2019.06.003},
journal = {Pattern Recogn.},
month = nov,
pages = {136–150},
numpages = {15},
keywords = {Optimization objective, Manifold regularization, Label correlations, Feature selection, Multi-label learning}
}

@article{10.1007/s10489-020-01863-5,
author = {Bai, Shengxing and Lin, Yaojin and Lv, Yan and Chen, Jinkun and Wang, Chenxi},
title = {Kernelized fuzzy rough sets based online streaming feature selection for large-scale hierarchical classification},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01863-5},
doi = {10.1007/s10489-020-01863-5},
abstract = {In recent years, many online streaming feature selection approaches focus on flat data, which means that all data are taken as a whole. However, in the era of big data, not only the feature space of data has unknown and evolutionary characteristics, but also the label space of data exists hierarchical structure. To address this problem, an online streaming feature selection framework for large-scale hierarchical classification task is proposed. The framework consists of three parts: (1) a new hierarchical data-oriented kernelized fuzzy rough model with sibling strategy is constructed, (2) the online important feature is selected based on feature correlation analysis, and (3) the online redundant feature is deleted based on feature redundancy. Finally, an empirical study using several hierarchical classification data sets manifests that the proposed method outperforms other state-of-the-art online streaming feature selection methods.},
journal = {Applied Intelligence},
month = mar,
pages = {1602–1615},
numpages = {14},
keywords = {Online feature selection, Hierarchical classification, Kernelized fuzzy rough sets, Sibling strategy}
}

@article{10.1016/j.knosys.2021.107443,
author = {Wang, Xiaodong and Wu, Pengtao and Xu, Qinghua and Zeng, Zhiqiang and Xie, Yong},
title = {Joint image clustering and feature selection with auto-adjoined learning for high-dimensional data},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {232},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107443},
doi = {10.1016/j.knosys.2021.107443},
journal = {Know.-Based Syst.},
month = nov,
numpages = {12},
keywords = {K-means, Dimension reduction, Feature selection, Clustering}
}

@inproceedings{10.1007/978-3-031-21517-9_12,
author = {Sujatha, G. and Sankareswari, K.},
title = {A Comparative Study on Machine Learning Based Classifier Model for Wheat Seed Classification},
year = {2021},
isbn = {978-3-031-21516-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-21517-9_12},
doi = {10.1007/978-3-031-21517-9_12},
abstract = {Seed classification is a process of categorizing different varieties of seeds into different classes on the basis of their morphological features. Seed identification is further complicated due to common object recognition constraints such as light, pose and orientation. Wheat has always been one of the globally common consumed foods in India. A large number of wheat varieties have been cultivated, exported and imported all around the world. Enormous studies have been done on identifying crop diseases and classifying the crop types. In the present work, wheat seed classification is performed to distinguish the three different Indian wheat varieties by their collected morphological features and applied machine learning models to develop wheat variety classification system. The seed features used here are length of kernel, compactness, asymmetry coefficient, width of kernel, length of kernel groove, area and perimeter. The present work carried out with different classifiers such as Decision Tree, Random Forest, Neural Net, Nearest Neighbors, Gaussian Process, AdaBoost, Naive Bayes, Support Vector Machine(SVM) Linear, SVM RBF(SVM with the Radial Basis Function) and SVM Sigmoid with 2&nbsp;K-fold cross validation. Also obtained the results using 5 fold and 10-fold Cross Validation.},
booktitle = {Mining Intelligence and Knowledge Exploration: 9th International Conference, MIKE 2021, Hammamet, Tunisia, November 1–3, 2021, Proceedings},
pages = {120–127},
numpages = {8},
keywords = {Classifiers, Decision tree, Random forest, K-fold cross validation, Gaussian, Neural net, SVM, Seed classification, Agriculture},
location = {Hammamet, Tunisia}
}

@article{10.1016/j.artmed.2021.102198,
author = {Peralta, Maxime and Jannin, Pierre and Baxter, John S.H.},
title = {Machine learning in deep brain stimulation: A systematic review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {122},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2021.102198},
doi = {10.1016/j.artmed.2021.102198},
journal = {Artif. Intell. Med.},
month = dec,
numpages = {13},
keywords = {Machine learning, Deep brain stimulation, Systematic review}
}

@article{10.1504/ijcistudies.2021.113826,
author = {Anand, Neeyati and Sehgal, Riya and Anand, Sanchit and Kaushik, Ajay},
title = {Feature selection on educational data using Boruta algorithm},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1},
issn = {1755-4977},
url = {https://doi.org/10.1504/ijcistudies.2021.113826},
doi = {10.1504/ijcistudies.2021.113826},
abstract = {Data mining in education deals with formulating strategies for students with the aim to increase the parameters affecting the learning and employability. It also helps the educational institutes in maintaining their reputation as it is directly linked to the student's grades. We need to identify the parameters involved in learning and the relationship among those parameters. In EDM, feature selection (FS) is one of the most important and needed method in EDM, as it removes the features which have no direct link with the student's performance. For example, the date of birth of a student does not impact his/her performance. In this paper, an attempt has been made to improve the performance of the classifiers for undergraduate students at Maharaja Agrasen Institute of Technology. We have applied several techniques of data mining to make some rules that increase the learning and employability of students. The results of our study have shown a significant increase in accuracy, recall, precision and F-measure for na\"{\i}ve Bayes and decision tree classifiers.},
journal = {Int. J. Comput. Intell. Stud.},
month = jan,
pages = {27–35},
numpages = {8},
keywords = {performance, feature selection, EDM, educational data mining, classifiers}
}

@article{10.1145/3446636,
author = {Nakashima, Makiya and Sim, Alex and Kim, Youngsoo and Kim, Jonghyun and Kim, Jinoh},
title = {Automated Feature Selection for Anomaly Detection in Network Traffic Data},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3446636},
doi = {10.1145/3446636},
abstract = {Variable selection (also known as feature selection) is essential to optimize the learning complexity by prioritizing features, particularly for a massive, high-dimensional dataset like network traffic data. In reality, however, it is not an easy task to effectively perform the feature selection despite the availability of the existing selection techniques. From our initial experiments, we observed that the existing selection techniques produce different sets of features even under the same condition (e.g., a static size for the resulted set). In addition, individual selection techniques perform inconsistently, sometimes showing better performance but sometimes worse than others, thereby simply relying on one of them would be risky for building models using the selected features. More critically, it is demanding to automate the selection process, since it requires laborious efforts with intensive analysis by a group of experts otherwise. In this article, we explore challenges in the automated feature selection with the application of network anomaly detection. We first present our ensemble approach that benefits from the existing feature selection techniques by incorporating them, and one of the proposed ensemble techniques based on greedy search works highly consistently showing comparable results to the existing techniques. We also address the problem of when to stop to finalize the feature elimination process and present a set of methods designed to determine the number of features for the reduced feature set. Our experimental results conducted with two recent network datasets show that the identified feature sets by the presented ensemble and stopping methods consistently yield comparable performance with a smaller number of features to conventional selection techniques.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {18},
numpages = {28},
keywords = {Feature selection, ensemble approach, network anomaly detection, cybersecurity analytics}
}

@article{10.1016/j.patcog.2019.107183,
author = {Georges, Nicolas and Mhiri, Islem and Rekik, Islem},
title = {Identifying the best data-driven feature selection method for boosting reproducibility in classification tasks},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {101},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107183},
doi = {10.1016/j.patcog.2019.107183},
journal = {Pattern Recogn.},
month = may,
numpages = {14},
keywords = {Cross-validation, Connectomics, Neurological disorders, Morphological brain network, Biomarker discovery, Feature reproducibility, Multi-graph topological analysis, Feature selection methods}
}

@article{10.1145/3418034,
author = {Cummings, Mary L. and Li, Songpo},
title = {Subjectivity in the Creation of Machine Learning Models},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1936-1955},
url = {https://doi.org/10.1145/3418034},
doi = {10.1145/3418034},
abstract = {Transportation analysts are inundated with requests to apply popular machine learning modeling techniques to datasets to uncover never-before-seen relationships that could potentially revolutionize safety, congestion, and mobility. However, the results from such models can be influenced not just by biases in underlying data, but also through practitioner-induced biases. To demonstrate the significant number of subjective judgments made in the development and interpretation of machine learning models, we developed Logistic Regression and Neural Network models for transportation-focused datasets including those looking at driving injury/fatalities and pedestrian fatalities. We then developed five different representations of feature importance for each dataset, including different feature interpretations commonly used in the machine learning community. Twelve distinct judgments were highlighted in the development and interpretation of these models, which produced inconsistent results. Such inconsistencies can lead to very different interpretations of the results, which can lead to errors of commission and omission, with significant cost and safety implications if policies are erroneously adapted from such outcomes.},
journal = {J. Data and Information Quality},
month = may,
articleno = {7},
numpages = {19},
keywords = {Bias, interpretable machine learning, transportation, logistic regression, subjectivity}
}

@article{10.1007/s11042-019-08549-2,
author = {Liu, Yanbei and Geng, Lei and Zhang, Fang and Wu, Jun and Zhang, Liang and Xiao, Zhitao},
title = {Unsupervised feature selection based on local structure learning},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {45–46},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-08549-2},
doi = {10.1007/s11042-019-08549-2},
abstract = {Unsupervised feature selection has become a significant and ambitious issue due to vast amounts of high-dimensional unlabeled data in machine learning. Traditional unsupervised feature selection algorithms usually make use of the similarity matrix for feature selection, and they heavily rely on the learned structure. However, a large amount of actual data always contains many noise samples or features that may make the similarity matrix obtained from the original data unreliable. Using Ideal Local Structure Learning (LSL) method ,we propose a novel unsupervised feature selection to perform feature selection and local structure learning at the same time in this paper. In order that we can earn more exactly structure information, an ideal local structure with precisely c connected components of data (c is the number of clusters) is utilized to refine the similarity matrix. Moreover, in order to optimize our algorithm, an effectual and plain iterative algorithm is developed. Experiments on multiple public baseline datasets, including biomedical data, letter recognition digit data and face image data, reveals the outstanding performance of our algorithms in the most advanced aspects.},
journal = {Multimedia Tools Appl.},
month = dec,
pages = {34571–34585},
numpages = {15},
keywords = {High-dimensional data, Unsupervised feature selection, Local structure learning, Similarity matrix}
}

@article{10.1007/s00521-020-05030-2,
author = {Punitha, V. and Mala, C.},
title = {Traffic classification in server farm using supervised learning techniques},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05030-2},
doi = {10.1007/s00521-020-05030-2},
abstract = {Server farms used in web hosting and commercial applications connect multiple servers. Edge computing being a realm of cloud technology is orchestrated with server farms to enhance network efficiency. Edge computing increases the availability of cloud resources and Internet services. The higher availability of services and their ease of access deeply affect the user’s requesting behavior. The anomalous requesting behavior is creating malicious traffic, and enormous amount of such traffics at server farm denies the services to the legitimate users. Categorizing the incoming traffic into malicious and non-malicious traffic at server farm is the foremost criteria to eliminate the attacks, which in turn improves the QoS of the server farm. In the light of preventing the biased usage of the server farm, this paper proposes a SVM classifier based on requesting statistics. The proposed classifier discovers the attacks that deny services to legitimate users in two levels, based on the user’s request behavior. The pattern of arrival, its statistical characteristics and security misbehaviors are investigated at both levels. An incremental learning algorithm is proposed to enhance the learning plasticity of the proposed classifier. The experimental results illustrate that the performance of the proposed two-level classifier with respect to classification accuracy is competently improved with incremental learning.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {1279–1296},
numpages = {18},
keywords = {Machine learning, Support vector machine, Denial of service, Traffic classification}
}

@article{10.1145/3451163,
author = {Abououf, Menatalla and Singh, Shakti and Otrok, Hadi and Mizouni, Rabeb and Damiani, Ernesto},
title = {Machine Learning in Mobile Crowd Sourcing: A Behavior-Based Recruitment Model},
year = {2021},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3451163},
doi = {10.1145/3451163},
abstract = {With the advent of mobile crowd sourcing (MCS) systems and its applications, the selection of the right crowd is gaining utmost importance. The increasing variability in the context of MCS tasks makes the selection of not only the capable but also the willing workers crucial for a high task completion rate. Most of the existing MCS selection frameworks rely primarily on reputation-based feedback mechanisms to assess the level of commitment of potential workers. Such frameworks select workers having high reputation scores but without any contextual awareness of the workers, at the time of selection, or the task. This may lead to an unfair selection of workers who will not perform the task. Hence, reputation on its own only gives an approximation of workers’ behaviors since it assumes that workers always behave consistently regardless of the situational context. However, following the concept of cross-situational consistency, where people tend to show similar behavior in similar situations and behave differently in disparate ones, this work proposes a novel recruitment system in MCS based on behavioral profiling. The proposed approach uses machine learning to predict the probability of the workers performing a given task, based on their learned behavioral models. Subsequently, a group-based selection mechanism, based on the genetic algorithm, uses these behavioral models in complementation with a reputation-based model to recruit a group of workers that maximizes the quality of recruitment of the tasks. Simulations based on a real-life dataset show that considering human behavior in varying situations improves the quality of recruitment achieved by the tasks and their completion confidence when compared with a benchmark that relies solely on reputation.},
journal = {ACM Trans. Internet Technol.},
month = nov,
articleno = {16},
numpages = {28},
keywords = {Machine learning, behavioral profiling, mobile crowd sourcing, selection management, quality of recruitment}
}

@inproceedings{10.1145/3132847.3133055,
author = {Wei, Xiaokai and Cao, Bokai and Yu, Philip S.},
title = {Unsupervised Feature Selection with Heterogeneous Side Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133055},
doi = {10.1145/3132847.3133055},
abstract = {Compared to supervised feature selection, unsupervised feature selection tends to be more challenging due to the lack of guidance from class labels. Along with the increasing variety of data sources, many datasets are also equipped with certain side information of heterogeneous structure. Such side information can be critical for feature selection when class labels are unavailable. In this paper, we propose a new feature selection method, SideFS, to exploit such rich side information. We model the complex side information as a heterogeneous network and derive instance correlations to guide subsequent feature selection. Representations are learned from the side information network and the feature selection is performed in a unified framework. Experimental results show that the proposed method can effectively enhance the quality of selected features by incorporating heterogeneous side information.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2359–2362},
numpages = {4},
keywords = {feature selection, heterogeneous information network, side information, unsupervised learning},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1016/j.infsof.2007.10.010,
author = {Snook, Colin and Poppleton, Michael and Johnson, Ian},
title = {Rigorous engineering of product-line requirements: A case study in failure management},
year = {2008},
issue_date = {January, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {1–2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.010},
doi = {10.1016/j.infsof.2007.10.010},
abstract = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set - for subsequent system instantiation - is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of EU project RODIN (This work is conducted in the setting of the EU funded Research Project: IST 511599 RODIN (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {112–129},
numpages = {18},
keywords = {Formal specification, Generic requirements, Product line, Refinement, Tools, UML-B, Verification}
}

@article{10.1016/j.patcog.2018.05.013,
author = {Kim, Younghoon and Kim, Seoung Bum},
title = {Collinear groupwise feature selection via discrete fusion group regression},
year = {2018},
issue_date = {Nov 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.05.013},
doi = {10.1016/j.patcog.2018.05.013},
journal = {Pattern Recogn.},
month = nov,
pages = {1–13},
numpages = {13},
keywords = {Multiple linear regression, Machine learning, Feature selection, Multicollinearity, Mixed-integer quadratic programming, Best subset selection}
}

@article{10.3233/JIFS-179743,
author = {Shukla, Alok Kumar and Pippal, Sanjeev Kumar and Gupta, Srishti and Ramachandra Reddy, B. and Tripathi, Diwakar and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Trajkovic, Ljiljana},
title = {Knowledge discovery in medical and biological datasets by integration of Relief-F and correlation feature selection techniques},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {38},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179743},
doi = {10.3233/JIFS-179743},
abstract = {Feature selection is a pre-processing method that identifies the significant features from high-dimensional data and able to diminish the computational cost of the learning algorithm because of removing the irrelevant and redundant features. It has traditionally been applied in a wide range of problems that include biological data processing, pattern recognition, and computer vision. The aim of this paper is to identify the best feature subsets from the benchmark datasets which improve the performance of the classifiers. Existing filter-based feature selection approaches fail to choose the relevant features from the original feature sets. To obtain the tiny subset of relevant features, we have introduced a novel filter-based feature selection method, called ReCFS. The proposed method is a combination of both feature-feature correlation and nearest neighbor weighted features to find an optimal subset of features to minimize correlation among features. The effectiveness of the selected feature subset by proposed method is evaluated by using two classifiers such as Na\"{\i}ve Bayes and K-Nearest Neighbour on real-life datasets. For the diverse performance measurements, the experiments are conducted on eight real-life datasets of varied dimensionality and number of instances. The result demonstrates that the proposed method has found promising feature subsets which improved the classification accuracy over competing feature selection methods},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6637–6648},
numpages = {12},
keywords = {na\"{\i}ve bayes, classification, correlation feature selection, relief-F, Machine learning}
}

@inproceedings{10.1145/3386164.3386177,
author = {Lall, Snehalika and Bandyopadhyay, Sanghamitra},
title = {An l1-Norm Regularized Copula Based Feature Selection},
year = {2020},
isbn = {9781450376617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386164.3386177},
doi = {10.1145/3386164.3386177},
abstract = {In this paper, we develop a novel feature selection method called RCFS (Regularized Copula based Feature Selection) based on regularized copula. We use l1 regularization, as it penalizes the redundant co-efficient of features and makes them zero, resulting in non-redundant effective features set. Scale-invariant property of copula ensures good performance in noisy data, thereby improving the stability of the method. Three different forms of copula viz., Gaussian copula, Empirical copula, and Archimedean copula are used with l1 regularization. Results prove a significant improvement in the accuracy of the prediction model than any non regularized feature selection method. The number of optimal features to achieve a fixed accuracy value is also less than any other non regularized feature selection techniques.},
booktitle = {Proceedings of the 2019 3rd International Symposium on Computer Science and Intelligent Control},
articleno = {30},
numpages = {6},
keywords = {Classification, Copula, Feature Selection, Micro- array Data, Regularization},
location = {Amsterdam, Netherlands},
series = {ISCSIC 2019}
}

@article{10.1145/3436891,
author = {Yu, Kui and Liu, Lin and Li, Jiuyong},
title = {A Unified View of Causal and Non-causal Feature Selection},
year = {2021},
issue_date = {August 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3436891},
doi = {10.1145/3436891},
abstract = {In this article, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we can interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-world data.},
journal = {ACM Trans. Knowl. Discov. Data},
month = apr,
articleno = {63},
numpages = {46},
keywords = {Bayesian network, Causal feature selection, Markov blanket, mutual information, non-causal feature selection}
}

@article{10.1016/j.eswa.2021.114820,
author = {Bertolini, Massimo and Mezzogori, Davide and Neroni, Mattia and Zammori, Francesco},
title = {Machine Learning for industrial applications: A comprehensive literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114820},
doi = {10.1016/j.eswa.2021.114820},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {29},
keywords = {Literature review, Industrial applications, Deep Learning, Machine Learning, Operation management}
}

@article{10.1016/j.knosys.2020.105512,
author = {Zhong, Guo and Pun, Chi-Man},
title = {Subspace clustering by simultaneously feature selection and similarity learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {193},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.105512},
doi = {10.1016/j.knosys.2020.105512},
journal = {Know.-Based Syst.},
month = apr,
numpages = {10},
keywords = {Subspace clustering, Feature selection, Graph learning, Similarity learning, Affinity matrix}
}

@inproceedings{10.1007/978-3-030-55130-8_38,
author = {Zheng, Xiaohan and Zhang, Li and Yan, Leilei},
title = {Feature Selection Using Sparse Twin Support Vector Machine with Correntropy-Induced Loss},
year = {2020},
isbn = {978-3-030-55129-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-55130-8_38},
doi = {10.1007/978-3-030-55130-8_38},
abstract = {Twin support vector machine (TSVM) has been widely applied to classification problems. But TSVM is sensitive to outliers and is not efficient enough to realize feature selection. To overcome the shortcomings of TSVM, we propose a novel sparse twin support vector machine with the correntropy-induced loss (C-STSVM), which is inspired by the robustness of the correntropy-induced loss and the sparsity of the ℓ1-norm regularization. The objective function of C-STSVM includes the correntropy-induced loss that replaces the hinge loss, and the ℓ1-norm regularization that can make the decision model sparse to realize feature selection. Experiments on real-world datasets with label noise and noise features demonstrate the effectiveness of C-STSVM in classification accuracy and confirm the above conclusion further.},
booktitle = {Knowledge Science, Engineering and Management: 13th International Conference, KSEM 2020, Hangzhou, China, August 28–30, 2020, Proceedings, Part I},
pages = {434–445},
numpages = {12},
keywords = {Twin support vector machine, Feature selection, Sparsity, Correntropy-induced loss},
location = {Hangzhou, China}
}

@article{10.1007/s11277-019-06864-3,
author = {Ghazy, Rania A. and El-Rabaie, El-Sayed M. and Dessouky, Moawad I. and El-Fishawy, Nawal A. and El-Samie, Fathi E. Abd},
title = {Feature Selection Ranking and Subset-Based Techniques with Different Classifiers for Intrusion Detection},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {111},
number = {1},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-019-06864-3},
doi = {10.1007/s11277-019-06864-3},
abstract = {This paper investigates the performance of different feature selection techniques such as ranking and subset-based techniques, aiming to find the optimum collection of features to detect attacks with an appropriate classifier. The results reveal that more accuracy of detection and less false alarms are obtained after eliminating the redundant features and determining the most useful set of features, which increases the intrusion detection system (IDS) performance.},
journal = {Wirel. Pers. Commun.},
month = mar,
pages = {375–393},
numpages = {19},
keywords = {Network attacks, Classifiers, Intrusion detection, Feature selection}
}

@article{10.1016/j.ins.2019.02.021,
author = {Lee, Jaesung and Yu, Injun and Park, Jaegyun and Kim, Dae-Won},
title = {Memetic feature selection for multilabel text categorization using label frequency difference},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {485},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.02.021},
doi = {10.1016/j.ins.2019.02.021},
journal = {Inf. Sci.},
month = jun,
pages = {263–280},
numpages = {18},
keywords = {Multi-label text categorization, Feature selection, Memetic search, Population-based incremental learning, 00-01, 99-00}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s11390-020-9688-x,
author = {Zhang, Shu-Zheng and Zhao, Zhen-Yu and Feng, Chao-Chao and Wang, Lei},
title = {A Machine Learning Framework with Feature Selection for Floorplan Acceleration in IC Physical Design},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {2},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9688-x},
doi = {10.1007/s11390-020-9688-x},
abstract = {Floorplan is an important process whose quality determines the timing closure in integrated circuit (IC) physical design. And generating a floorplan with satisfying timing result is time-consuming because much time is spent on the generation-evaluation iteration. Applying machine learning to the floorplan stage is a potential method to accelerate the floorplan iteration. However, there exist two challenges which are selecting proper features and achieving a satisfying model accuracy. In this paper, we propose a machine learning framework for floorplan acceleration with feature selection and model stacking to cope with the challenges, targeting to reduce time and effort in integrated circuit physical design. Specifically, the proposed framework supports predicting post-route slack of static random-access memory (SRAM) in the early floorplan stage. Firstly, we introduce a feature selection method to rank and select important features. Considering both feature importance and model accuracy, we reduce the number of features from 27 to 15 (44% reduction), which can simplify the dataset and help educate novice designers. Then, we build a stacking model by combining different kinds of models to improve accuracy. In 28 nm technology, we achieve the mean absolute error of slacks less than 23.03 ps and effectively accelerate the floorplan process by reducing evaluation time from 8 hours to less than 60 seconds. Based on our proposed framework, we can do design space exploration for thousands of locations of SRAM instances in few seconds, much more quickly than the traditional approach. In practical application, we improve the slacks of SRAMs more than 75.5 ps (177% improvement) on average than the initial design.},
journal = {J. Comput. Sci. Technol.},
month = mar,
pages = {468–474},
numpages = {7},
keywords = {design space exploration, feature selection, machine learning, physical design}
}

@article{10.1016/j.neucom.2015.07.155,
author = {Liu, Mingxia and Zhang, Daoqiang},
title = {Feature selection with effective distance},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {215},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.07.155},
doi = {10.1016/j.neucom.2015.07.155},
abstract = {As more features are introduced in pattern recognition and machine learning applications, feature selection remains a critically important task to find the most compact representation of data, especially in unsupervised learning scenarios without enough class labels. Although there are a number of unsupervised feature selection methods in the literature, most of existing methods only focus on using conventional distances (e.g., Euclidean distance) to measure the similarity between two samples, which could not capture the dynamic structure of data due to the static characteristics of conventional distances. To reflect the dynamic structure of data, in this paper, we propose a set of effective distance-based feature selection methods, where a probabilistically motivated effective distance is used to measure the similarity of samples. Specifically, we first develop a sparse representation-based algorithm to compute the effective distance. Then, we propose three new filter-type unsupervised feature selection methods using effective distance, including an effective distance-based Laplacian Score (EDLS), and two effective distance-based Sparsity Scores (i.e., EDSS-1, and EDSS-2). Experimental results of clustering and classification tasks on a series of benchmark data sets show that our effective distance-based feature selection methods can achieve better performance than conventional methods using Euclidean distance.},
journal = {Neurocomput.},
month = nov,
pages = {100–109},
numpages = {10},
keywords = {Classification, Clustering, Effective distance, Feature selection, Sparse representation}
}

@inproceedings{10.1145/3483207.3483220,
author = {Tong, Fu},
title = {A Comprehensive Comparison of Neural Network-Based Feature Selection Methods in Biological Omics Datasets},
year = {2021},
isbn = {9781450390170},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483207.3483220},
doi = {10.1145/3483207.3483220},
abstract = {Omics allows researchers to apply systems biology approaches to identify the novel pathophysiological mechanism of diseases. It yields an unprecedented view of the cellular inner workings and is now often incorporated into the everyday methodology of biological researchers. However, the curse of dimensionality caused by the lack of enough samples and a large number of features is a major impediment to using Omics. In order to improve performance and decrease impediments of dimensionality in machine learning using Omics data, feature selection techniques are adopted. There are different feature selection methods and deep learning-based methods that have been attracting increasing attention in the field. In this paper, I applied the neural network-based feature selection methods to extract the Stomach and Esophageal carcinoma (STES) gene and compared the results. Finally, through comprehensive comparison, I found that the use of neural network-based feature selection methods did not always help improve the performance.},
booktitle = {Proceedings of the 2021 4th International Conference on Signal Processing and Machine Learning},
pages = {77–81},
numpages = {5},
location = {Beijing, China},
series = {SPML '21}
}

@article{10.1007/s11042-020-09236-3,
author = {Arshaghi, Ali and Ashourian, Mohsen and Ghabeli, Leila},
title = {Feature selection based on buzzard optimization algorithm for potato surface defects detection},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {35–36},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09236-3},
doi = {10.1007/s11042-020-09236-3},
abstract = {Different methods of feature selection find the best subdivision from the candidate subset. In all methods, based on the application and the type of the definition, a subset is selected as the answer; which can optimize the value of an evaluation function. The large number of features, high spatial and temporal complexity, and even reduced accuracy are common problems in such systems. Therefore, research needs to be performed to optimize these systems. In this paper, for increasing the classification accuracy and reducing their complexity; feature selection techniques are used. In addition, a new feature selection method by using the buzzard optimization algorithm (BUOZA) is proposed. These features would be used in segmentation, feature extraction, and classification steps in related applications; to improve the system performance. The results of the performed experiment on the developed method have shown a high performance while optimizing the system’s working parameters.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {26623–26641},
numpages = {19},
keywords = {Buzzard optimization algorithm, Global optimization, Potato defect detection, Feature selection, Image processing}
}

@inproceedings{10.1145/3278312.3278316,
author = {Win, Thee Zin and Kham, Nang Saing Moon},
title = {Mutual Information-based Feature Selection Approach to Reduce High Dimension of Big Data},
year = {2018},
isbn = {9781450365567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278312.3278316},
doi = {10.1145/3278312.3278316},
abstract = {As increasing the massive amount of data demands effective and efficient mining strategies, practitioners and researchers are trying to develop scalable mining algorithms, machine learning algorithms and strategies to be successful data mining in turning mountains of data into nuggets. Data of high dimension significantly increases the memory storage requirements and computational costs for data analytics. Therefore, reducing dimension can mainly improve three data mining performance: speed of learning, predictive accuracy and simplicity and comprehensibility of mined result. Feature selection, data preprocessing technique, is effective and efficient in data mining, data analytics and machine learning problems particularly in high dimension reduction. Most feature selection algorithms can eliminate only irrelevant features but redundant features. Not only irrelevant features but also redundant features can degrade learning performance. Mutual information measured feature selection is proposed in this work to remove both irrelevant and redundant features.},
booktitle = {Proceedings of the 2018 International Conference on Machine Learning and Machine Intelligence},
pages = {3–7},
numpages = {5},
keywords = {Feature Selection, High Dimensional Data, Mutual Information, Redundant Features},
location = {Ha Noi, Viet Nam},
series = {MLMI '18}
}

@article{10.1016/j.compbiomed.2021.104985,
author = {Ali, Md Mamun and Ahmed, Kawsar and Bui, Francis M. and Paul, Bikash Kumar and Ibrahim, Sobhy M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
title = {Machine learning-based statistical analysis for early stage detection of cervical cancer},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {139},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104985},
doi = {10.1016/j.compbiomed.2021.104985},
journal = {Comput. Biol. Med.},
month = dec,
numpages = {17},
keywords = {Random tree, Schiller, Hinselmann, Cytology, Biopsy, Cervical cancer}
}

@inproceedings{10.1007/978-3-030-73973-7_18,
author = {Cui, Lixin and Zhang, Lichi and Bai, Lu and Wang, Yue and Hancock, Edwin R.},
title = {Alzheimer’s Brain Network Analysis Using Sparse Learning Feature Selection},
year = {2021},
isbn = {978-3-030-73972-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73973-7_18},
doi = {10.1007/978-3-030-73973-7_18},
abstract = {Accurate identification of Mild Cognitive Impairment (MCI) based on resting-state functional Magnetic Resonance Imaging (RS-fMRI) is crucial for reducing the risk of developing Alzheimer’s disease (AD). In the literature, functional connectivity (FC) is often used to extract brain network features. However, it still remains challenging for the estimation of FC because RS-fMRI data are often high-dimensional and small in sample size. Although various Lasso-type sparse learning feature selection methods have been adopted to identify the most discriminative features for brain disease diagnosis, they suffer from two common drawbacks. First, Lasso is instable and not very satisfactory for the high-dimensional and small sample size problem. Second, existing Lasso-type feature selection methods have not simultaneously encapsulate the joint correlations between pairwise features and the target, the correlations between pairwise features, and the joint feature interaction into the feature selection process, thus may lead to suboptimal solutions. To overcome these issues, we propose a novel sparse learning feature selection method for MCI classification in this work. It unifies the above measures into a minimization problem associated with a least square error and an Elastic Net regularizer. Experimental results demonstrate that the diagnosis accuracy for MCI subjects can be significantly improved using our proposed feature selection method.},
booktitle = {Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshops, S+SSPR 2020, Padua, Italy, January 21–22, 2021, Proceedings},
pages = {184–194},
numpages = {11},
keywords = {Alzheimer’s disease, Feature selection, Elastic net}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Dynamic Software Product Lines, Dynamic variability, Feature models, Software architecture}
}

@article{10.1016/j.patcog.2016.11.003,
author = {Sheikhpour, Razieh and Sarram, Mehdi Agha and Gharaghani, Sajjad and Chahooki, Mohammad Ali Zare},
title = {A Survey on semi-supervised feature selection methods},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {64},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.11.003},
doi = {10.1016/j.patcog.2016.11.003},
abstract = {Feature selection is a significant task in data mining and machine learning applications which eliminates irrelevant and redundant features and improves learning performance. In many real-world applications, collecting labeled data is difficult, while abundant unlabeled data are easily accessible. This motivates researchers to develop semi-supervised feature selection methods which use both labeled and unlabeled data to evaluate feature relevance. However, till-to-date, there is no comprehensive survey covering the semi-supervised feature selection methods. In this paper, semi-supervised feature selection methods are fully investigated and two taxonomies of these methods are presented based on two different perspectives which represent the hierarchical structure of semi-supervised feature selection methods. The first perspective is based on the basic taxonomy of feature selection methods and the second one is based on the taxonomy of semi-supervised learning methods. This survey can be helpful for a researcher to obtain a deep background in semi-supervised feature selection methods and choose a proper semi-supervised feature selection method based on the hierarchical structure of them. A comprehensive survey on semi-supervised feature selection methods is presented.Two categories of these methods are presented from two different perspectives.The hierarchical structure of semi-supervised feature selection methods is given.Advantage and disadvantage of the survey methods are presented.Future research directions are presented.},
journal = {Pattern Recogn.},
month = apr,
pages = {141–158},
numpages = {18},
keywords = {Survey, Semi-supervised learning, Feature selection}
}

@article{10.1016/j.asoc.2019.04.037,
author = {Pourpanah, Farhad and Shi, Yuhui and Lim, Chee Peng and Hao, Qi and Tan, Choo Jun},
title = {Feature selection based on brain storm optimization for data classification},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {80},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.04.037},
doi = {10.1016/j.asoc.2019.04.037},
journal = {Appl. Soft Comput.},
month = jul,
pages = {761–775},
numpages = {15},
keywords = {Feature selection, Brain storm optimization, Fuzzy ARTMAP, Data classification}
}

@article{10.1016/j.knosys.2021.107308,
author = {Mokhtia, Mahla and Eftekhari, Mahdi and Saberi-Movahed, Farid},
title = {Dual-manifold regularized regression models for feature selection based on hesitant fuzzy correlation},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {229},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107308},
doi = {10.1016/j.knosys.2021.107308},
journal = {Know.-Based Syst.},
month = oct,
numpages = {14},
keywords = {Regression, Hesitant fuzzy correlation matrix, Feature selection, Feature manifold, Data manifold}
}

@article{10.1016/j.datak.2021.101909,
author = {Maass, Wolfgang and Storey, Veda C.},
title = {Pairing conceptual modeling with machine learning},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {134},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2021.101909},
doi = {10.1016/j.datak.2021.101909},
journal = {Data Knowl. Eng.},
month = jul,
numpages = {35},
keywords = {Artificial intelligence, Framework for incorporating conceptual modeling into data science projects, Database management, Models, Methodologies and tools, Machine learning, Conceptual modeling}
}

@article{10.1145/3459664,
author = {Talbi, El-Ghazali},
title = {Machine Learning into Metaheuristics: A Survey and Taxonomy},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3459664},
doi = {10.1145/3459664},
abstract = {During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {129},
numpages = {32},
keywords = {ML-supported metaheuristics, Metaheuristics, machine learning, optimization}
}

@article{10.1007/s11047-019-09754-6,
author = {Niu, Ben and Yi, Wenjie and Tan, Lijing and Geng, Shuang and Wang, Hong},
title = {A multi-objective feature selection method based on bacterial foraging optimization},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {1},
issn = {1567-7818},
url = {https://doi.org/10.1007/s11047-019-09754-6},
doi = {10.1007/s11047-019-09754-6},
abstract = {Feature selection plays an important role in data preprocessing. The aim of feature selection is to recognize and remove redundant or irrelevant features. The key issue is to use as few features as possible to achieve the lowest classification error rate. This paper formulates feature selection as a multi-objective problem. In order to address feature selection problem, this paper uses the multi-objective bacterial foraging optimization algorithm to select the feature subsets and k-nearest neighbor algorithm as the evaluation algorithm. The wheel roulette mechanism is further introduced to remove duplicated features. Four information exchange mechanisms are integrated into the bacteria-inspired algorithm to avoid the individuals getting trapped into the local optima so as to achieve better results in solving high-dimensional feature selection problem. On six small datasets and ten high-dimensional datasets, comparative experiments with different conventional wrapper methods and several evolutionary algorithms demonstrate the superiority of the proposed bacteria-inspired based feature selection method.},
journal = {Natural Computing: An International Journal},
month = mar,
pages = {63–76},
numpages = {14},
keywords = {Information exchange mechanism, Bacterial foraging optimization, Multi-objective optimization, Feature selection}
}

@article{10.1016/j.neunet.2019.08.030,
author = {Razzak, Imran and Saris, Raghib Abu and Blumenstein, Michael and Xu, Guandong},
title = {Integrating joint feature selection into subspace learning: A&nbsp;formulation of 2DPCA for outliers robust feature selection},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {121},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.08.030},
doi = {10.1016/j.neunet.2019.08.030},
journal = {Neural Netw.},
month = jan,
pages = {441–451},
numpages = {11},
keywords = {Principal component analysis, Dimensionality reduction, Outliers, 2DPCA, PCA}
}

@article{10.1007/s11063-020-10389-3,
author = {Farokhmanesh, Fatemeh and Sadeghi, Mohammad Taghi},
title = {Deep Neural Networks Regularization Using a Combination of Sparsity Inducing Feature Selection Methods},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10389-3},
doi = {10.1007/s11063-020-10389-3},
abstract = {Deep learning is an important subcategory of machine learning approaches in which there is a hope of replacing man-made features with fully automatic extracted features. However, in deep learning, we are generally facing a very high dimensional feature space. This may lead to overfitting problem which is tried to be prevented by applying regularization techniques. In this framework, the sparse representation based feature selection and regularization methods are very attractive. This is because of the nature of the sparse methods which represent a data with as less as possible non-zero coefficients. In this paper, we utilize a variety of sparse representation based methods for regularizing of deep neural networks. For this purpose, first, the effects of three basic sparsity inducing methods are studied. These are the Least Square Regression, Sparse Group Lasso (SGL) and Correntropy inducing Robust Feature Selection (CRFS) methods. Then, in order to improve the regularization process, three combinations of the basic methods are proposed. This study is performed considering a simple fully connected deep neural network and a VGG-like network. Our experimental results show that, overall, the combined methods outperform the basic ones. Considering two important factors of the amount of induced sparsity and classification accuracy, the combination of the CRFS and SGL methods leads to very successful results in deep neural network.},
journal = {Neural Process. Lett.},
month = feb,
pages = {701–720},
numpages = {20},
keywords = {Deep learning, Regularization, Lasso, Sparse feature selection}
}

@article{10.1016/j.eswa.2021.114778,
author = {Hussain, Kashif and Neggaz, Nabil and Zhu, William and Houssein, Essam H.},
title = {An efficient hybrid sine-cosine Harris hawks optimization for low and high-dimensional feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114778},
doi = {10.1016/j.eswa.2021.114778},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {27},
keywords = {Feature selection, Harris hawks optimization, Sine-cosine algorithm, High-dimensional data, Optimization problems}
}

@inproceedings{10.1007/978-3-030-73197-7_9,
author = {Wang, Jiaqi and Zhang, Li},
title = {Discriminant Mutual Information for Text Feature Selection},
year = {2021},
isbn = {978-3-030-73196-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73197-7_9},
doi = {10.1007/978-3-030-73197-7_9},
abstract = {In text classification tasks, the high dimensionality of data would result in a high computational complexity and decrease the classification accuracy because of high correlation between features; so, it is necessary to execute feature selection. In this paper, we propose a Discriminant Mutual Information (DMI) criterion to select features for text classification tasks. DMI measures the discriminant ability of features from two aspects. One is the mutual information between features and the label information. The other is the discriminant correlation degree between a feature and a target feature subset based on the label information, which could be used for judging whether a feature is redundant in the target feature subset. Thus, DMI is a de-redundancy text feature selection method considering discriminant information. In order to prove the superiority of DMI, we compare it with the state-of-the-art filter methods for text feature selection and conduct experiments on two datasets: Reuters-21578 and WebKB. K-Nearest Neighbor (KNN) and Support Vector Machine (SVM) are taken as the subsequent classifiers. Experimental results shows that the proposed DMI has significantly improved the classification accuracy and F1-score of both Reuters-21578 and WebKB.},
booktitle = {Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II},
pages = {136–151},
numpages = {16},
keywords = {Redundant features, Discriminant information, Mutual information, Feature selection, Text classification},
location = {Taipei, Taiwan}
}

@article{10.3233/JIFS-189504,
author = {Fen, Zhou and Ramachandran, Varatharajan},
title = {Efficiency improvement of English online teaching system based on bagging learning flow feature selection},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189504},
doi = {10.3233/JIFS-189504},
abstract = {In the era of artificial intelligence, the traditional English teaching model can no longer meet the needs of society, and online English teaching has become the main development direction of English teaching in the future. In order to study the efficiency of English online teaching system, based on machine learning algorithms, this paper constructs an efficiency improvement model of English online teaching system. Moreover, in view of the shortcomings of current situation estimation algorithms that cannot coexist in terms of flexibility, causal interpretability and complexity, this paper proposes a biological immune algorithm framework that uses GBDT algorithm coding, which objectively and accurately shows the spread of the situation. In addition, for the problem that redundant information between features will reduce the accuracy of the framework, this paper proposes a streaming feature selection algorithm based on bagging learning. Finally, this paper designs a control experiment to analyze the performance of the model. The research results show that the model constructed in this paper is highly reliable.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6695–6705},
numpages = {11},
keywords = {Bagging learning, flow feature, feature selection, English online teaching, machine learning}
}

@inproceedings{10.1145/3307630.3342412,
author = {Ananieva, Sofia and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and L\"{o}nn, Henrik and Ramesh, S. and Burger, Andreas and Taentzer, Gabriele and Westfechtel, Bernhard},
title = {Towards a Conceptual Model for Unifying Variability in Space and Time},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342412},
doi = {10.1145/3307630.3342412},
abstract = {Effectively managing variability in space and time is among the main challenges of developing and maintaining large-scale yet long-living software-intensive systems. Over the last decades, two large research fields, Software Configuration Management (SCM) and Software Product Line Engineering (SPLE), have focused on version management and the systematic handling of variability, respectively. However, neither research community has been successful in producing unified management techniques that are effective in practice, and both communities have developed largely independently of each other. As a step towards overcoming this unfortunate situation, in this paper, we report on ongoing work on conceiving a conceptual yet integrated model of SCM and SPLE concepts, originating from a recent Dagstuhl seminar on the unification of version and variant management. Our goal is to provide discussion grounds for a wider exploration of a unified methodology supporting software evolution in both space and time.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {44–48},
numpages = {5},
keywords = {product lines, revision management, variability, version control},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10489-020-01800-6,
author = {Li, Hao and Wang, Yongli and Li, Yanchao and Hu, Peng and Zhao, Ruxin},
title = {Joint local structure preservation and redundancy minimization for unsupervised feature selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {12},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01800-6},
doi = {10.1007/s10489-020-01800-6},
abstract = {Unsupervised feature selection is an indispensable pre-processing step in many data mining and pattern recognition tasks where the unlabeled high dimensional data are ubiquitous. Most of existing methods fail to explore the local geometric structure consistency (preservation) of the input data and minimize redundancy of selected features simultaneously. In this paper we propose a novel unsupervised feature selection method which jointly integrates the local geometric structure consistency and redundancy minimization (JLSPRM) into an unified framework. JLSPRM utilizes nonnegative spectral analysis to learn the cluster labels of the input data, then the local geometric structure consistency is developed to make the learned cluster labels more accurate, during which the feature selection operation is performed. To minimize the redundancy rate among selected features, the maximal information coefficient (MIC) is utilized to evaluate the correlation of the pairwise features. Besides, the ℓ2,1-norm is exerted on feature selection matrix which makes the framework decent for selecting features. An efficient iterative optimization algorithm is designed to obtain the solution of the unsupervised feature selection model. The superiority and effectiveness of our proposed approach over the state-of-the-art feature selection methods have also been validated through the extensive experiments on nine benchmark datasets.},
journal = {Applied Intelligence},
month = dec,
pages = {4394–4411},
numpages = {18},
keywords = {Unsupervised feature selection, Local structure preservation, Redundancy minimization, Sparsity constraint, Maximal information coefficient}
}

@inproceedings{10.1145/3461001.3473059,
author = {Azanza, Maider and Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {20 years of industrial experience at SPLC: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473059},
doi = {10.1145/3461001.3473059},
abstract = {Software Product Lines (SPLs) have been around since the late 1970s and have established themselves as a way to deal with product variability. Tens of companies around the globe can pay testament to their advantages. Practitioners, however, have lamented the lack of data on other practitioners' experiences that would help them in the SPL journey. This work intends to analyze the application of SPLs in industry in the last 20 years. We departed from 194 industrial studies that were published at the Software Product Line Conference, the premier venue for SPL research. After the filtering process we selected 66 primary studies, from 43 different companies and 15 countries. The studies were classified to answer three research questions: (i) which contexts have SPLs been applied in?, (ii) what phenomena have been reported? and, (iii) what evidences have been collected in terms of obtained benefits, encountered issues and lessons learned? Regarding the context, SPLs have mainly been reported in USA and Germany (50%) and are used to develop embedded systems (76%). The most cited reason to adopt SPLs is the need to increase product variants (42.42%). As for the phenomena, the most reported problem area is adoption (39.39%). Last, as for evidences the most cited benefit is a cost reduction (53.03%), the issue is evolution (13.13%) and the learned lesson is that architecture is essential (24.24%). We believe the findings will be of interest to the community as a whole in quest to bridge the gap between industry and academia while balancing rigor, authenticity and relevance.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {172–183},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1287/mnsc.1090.0997,
author = {Boulding, William and Christen, Markus},
title = {Pioneering Plus a Broad Product Line Strategy: Higher Profits or Deeper Losses?},
year = {2009},
issue_date = {June 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {6},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.0997},
doi = {10.1287/mnsc.1090.0997},
abstract = {Previous research suggests firms can build a market share advantage by preempting later entrants with a broad product line and expanding rapidly into related markets. Whether such a strategy leads to a pioneering profit advantage relative to followers also depends on its cost effects. In this paper, we examine when the market share advantage of a pioneering firm with a broad product line strategy translates into a profit advantage by examining the cost effects of this strategy. Using the profit impact of marketing strategies data and an estimation method that controls for various unobserved factors, we find significant differences between different industry settings. From these contrasting findings, we generate an emerging theoretical framework that we subject to empirical testing. We conjecture, and empirically verify, that creating a broad product line with a versioning strategy---creating variety from a standard product in anticipating customer demand---does not increase the pioneering cost disadvantage, and thus results in a pioneering profit advantage. On the other hand, with a tailoring strategy---creating variety by customizing a product to actual customer demand---a broad product line substantially increases the pioneering cost disadvantage, thereby making a preemption strategy counterproductive.},
journal = {Manage. Sci.},
month = jun,
pages = {958–967},
numpages = {10},
keywords = {IV estimation, business unit profitability, pioneering, preemption, product line strategy}
}

@article{10.1016/j.neucom.2019.09.039,
author = {Chen, Chuangquan and Gan, Yanfen and Vong, Chi-Man},
title = {Extreme semi-supervised learning for multiclass classification},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {376},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.09.039},
doi = {10.1016/j.neucom.2019.09.039},
journal = {Neurocomput.},
month = feb,
pages = {103–118},
numpages = {16},
keywords = {Multiclass classification, Semi-supervised support vector machine, Extreme learning machine, Approximate empirical kernel map, Alternating optimization}
}

@inproceedings{10.1007/978-3-030-32391-2_1,
author = {Huang, Zhongwei and Lei, Haijun and Chen, Guoliang and Li, Shiqi and Li, Hancong and Elazab, Ahmed and Lei, Baiying},
title = {Unsupervised Feature Selection via Adaptive Embedding and Sparse Learning for Parkinson’s Disease Diagnosis},
year = {2019},
isbn = {978-3-030-32390-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32391-2_1},
doi = {10.1007/978-3-030-32391-2_1},
abstract = {Parkinson’s disease (PD) is known as a progressive neurodegenerative disease in elderly people. Apart from decelerating the disease exacerbation, early and accurate diagnosis also alleviates mental and physical sufferings and provides timely and appropriate medication. In this paper, we propose an unsupervised feature selection method via adaptive manifold embedding and sparse learning exploiting longitudinal multimodal neuroimaging data for classification and regression prediction. Specifically, the proposed method simultaneously carries out feature selection and dynamic local structure learning to obtain the structural information inherent in the neuroimaging data. We conduct extensive experiments on the publicly available Parkinson’s progression markers initiative (PPMI) dataset to validate the proposed method. Our proposed method outperforms other state-of-the-art methods in terms of classification and regression prediction performance.},
booktitle = {Connectomics in NeuroImaging: Third International Workshop, CNI 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 13, 2019, Proceedings},
pages = {1–9},
numpages = {9},
keywords = {Parkinson’s disease, Unsupervised feature selection, Adaptive manifold embedding, Classification, Regression prediction},
location = {Shenzhen, China}
}

@inproceedings{10.1007/978-3-030-66843-3_24,
author = {Anjum, Sadia and Hussain, Lal and Ali, Mushtaq and Abbasi, Adeel Ahmed},
title = {Automated Multi-class Brain Tumor Types Detection by Extracting RICA Based Features and Employing Machine Learning Techniques},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_24},
doi = {10.1007/978-3-030-66843-3_24},
abstract = {Brain tumor is the leading reason of mortality across the globe. It is obvious that the chances of survival can be increased if the tumor is identified and properly classified at an initial stage. Several factors such as type, texture and location help to categorize the brain tumor. In this study, we extracted reconstruction independent component analysis (RICA) base features from brain tumor types such as glioma, meningioma, pituitary and applied robust machine learning algorithms such as linear discriminant analysis (LDA) and support vector machine (SVM) with linear and quadratic kernels. The jackknife 10-fold cross validation was used for training and testing data validation. The SVM with quadratic kernel gives the highest multiclass detection performance. To detect pituitary, the highest detection performance was obtained with sensitivity (93.85%), specificity (100%), PPV (100%), NPV (97.27%), accuracy (98.07%) and AUC (96.92). To detect glioma, the highest detection performance was obtained with accuracy (94.35%), AUC (0.9508). To detect the meningioma, the highest was obtained with accuracy (96.18%), AUC (0.9095). The findings reveal that proposed methodology based on RICA features to detect multiclass brain tumor types will be very useful for treatment modification to achieve better clinical outcomes.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {249–258},
numpages = {10},
keywords = {Image analysis, Pituitary, Meningioma, Glioma, Machine learning, Feature extraction},
location = {Lima, Peru}
}

@article{10.1007/s10772-021-09808-0,
author = {Bhangale, Kishor Barasu and Mohanaprasad, K.},
title = {A review on speech processing using machine learning paradigm},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {2},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-021-09808-0},
doi = {10.1007/s10772-021-09808-0},
abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human–machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.},
journal = {Int. J. Speech Technol.},
month = jun,
pages = {367–388},
numpages = {22},
keywords = {Speech processing, Speech recognition, Machine learning, Speech feature extraction, Speech classification, Speech emotion recognition}
}

@article{10.1016/j.jksuci.2018.05.010,
author = {Bahassine, Said and Madani, Abdellah and Al-Sarem, Mohammed and Kissi, Mohamed},
title = {Feature selection using an improved Chi-square for Arabic text classification},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {2},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.05.010},
doi = {10.1016/j.jksuci.2018.05.010},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = feb,
pages = {225–231},
numpages = {7},
keywords = {Feature selection, Chi-square, Arabic text classification, Light stemming, Mutual information, Information gain, SVM, Decision tree}
}

@inproceedings{10.1007/978-3-030-66843-3_27,
author = {Park, Yae Won and Park, Ji Eun and Ahn, Sung Soo and Kim, Hwiyoung and Kim, Ho Sung and Lee, Seung-Koo},
title = {Differentiation of Recurrent Glioblastoma from Radiation Necrosis Using Diffusion Radiomics: Machine Learning Model Development and External Validation},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_27},
doi = {10.1007/978-3-030-66843-3_27},
abstract = {We aimed to establish a high-performing radiomics strategy to differentiate recurrent glioblastoma (GBM) from radiation necrosis (RN). Eighty-six patients with posttreatment GBM were enrolled in the training set (63 recurrent GBM and 23 RN). Another 41 patients (23 recurrent GBM and 18 RN) from a different institution were enrolled in the test set. Conventional MRI sequences (T2-weighted and postcontrast T1-weighted images) and ADC were analyzed to extract 263 radiomic features. After feature selection, various machine learning models were trained with combinations of MRI sequences and validated in the test set. In the independent test set, the model using ADC sequence showed the best diagnostic performance, with an AUC, accuracy, sensitivity, specificity of 0.800, 78%, 66.7%, and 87%, respectively. The radiomics models using other MRI sequences showed AUCs ranging from 0.650 to 0.662 in the test set. In conclusion, the diffusion radiomics may be helpful in differentiating recurrent GBM from RN.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {276–283},
numpages = {8},
keywords = {Glioblastoma, Machine learning, Magnetic resonance imaging, Radiomics},
location = {Lima, Peru}
}

@article{10.1016/j.future.2021.06.036,
author = {Janjua, Faisal and Masood, Asif and Abbas, Haider and Rashid, Imran and Khan, Malik M. Zaki Murtaza},
title = {Textual analysis of traitor-based dataset through semi supervised machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.06.036},
doi = {10.1016/j.future.2021.06.036},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {652–660},
numpages = {9},
keywords = {Text classification, TWOS dataset, Enron dataset, Machine learning, Insider threat, Malicious emails}
}

@article{10.1504/ijcse.2021.115654,
author = {Kaur, Prabhjot and Awasthi, Amit and Bijalwan, Anchit},
title = {Evaluation of feature selection techniques on network traffic for comparing model accuracy},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {3},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.115654},
doi = {10.1504/ijcse.2021.115654},
abstract = {The accuracy and performance of any machine learning model are highly dependent on the number of qualitative features taken into consideration while training the model. The selection of qualitative features depends on the considerate choice of feature selection technique. In this study, feature selection is performed using different techniques such as information gain, Gini decrease, Chi2 and FCBF on the same dataset, and subsequently, the accuracy has been measured. The results showed that the FCBF method has dramatically reduced the number of features and moderated the accuracy compared with other feature selection methods.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {228–243},
numpages = {15},
keywords = {information gain, Gini decrease, Chi2, network traffic, FCBF, fast correlation-based feature, feature selection}
}

@article{10.1007/s11042-020-09617-8,
author = {Zhong, Jing and Zhong, Ping and Xu, Yimin and Yang, Liran},
title = {Robust multiview feature selection via view weighted},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {1},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09617-8},
doi = {10.1007/s11042-020-09617-8},
abstract = {In recent years, combining the multiple views of data to perform feature selection has been popular. As the different views are the descriptions from different angles of the same data, the abundant information coming from multiple views instead of the single view can be used to improve the performance of identification. In this paper, through the view weighted strategy, we propose a novel robust supervised multiview feature selection method, in which the robust feature selection is performed under the effect of l2,1-norm. The proposed model has the following advantages. Firstly, different from the commonly used view concatenation that is liable to ignore the physical meaning of features and cause over-fitting, the proposed method divides the original space into several subspaces and performs feature selection in the subspaces, which can reduce the computational complexity. Secondly, the proposed method assigns different weights to views adaptively according to their importance, which shows the complementarity and the specificity of views. Then, the iterative algorithm is given to solve the proposed model, and in each iteration, the original large-scale problem is split into the small-scale subproblems due to the divided original space. The performance of the proposed method is compared with several related state-of-the-art methods on the widely used multiview datasets, and the experimental results demonstrate the effectiveness of the proposed method.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {1503–1527},
numpages = {25},
keywords = {Robustness, Specificity of views, View weighted strategy, Supervised multiview feature selection}
}

@article{10.1007/s11227-021-03773-w,
author = {Balakrishnan, K. and Dhanalakshmi, R. and Khaire, Utkarsh Mahadeo},
title = {Improved salp swarm algorithm based on the levy flight for feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {11},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03773-w},
doi = {10.1007/s11227-021-03773-w},
abstract = {The fields of data science and data mining are enduring high-dimensionality issues because of a high volume of data. Conventional machine learning techniques give disgruntled responses to high-dimensional datasets. Feature selection is used to get the appropriate information from the dataset to reduce the dimensionality of the data. The recently proposed Salp Swarm Algorithm (SSA) is a population-based meta-heuristic optimization technique inspired by the Sea Salps Swarming technique. SSA failed to converge initial random solutions to the global optimum owing to its complete dependency on the number of iterations for the process of exploration and exploitation. The proposed improved SSA (iSSA) aims to enhance the ability of Salps to explore divergent areas by randomly updating its location. Randomizing the Salps location via Levy flight enriches the exploitation potential of SSA resulting in it converging the model toward the global optima. The performance of the proposed iSSA is investigated using six different high-dimensional microarray datasets. While comparing the ability to converge, it is understood that the proposed model outperforms SSA providing 0.1033% more confidence in the selected features. The results of the simulation revealed that the iSSA can provide better competitive and significant results compared to SSA.},
journal = {J. Supercomput.},
month = nov,
pages = {12399–12419},
numpages = {21},
keywords = {Salp Swarm Algorithm, Meta-Heuristic Optimization, Levy flight, High-dimensional dataset, Feature selection}
}

@article{10.1007/s10044-018-0707-2,
author = {Hoseini, Elham and Mansoori, Eghbal G.},
title = {Unsupervised feature selection in linked biological data},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-018-0707-2},
doi = {10.1007/s10044-018-0707-2},
abstract = {Feature selection techniques have become an apparent need in many bioinformatics applications, especially when there exist a huge number of features. For instance, classification of hereditary disease genes/proteins plays a significant role in prediction and diagnosis of diseases. In this regard, some knowledge of features' goodness in making predictions is needed. Apparently, distinctive features and their relevancy to class labels are determinant in designing efficient classifiers. Indeed, excluding redundant and/or irrelevant features, without incurring much loss of information, can reduce the processing cost while improving the predictor's performance. Consequently, feature selection is a preliminary task in most biological studies. Traditionally, biological data analysis methods also use the common feature selection techniques which imagine the data instances as independent objects and so not consider their possibly inter-relations. For instance, protein---protein interactions (PPIs) handle a wide range of biological processes including cell-to-cell interactions and metabolic and developmental control. Apparently, linked data have more similar characteristics than uncorrelated ones and so accounting these inter-relations beside to data content will be beneficial in feature selection. To incorporate the data inter-relations (e.g., PPIs in biological data) along with the data content in selecting more effective features, a novel feature selection algorithm is proposed. This method works in unsupervised manner to handle the unlabeled biological data since most of the real-world genes/proteins have no label. For this purpose, we try to optimize a novel objective function which incorporates both the inter-relations of data instances and their content. The proposed method tries to identify the most relevant and non-redundant features and extract the top-ranked ones. For this purpose, an efficient iterative algorithm is developed to optimize the objective function. To assess our methods, three well-known evaluation criteria are examined on some real-world biological datasets and the results are compared against some of the state-of-the-art feature selection methods. The experiments demonstrate the effectiveness of our proposed approach.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {999–1013},
numpages = {15},
keywords = {Biological linked data, Feature selection, Protein---protein interaction, Unsupervised feature selection}
}

@article{10.1007/s10115-021-01616-x,
author = {Alalga, Abdelouahid and Benabdeslem, Khalid and Mansouri, Dou El Kefel},
title = {3-3FS: ensemble method for semi-supervised multi-label feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {11},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-021-01616-x},
doi = {10.1007/s10115-021-01616-x},
abstract = {Feature selection has received considerable attention over the past decade. However, it is continuously challenged by new emerging issues. Semi-supervised multi-label learning is one of these promising novel approaches. In this work, we refer to it as an approach that combines data consisting of a huge amount of unlabeled instances with a small number of multi-labeled instances. Semi-supervised multi-label feature selection, like conventional feature selection algorithms, has a rather poor record as regards stability (i.e. robustness with respect to changes in data). To address this weakness and improve the robustness of the feature selection process in high-dimensional data, this document develops an ensemble methodology based on a 3-way resampling of data: (1) Bagging, (2) a random subspace method (RSM) and (3) an additional random sub-labeling strategy (RSL). The proposed framework contributes to enhancing the stability of feature selection algorithms and to improving their performance. Our research findings illustrate that bagging and RSM help improve the stability of the feature selection process and increase learning accuracy, while RSL addresses label correlation, which is a major concern with multi-label data. The paper presents the key findings of a series of experiments, which we conducted on selected benchmark data sets in the classification task. Results are promising, highlighting that the proposed method either outperforms state-of-the-art algorithms or produces at least comparable results.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {2969–2999},
numpages = {31},
keywords = {Ensemble learning, Semi-supervised multi-label learning, Feature selection}
}

@inproceedings{10.1145/3380446.3430635,
author = {Kimmel, Richard and Li, Tong and Winston, David},
title = {An Enhanced Machine Learning Model for Adaptive Monte Carlo Yield Analysis},
year = {2020},
isbn = {9781450375191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380446.3430635},
doi = {10.1145/3380446.3430635},
abstract = {This paper presents a novel methodology for generating machine learning models used by an adaptive Monte Carlo analysis. The advantages of this methodology are that model generation occurs at the beginning of the analysis with no retraining required, it applies to both classification and regression models, and accuracy of the Monte Carlo analysis is not impacted by the accuracy of the model. This paper discusses the details of constructing and enhancing the machine learning model with emphasis on model training. It will then show how the model enables a Monte Carlo analysis that monitors and adapts to model mispredictions.},
booktitle = {Proceedings of the 2020 ACM/IEEE Workshop on Machine Learning for CAD},
pages = {89–94},
numpages = {6},
keywords = {Monte Carlo, machine learning, statistical blockade, support vector machine, yield analysis},
location = {Virtual Event, Iceland},
series = {MLCAD '20}
}

@article{10.1016/j.eswa.2021.114887,
author = {Miao, Fahui and Yao, Li and Zhao, Xiaojie},
title = {Symbiotic organisms search algorithm using random walk and adaptive Cauchy mutation on the feature selection of sleep staging},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114887},
doi = {10.1016/j.eswa.2021.114887},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {17},
keywords = {Sleep staging, Feature selection, Symbiotic organisms search algorithm, Random walk, Adaptive Cauchy mutation}
}

@inproceedings{10.1007/978-3-030-66843-3_21,
author = {Booth, Thomas C. and Akpinar, Bernice and Roman, Andrei and Shuaib, Haris and Luis, Aysha and Chelliah, Alysha and Al Busaidi, Ayisha and Mirchandani, Ayesha and Alparslan, Burcu and Mansoor, Nina and Ashkan, Keyoumars and Ourselin, Sebastien and Modat, Marc},
title = {Machine Learning and Glioblastoma: Treatment Response Monitoring Biomarkers in 2021},
year = {2020},
isbn = {978-3-030-66842-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66843-3_21},
doi = {10.1007/978-3-030-66843-3_21},
abstract = {The aim of the systematic review was to assess recently published studies on diagnostic test accuracy of glioblastoma treatment response monitoring biomarkers in adults, developed through machine learning (ML). Articles published 09/2018–09/2020 were searched for using MEDLINE, EMBASE, and the Cochrane Register. Included study participants were adult patients with high grade glioma who had undergone standard treatment (maximal resection, radiotherapy with concomitant and adjuvant temozolomide) and subsequently underwent follow-up imaging to determine treatment response status (specifically, distinguishing progression/recurrence from progression/recurrence mimics - the target condition). Risk of bias and applicability was assessed with QUADAS 2 methodology. Contingency tables were created for hold-out test sets and recall, specificity, precision, F1-score, balanced accuracy calculated. Fifteen studies were included with 1038 patients in training sets and 233 in test sets. To determine whether there was progression or a mimic, the reference standard combination of follow-up imaging and histopathology at re-operation was applied in 67% (10/15) of studies. External hold-out test sets were used in 27% (4/15) to give ranges of diagnostic accuracy measures: recall = 0.70–1.00; specificity = 0.67–0.90; precision = 0.78–0.88; F1 score = 0.74–0.94; balanced accuracy = 0.74–0.83; AUC = 0.80–0.85. The small numbers of patient included in studies, the high risk of bias and concerns of applicability in the study designs (particularly in relation to the reference standard and patient selection due to confounding), and the low level of evidence, suggest that limited conclusions can be drawn from the data. There is likely good diagnostic performance of machine learning models that use MRI features to distinguish between progression and mimics. The diagnostic performance of ML using implicit features did not appear to be superior to ML using explicit features. There are a range of ML-based solutions poised to become treatment response monitoring biomarkers for glioblastoma. To achieve this, the development and validation of ML models require large, well-annotated datasets where the potential for confounding in the study design has been carefully considered. Therefore, multidisciplinary efforts and multicentre collaborations are necessary.},
booktitle = {Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-Oncology: Third International Workshop, MLCN 2020, and Second International Workshop, RNO-AI 2020, Held in Conjunction with MICCAI 2020, Lima, Peru, October 4–8, 2020, Proceedings},
pages = {212–228},
numpages = {17},
keywords = {Biomarkers, Diagnostic monitoring, Machine learning, Neuro-oncology},
location = {Lima, Peru}
}

@article{10.1007/s11063-020-10250-7,
author = {Wen, Guoqiu and Zhu, Yonghua and Zhan, Mengmeng and Tan, Malong},
title = {Sparse Low-Rank and Graph Structure Learning for Supervised Feature Selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10250-7},
doi = {10.1007/s11063-020-10250-7},
abstract = {Spectral feature selection (SFS) is superior to conventional feature selection methods in many aspects, by extra importing a graph matrix to preserve the subspace structure of data. However, the graph matrix of classical SFS that is generally constructed by original data easily outputs a suboptimal performance of feature selection because of the redundancy. To address this, this paper proposes a novel feature selection method via coupling the graph matrix learning and feature data learning into a unified framework, where both steps can be iteratively update until achieving the stable solution. We also apply a low-rank constraint to obtain the intrinsic structure of data to improve the robustness of learning model. Besides, an optimization algorithm is proposed to solve the proposed problem and to have fast convergence. Compared to classical and state-of-the-art feature selection methods, the proposed method achieved the competitive results on twelve real data sets.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1793–1809},
numpages = {17},
keywords = {Spectral feature selection, Orthogonal constraint, Low-rank constraint, Graph learning}
}

@article{10.1007/s11390-020-9864-z,
author = {Chen, Yi-Fan and Zhao, Xiang and Liu, Jin-Yuan and Ge, Bin and Zhang, Wei-Ming},
title = {Item Cold-Start Recommendation with Personalized Feature Selection},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9864-z},
doi = {10.1007/s11390-020-9864-z},
abstract = {The problem of recommending new items to users (often referred to as item cold-start recommendation) remains a challenge due to the absence of users’ past preferences for these items. Item features from side information are typically leveraged to tackle the problem. Existing methods formulate regression methods, taking item features as input and user ratings as output. These methods are confronted with the issue of overfitting when item features are high-dimensional, which greatly impedes the recommendation experience. Availing of high-dimensional item features, in this work, we opt for feature selection to solve the problem of recommending top-N new items. Existing feature selection methods find a common set of features for all users, which fails to differentiate users’ preferences over item features. To personalize feature selection, we propose to select item features discriminately for different users. We study the personalization of feature selection at the level of the user or user group. We fulfill the task by proposing two embedded feature selection models. The process of personalized feature selection filters out the dimensions that are irrelevant to recommendations or unappealing to users. Experimental results on real-life datasets with high-dimensional side information reveal that the proposed method is effective in singling out features that are crucial to top-N recommendation and hence improving performance.},
journal = {J. Comput. Sci. Technol.},
month = oct,
pages = {1217–1230},
numpages = {14},
keywords = {personalized feature selection, item cold-start top-N recommendation, high-dimensionality}
}

@article{10.1016/j.eswa.2011.02.181,
author = {Yang, Yang and Liao, Yinxia and Meng, Guang and Lee, Jay},
title = {A hybrid feature selection scheme for unsupervised learning and its application in bearing fault diagnosis},
year = {2011},
issue_date = {September, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {9},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.02.181},
doi = {10.1016/j.eswa.2011.02.181},
abstract = {With the development of the condition-based maintenance techniques and the consequent requirement for good machine learning methods, new challenges arise in unsupervised learning. In the real-world situations, due to the relevant features that could exhibit the real machine condition are often unknown as priori, condition monitoring systems based on unimportant features, e.g. noise, might suffer high false-alarm rates, especially when the characteristics of failures are costly or difficult to learn. Therefore, it is important to select the most representative features for unsupervised learning in fault diagnostics. In this paper, a hybrid feature selection scheme (HFS) for unsupervised learning is proposed to improve the robustness and the accuracy of fault diagnostics. It provides a general framework of the feature selection based on significance evaluation and similarity measurement with respect to the multiple clustering solutions. The effectiveness of the proposed HFS method is demonstrated by a bearing fault diagnostics application and comparison with other features selection methods.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11311–11320},
numpages = {10},
keywords = {Unsupervised learning, Feature selection, Fault diagnostics}
}

@article{10.1016/j.compbiomed.2021.104505,
author = {Ding, Xiaojian and Yang, Fan and Jin, Sheng and Cao, Jie},
title = {An efficient alpha seeding method for optimized extreme learning machine-based feature selection algorithm},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104505},
doi = {10.1016/j.compbiomed.2021.104505},
journal = {Comput. Biol. Med.},
month = jul,
numpages = {10},
keywords = {Alpha Seeding, Optimized extreme learning machine, Feature selection, Computational cost}
}

@article{10.1134/S0005117919020103,
author = {Hodashinsky, I. A. and Sarin, K. S.},
title = {Feature Selection for Classification through Population Random Search with Memory},
year = {2019},
issue_date = {February  2019},
publisher = {Plenum Press},
address = {USA},
volume = {80},
number = {2},
issn = {0005-1179},
url = {https://doi.org/10.1134/S0005117919020103},
doi = {10.1134/S0005117919020103},
abstract = {We propose a new approach for feature selection. The proposed approach is based on a combination of random and heuristic search strategies. The solution is represented as a binary vector whose dimension is determined by the number of features in the dataset. New solutions are generated at random using a normal and uniform distribution. The heuristic underlying the proposed approach can be formulated as follows: the chance of a feature to get into the next generation is proportional to the frequency of this feature appearing in previous best solutions. The proposed approach has been tested on several datasets from the KEEL repository. We also show an experimental comparison with other methods.},
journal = {Autom. Remote Control},
month = feb,
pages = {324–333},
numpages = {10},
keywords = {random search with memory, metaheuristics, fuzzy classifier, feature selection, binary optimization}
}

@inproceedings{10.5555/2022115.2022133,
author = {Zhu, Jiayi and Peng, Xin and Jarzabek, Stan and Xing, Zhenchang and Xue, Yinxing and Zhao, Wenyun},
title = {Improving product line architecture design and customization by raising the level of variability modeling},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product Line Architecture (PLA) plays a central role in software product line development. In order to support architecture-level variability modeling, most architecture description languages (ADLs) introduce architectural variation elements, such as optional component, connector and interface, which must be customized during product derivation. Variation elements are many, and design and customization of PLA at the level of individual variation elements are difficult and error-prone. We observed that developers usually perceive architecture variability from the perspective of variant features or variant design decisions that are mapped into groups of architecture variation elements. In the paper, we describe heuristics to identify configurations of variation elements that typically form such groups. We call them variation constructs. We developed an architecture variability management method and a tool that allow developers to work at the variation construct level rather than at the level of individual variation elements. We have applied and evaluated the proposed method in the development and maintenance of a medium-size financial product line. Our experience indicates that by raising variability modeling from variation element to construct level, architecture design and customizations become more intuitive. Not only does our method reduce the design and customization effort, but also better ensures consistent configuration of architectural variation elements, avoiding errors.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {151–166},
numpages = {16},
keywords = {ADL, architecture, software product line, variability},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@inproceedings{10.1145/3472456.3472490,
author = {Zhou, Yang and Wang, Fang and Feng, Dan},
title = {ASLDP: An Active Semi-supervised Learning method for Disk Failure Prediction},
year = {2021},
isbn = {9781450390682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472456.3472490},
doi = {10.1145/3472456.3472490},
abstract = {Disk failure has always been a major problem for data centers, leading to data loss. Current research works used supervised learning to offline training through a large number of labeled samples. However, these offline methods are no longer suitable for disk failure prediction tasks in the current big data environment. Behind this explosive amount of data, most methods do not take into account the label values used in the model training phase may not be easy to obtain, or the obtained label values are not completely accurate. These problems further restrict the development of supervised learning and offline modeling in disk failure prediction. In this paper, ASLDP, a novel disk failure prediction method is proposed, which uses active learning and semi-supervised learning. According to the characteristics of data in the disk life cycle, ASLDP carries out active learning for those clear labeled samples, which selects valuable samples and eliminates redundancy. For those samples that are unclearly labeled or unlabeled, ASLDP combines with semi-supervised learning for pre-labeled, and enhances the generalization ability by active learning. The results on three realistic datasets demonstrate that ASLDP achieves stable failure detection rates of 80-85% with low false alarm rates, compared to current online learning methods. Furthermore, ASLDP can overcome the problems of the sample label missing and data redundancy in the massive data environment, compared to current offline learning methods.},
booktitle = {Proceedings of the 50th International Conference on Parallel Processing},
articleno = {18},
numpages = {11},
keywords = {Active Learning, Disk Failure Prediction, Machine Learning, Semi-supervised Learning},
location = {Lemont, IL, USA},
series = {ICPP '21}
}

@inproceedings{10.1007/978-3-030-65621-8_20,
author = {Roul, Rajendra Kumar and Sahoo, Jajati Keshari},
title = {Study the Significance of ML-ELM Using Combined PageRank and Content-Based Feature Selection},
year = {2021},
isbn = {978-3-030-65620-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-65621-8_20},
doi = {10.1007/978-3-030-65621-8_20},
abstract = {Scalable big data analysis frameworks are of paramount importance in the modern web society, which is characterized by a huge number of resources, including electronic text documents. Hence, choosing an adequate subset of features that provide a complete representation of the document while discarding the irrelevant one is of utmost importance. Aiming in this direction, this paper studies the suitability and importance of a deep learning classifier called Multilayer ELM (ML-ELM) by proposing a combined PageRank and content-based feature selection (CPRCFS) technique on all the terms present in a given corpus. Top k% terms are selected to generate a reduced feature vector which is then used to train different classifiers including ML-ELM. Experimental results show that the proposed feature selection technique is better or comparable with the baseline techniques and the performance of Multilayer ELM can outperform state-of-the-arts machine and deep learning classifiers.},
booktitle = {Distributed Computing and Internet Technology: 17th International Conference, ICDCIT 2021, Bhubaneswar, India, January 7–10, 2021, Proceedings},
pages = {299–307},
numpages = {9},
keywords = {PageRank, Multilayer ELM, Machine learning, Feature selection, Deep learning, Classification},
location = {Bhubaneswar, India}
}

@article{10.1016/j.compag.2019.105175,
author = {Fogarty, Eloise S. and Swain, David L. and Cronin, Greg M. and Moraes, Luis E. and Trotter, Mark},
title = {Behaviour classification of extensively grazed sheep using machine learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {169},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2019.105175},
doi = {10.1016/j.compag.2019.105175},
journal = {Comput. Electron. Agric.},
month = feb,
numpages = {10},
keywords = {Accelerometer, MEMS, Machine-learning, Sheep, Behaviour classification}
}

@article{10.1016/j.eswa.2021.114737,
author = {Nouri-Moghaddam, Babak and Ghazanfari, Mehdi and Fathian, Mohammad},
title = {A novel multi-objective forest optimization algorithm for wrapper feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {175},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114737},
doi = {10.1016/j.eswa.2021.114737},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {20},
keywords = {Dimension reduction, Wrapper method, Forest optimization algorithm, Multi-objective optimization, Feature selection}
}

@article{10.1504/ijcat.2020.111844,
author = {Kumar, Pradeep},
title = {Predictive analytics for spam email classification using machine learning techniques},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {64},
number = {3},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.111844},
doi = {10.1504/ijcat.2020.111844},
abstract = {Automated text classification is the most widely used approach to manage an enormous amount of unstructured text data in digital forms, which is continuously increasing across the globe. Machine learning techniques are applied for automatic email filtering effectively to detect the spam mail and prevent them from delivering into the user's inbox. This paper used Logistic regression, k-Nearest Neighbours (k-NN), Naive Bayes, Decision Trees, AdaBoost, ANNs, and SVMs for spam email classification. All the classifiers are learned, and the performance measured in terms of precision, recall, and accuracy using a set of systematic experiments conducted on the Spambase data set extracted from the UCI Machine Learning Repository. The effectiveness of each model is empirically illustrated to find a better and viable alternative model. The quantitative performance analysis of supervised and hybrid learning techniques is presented in detail. Experimental results indicate that ensemble methods outperform in terms of accuracy compared with other methods applied.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {282–296},
numpages = {14},
keywords = {text analytics, feature selection, predictive modelling, spam filtering, machine learning techniques}
}

@article{10.1016/j.patrec.2018.04.007,
author = {Cilia, Nicole Dalia and De Stefano, Claudio and Fontanella, Francesco and Scotto di Freca, Alessandra},
title = {A ranking-based feature selection approach for handwritten character recognition},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.04.007},
doi = {10.1016/j.patrec.2018.04.007},
journal = {Pattern Recogn. Lett.},
month = apr,
pages = {77–86},
numpages = {10},
keywords = {65D17, 65D05, 41A10, 41A05, Handwritten character recognition, Feature selection}
}

@article{10.1016/j.neucom.2019.10.018,
author = {Ding, Deqiong and Yang, Xiaogao and Xia, Fei and Ma, Tiefeng and Liu, Haiyun and Tang, Chang},
title = {Unsupervised feature selection via adaptive hypergraph regularized latent representation learning},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {378},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.10.018},
doi = {10.1016/j.neucom.2019.10.018},
journal = {Neurocomput.},
month = feb,
pages = {79–97},
numpages = {19},
keywords = {Unsupervised feature selection, Hypergraph learning, Latent representation learning, Local structure preservation, 00-01, 99-00}
}

@article{10.1007/s11042-021-10597-6,
author = {Aayesha and Qureshi, Muhammad Bilal and Afzaal, Muhammad and Qureshi, Muhammad Shuaib and Fayaz, Muhammad},
title = {Machine learning-based EEG signals classification model for epileptic seizure detection},
year = {2021},
issue_date = {May 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10597-6},
doi = {10.1007/s11042-021-10597-6},
abstract = {The detection of epileptic seizures by classifying electroencephalography (EEG) signals into ictal and interictal classes is a demanding challenge, because it identifies the seizure and seizure-free states of an epileptic patient. In previous works, several machine learning-based strategies were introduced to investigate and interpret EEG signals for the purpose of their accurate classification. However, non-linear and non-stationary characteristics of EEG signals make it complicated to get complete information about these dynamic biomedical signals. In order to address this issue, this paper focuses on extracting the most discriminating and distinguishing features of seizure EEG recordings to develop an approach that employs both fuzzy-based and traditional machine learning algorithms for epileptic seizure detection. The proposed framework classifies unknown EEG signal segments into ictal and interictal classes. The model is validated using empirical evaluation on two benchmark datasets, namely the Bonn and Children’s Hospital of Boston-Massachusetts Institute of Technology (CHB-MIT) datasets. The obtained results show that in both cases, K-Nearest Neighbor (KNN) and Fuzzy Rough Nearest Neighbor (FRNN) give the highest classification accuracy scores, with improved sensitivity and specificity percentages.},
journal = {Multimedia Tools Appl.},
month = may,
pages = {17849–17877},
numpages = {29},
keywords = {Machine learning, Epilepsy, Seizure detection, Signal processing, EEG, Classification}
}

@inproceedings{10.5555/2486788.2487011,
author = {Gonzalez-Sanchez, Javier},
title = {Toward a software product line for affective-driven self-adaptive systems},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {One expected characteristic in modern systems is self-adaptation, the capability of monitoring and reacting to changes into the environment. A particular case of self-adaptation is affective-driven self-adaptation. Affective-driven self-adaptation is about having consciousness of user’s affects (emotions) and drive self-adaptation reacting to changes in those affects. Most of the previous work around self-adaptive systems deals with performance, resources, and error recovery as variables that trigger a system reaction. Moreover, most effort around affect recognition has been put towards offline analysis of affect, and to date only few applications exist that are able to infer user’s affect in real-time and trigger self-adaptation mechanisms. In response to this deficit, this work proposes a software product line approach to jump-start the development of affect-driven self-adaptive systems by offering the definition of a domain-specific architecture, a set of components (organized as a framework), and guidelines to tailor those components. Study cases with systems for learning and gaming will confirm the capability of the software product line to provide desired functionalities and qualities.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1381–1384},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1007/978-3-030-72699-7_31,
author = {Wang, Peng and Xue, Bing and Liang, Jing and Zhang, Mengjie},
title = {Improved Crowding Distance in Multi-objective Optimization for Feature Selection in Classification},
year = {2021},
isbn = {978-3-030-72698-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72699-7_31},
doi = {10.1007/978-3-030-72699-7_31},
abstract = {Feature selection is an essential preprocessing step in data mining and machine learning. A feature selection task can be treated as a multi-objective optimization problem which simultaneously minimizes the classification error and the number of selected features. Many existing feature selection approaches including multi-objective methods neglect that there exists multiple optimal solutions in feature selection. There can be multiple different optimal feature subsets which achieve the same or similar classification performance. Furthermore, when using evolutionary multi-objective optimization for feature selection, a crowding distance metric is typically used to play a role in environmental selection. However, some existing calculations of crowding metrics based on continuous/numeric values are inappropriate for feature selection since the search space of feature selection is discrete. Therefore, this paper proposes a new environmental selection method to modify the calculation of crowding metrics. The proposed approach is expected to help a multi-objective feature selection algorithm to find multiple potential optimal feature subsets. Experiments on sixteen different datasets of varying difficulty show that the proposed approach can find more diverse feature subsets, achieving the same classification performance without deteriorating performance regarding hypervolume and inverted generational distance.},
booktitle = {Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {489–505},
numpages = {17},
keywords = {Crowding distance, Multi-objective optimization, Feature selection}
}

@article{10.1007/s00500-019-04628-6,
author = {Ding, Ye and Zhou, Kui and Bi, Weihong},
title = {Feature selection based on hybridization of genetic algorithm and competitive swarm optimizer},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {15},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04628-6},
doi = {10.1007/s00500-019-04628-6},
abstract = {Feature selection is one of the hottest machine learning topics in recent years. The main purposes of it are to simplify the original model, improve the readability of the model, and prevent over-fitting by searching for a suitable subset of features. There are many methods for this problem, including evolutionary algorithms and particle swarm optimization. Among them, the competitive swarm optimizer is a new optimization algorithm proposed in recent years, which is based on particle swarm optimization algorithm, and has achieved good results in high-dimensional feature selection problems, but it also has the problems of high computation time cost and easily being premature. Aiming at these problems, this paper proposes to add the crossover operator and mutation operator in the genetic algorithm to the competitive swarm optimization, so as to improve the generation speed of new individuals in the algorithm and prevent premature population. After testing on UC Irvine Machine Learning Repository, the new algorithm not only improves the computational efficiency, but also avoids the problem that the competitive swarm optimization algorithm is easy to fall into the local optimum, which greatly improves the calculation effect.},
journal = {Soft Comput.},
month = aug,
pages = {11663–11672},
numpages = {10},
keywords = {Feature selection, Competitive swarm optimization, Genetic algorithm, Classification, High dimensionality}
}

@article{10.1007/s10044-018-0695-2,
author = {Emary, E. and Zawbaa, Hossam M.},
title = {Feature selection via L\`{e}vy Antlion optimization},
year = {2019},
issue_date = {August    2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-018-0695-2},
doi = {10.1007/s10044-018-0695-2},
abstract = {In this paper, a modification of the newly proposed antlion optimization (ALO) is introduced and applied to feature selection relied on the L\`{e}vy flights. ALO method is one of the encouraging swarm intelligence algorithms which make use of random walking to perform the exploration and exploitation operations. Random walks based on uniform distribution is responsible for premature convergence and stagnation. A L\`{e}vy flight random walk is suggested as a permutation for performing a local search. L\`{e}vy random walking grants the optimization ability to generate several solutions that are apart from existing solutions and furthermore enables it to escape from local minima and much efficient in examining large search area. The proposed L\`{e}vy antlion optimization (LALO) algorithm is applied in a wrapper-based mode to select optimal feature combination that maximizing classification accuracy while minimizing the number of selected features. LALO algorithm is applied on 21 different benchmark datasets against genetic algorithm (GA), particle swarm optimization (PSO), and the native ALO methods. Different initialization methods and several evaluation criteria are employed to assess algorithm diversification and intensification of the optimization algorithms. The experimental results demonstrate the significant improvement in the proposed LALO over the native ALO and many well-known methods used in feature selection.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {857–876},
numpages = {20},
keywords = {Bio-inspired optimization, Feature selection, L\`{e}vy Antlion optimization, L\`{e}vy flight}
}

@inproceedings{10.1145/3336294.3336311,
author = {T\"{e}rnava, Xhevahire and Mortara, Johann and Collet, Philippe},
title = {Identifying and Visualizing Variability in Object-Oriented Variability-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336311},
doi = {10.1145/3336294.3336311},
abstract = {In many variability-intensive systems, variability is implemented in code units provided by a host language, such as classes or functions, which do not align well with the domain features. Annotating or creating an orthogonal decomposition of code in terms of features implies extra effort, as well as massive and cumbersome refactoring activities. In this paper, we introduce an approach for identifying and visualizing the variability implementation places within the main decomposition structure of object-oriented code assets in a single variability-rich system. First, we propose to use symmetry, as a common property of some main implementation techniques, such as inheritance or overloading, to identify uniformly these places. We study symmetry in different constructs (e.g., classes), techniques (e.g., subtyping, overloading) and design patterns (e.g., strategy, factory), and we also show how we can use such symmetries to find variation points with variants. We then report on the implementation and application of a toolchain, symfinder, which automatically identifies and visualizes places with symmetry. The publicly available application to several large open-source systems shows that symfinder can help in characterizing code bases that are variability-rich or not, as well as in discerning zones of interest w.r.t. variability.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {231–243},
numpages = {13},
keywords = {identifying software variability, object-oriented variability-rich systems, software product line engineering, tool support for understanding software variability, visualizing software variability},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.cmpb.2021.106329,
author = {Ventrella, Piervincenzo and Delgrossi, Giovanni and Ferrario, Gianmichele and Righetti, Marco and Masseroli, Marco},
title = {Supervised machine learning for the assessment of Chronic Kidney Disease advancement},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {209},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106329},
doi = {10.1016/j.cmpb.2021.106329},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {9},
keywords = {chronicity management, personalized care, predicting renal failure, supervised machine learning, Chronic Kidney Disease}
}

@article{10.3233/JIFS-191568,
author = {Subbiah, Siva Sankari and Chinnappan, Jayakumar},
title = {An improved short term load forecasting with ranker based feature selection technique},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-191568},
doi = {10.3233/JIFS-191568},
abstract = {The load forecasting is the significant task carried out by the electricity providing utility companies for estimating the future electricity load. The proper planning, scheduling, functioning, and maintenance of the power system rely on the accurate forecasting of the electricity load. In this paper, the clustering-based filter feature selection is proposed for assisting the forecasting models in improving the short term load forecasting performance. The Recurrent Neural Network based Long Short Term Memory (LSTM) is developed for forecasting the short term load and compared against Multilayer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Regression (SVR) and Random Forest (RF). The performance of the forecasting model is improved by reducing the curse of dimensionality using filter feature selection such as Fast Correlation Based Filter (FCBF), Mutual Information (MI), and RReliefF. The clustering is utilized to group the similar load patterns and eliminate the outliers. The feature selection identifies the relevant features related to the load by taking samples from each cluster. To show the generality, the proposed model is experimented by using two different datasets from European countries. The result shows that the forecasting models with selected features produce better performance especially the LSTM with RReliefF outperformed other models.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6783–6800},
numpages = {18},
keywords = {Load forecasting, feature selection, clustering, deep learning, long short term memory}
}

@article{10.1016/j.compbiomed.2021.104672,
author = {Ali, Md Mamun and Paul, Bikash Kumar and Ahmed, Kawsar and Bui, Francis M. and Quinn, Julian M.W. and Moni, Mohammad Ali},
title = {Heart disease prediction using supervised machine learning algorithms: Performance analysis and comparison},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104672},
doi = {10.1016/j.compbiomed.2021.104672},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {10},
keywords = {Cardiovascular disease, Machine learning, Random forest, Decision tree, KNN}
}

@article{10.1504/ijitm.2021.114161,
author = {Zaffar, Maryam and Hashmani, Manzoor Ahmed and Savita, K.S. and Khan, Sameer Ahmad},
title = {A review on feature selection methods for improving the performance of classification in educational data mining},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {20},
number = {1–2},
issn = {1461-4111},
url = {https://doi.org/10.1504/ijitm.2021.114161},
doi = {10.1504/ijitm.2021.114161},
abstract = {Educational data mining (EDM) evaluates and predicts students' performance that assists to discover important factors affecting students' academic performance and also guides educational managers to make appropriate decisions accordingly. The most common technique for discovering meaningful information from the educational database is classification. The accuracy of classification algorithms on educational data can be increased by applying feature selection algorithms. Feature selection algorithms help in selecting robots and meaningful features for predicting students' performance with high accuracy. This paper presents different EDM approaches for forecasting students' performance using different data mining techniques. In addition, this paper also presents an evaluation of recent classification algorithms and feature selection algorithms used in educational data mining. Furthermore, the paper will guide the researchers on new and possible dimensions in building a prediction model in EDM.},
journal = {Int. J. Inf. Technol. Manage.},
month = jan,
pages = {110–131},
numpages = {21},
keywords = {educational data mining, EDM, classification algorithms, feature selection in educational data mining, filter feature selection, wrapper feature selection}
}

@article{10.1007/s00521-018-3853-y,
author = {Li, Jiaye and Zhang, Shichao and Zhang, Leyuan and Lei, Cong and Zhang, Jilian},
title = {Unsupervised nonlinear feature selection algorithm via kernel function},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3853-y},
doi = {10.1007/s00521-018-3853-y},
abstract = {Feature selection is one of the important methods of data preprocessing, but the general feature selection algorithm has the following shortcomings: (1) Noise and outliers cannot be ruled out so that the algorithm does not work well. (2) They only consider the linear relationship between data without considering the nonlinear relationship between data. For this reason, an unsupervised nonlinear feature selection algorithm via kernel function is proposed in this paper. First, each data feature is mapped to a kernel space by a kernel function. In this way, nonlinear feature selection can be performed. Secondly, the low-rank processing of the kernel coefficient matrix is used to eliminate the interference of noise samples. Finally, the feature selection is performed through a sparse regularization factor in the kernel space. Experimental results show that our algorithm has better results than contrast algorithms.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6443–6454},
numpages = {12},
keywords = {Feature selection, Kernel function, Sparse regularization factor}
}

@article{10.1016/j.patcog.2019.107077,
author = {Mu\~{n}oz-Romero, Sergio and Gorostiaga, Arantza and Soguero-Ruiz, Cristina and Mora-Jim\'{e}nez, Inmaculada and Rojo-\'{A}lvarez, Jos\'{e} Luis},
title = {Informative variable identifier: Expanding interpretability in feature selection},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107077},
doi = {10.1016/j.patcog.2019.107077},
journal = {Pattern Recogn.},
month = feb,
numpages = {19},
keywords = {Feature selection, Interpretability, Explainable machine learning, Resampling, Classification}
}

@inproceedings{10.1007/978-3-030-59830-3_24,
author = {Li, Xinyi and You, Ning and Tan, Jun and Bi, Ning},
title = {Differences and Similarities Learning for Unsupervised Feature Selection},
year = {2020},
isbn = {978-3-030-59829-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-59830-3_24},
doi = {10.1007/978-3-030-59830-3_24},
abstract = {In this paper, a novel feature selection algorithm, named Feature Selection with Differences and Similarities (FSDS), is proposed. FSDS jointly exploits sample differences from global structure and similarities from local structure. To reduce the disturbance from noisy feature, a row-wise sparse constraint is also merged into the objective function. FSDS, then combines the underlying subspace features with original feature to construct a more reliable feature set. Furthermore, a joint version of FSDS (FSDS2) is introduced. To optimize the proposed two-step FSDS and the joint version FSDS2 we also design two efficient iterative algorithms. Experimental results on various datasets demonstrate the effectiveness of the proposed algorithms.},
booktitle = {Pattern Recognition and Artificial Intelligence: International Conference, ICPRAI 2020, Zhongshan, China, October 19–23, 2020, Proceedings},
pages = {276–292},
numpages = {17},
keywords = {Feature selection, Spectral learning, Row sparsity, Sample differences and similarities exploiting},
location = {Zhongshan, China}
}

@inproceedings{10.5555/3495724.3495928,
author = {Dinh, Vu and Ho, Lam Si Tung},
title = {Consistent feature selection for analytic deep neural networks},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {One of the most important steps toward interpretability and explainability of neural network models is feature selection, which aims to identify the subset of relevant features. Theoretical results in the field have mostly focused on the prediction aspect of the problem with virtually no work on feature selection consistency for deep neural networks due to the model's severe nonlinearity and unidentifiability. This lack of theoretical foundation casts doubt on the applicability of deep learning to contexts where correct interpretations of the features play a central role.In this work, we investigate the problem of feature selection for analytic deep networks. We prove that for a wide class of networks, including deep feed-forward neural networks, convolutional neural networks, and a major sub-class of residual neural networks, the Adaptive Group Lasso selection procedure with Group Lasso as the base estimator is selection-consistent. The work provides further evidence that Group Lasso might be inefficient for feature selection with neural networks and advocates the use of Adaptive Group Lasso over the popular Group Lasso.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {204},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}

@inproceedings{10.1145/3292500.3330856,
author = {Li, Jundong and Guo, Ruocheng and Liu, Chenghao and Liu, Huan},
title = {Adaptive Unsupervised Feature Selection on Attributed Networks},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330856},
doi = {10.1145/3292500.3330856},
abstract = {Attributed networks are pervasive in numerous of high-impact domains. As opposed to conventional plain networks where only pairwise node dependencies are observed, both the network topology and node attribute information are readily available on attributed networks. More often than not, the nodal attributes are depicted in a high-dimensional feature space and are therefore notoriously difficult to tackle due to the curse of dimensionality. Additionally, features that are irrelevant to the network structure could hinder the discovery of actionable patterns from attributed networks. Hence, it is important to leverage feature selection to find a high-quality feature subset that is tightly correlated to the network structure. Few of the existing efforts either model the network structure at a macro-level by community analysis or directly make use of the binary relations. Consequently, they fail to exploit the finer-grained tie strength information for feature selection and may lead to suboptimal results. Motivated by the sociology findings, in this work, we investigate how to harness the tie strength information embedded on the network structure to facilitate the selection of relevant nodal attributes. Methodologically, we propose a principled unsupervised feature selection framework ADAPT to find informative features that can be used to regenerate the observed links and further characterize the adaptive neighborhood structure of the network. Meanwhile, an effective optimization algorithm for the proposed ADAPT framework is also presented. Extensive experimental studies on various real-world attributed networks validate the superiority of the proposed ADAPT framework.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {92–100},
numpages = {9},
keywords = {adaptive neighborhood structure, attributed networks, tie strength, unsupervised feature selection},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@article{10.1016/j.eswa.2019.01.016,
author = {Manbari, Zhaleh and AkhlaghianTab, Fardin and Salavati, Chiman},
title = {Hybrid fast unsupervised feature selection for high-dimensional data},
year = {2019},
issue_date = {Jun 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {124},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.01.016},
doi = {10.1016/j.eswa.2019.01.016},
journal = {Expert Syst. Appl.},
month = jun,
pages = {97–118},
numpages = {22},
keywords = {Feature selection, High-dimensional data, Binary ant system, Clustering, Mutation}
}

@inproceedings{10.1145/3386723.3387840,
author = {Khairi, Nurilhami Izzatie and Mohamed, Azlinah and Yusof, Nor Nadiah},
title = {Feature Selection Methods in Sentiment Analysis: A Review},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387840},
doi = {10.1145/3386723.3387840},
abstract = {The development of digital tecnnologies nowadays assists people by suggesting opinion, choices, preferences and feelings. This opinion is useful for company's engagement to make certain analysis to know their potential users and personalized their need. However, the information needs extraction to make further analysis. Thus, sentiment analysis is used to extract opinion and others and transform it into meaningful data. During the process of analysis, feature selection method is required to select a subset which consists of relevant features to construct a predictive model. This process requires some conditions during the selection of feature subset. The required conditions for feature selection are that the selected feature subset must be small and relevant for a high dimensional dataset which considers the presence of noise plus there are no redundant features. However, some of the feature selection methods unable to fulfill all conditions. In this research, 40 papers were collected, classified and reviewed. We discussed on the feature selection methods in sentiment analysis based on its level of analysis and make comparison between these methods to know its limitation and advantages. The comparison made between methods are based on its accuracy and CPU performance. Finally, suggest the best/benchmark method for feature selection. The findings obtained from this research shows that hybrid methods obtain the best accuracy and CPU performance compared to the other methods.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems &amp; Security},
articleno = {21},
numpages = {7},
keywords = {Embedded, Feature Selection, Filter, Hybrid, Sentiment Analysis, Wrapper},
location = {Marrakech, Morocco},
series = {NISS '20}
}

@article{10.1504/ijaip.2021.116363,
author = {Praveena, M.D. Anto and Bharathi, B.},
title = {A hybrid feature selection algorithm for big data dimensionality reduction},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {19},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.116363},
doi = {10.1504/ijaip.2021.116363},
abstract = {Data dimensionality causes problems like large storage requirements due to data redundancy, noisy data visualisation and high computational cost in analytical processes. Hence, reduction of high data dimension data set is the vital task in big data analytics. Feature selection is a dimensionality reduction approach that targets to reduce the high data dimensions into l subset by attaining a set of uncorrelated principal variables based on key features from the data source by eliminating redundant, noisy features and irrelevant. In the proposed work, ant colony optimisation (ACO) and quick branch and bound (QBB) based hybrid algorithm for feature selection is presented to improve the efficiency of feature selection in big data analytics. ACO was implemented for feature selection by observing real ants in their search of their food resources. QBB algorithm is used to partition the large data set into two partitions initially. These two algorithms are combined and implemented to reduce the data dimensionality based on feature selection approach. This algorithm was simulated and compared against existing feature selection algorithms in terms of a set of performance metrics such as precision, recall, F-score, classification accuracy, size of selected feature and proved the efficacy of proposed hybrid algorithm.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {380–392},
numpages = {12},
keywords = {dimensionality reduction, feature selection, ant colony optimisation, ACO, quick branch and bound, QBB}
}

@article{10.1016/j.fss.2018.10.021,
author = {Wang, Ling and Meng, Jianyao and Huang, Ruixia and Zhu, Hui and Peng, Kaixiang},
title = {Incremental feature weighting for fuzzy feature selection},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {368},
number = {C},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2018.10.021},
doi = {10.1016/j.fss.2018.10.021},
journal = {Fuzzy Sets Syst.},
month = aug,
pages = {1–19},
numpages = {19},
keywords = {Feature selection, Fuzzy, Incremental feature weighting, Mutual information}
}

@article{10.1007/s10916-018-0940-7,
author = {Maniruzzaman, Md. and Rahman, Md. Jahanur and Al-Mehedihasan, Md. and Suri, Harman S. and Abedin, Md. Menhazul and El-Baz, Ayman and Suri, Jasjit S.},
title = {Accurate Diabetes Risk Stratification Using Machine Learning: Role of Missing Value and Outliers},
year = {2018},
issue_date = {May 2018},
publisher = {Plenum Press},
address = {USA},
volume = {42},
number = {5},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-018-0940-7},
doi = {10.1007/s10916-018-0940-7},
abstract = {Diabetes mellitus is a group of metabolic diseases in which blood sugar levels are too high. About 8.8% of the world was diabetic in 2017. It is projected that this will reach nearly 10% by 2045. The major challenge is that when machine learning-based classifiers are applied to such data sets for risk stratification, leads to lower performance. Thus, our objective is to develop an optimized and robust machine learning (ML) system under the assumption that missing values or outliers if replaced by a median configuration will yield higher risk stratification accuracy. This ML-based risk stratification is designed, optimized and evaluated, where: (i) the features are extracted and optimized from the six feature selection techniques (random forest, logistic regression, mutual information, principal component analysis, analysis of variance, and Fisher discriminant ratio) and combined with ten different types of classifiers (linear discriminant analysis, quadratic discriminant analysis, na ve Bayes, Gaussian process classification, support vector machine, artificial neural network, Adaboost, logistic regression, decision tree, and random forest) under the hypothesis that both missing values and outliers when replaced by computed medians will improve the risk stratification accuracy. Pima Indian diabetic dataset (768 patients: 268 diabetic and 500 controls) was used. Our results demonstrate that on replacing the missing values and outliers by group median and median values, respectively and further using the combination of random forest feature selection and random forest classification technique yields an accuracy, sensitivity, specificity, positive predictive value, negative predictive value and area under the curve as: 92.26%, 95.96%, 79.72%, 91.14%, 91.20%, and 0.93, respectively. This is an improvement of 10% over previously developed techniques published in literature. The system was validated for its stability and reliability. RF-based model showed the best performance when outliers are replaced by median values.},
journal = {J. Med. Syst.},
month = may,
pages = {1–17},
numpages = {17},
keywords = {Diabetes, Feature selection, Machine learning, Missing values, Outliers, Risk stratification}
}

@inproceedings{10.1007/978-3-030-70866-5_10,
author = {Elaeraj, Ouafae and Leghris, Cherkaoui and Renault, \'{E}ric},
title = {Performance Evaluation of Some Machine Learning Algorithms for Security Intrusion Detection},
year = {2020},
isbn = {978-3-030-70865-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70866-5_10},
doi = {10.1007/978-3-030-70866-5_10},
abstract = {The growth of the Internet and the opening of systems have led to an increasing number of attacks on computer networks. Security vulnerabilities are increasing, in the design of communication protocols as well as in their implementation. On another side, the knowledge, tools and scripts, to launch attacks, become readily available and more usable. Hence, the need for an intrusion detection system (IDS) is also more apparent. This technology consists in searching for a series of words or parameters characterizing an attack in a packet flow. Intrusion Detection Systems has become an essential and critical component in an IT security architecture. An IDS should be designed as part of a global security policy. The objective of an IDS is to detect any violation of the rules according to the local security policy, it thus makes it possible to report attacks. This last multi-faceted, difficult to pin down when not handled, but most of the work done in this area remains difficult to compare, that's why the aim of our article is to analyze and compare intrusion detection techniques with several machine learning algorithms. Our research indicates which algorithm offers better overall performance than the others with the IDS field.},
booktitle = {Machine Learning for Networking: Third International Conference, MLN 2020, Paris, France, November 24–26, 2020, Revised Selected Papers},
pages = {154–166},
numpages = {13},
keywords = {IDS, Machine learning, KNN, SVM and Decision tree},
location = {Paris, France}
}

@article{10.1016/j.ijinfomgt.2019.07.003,
author = {Lee, Keon Myung and Yoo, Jaesoo and Kim, Sang-Wook and Lee, Jee-Hyong and Hong, Jiman},
title = {Autonomic machine learning platform},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2019.07.003},
doi = {10.1016/j.ijinfomgt.2019.07.003},
journal = {Int. J. Inf. Manag.},
month = dec,
pages = {491–501},
numpages = {11},
keywords = {Autonomic machine learning platform, Autonomic level, Machine learning, Smart City}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1007/s11277-021-08308-3,
author = {Maanicshah, Kamal and Amayri, Manar and Bouguila, Nizar and Fan, Wentao},
title = {Unsupervised Learning Using Variational Inference on Finite Inverted Dirichlet Mixture Models with Component Splitting},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {119},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-021-08308-3},
doi = {10.1007/s11277-021-08308-3},
abstract = {Unsupervised learning has been one of the essentials of pattern recognition and data mining. The role of Dirichlet family of mixture models in this field is inevitable. In this article, we propose a finite Inverted Dirichlet mixture model for unsupervised learning using variational inference. In particular, we develop an incremental algorithm with a component splitting approach for local model selection, which makes the clustering algorithm more efficient. We illustrate our model and learning algorithm with synthetic data and some real applications for occupancy estimation in smart homes and topic learning in images and videos. Extensive comparisons with comparable recent approaches have shown the merits of our proposed model.},
journal = {Wirel. Pers. Commun.},
month = jul,
pages = {1817–1844},
numpages = {28},
keywords = {Unsupervised learning, Component splitting, Inverted Dirichlet distribution, Variational inference, Mixture models}
}

@article{10.5555/3546258.3546313,
author = {Wang, Minjie and Allen, Genevera I.},
title = {Integrative generalized convex clustering optimization and feature selection for mixed multi-view data},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {In mixed multi-view data, multiple sets of diverse features are measured on the same set of samples. By integrating all available data sources, we seek to discover common group structure among the samples that may be hidden in individualistic cluster analyses of a single data view. While several techniques for such integrative clustering have been explored, we propose and develop a convex formalization that enjoys strong empirical performance and inherits the mathematical properties of increasingly popular convex clustering methods. Specifically, our Integrative Generalized Convex Clustering Optimization (iGecco) method employs different convex distances, losses, or divergences for each of the different data views with a joint convex fusion penalty that leads to common groups. Additionally, integrating mixed multi-view data is often challenging when each data source is high-dimensional. To perform feature selection in such scenarios, we develop an adaptive shifted group-lasso penalty that selects features by shrinking them towards their loss-specific centers. Our so-called iGecco+ approach selects features from each data view that are best for determining the groups, often leading to improved integrative clustering. To solve our problem, we develop a new type of generalized multi-block ADMM algorithm using subproblem approximations that more efficiently fits our model for big data sets. Through a series of numerical experiments and real data examples on text mining and genomics, we show that iGecco+ achieves superior empirical performance for high-dimensional mixed multi-view data.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {55},
numpages = {74},
keywords = {integrative clustering, convex clustering, feature selection, convex optimization, sparse clustering, GLM deviance, Bregman divergences}
}

@inproceedings{10.1145/3486001.3486227,
author = {Shikha and Agrawal, Manan and Anwar, Mohd Ayaan and Sethia, Divyashikha},
title = {Stacked Sparse Autoencoder and Machine Learning Based Anxiety Classification Using EEG Signals},
year = {2021},
isbn = {9781450385947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486001.3486227},
doi = {10.1145/3486001.3486227},
abstract = {Anxiety is an emotion characterized by trepidation, stress, or uneasiness that involves extreme worry or fear over future unwanted events or an actual situation. Careful analysis for anxiety is critical since approximately 2 to 4% of the general population have experienced adequate symptoms indicating an anxiety disorder. This paper aims to classify anxiety levels based on machine learning and deep learning algorithms with improved performance. This work uses the publically available DASPS Database (Database for Anxious States based on a Psychological Stimulation). The dataset consists of EEG recordings from 23 participants during anxiety elicitation through face-to-face psychological stimuli. This work uses RFECV with the classifiers to reduce redundancy between features and improve results. We achieve the highest classification accuracy of 83.93% and 70.25% using Stacked Sparse Autoencoder and Decision Tree for two-class anxiety classification.},
booktitle = {Proceedings of the First International Conference on AI-ML Systems},
articleno = {7},
numpages = {7},
keywords = {anxiety detection, feature selection, machine learning, signal processing, stacked sparse autoencoders},
location = {Bangalore, India},
series = {AIMLSystems '21}
}

@inproceedings{10.1145/3382026.3425767,
author = {Marchezan, Luciano and Carbonell, Jo\~{a}o and Rodrigues, Elder and Bernardino, Maicon and Basso, F\'{a}bio Paulo and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing the Feature Retrieval Process with Scoping and Tool Support: PAxSPL_v2},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3425767},
doi = {10.1145/3382026.3425767},
abstract = {Software Product Lines (SPLs) are commonly adopted with an extractive approach, by performing a reengineering process in legacy systems, when dealing with variability and reuse became challenging. As a starting activity of the process, the legacy systems are analyzed to retrieve, categorize, and group their features in terms of commonality and variability. Due to the importance of this feature retrieving, we proposed the Prepare, Assemble, and Execute framework for SPL reengineering (PAxSPL). PAxSPL aims at guiding users to customize the feature retrieval for their scenario. In an initial evaluation of the PAxSPL in a real-world scenario, we could observe the need for including scoping activities and implementing a tool to make the framework more adoptable in practice. In this paper, we describe how we performed these improvements. We performed the evolution of PAxSPL by including SPL scoping concepts and activities into our framework as well as developing a supporting tool. We also conducted a pilot study to evaluate how PAxSPL allows instantiating a scenario where the SPL reengineering were conducted. The results show that all artifacts, activities, and techniques from the scenario could be properly represented. However, we also identified a potential limitation during the assembly of techniques regarding parallel activities. The main contribution is PAxSPL_v2 that makes the framework more adherent to industries performing the reengineering of legacy systems into SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {29–36},
numpages = {8},
keywords = {automated support, software product lines, variability management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-030-63833-7_7,
author = {Sanodiya, Rakesh Kumar and Paul, Debdeep and Yao, Leehter and Mathew, Jimson and Juhi, Aparna},
title = {A Feature Selection Approach to Visual Domain Adaptation in Classification},
year = {2020},
isbn = {978-3-030-63832-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-63833-7_7},
doi = {10.1007/978-3-030-63833-7_7},
abstract = {In machine learning, we presume datasets to be labeled while performing any operation. But, is it true in real-life scenarios? To its contrary, we have an enormous amount of unlabeled datasets available in the form of images, videos, audios, articles, and many more. The major challenge we face is to train our classification model with primitive machine learning algorithms because these algorithms only expect labeled data. To overcome these limitations visual domain adaptation algorithms such as MEDA (Manifold Embedded Distribution Alignment) have been introduced. The main motto of MEDA is to minimize the distribution difference between the source domain (an application that contains enough labeled data) and the target domain (an application that contains only unlabeled data). In this way, the source domain labeled data can be utilized to enhance the performance of the target domain classifier. Though MEDA (Manifold Embedded Distribution Alignment) approach shows remarkable improvement in classification accuracy, but still there is considerable scope of improvement. There are plenty of irrelevant features in both domains. These irrelevant features create a hole for this algorithm and prevent the target domain classifier from becoming more robust. Therefore, for the purpose of filling this hole, we propose a new feature selection based visual domain adaptation (FSVDA) method which uses particle swarm optimization (PSO), where the MEDA method is considered as a fitness function that leads to automatically select a good subset of features over both the domains. Extensive experimental results on two real-world domain adaptation (DA) data sets such as object recognition and digit recognition demonstrate that our proposed method outperforms state-of-the-art primitive and domain DA algorithms. It is a big challenge to train the classifier for a new unlabeled image dataset in image classification and computer vision. The two magnificent solutions to this challenge are transfer learning and domain adaptation. By transfer learning, we can use our knowledge from previously trained models for training newer models.},
booktitle = {Neural Information Processing: 27th International Conference, ICONIP 2020, Bangkok, Thailand, November 23–27, 2020, Proceedings, Part II},
pages = {77–89},
numpages = {13},
keywords = {Transfer learning, Feature selection, Domain adaptation, Classification.},
location = {Bangkok, Thailand}
}

@article{10.1016/j.asoc.2019.105936,
author = {Arora, Nisha and Kaur, Pankaj Deep},
title = {A Bolasso based consistent feature selection enabled random forest classification algorithm: An application to credit risk assessment},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105936},
doi = {10.1016/j.asoc.2019.105936},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {15},
keywords = {Bootstrap-lasso, Stable feature selection, BS-RF, Credit risk assessment, Random forest in credit risk}
}

@article{10.1177/0165551518770967,
author = {Barraza, N\'{e}stor and Moro, S\'{e}rgio and Ferreyra, Marcelo and de la Pe\~{n}a, Adolfo},
title = {Mutual information and sensitivity analysis for feature selection in customer targeting: A comparative study},
year = {2019},
issue_date = {Feb 2019},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {45},
number = {1},
issn = {0165-5515},
url = {https://doi.org/10.1177/0165551518770967},
doi = {10.1177/0165551518770967},
abstract = {Feature selection is a highly relevant task in any data-driven knowledge discovery project. The present research focuses on analysing the advantages and disadvantages of using mutual information (MI) and data-based sensitivity analysis (DSA) for feature selection in classification problems, by applying both to a bank telemarketing case. A logistic regression model is built on the tuned set of features identified by each of the two techniques as the most influencing set of features on the success of a telemarketing contact, in a total of 13 features for MI and 9 for DSA. The latter performs better for lower values of false positives while the former is slightly better for a higher false-positive ratio. Thus, MI becomes a better choice if the intention is reducing slightly the cost of contacts without risking losing a high number of successes. However, DSA achieved good prediction results with less features.},
journal = {J. Inf. Sci.},
month = feb,
pages = {53–67},
numpages = {15},
keywords = {Customer targeting, direct marketing, feature selection, modelling, mutual information, sensitivity analysis}
}

@article{10.1016/j.cmpb.2019.06.001,
author = {Cueto-L\'{o}pez, Nah\'{u}m and Garc\'{\i}a-Ord\'{a}s, Maria Teresa and D\'{a}vila-Batista, Ver\'{o}nica and Moreno, V\'{\i}ctor and Aragon\'{e}s, Nuria and Alaiz-Rodr\'{\i}guez, Roc\'{\i}o},
title = {A comparative study on feature selection for a risk prediction model for colorectal cancer},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {177},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.06.001},
doi = {10.1016/j.cmpb.2019.06.001},
journal = {Comput. Methods Prog. Biomed.},
month = aug,
pages = {219–229},
numpages = {11},
keywords = {Colorectal cancer, Risk prediction model, Feature selection, Stability}
}

@inproceedings{10.1007/978-3-030-86514-6_19,
author = {Theeuwes, Nikki and van Houtum, Geert-Jan and Zhang, Yingqian},
title = {Improving Ambulance Dispatching with Machine Learning and Simulation},
year = {2021},
isbn = {978-3-030-86513-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86514-6_19},
doi = {10.1007/978-3-030-86514-6_19},
abstract = {As an industry where performance improvements can save lives, but resources are often scarce, emergency medical services (EMS) providers continuously look for ways to deploy available resources more efficiently. In this paper, we report a case study executed at a Dutch EMS region to improve ambulance dispatching. We first capture the way in which dispatch human agents currently make decisions on which ambulance to dispatch to a request. We build a decision tree based on historical data to learn human agents’ dispatch decisions. Then, insights from the fitted decision tree are used to enrich the commonly assumed closest-idle dispatch policy. Subsequently, we use the captured dispatch policy as input to a discrete event simulation to investigate two enhancements to current practices and evaluate their performance relative to the current policy. Our results show that complementing the current dispatch policy with redispatching and reevaluation policies yields an improvement of the on-time performance of highly urgent ambulance requests of 0.77% points. The performance gain is significant, which is equivalent to adding additional seven weekly ambulance shifts.},
booktitle = {Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part IV},
pages = {302–318},
numpages = {17},
keywords = {Ambulance dispatching, Machine learning, Decision trees, Discrete event simulation, Logistics},
location = {Bilbao, Spain}
}

@article{10.1007/s10639-020-10260-x,
author = {Iatrellis, Omiros and Savvas, Ilias Κ. and Fitsilis, Panos and Gerogiannis, Vassilis C.},
title = {A two-phase machine learning approach for predicting student outcomes},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1360-2357},
url = {https://doi.org/10.1007/s10639-020-10260-x},
doi = {10.1007/s10639-020-10260-x},
abstract = {Learning analytics have proved promising capabilities and opportunities to many aspects of academic research and higher education studies. Data-driven insights can significantly contribute to provide solutions for curbing costs and improving education quality. This paper adopts a two-phase machine learning approach, which utilizes both unsupervised and supervised learning techniques for predicting outcomes of students following Higher Education programs of studies. The approach has been applied in a case-study which has been performed in the context of an undergraduate Computer Science curriculum offered by the University of Thessaly in Greece. Students involved in the case study were initially grouped based on the similarity of specific education-related factors and metrics. Using the K-Means algorithm, our clustering experiments revealed the presence of three coherent clusters of students. Subsequently, the discovered clusters were utilized to train prediction models for addressing each particular cluster of students individually. In this regard, two machine learning models were trained for every cluster of students in order to predict the time to degree completion and student enrollment in the offered educational programs. The developed models are claimed to produce predictions with relatively high accuracy. Finally, the paper discusses the potential usefulness of the clustering-aided approach for learning analytics in Higher Education.},
journal = {Education and Information Technologies},
month = jan,
pages = {69–88},
numpages = {20},
keywords = {Learning analytics, Unsupervised learning, Supervised learning, Higher education}
}

@article{10.1007/s00500-019-03757-2,
author = {Tsai, Cheng-Jung},
title = {New feature selection and voting scheme to improve classification accuracy},
year = {2019},
issue_date = {Nov 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {22},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-03757-2},
doi = {10.1007/s00500-019-03757-2},
abstract = {Classification is a classic technique employed in data mining. Many ensemble learning methods have been introduced to improve the predictive accuracy of classification. A typical ensemble learning method consists of three steps: selection, building, and integration. Of the three steps, the first and third significantly affect the predictive accuracy of the classification. In this paper, we propose a new selection and integration scheme. Our method can improve the accuracy of subtrees and maintain their diversity. Through a new voting scheme, the predictive accuracy of ensemble learning is improved. We also theoretically analyzed the selection and integration steps of our method. The results of experimental analyses show that our method can achieve better accuracy than two state-of-the-art tree-based ensemble learning approaches.},
journal = {Soft Comput.},
month = nov,
pages = {12017–12030},
numpages = {14},
keywords = {Data mining, Classification, Decision tree, Ensemble learning, Feature selection, Voting}
}

@article{10.1007/s00521-020-05309-4,
author = {Mahindru, Arvind and Sangal, A. L.},
title = {MLDroid—framework for Android malware detection using machine learning techniques},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {10},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05309-4},
doi = {10.1007/s00521-020-05309-4},
abstract = {This research paper presents MLDroid—a web-based framework—which helps to detect malware from Android devices. Due to increase in the popularity of Android devices, malware developers develop malware on daily basis to threaten the system integrity and user’s privacy. The proposed framework detects malware from Android apps by performing its dynamic analysis. To detect malware from real-world apps, we trained our proposed framework by selecting features which are gained by implementing feature selection approaches. Further, these selected features help to build a model by considering different machine learning algorithms. Experiment was performed on 5,00,000 plus Android apps. Empirical result reveals that model developed by considering all the four distinct machine learning algorithms parallelly (i.e., deep learning algorithm, farthest first clustering, Y-MLP and nonlinear ensemble decision tree forest approach) and rough set analysis as a feature subset selection algorithm achieved the highest detection rate of 98.8% to detect malware from real-world apps.},
journal = {Neural Comput. Appl.},
month = may,
pages = {5183–5240},
numpages = {58},
keywords = {Permissions, API calls, Feature selection methods, Android apps, Machine learning}
}

@article{10.1007/s00521-020-05609-9,
author = {Zheng, Xiaohan and Zhang, Li and Xu, Zhiqiang},
title = {L1-norm Laplacian support vector machine for data reduction in semi-supervised learning},
year = {2021},
issue_date = {Jun 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05609-9},
doi = {10.1007/s00521-020-05609-9},
abstract = {As a semi-supervised learning method, Laplacian support vector machine (LapSVM) is popular. Unfortunately, the model generated by LapSVM has a poor sparsity. A sparse decision model has always been fascinating because it could implement data reduction and improve performance. To generate a sparse model of LapSVM, we propose an ℓ1-norm Laplacian support vector machine (ℓ1-norm LapSVM), which replaces the ℓ2-norm with the ℓ1-norm in LapSVM. The ℓ1-norm LapSVM has two techniques that can induce sparsity: the ℓ1-norm regularization and the hinge loss function. We discuss two situations for the ℓ1-norm LapSVM, linear and nonlinear ones. In the linear ℓ1-norm LapSVM, the sparse decision model implies that features with nonzero coefficients are contributive. In other words, the linear ℓ1-norm LapSVM can perform feature selection to achieve the goal of data reduction. Moreover, the nonlinear (kernel) ℓ1-norm LapSVM can also implement data reduction in terms of sample selection. In addition, the optimization problem of the ℓ1-norm LapSVM is a convex quadratic programming one. That is, the ℓ1-norm LapSVM has a unique and global solution. Experimental results on semi-supervised classification tasks have shown a comparable performance of our ℓ1-norm LapSVM.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {12343–12360},
numpages = {18},
keywords = {Semi-supervised learning, Support vector machine, ℓ1-norm regularization, Laplacian regularization}
}

@article{10.1007/s11265-021-01656-0,
author = {Zhou, Feng and Qu, Hua and Liu, Hailong and Liu, Hong and Li, Bo},
title = {Fingerprinting IIoT Devices Through Machine Learning Techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {7},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01656-0},
doi = {10.1007/s11265-021-01656-0},
abstract = {From a security perspective, identifying Industrial Internet of Things (IIoT) devices connected to a network has multiple applications such as penetration testing, vulnerability assessment, etc. In this work, we propose a feature-based methodology to perform device-type fingerprinting. A device fingerprint consists of the TCP/IP header features and port-based features extracted from the network traffic of the device. These features are collected by a hybrid mechanism which has a negligible impact on device functionality and can avoid the problem of the long TCP connection. Once the fingerprint of a device is generated, it will be fed to the classifiers based on Gradient Boosting to predict its type details. Based on our proposed method, we implement a prototype application called IIoT Device Type Fingerprinting (IDTF) which capable of automatically identifying the types of devices being connected to an IIoT network. We collect a dataset consisting of 19,174 fingerprints from real-world Internet-facing IIoT devices indexed by Shodan to train and evaluate the classifiers using ten-fold cross-validation. And we conduct comparative experiments in an IIoT testbed to compare the effectiveness of IDTF with two famous fingerprinting tools. The experimental result shows that the ability of our approach is confirmed by a high mean F-Measure of 95.76%. It also demonstrates that IDTF achieves the highest identification rate in the testbed and is non-intrusive for IIoT devices. Compared with existing works, our approach is more generic as it does not rely on a specific protocol or deep packet inspection and can distinguish almost all IIoT device-types.},
journal = {J. Signal Process. Syst.},
month = jul,
pages = {779–794},
numpages = {16},
keywords = {Industrial Internet of Things (IIoT), Device-type fingerprinting, Machine learning}
}

@article{10.1007/s00521-020-05409-1,
author = {Ahmed, Shameem and Ghosh, Kushal Kanti and Garcia-Hernandez, Laura and Abraham, Ajith and Sarkar, Ram},
title = {Improved coral reefs optimization with adaptive β-hill climbing for feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05409-1},
doi = {10.1007/s00521-020-05409-1},
abstract = {For any classification problem, the dimension of the feature vector used for classification has great importance. This is because, in a high-dimensional feature vector, it is found that some are non-informative or even redundant as they do not contribute to the learning process of the classifier. Rather, they may be the reason for low classification accuracy and high training time of the learning model. To address this issue, researchers apply various feature selection (FS) methods as found in the literature. In recent years, meta-heuristic algorithms have been proven to be effective in solving FS problems. The Coral Reefs Optimizer (CRO) which is a cellular type evolutionary algorithms has good tuning between its exploration and exploitation ability. This has motivated us to present an improved version of CRO with the inclusion of adaptive β-hill climbing to increase the exploitation ability of CRO. The proposed method is assessed on 18 standard UCI-datasets by means of three distinct classifiers, KNN, Random Forest and Naive Bayes classifiers. It is also analyzed with 10 state-of-the-art meta-heuristics FS procedure, and the outputs show an excellent performance of the proposed FS method reaching better results than the previous methods considered here for comparison. The source code of this work is publicly available at .},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6467–6486},
numpages = {20},
keywords = {Meta-heuristic, Feature selection, UCI, Coral reefs optimization, Adaptive β-hill climbing, Hybrid optimization}
}

@article{10.5555/3288251.3288287,
author = {Lu, Guangquan and Li, Bo and Yang, Weiwei and Yin, Jian},
title = {Unsupervised feature selection with graph learning via low-rank constraint},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Feature selection is one of the most important machine learning procedure, and it has been successfully applied to make a preprocessing before using classification and clustering methods. High-dimensional features often appear in big data, and it's characters block data processing. So spectral feature selection algorithms have been increasing attention by researchers. However, most feature selection methods, they consider these tasks as two steps, learn similarity matrix from original feature space (may be include redundancy for all features), and then conduct data clustering. Due to these limitations, they do not get good performance on classification and clustering tasks in big data processing applications. To address this problem, we propose an Unsupervised Feature Selection method with graph learning framework, which can reduce the redundancy features influence and utilize a low-rank constraint on the weight matrix simultaneously. More importantly, we design a new objective function to handle this problem. We evaluate our approach by six benchmark datasets. And all empirical classification results show that our new approach outperforms state-of-the-art feature selection approaches.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29531–29549},
numpages = {19},
keywords = {Feature selection, Graph learning, Spectral clustering}
}

@article{10.1016/j.knosys.2021.107577,
author = {Li, Wangwang and Chai, Zhengyi and Tang, Zengjie},
title = {A decomposition-based multi-objective immune algorithm for feature selection in learning to rank},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {234},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107577},
doi = {10.1016/j.knosys.2021.107577},
journal = {Know.-Based Syst.},
month = dec,
numpages = {11},
keywords = {Learning-to-rank, Multi-objective optimization, Immune algorithm, Decomposition, Large-scale data}
}

@article{10.1016/j.eswa.2019.113128,
author = {Matos, Tales and Macedo, Jose Antonio and Lettich, Francesco and Monteiro, Jose Maria and Renso, Chiara and Perego, Raffaele and Nardini, Franco Maria},
title = {Leveraging feature selection to detect potential tax fraudsters},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113128},
doi = {10.1016/j.eswa.2019.113128},
journal = {Expert Syst. Appl.},
month = may,
numpages = {13},
keywords = {Feature selection, Tax fraud detection, Association rules, Graph centrality measure, 00-01, 99-00}
}

@article{10.1145/3430376,
author = {K., Mythili and Narwaria, Manish},
title = {Assessment of Machine Learning-Based Audiovisual Quality Predictors: Why Uncertainty Matters},
year = {2021},
issue_date = {May 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3430376},
doi = {10.1145/3430376},
abstract = {Quality assessment of audiovisual (AV) signals is important from the perspective of system design, optimization, and management of a modern multimedia communication system. However, automatic prediction of AV quality via the use of computational models remains challenging. In this context, machine learning (ML) appears to be an attractive alternative to the traditional approaches. This is especially when such assessment needs to be made in no-reference (i.e., the original signal is unavailable) fashion. While development of ML-based quality predictors is desirable, we argue that proper assessment and validation of such predictors is also crucial before they can be deployed in practice. To this end, we raise some fundamental questions about the current approach of ML-based model development for AV quality assessment and signal processing for multimedia communication in general. We also identify specific limitations associated with the current validation strategy which have implications on analysis and comparison of ML-based quality predictors. These include a lack of consideration of: (a) data uncertainty, (b) domain knowledge, (c) explicit learning ability of the trained model, and (d) interpretability of the resultant model. Therefore, the primary goal of this article is to shed some light into mentioned factors. Our analysis and proposed recommendations are of particular importance in the light of significant interests in ML methods for multimedia signal processing (specifically in cases where human-labeled data is used), and a lack of discussion of mentioned issues in existing literature.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
articleno = {45},
numpages = {22},
keywords = {Audiovisual quality, machine learning, uncertainty, validation}
}

@inproceedings{10.1145/3132847.3132858,
author = {Braytee, Ali and Liu, Wei and Catchpoole, Daniel R. and Kennedy, Paul J.},
title = {Multi-Label Feature Selection using Correlation Information},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3132858},
doi = {10.1145/3132847.3132858},
abstract = {High-dimensional multi-labeled data contain instances, where each instance is associated with a set of class labels and has a large number of noisy and irrelevant features. Feature selection has been shown to have great benefits in improving the classification performance in machine learning. In multi-label learning, to select the discriminative features among multiple labels, several challenges should be considered: interdependent labels, different instances may share different label correlations, correlated features, and missing and flawed labels. This work is part of a project at The Children's Hospital at Westmead (TB-CHW), Australia to explore the genomics of childhood leukaemia. In this paper, we propose a CMFS (Correlated- and Multi-label Feature Selection method), based on non-negative matrix factorization (NMF) for simultaneously performing feature selection and addressing the aforementioned challenges. Significantly, a major advantage of our research is to exploit the correlation information contained in features, labels and instances to select the relevant features among multiple labels. Furthermore, l2,1 -norm regularization is incorporated in the objective function to undertake feature selection by imposing sparsity on the feature matrix rows. We employ CMFS to decompose the data and multi-label matrices into a low-dimensional space. To solve the objective function, an efficient iterative optimization algorithm is proposed with guaranteed convergence. Finally, extensive experiments are conducted on high-dimensional multi-labeled datasets. The experimental results demonstrate that our method significantly outperforms state-of-the-art multi-label feature selection methods.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {1649–1656},
numpages = {8},
keywords = {high dimensional data, multi-label classification, multi-label feature selection, new application},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@article{10.1016/j.neucom.2020.01.044,
author = {Bai, Xiangpin and Zhu, Lei and Liang, Cheng and Li, Jingjing and Nie, Xiushan and Chang, Xiaojun},
title = {Multi-view feature selection via Nonnegative Structured Graph Learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {387},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2020.01.044},
doi = {10.1016/j.neucom.2020.01.044},
journal = {Neurocomput.},
month = apr,
pages = {110–122},
numpages = {13},
keywords = {Unsupervised multi-view feature selection, Structured graph, Pseudo label learning, Alternate optimization}
}

@article{10.1007/s10766-018-0607-5,
author = {Chidambaram, S. and Sumathi, A.},
title = {Optimal Feature Selection for the Classification of Hyperspectral Imagery Using Adaptive Spectral–Spatial Clustering},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {5},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-018-0607-5},
doi = {10.1007/s10766-018-0607-5},
abstract = {Hyperspectral images captured through the hyperspectral sensors play an imperative part in remote sensing applications in the present context. Unlike traditional images sensed with few bands in the visible spectrum, the hyperspectral (HS) images are obtained with hundreds of spectral band ranges from infrared to ultraviolet regions. Because of its vast spatial and spectral data, it requires an extensive computational system for processing and its hidden features are needed to be unveiled in an effective manner specifically for the classification of HS imagery. This approach exploits the high spectral band correlation and rich spatial information of the HS images for the generation of feature vectors. To attain optimal feature space for the best probable classification, an adaptive approach is incorporated to adaptively choose spectral–spatial features for feature selection to classify the pixels effectively. Furthermore, the HS image encompasses several bands including noisy bands. To categorize the images with great accuracy, it is suggested to eradicate the noisy bands whilst retaining the informative bands. In this research, an adaptive spectral–spatial feature selection scheme is proposed for HS images where the extremely correlated representative bands are considered for analysis with uncorrelated and noisy spectral bands are judiciously discarded during its classification process. This hybrid approach not merely diminishes the computational time and also improves the general classification accuracy significantly. The empirical result displays that the proposed work surpasses the conventional approach of HS image classification systems.},
journal = {Int. J. Parallel Program.},
month = oct,
pages = {813–832},
numpages = {20},
keywords = {Spectral–spatial features, Hyperspectral image, Clustering, Parallel classifiers, Classification}
}

@article{10.1145/3314202,
author = {Wang, Zheng and Ye, Xiaojun and Wang, Chaokun and Yu, Philip S.},
title = {Feature Selection via Transferring Knowledge Across Different Classes},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1556-4681},
url = {https://doi.org/10.1145/3314202},
doi = {10.1145/3314202},
abstract = {The problem of feature selection has attracted considerable research interest in recent years. Supervised information is capable of significantly improving the quality of selected features. However, existing supervised feature selection methods all require that classes in the labeled data (source domain) and unlabeled data (target domain) to be identical, which may be too restrictive in many cases. In this article, we consider a more challenging cross-class setting where the classes in these two domains are related but different, which has rarely been studied before. We propose a cross-class knowledge transfer feature selection framework which transfers the cross-class knowledge from the source domain to guide target domain feature selection. Specifically, high-level descriptions, i.e., attributes, are used as the bridge for knowledge transfer. To further improve the quality of the selected features, our framework jointly considers the tasks of cross-class knowledge transfer and feature selection. Experimental results on four benchmark datasets demonstrate the superiority of the proposed method.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {22},
numpages = {29},
keywords = {Feature selection, dimension reduction, supervision transfer}
}

@article{10.1007/s11634-017-0285-y,
author = {Ben Brahim, Afef and Limam, Mohamed},
title = {Ensemble feature selection for high dimensional data: a new method and a comparative study},
year = {2018},
issue_date = {December  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {4},
issn = {1862-5347},
url = {https://doi.org/10.1007/s11634-017-0285-y},
doi = {10.1007/s11634-017-0285-y},
abstract = {The curse of dimensionality is based on the fact that high dimensional data is often difficult to work with. A large number of features can increase the noise of the data and thus the error of a learning algorithm. Feature selection is a solution for such problems where there is a need to reduce the data dimensionality. Different feature selection algorithms may yield feature subsets that can be considered local optima in the space of feature subsets. Ensemble feature selection combines independent feature subsets and might give a better approximation to the optimal subset of features. We propose an ensemble feature selection approach based on feature selectors' reliability assessment. It aims at providing a unique and stable feature selection without ignoring the predictive accuracy aspect. A classification algorithm is used as an evaluator to assign a confidence to features selected by ensemble members based on their associated classification performance. We compare our proposed approach to several existing techniques and to individual feature selection algorithms. Results show that our approach often improves classification performance and feature selection stability for high dimensional data sets.},
journal = {Adv. Data Anal. Classif.},
month = dec,
pages = {937–952},
numpages = {16},
keywords = {41A05, 41A10, 65D05, 65D17, Classification, Ensemble methods, Feature selection, High dimensionality, Stability}
}

@inproceedings{10.1007/978-3-030-89691-1_23,
author = {Horio, Tomoya and Kudo, Mineichi},
title = {Feature Selection with Class Hierarchy for Imbalance Problems},
year = {2021},
isbn = {978-3-030-89690-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89691-1_23},
doi = {10.1007/978-3-030-89691-1_23},
abstract = {In this paper, we aim to improve the classification performance in imbalance data by mitigating the impact of the curse of dimensionality especially in minority classes of a few samples. We exploit a class hierarchy realized as a binary tree whose node has a subset of classes. We construct such a binary tree in a top-down way by taking into consideration the separability of classes and the size of the feature subset. It is expected that the generalization performance is improved, especially in minority classes having a small number of samples, and that the interpretability of the decision rule is enhanced by the smallness of the number of features. Experimental results showed a remarkable improvement is by the proposed method in large-scale problems with many classes, e.g. from 48% to 62% in the balanced accuracy. In addition, only one feature was chosen in every node of the class hierarchy in all the four datasets, bringing a high interpretability of the classification rules.},
booktitle = {Progress in Artificial Intelligence and Pattern Recognition: 7th International Workshop on Artificial Intelligence and Pattern Recognition, IWAIPR 2021, Havana, Cuba, October 5–7, 2021, Proceedings},
pages = {229–238},
numpages = {10},
keywords = {Imbalanced problems, Class-dependent feature selection, Class hierarchy},
location = {Havana, Cuba}
}

@article{10.1016/j.patcog.2018.11.006,
author = {Li, Yanchao and Wang, Yongli and Liu, Qi and Bi, Cheng and Jiang, Xiaohui and Sun, Shurong},
title = {Incremental semi-supervised learning on streaming data},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.11.006},
doi = {10.1016/j.patcog.2018.11.006},
journal = {Pattern Recogn.},
month = apr,
pages = {383–396},
numpages = {14},
keywords = {Semi-supervised learning, Dynamic feature learning, Streaming data, Classification}
}

@article{10.1007/s11042-020-09079-y,
author = {Umer, Saiyed and Mohanta, Partha Pratim and Rout, Ranjeet Kumar and Pandey, Hari Mohan},
title = {Machine learning method for cosmetic product recognition: a visual searching approach},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {28–29},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09079-y},
doi = {10.1007/s11042-020-09079-y},
abstract = {A cosmetic product recognition system is proposed in this paper. For this recognition system, we have proposed a cosmetic product database that contains image samples of forty different cosmetic items. The purpose of this recognition system is to recognize Cosmetic products with there types, brands and retailers such that to analyze a customer experience what kind of products and brands they need. This system has various applications in such as brand recognition, product recognition and also the availability of the products to the vendors. The implementation of the proposed system is divided into three components: preprocessing, feature extraction and classification. During preprocessing we have scaled and transformed the color images into gray-scaled images to speed up the process. During feature extraction, several different feature representation schemes: transformed, structural and statistical texture analysis approaches have been employed and investigated by employing the global and local feature representation schemes. Various machine learning supervised classification methods such as Logistic Regression, Linear Support Vector Machine, Adaptive k-Nearest Neighbor, Artificial Neural Network and Decision Tree classifiers have been employed to perform the classification tasks. Apart from this, we have also performed some data analytic tasks for Brand Recognition as well as Retailer Recognition and for these experimentation, we have employed some datasets from the ‘Kaggle’ website and have obtained the performance due to the above-mentioned classifiers. Finally, the performance of the cosmetic product recognition system, Brand Recognition and Retailer Recognition have been aggregated for the customer decision process in the form of the state-of-the-art for the proposed system.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {34997–35023},
numpages = {27},
keywords = {Cosmetic products, E-commerce application, Feature extraction, Machine learning, Visual search}
}

@inproceedings{10.1007/978-3-030-72699-7_2,
author = {Renau, Quentin and Dreo, Johann and Doerr, Carola and Doerr, Benjamin},
title = {Towards Explainable Exploratory Landscape Analysis: Extreme Feature Selection for Classifying BBOB Functions},
year = {2021},
isbn = {978-3-030-72698-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72699-7_2},
doi = {10.1007/978-3-030-72699-7_2},
abstract = {Facilitated by the recent advances of Machine Learning&nbsp;(ML), the automated design of optimization heuristics is currently shaking up evolutionary computation&nbsp;(EC). Where the design of hand-picked guidelines for choosing a most suitable heuristic has long dominated research activities in the field, automatically trained heuristics are now seen to outperform human-derived choices even for well-researched optimization tasks. ML-based EC is therefore not any more a futuristic vision, but has become an integral part of our community.A key criticism that ML-based heuristics are often faced with is their potential lack of explainability, which may hinder future developments. This applies in particular to supervised learning techniques which extrapolate algorithms’ performance based on exploratory landscape analysis&nbsp;(ELA). In such applications, it is not uncommon to use dozens of problem features to build the models underlying the specific algorithm selection or configuration task. Our goal in this work is to analyze whether this many features are indeed needed. Using the classification of the BBOB test functions as testbed, we show that a surprisingly small number of features – often less than four – can suffice to achieve a 98% accuracy. Interestingly, the number of features required to meet this threshold is found to decrease with the problem dimension. We show that the classification accuracy transfers to settings in which several instances are involved in training and testing. In the leave-one-instance-out setting, however, classification accuracy drops significantly, and the transformation-invariance of the features becomes a decisive success factor.},
booktitle = {Applications of Evolutionary Computation: 24th International Conference, EvoApplications 2021, Held as Part of EvoStar 2021, Virtual Event, April 7–9, 2021, Proceedings},
pages = {17–33},
numpages = {17},
keywords = {Exploratory landscape analysis, Feature selection, Black-box optimization}
}

@inproceedings{10.1609/aaai.v33i01.33013983,
author = {Jiang, Bingbing and Wu, Xingyu and Yu, Kui and Chen, Huanhuan},
title = {Joint semi-supervised feature selection and classification through Bayesian approach},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33013983},
doi = {10.1609/aaai.v33i01.33013983},
abstract = {With the increasing data dimensionality, feature selection has become a fundamental task to deal with high-dimensional data. Semi-supervised feature selection focuses on the problem of how to learn a relevant feature subset in the case of abundant unlabeled data with few labeled data. In recent years, many semi-supervised feature selection algorithms have been proposed. However, these algorithms are implemented by separating the processes of feature selection and classifier training, such that they cannot simultaneously select features and learn a classifier with the selected features. Moreover, they ignore the difference of reliability inside unlabeled samples and directly use them in the training stage, which might cause performance degradation. In this paper, we propose a joint semi-supervised feature selection and classification algorithm (ISFS) which adopts a Bayesian approach to automatically select the relevant features and simultaneously learn a classifier. Instead of using all unlabeled samples indiscriminately, ISFS associates each unlabeled sample with a self-adjusting weight to distinguish the difference between them, which can effectively eliminate the irrelevant unlabeled samples via introducing a left-truncated Gaussian prior. Experiments on various datasets demonstrate the effectiveness and superiority of ISFS.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {489},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1007/s10586-021-03308-1,
author = {Alamuru, Susmitha and Jain, Sanjay},
title = {Video event detection, classification and retrieval using ensemble feature selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03308-1},
doi = {10.1007/s10586-021-03308-1},
abstract = {In recent times, video analysis gains more popularity among the researchers, due to its social impact and widespread applications. In this research article, a novel algorithm is proposed to detect the multiple events in the videos. At first, histogram equalization technique is used for contrast enhancement and smoothing of the videos, which are collected from Columbia consumer video, and UCF101 databases. The histogram equalization delivers a better quality of frames without loss of any local information like edges, patches and points. Further, feature extraction is accomplished using gradient local ternary pattern, histogram of oriented gradients, and Tamura features to extract the feature vectors from the enhanced frames that speed up the training process. The extracted feature vectors are high dimension in nature, so an ensemble feature selection algorithm is proposed to select optimal features from the extracted features. The selected optimal feature sub-sets are fed to multi support vector machine classifier to classify the multiple events. Finally, the relevant events are retrieved utilizing Euclidean distance measure and the simulation result showed that the proposed algorithm achieved better performance in multi-event recognition. The proposed algorithm almost showed a maximum of 14.21 % improvement in video event classification by means of accuracy compared to the existing algorithms like two-stage neural network, multi-stream deep learning system, improved two-stream model, Concept wise Power Law Normalization (CPN) with Convolutional Neural Network (CNN), Motion based Shot Boundary Detection (MSBD) algorithm, and motion features with Recurrent Neural Network (RNN).},
journal = {Cluster Computing},
month = dec,
pages = {2995–3010},
numpages = {16},
keywords = {Ensemble Feature Selection, Gradient Local Directional Ternary Pattern, Histogram of Oriented Gradients, Multi Support Vector Machine, Video Event Detection}
}

@article{10.1007/s00521-021-06047-x,
author = {Batur \c{S}ahin, Canan and Abualigah, Laith},
title = {A novel deep learning-based feature selection model for improving the static analysis of vulnerability detection},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06047-x},
doi = {10.1007/s00521-021-06047-x},
abstract = {The automatic detection of software vulnerabilities is considered a complex and common research problem. It is possible to detect several security vulnerabilities using static analysis (SA) tools, but comparatively high false-positive rates are observed in this case. Existing solutions to this problem depend on human experts to identify functionality, and as a result, several vulnerabilities are often overlooked. This paper introduces a novel approach for effectively and reliably finding vulnerabilities in open-source software programs. In this paper, we are motivated to examine the potential of the clonal selection theory. A novel deep learning-based vulnerability detection model is proposed to define features using the clustering theory of the clonal selection algorithm. To our knowledge, this is the first time we have used deep-learned long-lived team-hacker features to process memories of sequential features and mapping from the entire history of previous inputs to target vectors in theory. With an immune-based feature selection model, the proposed approach aimed to improve static analyses' detection abilities. A real-world SA dataset is used based on three open-source PHP applications. Comparisons are conducted based on using a classification model for all features to measure the proposed feature selection methods' classification improvement. The results demonstrated that the proposed method got significant enhancements, which occurred in the classification accuracy also in the true positive rate.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {14049–14067},
numpages = {19},
keywords = {Software vulnerability prediction, Deep learning, Feature selection, Immune systems}
}

@article{10.3233/THC-202237,
author = {Choudhury, Avishek},
title = {Predicting cancer using supervised machine learning: Mesothelioma},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {29},
number = {1},
issn = {0928-7329},
url = {https://doi.org/10.3233/THC-202237},
doi = {10.3233/THC-202237},
journal = {Technol. Health Care},
month = jan,
pages = {45–58},
numpages = {14},
keywords = {Mesothelioma, predictive modeling, decision support system, machine learning, artificial intelligence, lung cancer}
}

@article{10.1007/s10994-019-05795-1,
author = {Sechidis, Konstantinos and Azzimonti, Laura and Pocock, Adam and Corani, Giorgio and Weatherall, James and Brown, Gavin},
title = {Efficient feature selection using shrinkage estimators},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {108},
number = {8–9},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-019-05795-1},
doi = {10.1007/s10994-019-05795-1},
abstract = {Information theoretic feature selection methods quantify the importance of each feature by estimating mutual information terms to capture: the relevancy, the redundancy and the complementarity. These terms are commonly estimated by maximum likelihood, while an under-explored area of research is how to use shrinkage methods instead. Our work suggests a novel shrinkage method for data-efficient estimation of information theoretic terms. The small sample behaviour makes it particularly suitable for estimation of discrete distributions with large number of categories (bins). Using our novel estimators we derive a framework for generating feature selection criteria that capture any high-order feature interaction for redundancy and complementarity. We perform a thorough empirical study across datasets from diverse sources and using various evaluation measures. Our first finding is that our shrinkage based methods achieve better results, while they keep the same computational cost as the simple maximum likelihood based methods. Furthermore, under our framework we derive efficient novel high-order criteria that outperform state-of-the-art methods in various tasks.},
journal = {Mach. Learn.},
month = sep,
pages = {1261–1286},
numpages = {26},
keywords = {Feature selection, High order feature selection, Mutual information, Shrinkage estimators}
}

@article{10.1007/s11042-020-09512-2,
author = {Khan, Ayaz H. and Zubair, Muhammad},
title = {Classification of multi-lingual tweets, into multi-class model using Na\"{\i}ve Bayes and semi-supervised learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {43–44},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09512-2},
doi = {10.1007/s11042-020-09512-2},
abstract = {Twitter is a social media platform which has been proven to be a great tool for insights of emotions about products, policies etc. through a 280-character message called tweet, containing direct and unfiltered emotions by a large amount of user population. Twitter has attracted the attention of many researchers owing to the fact that every tweet is by default, public in nature which is not the case with Facebook. This paper proposes a model for multi-lingual (English and Roman Urdu) classification of tweets over diversely ranged classes (non-hierarchical architecture). Previous work in tweet classification is narrowly focused either on single language or either on uniform set of classes at most (Positive, Extremely Positive, Negative and Extremely Negative). The proposed model is based on semi-supervised learning and proposed feature selection approach makes it less dependent and highly adaptive for grabbing trending terms. This makes it a strong contender of choice for streaming data. In the methodology, using Na\"{\i}ve Bayes learning algorithm for each phase, obtained remarkable accuracy of up to 87.16% leading from both KNN and SVM models which are popular for NLP and Text classification domains.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {32749–32767},
numpages = {19},
keywords = {Twitter, Sentiment analysis, Sentiment classification, Semi-supervised learning}
}

@article{10.1016/j.cor.2018.03.005,
author = {Ben\'{\i}tez-Pe\~{n}a, S. and Blanquero, R. and Carrizosa, E. and Ram\'{\i}rez-Cobo, P.},
title = {Cost-sensitive Feature Selection for Support Vector Machines},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {106},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2018.03.005},
doi = {10.1016/j.cor.2018.03.005},
journal = {Comput. Oper. Res.},
month = jun,
pages = {169–178},
numpages = {10},
keywords = {Classification, Data Science, Support Vector Machines, Feature Selection, Integer Programming, Sparsity}
}

@article{10.1016/j.ins.2021.04.006,
author = {Duarte, Jos\'{e} Marcio and Sousa, Samuel and Milios, Evangelos and Berton, Lilian},
title = {Deep analysis of word sense disambiguation via semi-supervised learning and neural word representations},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.04.006},
doi = {10.1016/j.ins.2021.04.006},
journal = {Inf. Sci.},
month = sep,
pages = {278–297},
numpages = {20},
keywords = {Natural language processing, Word sense disambiguation, Semi-supervised learning, Word embeddings, Graph-based methods}
}

@inproceedings{10.1609/aaai.v33i01.33012438,
author = {Correia, Alvaro H. C. and Lecue, Freddy},
title = {Human-in-the-loop feature selection},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33012438},
doi = {10.1609/aaai.v33i01.33012438},
abstract = {Feature selection is a crucial step in the conception of Machine Learning models, which is often performed via data-driven approaches that overlook the possibility of tapping into the human decision-making of the model's designers and users. We present a human-in-the-loop framework that interacts with domain experts by collecting their feedback regarding the variables (of few samples) they evaluate as the most relevant for the task at hand. Such information can be modeled via Reinforcement Learning to derive a per-example feature selection method that tries to minimize the model's loss function by focusing on the most pertinent variables from a human perspective. We report results on a proof-of-concept image classification dataset and on a real-world risk classification task in which the model successfully incorporated feedback from experts to improve its accuracy.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {301},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1016/j.compbiomed.2021.104650,
author = {Singh, Om Prakash and Vallejo, Marta and El-Badawy, Ismail M. and Aysha, Ali and Madhanagopal, Jagannathan and Mohd Faudzi, Ahmad Athif},
title = {Classification of SARS-CoV-2 and non-SARS-CoV-2 using machine learning algorithms},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104650},
doi = {10.1016/j.compbiomed.2021.104650},
journal = {Comput. Biol. Med.},
month = sep,
numpages = {9},
keywords = {COVID-19, Signal processing, Biomarker, Machine learning}
}

@article{10.1016/j.neucom.2018.11.060,
author = {Li, Shaoyong and Tang, Chang and Liu, Xinwang and Liu, Yaping and Chen, Jiajia},
title = {Dual graph regularized compact feature representation for unsupervised feature selection},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {331},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.11.060},
doi = {10.1016/j.neucom.2018.11.060},
journal = {Neurocomput.},
month = feb,
pages = {77–96},
numpages = {20},
keywords = {Unsupervised feature selection, Dictionary learning, Local geometrical structure preservation, Feature representation}
}

@article{10.1016/j.neucom.2018.04.001,
author = {Lu, Quanmao and Li, Xuelong and Dong, Yongsheng},
title = {Structure preserving unsupervised feature selection},
year = {2018},
issue_date = {August 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {301},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.001},
doi = {10.1016/j.neucom.2018.04.001},
abstract = {Spectral analysis was usually used to guide unsupervised feature selection. However, the performances of these methods are not always satisfactory due to that they may generate continuous pseudo labels to approximate the discrete real labels. In this paper, a novel unsupervised feature selection method is proposed based on self-expression model. Unlike existing spectral analysis based methods, we utilize self-expression model to capture the relationships between the features without learning the cluster labels. Specifically, each feature can be reconstructed by using a linear combination of all the features in the original feature space, and a representative feature should give a large weight to reconstruct other features. Besides, a structure preserved constraint is incorporated into our model for keeping the local manifold structure of the data. Then an efficient alternative iterative algorithm is utilized to solve our proposed model with the theoretical analysis on its convergence. The experimental results on different datasets show the effectiveness of our method.},
journal = {Neurocomput.},
month = aug,
pages = {36–45},
numpages = {10},
keywords = {Self-expression model, Structure preserving, Unsupervised feature selection}
}

@article{10.1007/s10115-020-01526-4,
author = {Morillo-Salas, Jos\'{e} Luis and Bol\'{o}n-Canedo, Ver\'{o}nica and Alonso-Betanzos, Amparo},
title = {Dealing with heterogeneity in the context of distributed feature selection for classification},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-020-01526-4},
doi = {10.1007/s10115-020-01526-4},
abstract = {Advances in the information technologies have greatly contributed to the advent of larger datasets. These datasets often come from distributed sites, but even so, their large size usually means they cannot be handled in a centralized manner. A possible solution to this problem is to distribute the data over several processors and combine the different results. We propose a methodology to distribute feature selection processes based on selecting relevant and discarding irrelevant features. This preprocessing step is essential for current high-dimensional sets, since it allows the input dimension to be reduced. We pay particular attention to the problem of data imbalance, which occurs because the original dataset is unbalanced or because the dataset becomes unbalanced after data partitioning. Most works approach unbalanced scenarios by oversampling, while our proposal tests both over- and undersampling strategies. Experimental results demonstrate that our distributed approach to classification obtains comparable accuracy results to a centralized approach, while reducing computational time and efficiently dealing with data imbalance.},
journal = {Knowl. Inf. Syst.},
month = jan,
pages = {233–276},
numpages = {44},
keywords = {Feature selection, Distributed learning, Unbalanced data, Oversampling}
}

@article{10.1016/j.jss.2007.10.031,
author = {Karam, Marcel and Dascalu, Sergiu and Safa, Haidar and Santina, Rami and Koteich, Zeina},
title = {A product-line architecture for web service-based visual composition of web applications},
year = {2008},
issue_date = {June, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.10.031},
doi = {10.1016/j.jss.2007.10.031},
abstract = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
journal = {J. Syst. Softw.},
month = jun,
pages = {855–867},
numpages = {13},
keywords = {Agile methods, Product line architecture, Product line engineering, Visual languages, Web services}
}

@inbook{10.1145/3447404.3447414,
author = {Chatzilygeroudis, Konstantinos and Hatzilygeroudis, Ioannis and Perikos, Isidoros},
title = {Machine Learning Basics},
year = {2021},
isbn = {9781450390293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3447404.3447414},
booktitle = {Intelligent Computing for Interactive System Design: Statistics, Digital Signal Processing, and Machine Learning in Practice},
pages = {143–193},
numpages = {51}
}

@article{10.1007/s00521-020-05375-8,
author = {Agrawal, Prachi and Ganesh, Talari and Mohamed, Ali Wagdy},
title = {A novel binary gaining–sharing knowledge-based optimization algorithm for feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05375-8},
doi = {10.1007/s00521-020-05375-8},
abstract = {To obtain the optimal set of features in feature selection problems is the most challenging and prominent problem in machine learning. Very few human-related metaheuristic algorithms were developed and solved this type of problem. It motivated us to check the performance of recently developed gaining–sharing knowledge-based optimization algorithm (GSK), which is based on the concept of gaining and sharing knowledge of humans throughout their lifespan. It depends on two stages: beginners–intermediate gaining and sharing stage and intermediate–experts gaining and sharing stage. In this study, two approaches are proposed to solve feature selection problems: FS-BGSK: a novel binary version of GSK algorithm that relies on these two stages with knowledge factor 1 and FS-pBGSK: a population reduction technique that is employed on BGSK algorithm to enhance the exploration and exploitation quality of FS-BGSK. The proposed approaches are checked on twenty two feature selection benchmark datasets from UCI repository that contains small, medium and large dimensions datasets. The obtained results are compared with seven state-of-the-art metaheuristic algorithms; binary differential evolution, binary particle swarm optimization algorithm, binary bat algorithm, binary grey wolf optimizer, binary ant lion optimizer, binary dragonfly algorithm and binary salp swarm algorithm. It concludes that FS-pBGSK and FS-BGSK outperform other algorithms in terms of accuracy, convergence and robustness in most of the datasets.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {5989–6008},
numpages = {20},
keywords = {Gaining–sharing knowledge-based optimization algorithm, Feature selection, Classification, K-NN classifier}
}

@article{10.1016/j.asoc.2017.04.055,
author = {Lee, Chia-Yen and Chen, Bo-Syun},
title = {Mutually-exclusive-and-collectively-exhaustive feature selection scheme},
year = {2018},
issue_date = {Jul 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.04.055},
doi = {10.1016/j.asoc.2017.04.055},
journal = {Appl. Soft Comput.},
month = jul,
pages = {961–971},
numpages = {11},
keywords = {Feature selection, Mutually-exclusive-and-collectively-exhaustive, Data mining, Semiconductor manufacturing, Bioinformatics}
}

@article{10.1016/j.knosys.2017.03.002,
author = {Wang, Shiping and Wang, Han},
title = {Unsupervised feature selection via low-rank approximation and structure learning},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {124},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.002},
doi = {10.1016/j.knosys.2017.03.002},
abstract = {Feature selection is an important research topic in machine learning and computer vision in that it can reduce the dimensionality of input data and improve the performance of learning algorithms. Low-rank approximation techniques can well exploit the low-rank property of input data, which coincides with the internal consistency of dimensionality reduction. In this paper, we propose an efficient unsupervised feature selection algorithm, which incorporates low-rank approximation as well as structure learning. First, using the self-representation of data matrix, we formalize the feature selection problem as a matrix factorization with low-rank constraints. This matrix factorization formulation also embeds structure learning regularization as well as a sparse regularized term. Second, we present an effective technique to approximate low-rank constraints and propose a convergent algorithm in a batch mode. This technique can serve as an algorithmic framework for general low-rank recovery problems as well. Finally, the proposed algorithm is validated in twelve publicly available datasets from machine learning repository. Extensive experimental results demonstrate that the proposed method is capable to achieve competitive performance compared to existing state-of-the-art feature selection methods in terms of clustering performance.},
journal = {Know.-Based Syst.},
month = may,
pages = {70–79},
numpages = {10},
keywords = {Feature selection, Low-rank approximation, Machine learning, Structure learning, Unsupervised learning}
}

@article{10.1007/s11063-020-10297-6,
author = {Luo, Qimin and Wen, Guoqiu and Zhang, Leyuan and Zhan, Mengmeng},
title = {An Efficient Algorithm Combining Spectral Clustering with Feature Selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10297-6},
doi = {10.1007/s11063-020-10297-6},
abstract = {Traditional clustering algorithms have some limitations, which are sensitive to noise and mostly applicable to convex data sets. To solve these problems, the paper proposes a novel algorithm combining spectral clustering with feature selection. Specifically, the loss item is marked with a root that can reduce the deviation value then improve the robustness of the model. And in the algorithm optimization, there is one parameter is represented by other known parameters, which can reduce the time of parameter adjustment. Then, the regular term ℓ2,p-norm is applied to reduce the influence of noise and redundant features and prevent the model from overfitting. Finally, Laplace matrix is constructed by kNN algorithm which is used to learn subspace and to preserve the local structure among samples, and the data after dimension reduction is used to spectral clustering. Experimental analysis on 10 benchmark datasets show that the proposed algorithm is more outperformed than the algorithms of the state-of-the-art.},
journal = {Neural Process. Lett.},
month = dec,
pages = {1913–1925},
numpages = {13},
keywords = {Spectral clustering, Sparse learning, Locally preserved projection, kNN}
}

@inproceedings{10.1145/1858996.1859021,
author = {Kim, Chang Hwan Peter and Batory, Don and Khurshid, Sarfraz},
title = {Eliminating products to test in a software product line},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859021},
doi = {10.1145/1858996.1859021},
abstract = {A Software Product Line (SPL) is a family of programs where each program is defined by a unique combination of features. Developing a set of programs with commonalities and variabilities in this way can significantly reduce both the time and cost of software development. However, as the number of programs may be exponential in the number of features, testing an SPL, the phase to which the majority of software development is dedicated, becomes especially challenging [12].Indeed, scale is the biggest challenge in testing or checking the properties of programs in a product line. Even a product line with just 10 optional features has over a thousand (210) distinct programs. As an example of a situation where every program must be considered, suppose that every program of an SPL outputs a String that each feature might modify.},
booktitle = {Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering},
pages = {139–142},
numpages = {4},
keywords = {feature oriented programming, software product lines, static analysis, testing},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2513228.2513313,
author = {Trivedi, Shrawan Kumar and Dey, Shubhamoy},
title = {Effect of feature selection methods on machine learning classifiers for detecting email spams},
year = {2013},
isbn = {9781450323482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513228.2513313},
doi = {10.1145/2513228.2513313},
abstract = {This research presents the effects of using features selected by two feature selection methods i.e. Genetic Search and Greedy Stepwise Search on popular Machine Learning Classifiers like Bayesian, Naive Bayes, Support Vector Machine and Genetic Algorithm. Tests were performed on two different publicly available spam email datasets: "Enron" and "SpamAssassin". Results show that, Greedy Stepwise Search is a good method for feature selection for spam email detection. Among the Machine Learning Classifiers, Support Vector Machine has been found to be the best both in terms of accuracy and False Positive rate},
booktitle = {Proceedings of the 2013 Research in Adaptive and Convergent Systems},
pages = {35–40},
numpages = {6},
keywords = {classification accuracy, email spam classification, evolutionary algorithms, false positive rate, feature selection},
location = {Montreal, Quebec, Canada},
series = {RACS '13}
}

@inproceedings{10.1007/978-3-030-53956-6_25,
author = {Wang, Hong and Ou, Yikun},
title = {An Adapting Chemotaxis Bacterial Foraging Optimization Algorithm for Feature Selection in Classification},
year = {2020},
isbn = {978-3-030-53955-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-53956-6_25},
doi = {10.1007/978-3-030-53956-6_25},
abstract = {Efficient classification methods can improve the data quality or relevance to better optimize some Internet applications such as fast searching engine and accurate identification. However, in the big data era, difficulties and volumes of data processing increase drastically. To decrease the huge computational cost, heuristic algorithms have been used. In this paper, an Adapting Chemotaxis Bacterial Foraging Optimization (ACBFO) algorithm is proposed based on basic Bacterial Foraging Optimization (BFO) algorithm. The aim of this work is to design a modified algorithm which is more suitable for data classification. The proposed algorithm has two updating strategies and one structural changing. First, the adapting chemotaxis step updating strategy is responsible to increase the flexibility of searching. Second, the feature subsets updating strategy better combines the proposed heuristic algorithm with the KNN classifier. Third, the nesting structure of BFO has been simplified to reduce the computation complexity. The ACBFO has been compared with BFO, BFOLIW and BPSO by testing on 12 widely used benchmark datasets. The result shows that ACBFO has a good ability of solving classification problems and gets higher accuracy than the other comparation algorithm.},
booktitle = {Advances in Swarm Intelligence: 11th International Conference, ICSI 2020, Belgrade, Serbia, July 14–20, 2020, Proceedings},
pages = {275–286},
numpages = {12},
keywords = {Bacterial foraging optimization, Feature selection, Classification},
location = {Belgrade, Serbia}
}

@article{10.1007/s10489-018-1348-2,
author = {Singh, Dalwinder and Singh, Birmohan},
title = {Hybridization of feature selection and feature weighting for high dimensional data},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1348-2},
doi = {10.1007/s10489-018-1348-2},
abstract = {The classification of high dimensional data is a challenging problem due to the presence of redundant and irrelevant features in a higher amount. These unwanted features degrade accuracy and increase the computational complexity of machine learning algorithms. In this paper, we propose a hybrid method that integrates the complementary strengths of feature selection and feature weighting approaches for improving the classification of high dimensional data on the Nearest Neighbor classifier. Specifically, we suggest four strategies that combine filter and wrapper methods of feature selection and feature weighting. Experiments are performed on 12 high dimensional datasets and outcomes are supported by Friedman as well as Holm statistical tests for validation. Extended Adjusted Ratio of Ratios is used to recognize the best method considering accuracy, feature selection, and runtime. The results show that two proposed strategies outperform other well-known methods in accuracy and features reduction. The hybrid feature selection-feature weighting wrapper method is the best among all in accuracy while the hybrid feature selection filter-feature weighting wrapper method is the most suitable for reducing features and runtime. Thus, the promising outcomes validate the importance of hybridizing feature selection and feature weighting while dealing with high dimensional data.},
journal = {Applied Intelligence},
month = apr,
pages = {1580–1596},
numpages = {17},
keywords = {Feature selection, Feature weighting, Hybrid method, Optimization algorithm}
}

@inproceedings{10.1145/3365109.3368792,
author = {Cherrington, Marianne and Airehrour, David and Lu, Joan and Xu, Qiang and Wade, Stephen and Madanian, Samaneh},
title = {Feature Selection Methods for Linked Data: Limitations, Capabilities and Potentials},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368792},
doi = {10.1145/3365109.3368792},
abstract = {Feature selection is an important pre-processing, data mining, and knowledge discovery tool for data analysis. By eliminating redundant and irrelevant features from high-dimensional data, feature selection diminishes the 'curse of dimensionality' to improve performance. Data are becoming increasingly complex; heterogeneous data may often be viewed as natural collections of linked objects. Linked data are structured data that are connected with other data sources through the use of semantic queries. It is increasingly prevalent in social media websites and biological networks. Many feature selection methods assume independent and identically distributed data (IID), a condition violated with linked data. In this paper, a review of current feature selection techniques for linked data is presented. Several approaches are examined in various contexts so that performance issues and ongoing challenges can be assessed. The major contribution of this paper is to underscore contemporary uses and limitations of linked data feature selection techniques with the purpose of informing existing capabilities and current potentials for key areas of future research and application.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {103–112},
numpages = {10},
keywords = {dimensionality reduction, feature selection (fs), heterogeneous data, high-dimensional data (hdd), linked data (ld)},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@inproceedings{10.1007/978-3-030-79463-7_48,
author = {Tran, Khanh Quoc and Duong, Binh Van and Tran, Linh Quang and Tran, An Le-Hoai and Nguyen, An Trong and Nguyen, Kiet Van},
title = {Machine Learning-Based Empirical Investigation for Credit Scoring in&nbsp;Vietnam’s Banking},
year = {2021},
isbn = {978-3-030-79462-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79463-7_48},
doi = {10.1007/978-3-030-79463-7_48},
abstract = {In thons for credit scoring in Vietnam with machine learning models based on our submissions for the Kalapa Credit Score Challenge. We conduct experiments with modern machine learning methods based on ensemble learning models: LightGBM, CatBoost, and Random Forest. Our experimental results are better than single-model algorithms such as Support Vector Machine (SVM) or Logistic Regression. As a result, we achieve the F1-Score of 0.83 (Random Forest) with the sixth place on the leaderboard. Subsequently, we analyze the advantages and disadvantages of the used models, propose suitable measures to use for similar problems in the future, and evaluate the results to select the best model. To the best of our knowledge, this is the first work of the field in Vietnamese banking.},
booktitle = {Advances and Trends in Artificial Intelligence. From Theory to Practice: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part II},
pages = {564–574},
numpages = {11},
keywords = {Credit scoring, Prediction, Machine learning, Ensemble models, Data mining},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1504/IJDATS.2013.055346,
author = {Praharsi, Yugowati and Miaou, Shaou-Gang and Wee, Hui-Ming},
title = {Supervised learning approaches and feature selection - a case study in diabetes},
year = {2013},
issue_date = {July 2013},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {3},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2013.055346},
doi = {10.1504/IJDATS.2013.055346},
abstract = {Data description and classification are important tasks in supervised learning. In this study, three supervised learning methods such as k-nearest neighbour k-NN, support vector data description SVDD and support vector machine SVM are considered because they do not suffer from the problem of introducing a new class. The data sample chosen is Pima Indians diabetes. The results show that feature selection based on mean information gain and a standard deviation threshold can be considered as a substitute for forward selection. This indicates that data variation using information gain is an important factor that must be considered in selecting feature subset. Finally, among eight candidate features, glucose level is the most prominent feature for diabetes detection in all classifiers and feature selection methods under consideration. Relevancy measurement in information gain can sort out the most important feature to the least significant one. It can be very useful in medical applications such as defining feature prioritisation for symptom recognition.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = jul,
pages = {323–337},
numpages = {15}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {feature models, software product lines, unified language},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1504/ijdmb.2020.107378,
author = {\"{O}zt\"{u}rk, Celal and Tar\i{}m, Mustafa and Arslan, Sibel},
title = {Feature selection and classification of metabolomics data using artificial bee colony programming (ABCP)},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {23},
number = {2},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2020.107378},
doi = {10.1504/ijdmb.2020.107378},
abstract = {One area of metabolic data analysis is processes that involve the detection and discovery of biomarkers used in the early diagnosis of diseases and development of alternative treatments. Classification and feature selection are frequently used in the statistical analysis of metabolomics data for the detection and discovery of biomarkers. Recently, automatic programming methods have begun to be used instead of conventional methods. In this paper, three conventional classification and feature selection methods (PLS-DA, RF, SVM) and two automatic programming methods (ABCP and GP) are applied to classification problems where they are evaluated on synthetic and real data sets. The selection performances on the biomarker discovery of the algorithms have been compared. It has been found that automatic programming methods are more successful in classifying metabolic data and ABCP is superior to GP in biomarker discovery.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {101–118},
numpages = {17},
keywords = {metabolomics data, biomarker discovery, feature selection, classification, artificial bee colony programming, genetic programming, bioinformatics}
}

@inproceedings{10.1145/1176617.1176733,
author = {Trask, Bruce and Paniscotti, Dominick and Roman, Angel and Bhanot, Vikram},
title = {Using model-driven engineering to complement software product line engineering in developing software defined radio components and applications},
year = {2006},
isbn = {159593491X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176617.1176733},
doi = {10.1145/1176617.1176733},
abstract = {This paper details the application of Software Product Lines (SPL)16 and Model-Driven Engineering (MDE)15 to the software defined radio domain. More specifically it is an experience report emphasizing the synergy 17 resulting from combining MDE and SPL technologies. The software defined radio domain has very unique characteristics as its systems typically are a confluence of a number of typically challenging aspects of software development. To name a few, these systems are usually described by modifiers such as, embedded, real-time, distributed, object-oriented, portable, heterogeneous, multithreaded, high performance, dynamic, resource-constrained, safety-critical, secure, networked, component based and fault-tolerant. Each one of these modifiers by themselves carries with it a set of unique challenges, but building systems characterized by all of these modifiers all at the same time makes for a daunting task in software development. In addition to all of these, it is quite common in these embedded systems for components to have multiple implementations that must run on disparate processing elements. With all of this taken into account, it stands to reason that these systems could and should benefit greatly from advances in software technology such as product line engineering, domain-specific modeling and model-driven engineering. It is our experience that one big benefit to the software development industry is the combination of the Software Product Lines and Model Driven Engineering technologies.},
booktitle = {Companion to the 21st ACM SIGPLAN Symposium on Object-Oriented Programming Systems, Languages, and Applications},
pages = {846–853},
numpages = {8},
keywords = {development, domain, generation, language, model},
location = {Portland, Oregon, USA},
series = {OOPSLA '06}
}

@inproceedings{10.5555/2022115.2022130,
author = {Wu, Yijian and Yang, Yiming and Peng, Xin and Qiu, Cheng and Zhao, Wenyun},
title = {Recovering object-oriented framework for software product line reengineering},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A large number of software product lines (SPL) in practice are not constructed from scratch, but reengineered from legacy variant products. In order to transfer legacy products to SPL core assets, reverse variability analysis should be involved to find commonality and differences among variant artifacts. In this paper we concentrate on the recovery of SPL framework which can be represented by an object-oriented design model with variation points. We propose a semi-automatic SPL framework recovery approach with the assumption that involved legacy products have similar designs and implementations. In this approach, we adopt a bottom-up process based on clone detection and context analysis to identify corresponding mappings among design elements in different products. Then we use a top-down process from class level to method level with some heuristic rules to determine the commonality/variability classification and the variability type for each design element. In order to evaluate the effectiveness of our approach, we conduct a case study on an industrial product line and present comprehensive analysis and discussions on the results.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {119–134},
numpages = {16},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@article{10.1007/s00500-020-05183-1,
author = {Guha, Ritam and Ghosh, Manosij and Mutsuddi, Shyok and Sarkar, Ram and Mirjalili, Seyedali},
title = {Embedded chaotic whale survival algorithm for filter–wrapper feature selection},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {17},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05183-1},
doi = {10.1007/s00500-020-05183-1},
abstract = {Classification accuracy provided by a machine learning model depends a lot on the feature set used in the learning process. Feature selection (FS) is an important and challenging preprocessing technique which helps to identify only the relevant features from a dataset, thereby reducing the feature dimension as well as improving the classification accuracy at the same time. The binary version of whale optimization algorithm (WOA) is a popular FS technique which is inspired from the foraging behavior of humpback whales. In this paper, an embedded version of WOA called embedded chaotic whale survival algorithm (ECWSA) has been proposed which uses its wrapper process to achieve high classification accuracy and a filter approach to further refine the selected subset with low computation cost. Chaos has been introduced in the ECWSA to guide selection of the type of movement followed by the whales while searching for prey. A fitness-dependent death mechanism has also been introduced in the system of whales which is inspired from the real-life scenario in which whales die if they are unable to catch their prey. The proposed method has been evaluated on 18 well-known UCI datasets and compared with its predecessors as well as some other popular FS methods. The source code of ECWSA can be found in .},
journal = {Soft Comput.},
month = sep,
pages = {12821–12843},
numpages = {23},
keywords = {Whale optimization algorithm, Feature selection, Embedded systems, Chaotic mapping, UCI dataset, Optimization, Heuristic, Algorithm, Benchmark}
}

@article{10.1007/s10922-020-09566-5,
author = {Safari Khatouni, Ali and Seddigh, Nabil and Nandy, Biswajit and Zincir-Heywood, Nur},
title = {Machine Learning Based Classification Accuracy of Encrypted Service Channels: Analysis of Various Factors},
year = {2021},
issue_date = {Jan 2021},
publisher = {Plenum Press},
address = {USA},
volume = {29},
number = {1},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-020-09566-5},
doi = {10.1007/s10922-020-09566-5},
abstract = {Visibility into network traffic is a key requirement for different security and network monitoring tools. Recent trends in the evolution of Internet traffic present a challenge for traditional traffic analysis methods to achieve accurate classification of Internet traffic including Voice over IP (VoIP), text messaging, video, and audio services among others. A key aspect of this trend is the rising levels of encrypted multiple service channels where the payload is opaque to middleboxes in the network. In such scenarios, traditional approaches such as Deep Packet Inspection (DPI) or examination of Port numbers are unable to achieve the classification accuracy required. This work investigates Machine Learning-based network traffic classifiers as a means of accurately classifying encrypted multiple service channels. The study carries out a thorough study which (i) proposes and evaluates two machine learning-based frameworks for multiple service channels analysis; (ii) undertakes feature engineering to identify the minimum number of features required to obtain high accuracy while reducing the effects of over-fitting; (iii) explores the portability and robustness of the frameworks trained models under different network conditions: location, time, and volume; and (iv) collects and analyzes a large-scale dataset including nine classes of services, for benchmarking purposes.},
journal = {J. Netw. Syst. Manage.},
month = jan,
numpages = {27},
keywords = {Multiple service channels, Encrypted traffic classification, Encrypted traffic analysis, Feature selection, Robust traffic classifier, Machine Learning based traffic analysis}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Metamodels, Product Line Architecture, Software Product Lines, Systematic Literature Review, Variability},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@article{10.1016/j.eswa.2021.114583,
author = {Bera, Asish and Bhattacharjee, Debotosh and Shum, Hubert P.H.},
title = {Two-stage human verification using HandCAPTCHA and anti-spoofed finger biometrics with feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {171},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114583},
doi = {10.1016/j.eswa.2021.114583},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {18},
keywords = {Attack, Feature Selection, Finger Geometry, HandCAPTCHA, Image Quality, Spoofing, Verification}
}

@article{10.1016/j.compeleceng.2021.107527,
author = {Mohammad, Abdul Salam and Pradhan, Manas Ranjan},
title = {Machine learning with big data analytics for cloud security},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {PA},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107527},
doi = {10.1016/j.compeleceng.2021.107527},
journal = {Comput. Electr. Eng.},
month = dec,
numpages = {15},
keywords = {Big data, Cloud computing, Cloud security, Data security, Data management, Data storage, Machine learning}
}

@inproceedings{10.1007/978-3-030-22796-8_6,
author = {Yu, Ling and Zhang, Zhen and Xie, Xuetao and Chen, Hua and Wang, Jian},
title = {Unsupervised Feature Selection Using RBF Autoencoder},
year = {2019},
isbn = {978-3-030-22795-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-22796-8_6},
doi = {10.1007/978-3-030-22796-8_6},
abstract = {In this paper, a novel learning approach to solve unsupervised feature selection in high-dimensional data is proposed, namely Radial Basis Function Autoencoder feature selection (RAFS). This method based on autoencoder uses the radial basis function to achieve mapping instead of the weight. We also consider penalty to give a powerful constraint on redundant features. In extensive experiments, our method shows its outperformance in fair comparison with several other methods.},
booktitle = {Advances in Neural Networks – ISNN 2019: 16th International Symposium on Neural Networks, ISNN 2019, Moscow, Russia, July 10–12, 2019, Proceedings, Part I},
pages = {48–57},
numpages = {10},
keywords = {Unsupervised, Feature selection, Radial basis function, Autoencoder, Penalty},
location = {Moscow, Russia}
}

@article{10.1007/s00500-020-05253-4,
author = {Al-Yarimi, Fuad Ali Mohammed and Munassar, Nabil Mohammed Ali and Bamashmos, Mohammed Hasan Mohammed and Ali, Mohammed Yousef Salem},
title = {RETRACTED ARTICLE: Feature optimization by discrete weights for heart disease prediction using supervised learning},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {3},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05253-4},
doi = {10.1007/s00500-020-05253-4},
abstract = {The topic predictive analytics is the ray that lightning the way to patch the gap between accuracy in decision-making by the expertise and the inexperience. In particular, the health domain is more crucial about disease prediction accuracy. The disease diagnosis by clinical practitioner correlates to his exposer toward the clinical observations of the disease. However, the perceptions of an experienced clinical practitioner on a medical record often fail to identify the premature states of the disease, which costs patient life in the sector of critical diseases such as heart diseases. Hence, contemporary computer science engineering research has more attention to define substantial predictive analytics built by machine learning toward heart disease prediction. The critical objective to define predictive analytics with minimal false alarming is centric to potential training data corpus, and the optimal feature selection. In order to these arguments, the contribution of this manuscript aimed to portray the feature selection approach to perform supervised learning and label the given patient record is prone to heart disease or not with minimal false alarming. The contribution is a dynamic n-gram Features Optimization by Discrete Weights of the feature correlation. The experimental study signified the performance of the proposed model compared to the contemporary methods of feature selection for heart disease prediction.},
journal = {Soft Comput.},
month = feb,
pages = {1821–1831},
numpages = {11},
keywords = {Coronary Heart Disease (CHD), Decision Trees (DT), Nearest Neighbor Algorithm (K-NN), Support vector machines (SVM), Decision support system (DSS)}
}

@article{10.1007/s11265-021-01637-3,
author = {Chen, Jiwei and Tang, Guojian},
title = {A Feature Selection Model to Filter Periodic Variable Stars with Data-sensitive Light-variable Characteristics},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {93},
number = {7},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-021-01637-3},
doi = {10.1007/s11265-021-01637-3},
abstract = {At present, autonomous management and operation of spacecraft are the main direction and objective of the development of space technology to lighten the burden of ground measurement and control, reduce the cost of operation and management, and expand the application scope of spacecraft. To select the suitable periodic variable stars with a certain quantity at the given conditions of spacecraft, we study the autonomous navigation method of optical variable spacecraft and propose a feature selection model to filter periodic variable stars with light-variable characteristics. It mainly focuses on the learning processes of the pulsating optical variable light variation star clock model, the high precision pulsating optical variable autonomous navigation algorithm and the optical variable light variation characteristic mechanism with the measurement method. From experiments, the sample of the periodic variable star is selected, forms a database of 132 initial candidate samples and 16 navigation sample stars. So, time measurement can be conducted to take advantage of the nature of periodic variable stars that take days as its cycle, ground-based observation and ground-based application can be conducted with the wide spectrum of periodic variable star observation. It can meet the requirements of spacecraft.},
journal = {J. Signal Process. Syst.},
month = jul,
pages = {733–744},
numpages = {12},
keywords = {Machine learning filter, Feature selection model, Periodic variable stars}
}

@article{10.1007/s11227-021-03626-6,
author = {Mohmmadzadeh, Hekmat and Gharehchopogh, Farhad Soleimanian},
title = {An efficient binary chaotic symbiotic organisms search algorithm approaches for feature selection problems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03626-6},
doi = {10.1007/s11227-021-03626-6},
abstract = {Feature selection is one of the main steps in preprocessing data in machine learning, and its goal is to reduce features by removing additional and noisy features. Feature selection methods and feature reduction in a dataset must consider the accuracy of the classifying algorithms. Meta-heuristic algorithms serve as the most successful and promising methods to solve this problem. Symbiotic Organisms Search (SOS) is one of the most successful meta-heuristic algorithms inspired by organisms' interaction in nature called mutualism, commensalism, and parasitism. In this paper, three SOS-based binary approaches are offered to solve the feature selection problem. In the first and second approaches, several S-shaped transfer functions and several Chaotic Tent Function-based V-shaped transfer functions called BSOSST and BSOSVT are used to make the binary SOS (BSOS). In the third approach, an advanced BSOS based on changing SOS and the chaotic Tent function operators called EBCSOS is provided. The EBCSOS algorithm uses the chaotic Tent function and the Gaussian mutation to increase usefulness and exploration. Moreover, two new operators, i.e., BMPT and BCPT, are suggested to make the commensalism and mutualism stage binary based on a chaotic function to solve the feature selection problem. Finally, the proposed BSOSST and BSOSVT methods and the advanced version of EBCSOS were implemented on 25 datasets than the basic algorithm's binary meta-heuristic algorithms. Various experiments demonstrated that the proposed EBCSOS algorithm outperformed other methods in terms of several features and accuracy. To further confirm the proposed EBCSOS algorithm, the problem of detecting spam E-mails was applied, with the results of this experiment indicating that the proposed EBCSOS algorithm significantly improved the accuracy and speed of all categories in detecting spam E-mails.},
journal = {J. Supercomput.},
month = aug,
pages = {9102–9144},
numpages = {43},
keywords = {Binary symbiotic organisms search, Feature selection, Classification, Optimization}
}

@article{10.1016/j.knosys.2017.02.013,
author = {Das, Asit K and Das, Sunanda and Ghosh, Arka},
title = {Ensemble feature selection using bi-objective genetic algorithm},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {123},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.02.013},
doi = {10.1016/j.knosys.2017.02.013},
abstract = {An ensemble parallel processing bi-objective genetic algorithm based feature selection method is proposed.Rough set theory and Mutual information gain are used to select informative data removing the vague one.Parallel processing in genetic algorithm reduces time complexity.The method is compared with the existing state-of-the-art methods using suitable datasets.Classification accuracy and statistical measures outperforms that of other state-of-the-art methods. Feature selection problem in data mining is addressed here by proposing a bi-objective genetic algorithm based feature selection method. Boundary region analysis of rough set theory and multivariate mutual information of information theory are used as two objective functions in the proposed work, to select only precise and informative data from the data set. Data set is sampled with replacement strategy and the method is applied to determine non-dominated feature subsets from each sampled data set. Finally, ensemble of such bi-objective genetic algorithm based feature selectors is developed with the help of parallel implementations to produce much generalized feature subset. In fact, individual feature selector outputs are aggregated using a novel dominance based principle to produce final feature subset. Proposed work is validated using repository especially for feature selection datasets as well as on UCI machine learning repository datasets and the experimental results are compared with related state of art feature selection methods to show effectiveness of the proposed ensemble feature selection method.},
journal = {Know.-Based Syst.},
month = may,
pages = {116–127},
numpages = {12},
keywords = {Evolutionary optimization, Feature selection, Genetic algorithm, Mutual information, Rough set theory, Supervised learning}
}

@article{10.1016/j.cosrev.2021.100370,
author = {Garg, Arunim and Mago, Vijay},
title = {Role of machine learning in medical research: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100370},
doi = {10.1016/j.cosrev.2021.100370},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {17},
keywords = {00-01, 99-00, Medical research, Machine learning, Deep learning, Medical data}
}

@article{10.1016/j.future.2018.05.060,
author = {Sun, Guanglu and Li, Jiabin and Dai, Jian and Song, Zhichao and Lang, Fei},
title = {Feature selection for IoT based on maximal information coefficient},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.05.060},
doi = {10.1016/j.future.2018.05.060},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {606–616},
numpages = {11},
keywords = {Internet of Things, Feature selection, Maximal information coefficient, Feature relevance, Feature redundancy}
}

@article{10.1007/s10489-020-01726-z,
author = {Zhou, HongFang and Wen, Jing},
title = {Dynamic feature selection method with minimum redundancy information for linear data},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01726-z},
doi = {10.1007/s10489-020-01726-z},
abstract = {Feature selection plays a fundamental role in many data mining and machine learning tasks. In this paper, we proposed a novel feature selection method, namely, Dynamic Feature Selection Method with Minimum Redundancy Information (MRIDFS). In MRIDFS, the conditional mutual information is used to calculate the relevance and the redundancy among multiple features, and a new concept, the feature-dependent redundancy ratio, was introduced. Such ratio can represent redundancy more accurately. To evaluate our method, MRIDFS is tested and compared with seven popular methods on 16 benchmark data sets. Experimental results show that MRIDFS outperforms in terms of average classification accuracy.},
journal = {Applied Intelligence},
month = nov,
pages = {3660–3677},
numpages = {18},
keywords = {Feature selection, Mutual information, Conditional redundancy, Linear data}
}

@inproceedings{10.1145/3443467.3443843,
author = {Cai, Wei and Xu, Shengbing and Liu, Jiongzhi and Du, Qingping and Chen, Hefeng and Lin, Yinyun},
title = {An adaptive approach of Feature Selection applied to Semi-Supervised Fuzzy Clustering},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443843},
doi = {10.1145/3443467.3443843},
abstract = {Label information in the corresponding semi-supervised fuzzy clustering cannot be used efficiently due to feature redundancy. To address the problem, we propose an adaptive approach of feature selection applied to semi-supervised fuzzy clustering. There are three phases in our approach: 1) feature-score by fisher-score; 2) Min Mean Square Error of Feature Select criterion; 3) the number of features is selected by Min Mean Square Error of Feature Select criterion. We apply our approach to three semi-supervised fuzzy clustering methods. Experiments show that the adaptive approach of feature selection applied to semi-supervised fuzzy clustering can improve the clustering performance.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {723–727},
numpages = {5},
keywords = {Adaptive Feature Selection, Fisher Score, Fuzzy Clustering, Semi-supervised Clustering},
location = {Xiamen, China},
series = {EITCE '20}
}

@article{10.1016/j.eswa.2019.02.005,
author = {Chen, Lan-lan and Zhang, Ao and Lou, Xiao-guang},
title = {Cross-subject driver status detection from physiological signals based on hybrid feature selection and transfer learning},
year = {2019},
issue_date = {Dec 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.02.005},
doi = {10.1016/j.eswa.2019.02.005},
journal = {Expert Syst. Appl.},
month = dec,
pages = {266–280},
numpages = {15},
keywords = {Driver status, Physiological signals, Cross-subject feature evaluation, Hybrid feature selection, Transfer learning}
}

@article{10.1016/j.eswa.2021.115648,
author = {Fern\'{a}ndez-Edreira, Diego and Li\~{n}ares-Blanco, Jose and Fernandez-Lozano, Carlos},
title = {Machine Learning analysis of the human infant gut microbiome identifies influential species in type 1 diabetes},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115648},
doi = {10.1016/j.eswa.2021.115648},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {11},
keywords = {Machine Learning, Diabetes, T1D, Microbiota, Metagenomics, Feature selection, Random forest, Generalized Linear Model}
}

@article{10.1007/s10462-019-09800-w,
author = {Hancer, Emrah and Xue, Bing and Zhang, Mengjie},
title = {A survey on feature selection approaches for clustering},
year = {2020},
issue_date = {Aug 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {6},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-019-09800-w},
doi = {10.1007/s10462-019-09800-w},
abstract = {The massive growth of data in recent years has led challenges in data mining and machine learning tasks. One of the major challenges is the selection of relevant features from the original set of available features that maximally improves the learning performance over that of the original feature set. This issue attracts researchers’ attention resulting in a variety of successful feature selection approaches in the literature. Although there exist several surveys on unsupervised learning (e.g., clustering), lots of works concerning unsupervised feature selection are missing in these surveys (e.g., evolutionary computation based feature selection for clustering) for identifying the strengths and weakness of those approaches. In this paper, we introduce a comprehensive survey on feature selection approaches for clustering by reflecting the advantages/disadvantages of current approaches from different perspectives and identifying promising trends for future research.},
journal = {Artif. Intell. Rev.},
month = aug,
pages = {4519–4545},
numpages = {27},
keywords = {Clustering, Feature selection, Data mining, Evolutionary computation}
}

@article{10.1016/j.cmpb.2016.09.016,
author = {Dumortier, Antoine and Beckjord, Ellen and Shiffman, Saul and Sejdi, Ervin},
title = {Classifying smoking urges via machine learning},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2016.09.016},
doi = {10.1016/j.cmpb.2016.09.016},
abstract = {We examine different machine learning approaches to classify smoking urges.We use situational features associated with urges to smoke during a quit attempt.Feature selection algorithms make possible to obtain high classification rates.The classification tree method provides the best classification results. Background and objectiveSmoking is the largest preventable cause of death and diseases in the developed world, and advances in modern electronics and machine learning can help us deliver real-time intervention to smokers in novel ways. In this paper, we examine different machine learning approaches to use situational features associated with having or not having urges to smoke during a quit attempt in order to accurately classify high-urge states. MethodsTo test our machine learning approaches, specifically, Bayes, discriminant analysis and decision tree learning methods, we used a dataset collected from over 300 participants who had initiated a quit attempt. The three classification approaches are evaluated observing sensitivity, specificity, accuracy and precision. ResultsThe outcome of the analysis showed that algorithms based on feature selection make it possible to obtain high classification rates with only a few features selected from the entire dataset. The classification tree method outperformed the naive Bayes and discriminant analysis methods, with an accuracy of the classifications up to 86%. These numbers suggest that machine learning may be a suitable approach to deal with smoking cessation matters, and to predict smoking urges, outlining a potential use for mobile health applications. ConclusionsIn conclusion, machine learning classifiers can help identify smoking situations, and the search for the best features and classifier parameters significantly improves the algorithms' performance. In addition, this study also supports the usefulness of new technologies in improving the effect of smoking cessation interventions, the management of time and patients by therapists, and thus the optimization of available health care resources. Future studies should focus on providing more adaptive and personalized support to people who really need it, in a minimum amount of time by developing novel expert systems capable of delivering real-time interventions.},
journal = {Comput. Methods Prog. Biomed.},
month = dec,
pages = {203–213},
numpages = {11},
keywords = {Feature selection, Machine learning, Smoking cessation, Smoking urges, Supervised learning}
}

@article{10.1016/j.eswa.2017.09.047,
author = {Kumova Metin, Senem},
title = {Feature selection in multiword expression recognition},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {92},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.09.047},
doi = {10.1016/j.eswa.2017.09.047},
abstract = {A feature selection procedure to be used in MWE recognition is presented.Feature selection is performed by both wrappers and filtering.A MWE Turkish data set is prepared.The performance of MWE recognition is improved by feature selection procedure. In multiword expression (MWE) recognition, there exist many studies where different learning methods are employed to decide whether given word combination is a multiword expression. The recognition methods commonly utilize a number of features that are extracted from a data source, frequently from the given text. Though the recognition methods and the features are well studied, we believe that to achieve the best possible performance with a learning method, different subsets of features should also be considered and the best performing subset must be selected.In this paper, we propose a procedure that covers the performance comparison of well-known feature selection methods to obtain the best feature subset in MWE recognition. The evaluation tests are performed on a Turkish MWE data set and the performance is measured by precision, recall and F1 values. The highest F1 value =0.731 is obtained by C4.5 classifier employing either wrapper or filtering method in feature selection. In the regarding setting(s), it is examined that the performance is increased by 1.11% compared to the setting where all features are employed in classification.Based on the experimental results, it may be stated that feature selection improves the performance of MWE recognition by eliminating the noisy/non-effective features. Moreover, it is obvious that proposed feature selection method contributes to the overall MWE recognition system by reducing the measurement and storage requirements due to the lower number of features in classification, providing a faster and more-cost effective learning model.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {106–123},
numpages = {18},
keywords = {Feature selection, Learning algorithms, Multiword expression, Multiword expression recognition}
}

@inproceedings{10.1145/3389189.3397996,
author = {Drosouli, Ifigenia and Voulodimos, Athanasios and Miaoulis, Georgios},
title = {Transportation mode detection using machine learning techniques on mobile phone sensor data},
year = {2020},
isbn = {9781450377737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3389189.3397996},
doi = {10.1145/3389189.3397996},
abstract = {The everyday use of means of transportation by millions of people combined with the continuous spreading of smartphones which are now equipped with various sensors, imply the existence of abundance of real-world transportation-related data and make Transportation Mode Detection (TMD) an interesting research field, essential to urban transportation planning, development of context-aware applications and physical and mental health improvement. The main objective of this work is to develop a machine learning methodology for classifying eight different transportation modes, including: still, walk, run, bike, car, bus, train, and subway, using data from smartphones sensors. To this end, publicly available datasets were used. For example, a subset of the original SHL dataset, including data obtained from one participant's smartphone embedded sensors (accelerometer, magnetometer, gyroscope, pressure sensor), being recorded for 68 days. As classifiers, eight Machine Learning algorithms were employed. The classifiers were firstly developed without Dimensionality Reduction (DR) and then with a DR feature extraction algorithm (Principal Component Analysis - PCA) so as to explore the possibility of using lighter models and potentially improve performance. After dimensionality reduction, the algorithms that performed best, accomplished a very good classification result in all classes while training time was significantly reduced.},
booktitle = {Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {65},
numpages = {8},
keywords = {classification, machine learning, smartphone sensors, transportation mode detection, urban informatics, visualization},
location = {Corfu, Greece},
series = {PETRA '20}
}

@article{10.1007/s11042-021-10619-3,
author = {Ullah, Waseem and Muhammad, Khan and Ul Haq, Ijaz and Ullah, Amin and Ullah Khattak, Saeed and Sajjad, Muhammad},
title = {Splicing sites prediction of human genome using machine learning techniques},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {20},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10619-3},
doi = {10.1007/s11042-021-10619-3},
abstract = {The accurate splice site prediction has several applications in the field of medical sciences and biochemistry. For instance, any mutation affecting the splice site will lead to genetic diseases and cancer such as Lynch syndrome and breast cancer. For this purpose, collecting the Ribonucleic Acid (RNA) samples is an efficient and convenient method to detect the involvement of splicing defects in disease formation. Therefore, the present study aims to develop an accurate and robust Computer-Aided Diagnosis (CAD) method for swift and precise targeting of splice site sequences. A composite features-based model is proposed by integrating three different sample representation methods i.e., Dinucleotide Composition (DNC), Trinucleotide Composition (TNC) and Tetranucleotide Composition (TetraNC) for precise splice site prediction after converting the DNA sequences into numerical descriptors. The precision and accuracy of these features are analyzed by applying different machine learning algorithms such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN) and Na\"{\i}ve Bayes (NB). Results show that the proposed model of composite features vector with SVM classifier achieved an accuracy of 95.20% and 97.50% for donor and acceptor sites datasets, respectively.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {30439–30460},
numpages = {22},
keywords = {Biomedical data, Big data analysis, Computer-aided diagnosis, Genomics, Machine learning, Pattern recognition, Splicing sites}
}

@article{10.1007/s11390-020-9323-x,
author = {Peynirci, G\"{o}k\c{c}er and Emina\u{g}ao\u{g}lu, Mete and Karabulut, Korhan},
title = {Feature Selection for Malware Detection on the Android Platform Based on Differences of IDF Values},
year = {2020},
issue_date = {Jul 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {4},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9323-x},
doi = {10.1007/s11390-020-9323-x},
abstract = {Android is the mobile operating system most frequently targeted by malware in the smartphone ecosystem, with a market share significantly higher than its competitors and a much larger total number of applications. Detection of malware before being published on official or unofficial application markets is critically important due to the typical end users’ widespread security inadequacy. In this paper, a novel feature selection method is proposed along with an Android malware detection approach. The feature selection method proposed in this study makes use of permissions, API calls, and strings as features, which are statically extractable from the Android executables (APK files) and it can be used in a machine learning process with different algorithms to detect malware on the Android platform. A novel document frequencybased approach, namely Delta IDF, was designed and implemented for feature selection. Delta IDF was tested upon three universal benchmark datasets that contain Android malware samples and highly promising results were obtained by using several binary classification algorithms.},
journal = {J. Comput. Sci. Technol.},
month = jul,
pages = {946–962},
numpages = {17},
keywords = {malware detection, Android, feature selection, inverse document frequency, static analysis}
}

@inproceedings{10.1145/2491627.2491629,
author = {Clements, Paul and Krueger, Charles and Shepherd, James and Winkler, Andrew},
title = {A PLE-based auditing method for protecting restricted content in derived products},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491629},
doi = {10.1145/2491627.2491629},
abstract = {Many organizations that produce a portfolio of products for different customers need to ensure that sensitive or restricted content that may appear in some products must not appear in others. Examples of this need include complying with statutes in different countries of sale, protection of intellectual property developed specifically for one customer, and more. For organizations operating under these requirements and producing their products under a product line engineering paradigm that relies on automation in product derivation, there is a need for a method to ensure that the content restrictions have been met in the derived products. This paper describes an auditing method that meets this need. It was created for use in the Second Generation Product Line Engineering approach that is being applied by Lockheed Martin in their AEGIS ship combat system product line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {218–226},
numpages = {9},
keywords = {bill-of-features, feature modeling, feature profiles, hierarchical product lines, product audit, product baselines, product configurator, product derivation, product line engineering, product portfolio, second generation product line engineering, software product lines, variation points},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1016/j.procs.2019.09.180,
author = {Apostolo, Guilherme Henrique and Sampaio, Igor Garcia Ballhausen and Viterbo, Jos\'{e}},
title = {Feature selection on database optimization for Wi-Fi fingerprint indoor positioning},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {159},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.09.180},
doi = {10.1016/j.procs.2019.09.180},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {251–260},
numpages = {10},
keywords = {Wi-fi Indoor Localization, Database Optimization, Machine Learning Algorithms, Feature Selection}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {generating feature models, inductive generalization from examples, machine learning},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.cie.2020.106536,
author = {Aremu, Oluseun Omotola and Cody, Roya Allison and Hyland-Wood, David and McAree, Peter Ross},
title = {A relative entropy based feature selection framework for asset data in predictive maintenance},
year = {2020},
issue_date = {Jul 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2020.106536},
doi = {10.1016/j.cie.2020.106536},
journal = {Comput. Ind. Eng.},
month = jul,
numpages = {13},
keywords = {Predictive maintenance, Asset management, Machine learning, Feature selection, Feature engineering, Information theory, Relative entropy}
}

@article{10.1007/s10489-021-02302-9,
author = {Pan, Jeng-Shyang and Tian, Ai-Qing and Chu, Shu-Chuan and Li, Jun-Bao},
title = {Improved binary pigeon-inspired optimization and its application for feature selection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {12},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02302-9},
doi = {10.1007/s10489-021-02302-9},
abstract = {The Pigeon-Inspired Optimization (PIO) algorithm is an intelligent algorithm inspired by the behavior of pigeons returned to the nest. The binary pigeon-inspired optimization (BPIO) algorithm is a binary version of the PIO algorithm, it can be used to optimize binary application problems. The transfer function plays a very important part in the BPIO algorithm. To improve the solution quality of the BPIO algorithm, this paper proposes four new transfer function, an improved speed update scheme, and a second-stage position update method. The original BPIO algorithm is easier to fall into the local optimal, so a new speed update equation is proposed. In the simulation experiment, the improved BPIO is compared with binary particle swarm optimization (BPSO) and binary grey wolf optimizer (BGWO). In addition, the benchmark test function, statistical analysis, Friedman’s test and Wilcoxon rank-sum test are used to prove that the improved algorithm is quite effective, and it also verifies how to set the speed of dynamic movement. Finally, feature selection was successfully implemented in the UCI data set, and higher classification results were obtained with fewer feature numbers.},
journal = {Applied Intelligence},
month = dec,
pages = {8661–8679},
numpages = {19},
keywords = {Pigeon-inspired optimization, Transfer function, Binary version, Wilcoxon rank sum test, Feature selection}
}

@article{10.1016/j.neucom.2018.12.043,
author = {Lin, Qiang and Xue, Yiming and Wen, Juan and Zhong, Ping},
title = {A sharing multi-view feature selection method via Alternating Direction Method of Multipliers},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {333},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.12.043},
doi = {10.1016/j.neucom.2018.12.043},
journal = {Neurocomput.},
month = mar,
pages = {124–134},
numpages = {11},
keywords = {Feature selection, Multi-view, Sharing strategy, Alternating Direction Method of Multipliers (ADMM)}
}

@article{10.1016/j.is.2019.02.003,
author = {Barddal, Jean Paul and Enembreck, Fabr\'{\i}cio and Gomes, Heitor Murilo and Bifet, Albert and Pfahringer, Bernhard},
title = {Boosting decision stumps for dynamic feature selection on data streams},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {83},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2019.02.003},
doi = {10.1016/j.is.2019.02.003},
journal = {Inf. Syst.},
month = jul,
pages = {13–29},
numpages = {17},
keywords = {Data stream mining, Feature selection, Concept drift, Feature drift}
}

@inproceedings{10.5555/2022115.2022129,
author = {Gamez, Nadia and Fuentes, Lidia},
title = {Software product line evolution with cardinality-based feature models},
year = {2011},
isbn = {9783642213465},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature models are widely used for modelling variability present in a Software Product Line family. We propose using cardinality-based feature models and clonable features to model and manage the evolution of the structural variability present in pervasive systems, composed by a large variety of heterogeneous devices. The use of clonable features increases the expressiveness of feature models, but also greatly increases the complexity of the resulting configurations. So, supporting the evolution of product configurations becomes an intractable task to do it manually. In this paper, we propose a model driven development process to propagate changes made in an evolved feature model, into existing configurations. Furthermore, our process allows us to calculate the effort needed to perform the evolution changes in the customized products. To do this, we have defined two operators, one to calculate the differences between two configurations and another to create a new configuration from a previous one. Finally, we validate our approach, showing that by using our tool support we can generate new configurations for a family of products with thousands of cloned features.},
booktitle = {Proceedings of the 12th International Conference on Top Productivity through Software Reuse},
pages = {102–118},
numpages = {17},
keywords = {evolution, feature models, software product lines},
location = {Pohang, South Korea},
series = {ICSR'11}
}

@article{10.1016/j.compbiomed.2020.104151,
author = {Lee, You Won and Choi, Jae Woo and Shin, Eun-Hee},
title = {Machine learning model for predicting malaria using clinical information},
year = {2021},
issue_date = {Feb 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104151},
doi = {10.1016/j.compbiomed.2020.104151},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {7},
keywords = {Machine learning, Malaria, Diagnosis, Case reports, Patient information}
}

@article{10.1007/s42979-021-00465-3,
author = {Chaurasia, Vikas and Pal, Saurabh},
title = {Stacking-Based Ensemble Framework and Feature Selection Technique for the Detection of Breast Cancer},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {2},
url = {https://doi.org/10.1007/s42979-021-00465-3},
doi = {10.1007/s42979-021-00465-3},
abstract = {Breast cancer is the second most common cancer in women worldwide. The uncontrolled growth of breast cells is called breast cancer. The treatment of human breast cancer is a very critical process, and sometimes certain indicators may produce negative results. To avoid this misleading outcome situation, a reliable and accurate breast cancer diagnosis system must be available. The machine learning (ML) method is a modern and accurate technique that researchers have recently applied to predict and diagnose breast cancer. In this research article, we developed stack-based ensemble techniques and feature selection methods for the comprehensive performance of the algorithm and comparative analysis of breast cancer datasets with reduced attributes and all attributes. In this article, we first take the SVM, k nearest neighbors, Naive Bayes and perceptron as four ML algorithms as sub-models that have been trained and predicted from, and then combine them to make a new model called blending (stacking). Finally, logistic regression is used to predict the stacked model. It is significant that sub-models produce different results that are not correlated predictions. The stacking technique is best when all the sub-models are skillfully combined together. This article uses the five-feature selection technique because it affects the overall performance of the model. Unrelated or moderately related features may adversely affect the behavior of the model. After applying the feature selection method, now we have data set with reduced features as well as all features. We implemented logistic regression on a dataset with all features and a dataset with reduced features. Finally, we see that the dataset with reduced features has got improved accuracy.},
journal = {SN Comput. Sci.},
month = feb,
numpages = {13},
keywords = {Breast cancer, KNN, Perceptron, SVM, Na\"{\i}ve Bayes, Stacking, Machine learning, Feature selection}
}

@inproceedings{10.1007/978-3-030-72610-2_3,
author = {Kwuida, L\'{e}onard and Ignatov, Dmitry I.},
title = {On Interpretability and Similarity in Concept-Based Machine Learning},
year = {2020},
isbn = {978-3-030-72609-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72610-2_3},
doi = {10.1007/978-3-030-72610-2_3},
abstract = {Machine Learning (ML) provides important techniques for classification and predictions. Most of these are black-box models for users and do not provide decision-makers with an explanation. For the sake of transparency or more validity of decisions, the need to develop explainable/interpretable ML-methods is gaining more and more importance. Certain questions need to be addressed:How does an ML procedure derive the class for a particular entity?Why does a particular clustering emerge from a particular unsupervised ML procedure?What can we do if the number of attributes is very large?What are the possible reasons for the mistakes for concrete cases and models?For binary attributes, Formal Concept Analysis (FCA) offers techniques in terms of intents of formal concepts, and thus provides plausible reasons for model prediction. However, from the interpretable machine learning viewpoint, we still need to provide decision-makers with the importance of individual attributes to the classification of a particular object, which may facilitate explanations by experts in various domains with high-cost errors like medicine or finance.We discuss how notions from cooperative game theory can be used to assess the contribution of individual attributes in classification and clustering processes in concept-based machine learning. To address the 3rd question, we present some ideas on how to reduce the number of attributes using similarities in large contexts.},
booktitle = {Analysis of Images, Social Networks and Texts: 9th International Conference, AIST 2020, Skolkovo, Moscow, Russia, October 15–16, 2020, Revised Selected Papers},
pages = {28–54},
numpages = {27},
keywords = {Interpretable Machine Learning, Concept learning, Formal concepts, Shapley values, Explainable AI},
location = {Moscow, Russia}
}

@article{10.1007/s00521-021-06406-8,
author = {Abiodun, Esther Omolara and Alabdulatif, Abdulatif and Abiodun, Oludare Isaac and Alawida, Moatsum and Alabdulatif, Abdullah and Alkhawaldeh, Rami S.},
title = {A systematic review of emerging feature selection optimization methods for optimal text classification: the present state and prospective opportunities},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {22},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06406-8},
doi = {10.1007/s00521-021-06406-8},
abstract = {Specialized data preparation techniques, ranging from data cleaning, outlier detection, missing value imputation, feature selection (FS), amongst others, are procedures required to get the most out of data and, consequently, get the optimal performance of predictive models for classification tasks. FS is a vital and indispensable technique that enables the model to perform faster, eliminate noisy data, remove redundancy, reduce overfitting, improve precision and increase generalization on testing data. While conventional FS techniques have been leveraged for classification tasks in the past few decades, they fail to optimally reduce the high dimensionality of the feature space of texts, thus breeding inefficient predictive models. Emerging technologies such as the metaheuristics and hyper-heuristics optimization methods provide a new paradigm for FS due to their efficiency in improving the accuracy of classification, computational demands, storage, as well as functioning seamlessly in solving complex optimization problems with less time. However, little details are known on best practices for case-to-case usage of emerging FS methods. The literature continues to be engulfed with clear and unclear findings in leveraging effective methods, which, if not performed accurately, alters precision, real-world-use feasibility, and the predictive model's overall performance. This paper reviews the present state of FS with respect to metaheuristics and hyper-heuristic methods. Through a systematic literature review of over 200 articles, we set out the most recent findings and trends to enlighten analysts, practitioners and researchers in the field of data analytics seeking clarity in understanding and implementing effective FS optimization methods for improved text classification tasks.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {15091–15118},
numpages = {28},
keywords = {Feature selection, Hyper-heuristics, Metaheuristic algorithm, Optimization, Text classification}
}

@article{10.1016/j.eswa.2019.112898,
author = {Faris, Hossam and Heidari, Ali Asghar and Al-Zoubi, Ala’ M. and Mafarja, Majdi and Aljarah, Ibrahim and Eshtay, Mohammed and Mirjalili, Seyedali},
title = {Time-varying hierarchical chains of salps with random weight networks for feature selection},
year = {2020},
issue_date = {Feb 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {140},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112898},
doi = {10.1016/j.eswa.2019.112898},
journal = {Expert Syst. Appl.},
month = feb,
numpages = {17},
keywords = {Feature selection, Salp swarm algorithm, Optimization, Evolutionary algorithms}
}

@inproceedings{10.1145/3377930.3389832,
author = {Nguyen, Tien Thanh and Van Pham, Nang and Dang, Manh Truong and Luong, Anh Vu and McCall, John and Liew, Alan Wee Chung},
title = {Multi-layer heterogeneous ensemble with classifier and feature selection},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389832},
doi = {10.1145/3377930.3389832},
abstract = {Deep Neural Networks have achieved many successes when applying to visual, text, and speech information in various domains. The crucial reasons behind these successes are the multi-layer architecture and the in-model feature transformation of deep learning models. These design principles have inspired other sub-fields of machine learning including ensemble learning. In recent years, there are some deep homogenous ensemble models introduced with a large number of classifiers in each layer. These models, thus, require a costly computational classification. Moreover, the existing deep ensemble models use all classifiers including unnecessary ones which can reduce the predictive accuracy of the ensemble. In this study, we propose a multi-layer ensemble learning framework called MUlti-Layer heterogeneous Ensemble System (MULES) to solve the classification problem. The proposed system works with a small number of heterogeneous classifiers to obtain ensemble diversity, therefore being efficiency in resource usage. We also propose an Evolutionary Algorithm-based selection method to select the subset of suitable classifiers and features at each layer to enhance the predictive performance of MULES. The selection method uses NSGA-II algorithm to optimize two objectives concerning classification accuracy and ensemble diversity. Experiments on 33 datasets confirm that MULES is better than a number of well-known benchmark algorithms.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {725–733},
numpages = {9},
keywords = {classifier selection, deep learning, ensemble method, ensemble of classifiers, feature selection, multiple classifiers},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1016/j.compbiomed.2021.104771,
author = {Narin, Ali},
title = {Accurate detection of COVID-19 using deep features based on X-Ray images and feature selection methods},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104771},
doi = {10.1016/j.compbiomed.2021.104771},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {11},
keywords = {COVID-19, X-ray, Deep features, Feature selection, PSO, ACO}
}

@inproceedings{10.1007/978-3-030-74251-5_19,
author = {Verbruggen, Gust and Van Wolputte, Elia and Duman\v{c}i\'{c}, Sebastijan and De Raedt, Luc},
title = {avatar—Automated Feature Wrangling for Machine Learning},
year = {2021},
isbn = {978-3-030-74250-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-74251-5_19},
doi = {10.1007/978-3-030-74251-5_19},
abstract = {A large part of the time invested in data science is spent on manual preparation of data. Transforming wrongly formatted columns into useful features takes up a significant part of this time. We present the avatar algorithm for automatically learning programs that perform this type of feature wrangling. Instead of relying on users to guide the wrangling process, avatar directly uses the predictive performance of machine learning models to measure its progress during wrangling. We use datasets from Kaggle to show that avatar improves raw data for prediction, and square it off against human data scientists.},
booktitle = {Advances in Intelligent Data Analysis XIX: 19th International Symposium on Intelligent Data Analysis, IDA 2021, Porto, Portugal, April 26–28, 2021, Proceedings},
pages = {235–247},
numpages = {13},
keywords = {Data wrangling, Program synthesis, Machine learning},
location = {Porto, Portugal}
}

@article{10.1016/j.eswa.2007.01.036,
author = {Liao, Shu-Hsien and Chen, Chyuan-Meei and Wu, Chung-Hsin},
title = {Mining customer knowledge for product line and brand extension in retailing},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.01.036},
doi = {10.1016/j.eswa.2007.01.036},
abstract = {Retailing consists of the final activities and steps needed to place a product in the hands of the consumer or to provide services to the consumer. In fact, retailing is actually the last step in a supply chain that may stretch from Europe or Asia to the customer's hometown. Therefore, any firm that sells a product or provides a service to the final consumer is performing the retailing function. On the other hand, product line extension, which adds depth to an existing product line by introducing new products in the same product category, can give customers greater choice and help to protect the firm from flanking attack by a competitor. In addition, a product line extension is marketed under the same general brand as a previous item or items. Thus, to distinguish the brand extension from the other item(s) under the primary brand, the retailer can either add secondary brand identification or add a generic brand. This paper investigates product line and brand extension issues in the Taiwan branch of a leading international retailing company, Carrefour, which is a hypermarket retailer. This paper develops a relational database and proposes Apriori algorithm and K-means as methodologies for association rule and cluster analysis for data mining, which is then implemented to mine customer knowledge from household customers. Knowledge extraction by data mining results is illustrated as knowledge patterns/rules and clusters in order to propose suggestions and solutions to the case firm for product line and brand extensions and knowledge management.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {1763–1776},
numpages = {14},
keywords = {Association rules, Brand extension, Cluster analysis, Data mining, Knowledge extraction, Product line extension, Retailing}
}

@inproceedings{10.1145/3382025.3414952,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Gasca, Rafael M. and Carmona-Fombella, Jose Antonio and G\'{o}mez-L\'{o}pez, Mar\'{\i}a Teresa},
title = {AMADEUS: towards the AutoMAteD secUrity teSting},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414952},
doi = {10.1145/3382025.3414952},
abstract = {The proper configuration of systems has become a fundamental factor to avoid cybersecurity risks. Thereby, the analysis of cybersecurity vulnerabilities is a mandatory task, but the number of vulnerabilities and system configurations that can be threatened is extremely high. In this paper, we propose a method that uses software product line techniques to analyse the vulnerable configuration of the systems. We propose a solution, entitled AMADEUS, to enable and support the automatic analysis and testing of cybersecurity vulnerabilities of configuration systems based on feature models. AMADEUS is a holistic solution that is able to automate the analysis of the specific infrastructures in the organisations, the existing vulnerabilities, and the possible configurations extracted from the vulnerability repositories. By using this information, AMADEUS generates automatically the feature models, that are used for reasoning capabilities to extract knowledge, such as to determine attack vectors with certain features. AMADEUS has been validated by demonstrating the capacities of feature models to support the threat scenario, in which a wide variety of vulnerabilities extracted from a real repository are involved. Furthermore, we open the door to new applications where software product line engineering and cybersecurity can be empowered.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {11},
numpages = {12},
keywords = {cybersecurity, feature model, pentesting, reasoning, testing, vulnerabilities, vulnerable configuration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.knosys.2018.12.031,
author = {Wang, Mingwei and Wu, Chunming and Wang, Lizhe and Xiang, Daxiang and Huang, Xiaohui},
title = {A feature selection approach for hyperspectral image based on modified ant lion optimizer},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {168},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.12.031},
doi = {10.1016/j.knosys.2018.12.031},
journal = {Know.-Based Syst.},
month = mar,
pages = {39–48},
numpages = {10},
keywords = {Hyperspectral image, Feature selection, Wavelet support vector machine, Ant lion optimizer, L\'{e}vy flight}
}

@inproceedings{10.1007/978-3-030-70881-8_1,
author = {Kherbache, Meriem and Espes, David and Amroun, Kamal},
title = {New Wrapper Feature Selection Algorithm for Anomaly-Based Intrusion Detection Systems},
year = {2020},
isbn = {978-3-030-70880-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-70881-8_1},
doi = {10.1007/978-3-030-70881-8_1},
abstract = {With advanced persistent and zero-days threats, the threat landscape is constantly evolving. Signature-based defense is ineffective against these new attacks. Anomaly-based intrusion detection systems rely on classification models, trained on specific datasets, to detect them. Their efficiency is related to the features used by the classifier. Feature selection is a fundamental phase of anomaly-based intrusion detection systems. It selects the near-optimal subset of features in order to improve the detection accuracy and reduce the classification time. This paper introduces a new wrapper method based on two phases. The first phase adopts a correlation analysis between two variables as a measure of feature quality. This phase aims to select the features that contribute the most to the classification by selecting the ones that highly correlated to either the normal or attack traffic but not both. The second phase is used to search for a proper subset that improves the detection accuracy. Our approach is evaluated using three well-known datasets: NSL-KDD, UNSW-NB15 and CICIDS2017. The evaluation results show that our algorithm significantly increases the detection accuracy and improves the detection time. Moreover, it is particularly efficient on stealthy attacks.},
booktitle = {Foundations and Practice of Security: 13th International Symposium, FPS 2020, Montreal, QC, Canada, December 1–3, 2020, Revised Selected Papers},
pages = {3–19},
numpages = {17},
keywords = {Intrusion detection system, Correlation, Support vector machine, Feature selection},
location = {Montreal, QC, Canada}
}

@article{10.1016/j.patrec.2019.08.025,
author = {Happy, S L and Dantcheva, Antitza and Bremond, Francois},
title = {A Weakly Supervised learning technique for classifying facial expressions},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2019.08.025},
doi = {10.1016/j.patrec.2019.08.025},
journal = {Pattern Recogn. Lett.},
month = dec,
pages = {162–168},
numpages = {7},
keywords = {Weakly supervised learning, Facial expression recognition, Label smoothing, 41A05, 41A10, 65D05, 65D17}
}

@inproceedings{10.1007/978-3-030-64984-5_35,
author = {Wang, Hongyu and Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Mayo, Michael},
title = {A Comparison of Machine Learning Methods for Cross-Domain Few-Shot Learning},
year = {2020},
isbn = {978-3-030-64983-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64984-5_35},
doi = {10.1007/978-3-030-64984-5_35},
abstract = {We present an empirical evaluation of machine learning algorithms in cross-domain few-shot learning based on a fixed pre-trained feature extractor. Experiments were performed in five target domains (CropDisease, EuroSAT, Food101, ISIC and ChestX) and using two feature extractors: a ResNet10 model trained on a subset of ImageNet known as miniImageNet and a ResNet152 model trained on the ILSVRC 2012 subset of ImageNet. Commonly used machine learning algorithms including logistic regression, support vector machines, random forests, nearest neighbour classification, na\"{\i}ve Bayes, and linear and quadratic discriminant analysis were evaluated on the extracted feature vectors. We also evaluated classification accuracy when subjecting the feature vectors to normalisation using p-norms. Algorithms originally developed for the classification of gene expression data—the nearest shrunken centroid algorithm and LDA ensembles obtained with random projections—were also included in the experiments, in addition to a cosine similarity classifier that has recently proved popular in few-shot learning. The results enable us to identify algorithms, normalisation methods and pre-trained feature extractors that perform well in cross-domain few-shot learning. We show that the cosine similarity classifier and ℓ2-regularised 1-vs-rest logistic regression are generally the best-performing algorithms. We also show that algorithms such as LDA yield consistently higher accuracy when applied to ℓ2-normalised feature vectors. In addition, all classifiers generally perform better when extracting feature vectors using the ResNet152 model instead of the ResNet10 model.},
booktitle = {AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint Conference, AI 2020, Canberra, ACT, Australia, November 29–30, 2020, Proceedings},
pages = {445–457},
numpages = {13},
keywords = {Cross-domain few-shot learning, Pre-trained feature extractors, Normalisation, Transfer learning},
location = {Canberra, ACT, Australia}
}

@inproceedings{10.1145/3205455.3205552,
author = {Lensen, Andrew and Xue, Bing and Zhang, Mengjie},
title = {Automatically evolving difficult benchmark feature selection datasets with genetic programming},
year = {2018},
isbn = {9781450356183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205455.3205552},
doi = {10.1145/3205455.3205552},
abstract = {There has been a wealth of feature selection algorithms proposed in recent years, each of which claims superior performance in turn. A wide range of datasets have been used to compare these algorithms, each with different characteristics and quantities of redundant and noisy features. Hence, it is very difficult to comprehensively and fairly compare these feature selection methods in order to find which are most robust and effective. In this work, we examine using Genetic Programming to automatically synthesise redundant features for augmenting existing datasets in order to more scientifically test feature selection performance. We develop a method for producing complex multi-variate redundancies, and present a novel and intuitive approach to ensuring a range of redundancy relationships are automatically created. The application of these augmented datasets to well-established feature selection algorithms shows a number of interesting and useful results and suggests promising directions for future research in this area.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {458–465},
numpages = {8},
keywords = {feature creation, feature selection, genetic programming, mutual information},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.eswa.2021.115658,
author = {Lee, You Won and Choi, Jae Woo and Shin, Eun-Hee},
title = {Machine learning model for diagnostic method prediction in parasitic disease using clinical information},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {185},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115658},
doi = {10.1016/j.eswa.2021.115658},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {11},
keywords = {Machine learning, Parasite, Diagnosis, Multi-classification, Binary-classification}
}

@article{10.5555/3288897.3288922,
author = {Zheng, Yuefeng and Li, Ying and Wang, Gang and Chen, Yupeng and Xu, Qian and Fan, Jiahao and Cui, Xueting},
title = {A novel hybrid algorithm for feature selection},
year = {2018},
issue_date = {Oct 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {5–6},
issn = {1617-4909},
abstract = {Feature selection is an important filtering method for data analysis, pattern classification, data mining, and so on. Feature selection reduces the number of features by removing irrelevant and redundant data. In this paper, we propose a hybrid filter---wrapper feature subset selection algorithm called the maximum Spearman minimum covariance cuckoo search (MSMCCS). First, based on Spearman and covariance, a filter algorithm is proposed called maximum Spearman minimum covariance (MSMC). Second, three parameters are proposed in MSMC to adjust the weights of the correlation and redundancy, improve the relevance of feature subsets, and reduce the redundancy. Third, in the improved cuckoo search algorithm, a weighted combination strategy is used to select candidate feature subsets, a crossover mutation concept is used to adjust the candidate feature subsets, and finally, the filtered features are selected into optimal feature subsets. Therefore, the MSMCCS combines the efficiency of filters with the greater accuracy of wrappers. Experimental results on eight common data sets from the University of California at Irvine Machine Learning Repository showed that the MSMCCS algorithm had better classification accuracy than the seven wrapper methods, the one filter method, and the two hybrid methods. Furthermore, the proposed algorithm achieved preferable performance on the Wilcoxon signed-rank test and the sensitivity---specificity test.},
journal = {Personal Ubiquitous Comput.},
month = oct,
pages = {971–985},
numpages = {15},
keywords = {Classification, Cuckoo search algorithm, Dimensionality reduction, Feature selection, Maximum Spearman and minimum covariance}
}

@article{10.1007/s00521-021-05749-6,
author = {Karlos, Stamatis and Aridas, Christos and Kanas, Vasileios G. and Kotsiantis, Sotiris},
title = {Classification of acoustical signals by combining active learning strategies with semi-supervised learning schemes},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05749-6},
doi = {10.1007/s00521-021-05749-6},
abstract = {In real-world cases, handling both labeled and unlabeled data has raised the interest of several Data Scientists and Machine Learning engineers, leading to several demonstrations that apply data-augmenting approaches in order to obtain a robust and, at the same time, accurate enough learning behavior. The main reason is the existence of much unlabeled data that are ignored by conventional supervised approaches, reducing the chance of enriching the final formatted hypothesis. However, the majority of the proposed methods that operate using both kinds of these data are oriented toward exploiting only one category of these algorithms, without combining their strategies. Since the most popular of them regarding the classification task are Active and Semi-supervised Learning approaches, we aim to design a framework that combines both of them trying to fuse their advantages during the main core of the learning process. Thus, we conduct an empirical evaluation of such a combinatory approach over three problems, which stem from various fields but are all tackled through the use of acoustical signals, operating under the pool-based scenario: gender identification, emotion detection and automatic speaker recognition. Into the proposed combinatory framework, which operates under training sets with small cardinality, our results prove the benefits of adopting such kind of semi-automated approaches regarding both the achieved predictive correctness when reduced consumption of resources takes place, as well as the smoothness of the learning convergence. Several learners have been examined for reaching to more general conclusions, and a variant of self-training scheme has been also examined.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {3–20},
numpages = {18},
keywords = {Combined learning framework, Self-training scheme, Active learning queries, Acoustical signal classification, Data augmentation techniques, Semi-automated approaches}
}

@article{10.1016/j.eswa.2019.113122,
author = {Tubishat, Mohammad and Idris, Norisma and Shuib, Liyana and Abushariah, Mohammad A.M. and Mirjalili, Seyedali},
title = {Improved Salp Swarm Algorithm based on opposition based learning and novel local search algorithm for feature selection},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {145},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113122},
doi = {10.1016/j.eswa.2019.113122},
journal = {Expert Syst. Appl.},
month = may,
numpages = {10},
keywords = {Salp Swarm Algorithm, Classification, Feature selection, Optimization, Machine Learning, Algorithm, Opposition Based Learning}
}

@inproceedings{10.1007/978-3-319-24888-2_5,
author = {Wade, Benjamin S. C. and Joshi, Shantanu H. and Gutman, Boris A. and Thompson, Paul M.},
title = {Machine Learning on High Dimensional Shape Data from Subcortical Brain Surfaces: A Comparison of Feature Selection and Classification Methods},
year = {2015},
isbn = {978-3-319-24887-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-24888-2_5},
doi = {10.1007/978-3-319-24888-2_5},
abstract = {Recently, high-dimensional shape data (HDSD) has been demonstrated to be informative in describing subcortical brain morphometry in several disorders. While HDSD may serve as a biomarker of disease, its high dimensionality may require careful treatment in its application to machine learning. Here, we compare several possible approaches for feature selection and pattern classification using HDSD. We explore the efficacy of three candidate feature selection (FS) methods: Guided Random Forest (GRF), LASSO and no feature selection (NFS). Each feature set was applied to three classifiers: Random Forest (RF), Support Vector Machines (SVM) and Na\"{\i}ve Bayes (NB). Each model was cross-validated using two diagnostic contrasts: Alzheimer’s Disease and mild cognitive impairment; each relative to matched controls. GRF and NFS outperformed LASSO as FS methods and were comparably competitive. NB underperformed relative to RF and SVM, which were comparable in performance. Our results advocate the NFS-RF approach for its speed, simplicity and interpretability.},
booktitle = {Machine Learning in Medical Imaging: 6th International Workshop, MLMI 2015, Held in Conjunction with MICCAI 2015, Munich, Germany, October 5, 2015, Proceedings},
pages = {36–43},
numpages = {8},
location = {Munich, Germany}
}

@inproceedings{10.1145/2695664.2695991,
author = {Labib, A. Ezzat and Penad\'{e}s, M. Carmen and Can\'{o}s, Jos\'{e} H. and G\'{o}mez, Abel},
title = {Enforcing reuse and customization in the development of learning objects: a product line approach},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695991},
doi = {10.1145/2695664.2695991},
abstract = {The growing use of information technologies in the educational cycles has raised new requirements for the development of Interactive Learning Materials in terms of content reuse, customization, and ease of creation and efficiency of production. In practical terms, the goal is the development of tools for creating reusable, granular, durable, and interoperable learning objects, and to compose such objects into meaningful courseware pieces. Current learning object development tools require special technical skills in the instructors to exploit reuse and customization features, leading sometimes to unsatisfactory user experiences.In this paper, we explore a new way to reuse and customization following Product Line Engineering principles and tools. We have applied product line-based document engineering tools to create the so-called Learning Object Authoring Tool (LOAT), which supports the development of learning materials following the Cisco's Reusable Information Object strategy. We describe the principles behind LOAT, outline its design, and give clues about how it may be used by instructors to create learning objects in their own disciplines.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {261–263},
numpages = {3},
keywords = {authoring tool, content model, document product line, e-learning, learning object},
location = {Salamanca, Spain},
series = {SAC '15}
}

@article{10.1016/j.cosrev.2021.100395,
author = {T.K., Balaji and Annavarapu, Chandra Sekhara Rao and Bablani, Annushree},
title = {Machine learning algorithms for social media analysis: A survey},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100395},
doi = {10.1016/j.cosrev.2021.100395},
journal = {Comput. Sci. Rev.},
month = may,
numpages = {32},
keywords = {Social Media, Machine learning, Social network analysis, Applications of social media analysis}
}

@inproceedings{10.1007/978-3-030-29726-8_19,
author = {Iravani, Sahar and Conrad, Tim O. F.},
title = {Deep Learning for Proteomics Data for Feature Selection and Classification},
year = {2019},
isbn = {978-3-030-29725-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29726-8_19},
doi = {10.1007/978-3-030-29726-8_19},
abstract = {Todays high-throughput molecular profiling technologies allow to routinely create large datasets providing detailed information about a given biological sample, e.g. about the concentrations of thousands contained proteins. A standard task in the context of precision medicine is to identify a set of biomarkers (e.g. proteins) from these datasets that can be used for disease diagnosis, prognosis or to monitor treatment response. However, finding good biomarker sets is still a challenging task due to the high dimensionality and complexity of the data and the often quite high noise level.In this work, we present an approach to this problem based on Deep Neural Networks (DNN) and a transfer learning strategy using simulation data. To allow interpretation of the results, we compare different approaches to analyze the learned DNN. Based on these interpretation approaches, we describe how to extract biomarker sets.Comparison of our method to a state-of-the-art L1-SVM approach shows that the new approach is able to find better biomarker sets for classification when small sets are desired. Compared to a state-of-the-art -support vector machine (-SVM) approach, our method achieves better results for the classification task when a small number of features are needed.},
booktitle = {Machine Learning and Knowledge Extraction: Third IFIP TC 5, TC 12, WG 8.4, WG 8.9, WG 12.9 International Cross-Domain Conference, CD-MAKE 2019, Canterbury, UK, August 26–29, 2019, Proceedings},
pages = {301–316},
numpages = {16},
keywords = {Deep learning, Attribution, LRP, Interpretation, Feature selection, Transfer learning, Mass spectrometry, Proteomics},
location = {Canterbury, United Kingdom}
}

@article{10.2478/cait-2019-0001,
author = {Venkatesh, B. and Anuradha, J.},
title = {A Review of Feature Selection and Its Methods},
year = {2019},
issue_date = {Mar 2019},
publisher = {Walter de Gruyter GmbH},
address = {Berlin, DEU},
volume = {19},
number = {1},
issn = {1314-4081},
url = {https://doi.org/10.2478/cait-2019-0001},
doi = {10.2478/cait-2019-0001},
abstract = {Nowadays, being in digital era the data generated by various applications are increasing drastically both row-wise and column wise; this creates a bottleneck for analytics and also increases the burden of machine learning algorithms that work for pattern recognition. This cause of dimensionality can be handled through reduction techniques. The Dimensionality Reduction (DR) can be handled in two ways namely Feature Selection (FS) and Feature Extraction (FE). This paper focuses on a survey of feature selection methods, from this extensive survey we can conclude that most of the FS methods use static data. However, after the emergence of IoT and web-based applications, the data are generated dynamically and grow in a fast rate, so it is likely to have noisy data, it also hinders the performance of the algorithm. With the increase in the size of the data set, the scalability of the FS methods becomes jeopardized. So the existing DR algorithms do not address the issues with the dynamic data. Using FS methods not only reduces the burden of the data but also avoids overfitting of the model.},
journal = {Cybern. Inf. Technol.},
month = mar,
pages = {3–26},
numpages = {24},
keywords = {Dimensionality Reduction (DR), Feature Selection (FS), Feature Extraction (FE)}
}

@article{10.1016/j.ins.2019.05.072,
author = {Sun, Lin and Zhang, Xiaoyu and Qian, Yuhua and Xu, Jiucheng and Zhang, Shiguang},
title = {Feature selection using neighborhood entropy-based uncertainty measures for gene expression data classification},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {502},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.05.072},
doi = {10.1016/j.ins.2019.05.072},
journal = {Inf. Sci.},
month = oct,
pages = {18–41},
numpages = {24},
keywords = {Granular computing, Neighborhood rough sets, Feature selection, Neighborhood entropy, Uncertainty measure, Cancer classification}
}

@article{10.4018/IJWSR.2019010103,
author = {Sun, Chang-ai and Wang, Zhen and Wang, Ke and Xue, Tieheng and Aiello, Marco},
title = {Adaptive BPEL Service Compositions via Variability Management: A Methodology and Supporting Platform},
year = {2019},
issue_date = {January 2019},
publisher = {IGI Global},
address = {USA},
volume = {16},
number = {1},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2019010103},
doi = {10.4018/IJWSR.2019010103},
abstract = {Service-Oriented Architectures are a popular development paradigm to enable distributed applications constructed from independent web services. When coordinated, web services are an infrastructure to fulfill dynamic and vertical integration of business. They may face frequent changes of both requirements and execution environments. Static and predefined service compositions using business process execution language BPEL are not able to cater for such rapid and unpredictable context shifts. The authors propose a variability management-based adaptive and configurable service composition approach that treats changes as first-class citizens and consists of identifying, expressing, realizing, and managing changes of service compositions. The proposed approach is realized with a language called VxBPEL to support variability in service compositions and a platform for design, execution, analysis, and maintenance of VxBPEL-based service compositions. Four case studies validate the feasibility of the proposed approach while exhibiting good performance of the supporting platform.},
journal = {Int. J. Web Serv. Res.},
month = jan,
pages = {37–69},
numpages = {33},
keywords = {Adaptive Systems, Business Process Execution Language, Service Composition, Service Oriented Architectures, Variability Management}
}

@inproceedings{10.1007/978-3-030-77211-6_24,
author = {Hackl, Melanie and Datta, Suparno and Miotto, Riccardo and Bottinger, Erwin},
title = {Unsupervised Learning to Subphenotype Heart Failure Patients from Electronic Health Records},
year = {2021},
isbn = {978-3-030-77210-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77211-6_24},
doi = {10.1007/978-3-030-77211-6_24},
abstract = {Heart failure (HF) is a deadly disease and its prevalence is slowly increasing. The sub-types of HF are currently mostly determined by the so-called ejection fraction (EF). In this work, we try to find novel subgroups of heart failure following a complete data-driven approach of clustering patients based on their electronic health records (EHRs). Using a validated phenotyping algorithm we were able to identify 14,334 adult patients with heart failure in our database. We derived embeddings of patients using two different strategies, one processing aggregated clinical features using principal component analysis (PCA) and uniform manifold approximation and projection (UMAP), and one where we learn embeddings from the sequence of medical events using a long short-term memory (LSTM) autoencoder. Then we evaluated different clustering strategies like k-means and agglomerative hierarchical to derive the most informative subtypes. The results were compared based on different metrics such as silhouette coefficient and so on and also based on comparing outcomes such as hospitalization, EF etc. between the clusters. In the most promising result, we were able to identify 3 subclusters using the aggregated data approach in combination with UMAP as dimension reduction method and k-means as cluster method. Patients in cluster 1 had the lowest number of hospital days and comorbidities, while patients in cluster 3 had a significantly higher number of hospital days together with a higher prevalence of comorbidities such as chronic kidney disease and atrial fibrillation. Patients in cluster 2 had a high prevalence of drug allergies in their medical history.},
booktitle = {Artificial Intelligence in Medicine: 19th International Conference on Artificial Intelligence in Medicine, AIME 2021, Virtual Event, June 15–18, 2021, Proceedings},
pages = {219–228},
numpages = {10},
keywords = {Unsupervised learning, Electronic health records, Heart failure}
}

@inproceedings{10.1145/3321707.3321713,
author = {Tran, Binh and Xue, Bing and Zhang, Mengjie},
title = {Adaptive multi-subswarm optimisation for feature selection on high-dimensional classification},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321713},
doi = {10.1145/3321707.3321713},
abstract = {Feature space is an important factor influencing the performance of any machine learning algorithm including classification methods. Feature selection aims to remove irrelevant and redundant features that may negatively affect the learning process especially on high-dimensional data, which usually suffers from the curse of dimensionality. Feature ranking is one of the most scalable feature selection approaches to high-dimensional problems, but most of them fail to automatically determine the number of selected features as well as detect redundancy between features. Particle swarm optimisation (PSO) is a population-based algorithm which has shown to be effective in addressing these limitations. However, its performance on high-dimensional data is still limited due to the large search space and high computation cost. This study proposes the first adaptive multi-swarm optimisation (AMSO) method for feature selection that can automatically select a feature subset of high-dimensional data more effectively and efficiently than the compared methods. The subswarms are automatically and dynamically changed based on their performance during the evolutionary process. Experiments on ten high-dimensional datasets of varying difficulties have shown that AMSO is more effective and more efficient than the compared PSO-based and traditional feature selection methods in most cases.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {481–489},
numpages = {9},
keywords = {classification, feature selection, high-dimensional data, particle swarm optimisation},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1007/s00521-018-04006-7,
author = {Zhong, Zhi},
title = {Adaptive graph learning and low-rank constraint for supervised spectral feature selection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-04006-7},
doi = {10.1007/s00521-018-04006-7},
abstract = {Spectral feature selection (SFS) effectively improves performance of feature selection by introducing a graph matrix to preserve information of data. However, conventional SFS (1) generally preserves either global structure or local structure of data in selected subset, which is not capable of providing comprehensive information for model to output a robust result; (2) constructs graph matrix by original data, which usually lead to a suboptimal graph matrix because of redundant information; (3) conducts feature selection task depending on the fixed graph matrix, which is easily trapped in local optimization. Thus, we have proposed a novel SFS to (1) preserve both local information and global information of original data in feature-selected subset to provide comprehensive information for learning model; (2) integrate graph construction and feature selection to propose a robust spectral feature selection easily obtaining global optimization of feature selection. Besides, for the proposed problem, we further provide a optimization algorithm to effectively tackle the problem with a fast convergence. The extensive experimental results showed that our proposed method outperforms state-of-the-art feature selection methods, in terms of classification performance.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6503–6512},
numpages = {10},
keywords = {Graph learning, Low-rank constraint, Spectral feature selection}
}

@inproceedings{10.1145/1062455.1062551,
author = {Verlage, Martin and Kiesgen, Thomas},
title = {Five years of product line engineering in a small company},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1062455.1062551},
doi = {10.1145/1062455.1062551},
abstract = {In 1999, a new team at MARKET MAKER Software AG began to develop a software product line for managing and displaying stock market data and financial market news. The basic idea was to use web technology in all applications for delivering services to customers. It soon turned out that the company had to change both the processes and the organization. This report summarizes the changes made and the lessons learned over the past five years, when the product line idea was introduced into a small company which faced the pressure to quickly market the first product line instances.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering},
pages = {534–543},
numpages = {10},
keywords = {SME, experience report, product line engineering, project management},
location = {St. Louis, MO, USA},
series = {ICSE '05}
}

@article{10.1007/s10586-021-03254-y,
author = {Abualigah, Laith and Dulaimi, Akram Jamal},
title = {A novel feature selection method for data mining tasks using hybrid Sine Cosine Algorithm and Genetic Algorithm},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03254-y},
doi = {10.1007/s10586-021-03254-y},
abstract = {Feature selection (FS) is a real-world problem that can be solved using optimization techniques. These techniques proposed solutions to make a predictive model, which minimizes the classifier's prediction errors by selecting informative or important features by discarding redundant, noisy, and irrelevant attributes in the original dataset. A new hybrid feature selection method is proposed using the Sine Cosine Algorithm (SCA) and Genetic Algorithm (GA), called SCAGA. Typically, optimization methods have two main search strategies; exploration of the search space and exploitation to determine the optimal solution. The proposed SCAGA resulted in better performance when balancing between exploitation and exploration strategies of the search space. The proposed SCAGA has also been evaluated using the following evaluation criteria: classification accuracy, worst fitness, mean fitness, best fitness, the average number of features, and standard deviation. Moreover, the maximum accuracy of a classification and the minimal features were obtained in the results. The results were also compared with a basic Sine Cosine Algorithm (SCA) and other related approaches published in literature such as Ant Lion Optimization and Particle Swarm Optimization. The comparison showed that the obtained results from the SCAGA method were the best overall the tested datasets from the UCI machine learning repository.},
journal = {Cluster Computing},
month = sep,
pages = {2161–2176},
numpages = {16},
keywords = {Optimization problems, Feature selection, Sine Cosine algorithm, Genetic algorithm, Hybridization}
}

@article{10.1504/IJIIDS.2008.021444,
author = {Bubak, Oldrich and Gomaa, Hassan},
title = {Applying software product line concepts in service orientation},
year = {2008},
issue_date = {November 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {4},
issn = {1751-5858},
url = {https://doi.org/10.1504/IJIIDS.2008.021444},
doi = {10.1504/IJIIDS.2008.021444},
abstract = {Today's competitive business environment commands innovation, increasingly shorter time-to-market and efficiency. Product line technology, pioneered in manufacturing, is increasingly finding its way to the software sector allowing companies to sustain growth and achieve market success. Regardless of the domain, however, information systems have been behind all facets of business operations. Here, the emerging service oriented architectures can help provide the answers to the need for agility, versatility and economies. This effort introduces the concepts of software product lines and service orientation and explores their parallels. Next, it attempts to show the applicability of software product line methods to service orientation. Finally, the work discusses the main obstacles on the road to realising the synergy between these cutting-edge technologies.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = nov,
pages = {383–396},
numpages = {14},
keywords = {SOA, SPL, enterprise architecture, reusable architecture, service orientation, service oriented architecture, software product lines}
}

@article{10.1016/j.asoc.2018.12.008,
author = {M\'{e}ndez, Jos\'{e} R. and Cotos-Ya\~{n}ez, Tom\'{a}s R. and Ruano-Ord\'{a}s, David},
title = {A new semantic-based feature selection method for spam filtering},
year = {2019},
issue_date = {Mar 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {76},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.12.008},
doi = {10.1016/j.asoc.2018.12.008},
journal = {Appl. Soft Comput.},
month = mar,
pages = {89–104},
numpages = {16},
keywords = {Feature selection methods, Text mining, Spam filtering, e-mail, Classification, Machine learning}
}

@inproceedings{10.1145/2791060.2791103,
author = {Mazo, Ra\'{u}l and Mu\~{n}oz-Fern\'{a}ndez, Juan C. and Rinc\'{o}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
title = {VariaMos: an extensible tool for engineering (dynamic) product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791103},
doi = {10.1145/2791060.2791103},
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {374–379},
numpages = {6},
keywords = {constraints, dynamic product line models, product line engineering, simulation, tool, variability},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1145/3408292,
author = {Wang, Jingwen and Jing, Xuyang and Yan, Zheng and Fu, Yulong and Pedrycz, Witold and Yang, Laurence T.},
title = {A Survey on Trust Evaluation Based on Machine Learning},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408292},
doi = {10.1145/3408292},
abstract = {Trust evaluation is the process of quantifying trust with attributes that influence trust. It faces a number of severe issues such as lack of essential evaluation data, demand of big data process, request of simple trust relationship expression, and expectation of automation. In order to overcome these problems and intelligently and automatically evaluate trust, machine learning has been applied into trust evaluation. Researchers have proposed many methods to use machine learning for trust evaluation. However, the literature still lacks a comprehensive literature review on this topic. In this article, we perform a thorough survey on trust evaluation based on machine learning. First, we cover essential prerequisites of trust evaluation and machine learning. Then, we justify a number of requirements that a sound trust evaluation method should satisfy, and propose them as evaluation criteria to assess the performance of trust evaluation methods. Furthermore, we systematically organize existing methods according to application scenarios and provide a comprehensive literature review on trust evaluation from the perspective of machine learning’s function in trust evaluation and evaluation granularity. Finally, according to the completed review and evaluation, we explore some open research problems and suggest the directions that are worth our research effort in the future.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {107},
numpages = {36},
keywords = {Trust evaluation, evaluation requirements, machine learning, performance metrics}
}

@article{10.1016/j.knosys.2019.104942,
author = {Sun, Lin and Wang, Lanying and Qian, Yuhua and Xu, Jiucheng and Zhang, Shiguang},
title = {Feature selection using Lebesgue and entropy measures for incomplete neighborhood decision systems},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {186},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.104942},
doi = {10.1016/j.knosys.2019.104942},
journal = {Know.-Based Syst.},
month = dec,
numpages = {19},
keywords = {Neighborhood rough sets, Feature selection, Neighborhood entropy, Lebesgue measure, Incomplete neighborhood decision systems}
}

@inproceedings{10.1145/3479645.3479668,
author = {Krisnabayu, Rifky Yunus and Ridok, Achmad and Setia Budi, Agung},
title = {Hepatitis Detection using Random Forest based on SVM-RFE (Recursive Feature Elimination) Feature Selection and SMOTE},
year = {2021},
isbn = {9781450384070},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479645.3479668},
doi = {10.1145/3479645.3479668},
abstract = {Hepatitis is a dangerous disease because it is a contagious disease and it is not easy to diagnose the disease early. Due to the difficulty of making an early diagnosis, the disease has the potential to become even more severe and increase the mortality rate. Therefore, it is necessary to develop predictive methods that can be used for the early detection of this disease. In this study, a hepatitis prediction method was developed using a random forest (RF) algorithm combined with feature selection using SVM-RFE (recursive feature elimination). Then, because the dataset used does not have a balanced distribution between classes, which is only 20% for the minority class, SMOTE (synthetic minority oversampling technique) is used to deal with this problem. To determine the best parameters in the model, Grid-Search is used as the tuning hyper-parameters. The classifier built with this approach produces 0.879 accuracy, 0.902 precision, and 0.966 ROC performance. This classifier proved to be better than the other classifiers.},
booktitle = {Proceedings of the 6th International Conference on Sustainable Information Engineering and Technology},
pages = {151–156},
numpages = {6},
keywords = {Hepatitis prediction, Random forest, Recursive feature elimination, Synthetic minority oversampling technique},
location = {Malang, Indonesia},
series = {SIET '21}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/2283516.2283660,
author = {Yang, Yi and Shen, Heng Tao and Ma, Zhigang and Huang, Zi and Zhou, Xiaofang},
title = {l2,1-norm regularized discriminative feature selection for unsupervised learning},
year = {2011},
isbn = {9781577355144},
publisher = {AAAI Press},
abstract = {Compared with supervised learning for feature selection, it is much more difficult to select the discriminative features in unsupervised learning due to the lack of label information. Traditional unsupervised feature selection algorithms usually select the features which best preserve the data distribution, e.g., manifold structure, of the whole feature set. Under the assumption that the class label of input data can be predicted by a linear classifier, we incorporate discriminative analysis and l2,1-norm minimization into a joint framework for unsupervised feature selection. Different from existing unsupervised feature selection algorithms, our algorithm selects the most discriminative feature subset from the whole feature set in batch mode. Extensive experiment on different data types demonstrates the effectiveness of our algorithm.},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Two},
pages = {1589–1594},
numpages = {6},
location = {Barcelona, Catalonia, Spain},
series = {IJCAI'11}
}

@inproceedings{10.1145/3318299.3318343,
author = {Khan, Younas and Qamar, Usman and Yousaf, Nazish and Khan, Aimal},
title = {Machine Learning Techniques for Heart Disease Datasets: A Survey},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318343},
doi = {10.1145/3318299.3318343},
abstract = {Heart Failure (HF) has been proven one of the leading causes of death that is why an accurate and timely prediction of HF risks is extremely essential. Clinical methods, for instance, angiography is the best and most effective way of diagnosing HF, however, studies show that it is not only costly but has side effects as well. Lately, machine learning techniques have been used for the stated purpose. This survey paper aims to present a systematic literature review based on 35 journal articles published since 2012, where state of the art machine learning classification techniques have been implemented on heart disease datasets. This study critically analyzes the selected papers and finds gaps in the existing literature and is assistive for researchers who intend to apply machine learning in medical domains, particularly on heart disease datasets. The survey finds out that the most popular classification techniques are Support Vector Machine, Neural Networks, and ensemble classifiers.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {27–35},
numpages = {9},
keywords = {Heart failure, deep learning, healthcare, heart diseases, machine learning, neural network, risk prediction},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.1504/ijbra.2020.109100,
author = {Barnali, Sahu and Satchidananda, Dehuri and Kumar, Jagadev Alok},
title = {Usage of ensemble model and genetic algorithm in pipeline for feature selection from cancer microarray data},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {3},
issn = {1744-5485},
url = {https://doi.org/10.1504/ijbra.2020.109100},
doi = {10.1504/ijbra.2020.109100},
abstract = {This paper proposes an ensemble of feature selection techniques with genetic algorithm (GA) in pipeline for selecting features from microarray data. The ensemble is a combination of filter and wrapper-based feature selection methods. In addition, GA in pipeline has been used for refinement of ensemble output to produce a non-local set of robust feature subset. An extensive computational experiment has been carried out on a prostate cancer dataset for validation of the method and comparison with group genetic algorithm (GGA). Finally, the resultant feature subsets of GA, GGA, and other constituents of the ensemble in standalone mode have been used for uncovering frequent patterns based on Apriori and FP-growth. The experimental study confirms that the proposed method gives classification accuracy of 100%, 98.34%, 98.02%, and 97% based on an ensemble of classifiers w. r. t. 5, 10, 15, and 20 features, respectively, vis-\`{a}-vis 92.34%, 90.34%, 86.54%, and 87.21% of GGA.},
journal = {Int. J. Bioinformatics Res. Appl.},
month = jan,
pages = {217–244},
numpages = {27},
keywords = {microarray data, differentially expressed genes, ensemble feature selection, Apriori, FP-growth}
}

@article{10.1016/j.cie.2019.06.052,
author = {Chen, Chong and Liu, Ying and Kumar, Maneesh and Qin, Jian and Ren, Yunxia},
title = {Energy consumption modelling using deep learning embedded semi-supervised learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2019.06.052},
doi = {10.1016/j.cie.2019.06.052},
journal = {Comput. Ind. Eng.},
month = sep,
pages = {757–765},
numpages = {9},
keywords = {Energy modelling, Intelligent manufacturing, Deep learning, Semi-supervised learning, Data mining}
}

@article{10.1016/j.procs.2021.06.033,
author = {Domashova, Jenny and Mikhailina, Natalia},
title = {Usage of machine learning methods for early detection of money laundering schemes},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {190},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.06.033},
doi = {10.1016/j.procs.2021.06.033},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {184–192},
numpages = {9},
keywords = {legalization of criminal profits, aml/cft, money laundering, machine learning methods, categorial features, classification, logistic regression, gradient boosting}
}

@inproceedings{10.5555/1885639.1885650,
author = {Svendsen, Andreas and Zhang, Xiaorui and Lind-Tviberg, Roy and Fleurey, Franck and Haugen, \O{}ystein and M\o{}ller-Pedersen, Birger and Olsen, G\o{}ran K.},
title = {Developing a software product line for train control: a case study of CVL},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents a case study of creating a software product line for the train signaling domain. The Train Control Language (TCL) is a DSL which automates the production of source code for computers controlling train stations. By applying the Common Variability Language (CVL), which is a separate and generic language to define variability on base models, we form a software product line of stations. We discuss the process and experience of using CVL to automate the production of three real train stations. A brief discussion about the verification needed for the generated products is also included.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {106–120},
numpages = {15},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1007/s10489-021-02210-y,
author = {Dornaika, F.},
title = {Flexible data representation with feature convolution for semi-supervised learning},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02210-y},
doi = {10.1007/s10489-021-02210-y},
abstract = {Data representation plays a crucial role in semi-supervised learning. This paper proposes a framework for semi-supervised data representation. It introduces a flexible nonlinear embedding model that integrates graph-based data convolutions. The proposed approach exploits structured data in order to estimate a nonlinear data representation as well as a linear transformation, enabling an inductive semi-supervised model. The introduced approach exploits data graphs at two different levels. First, it integrates manifold regularization that is encoded by the graph itself. Second, it optimizes a flexible linear transformation that maps the convolved data samples to their nonlinear representations. These convolved data are generated by the joint use of the graph and data. The proposed semi-supervised model overcomes some challenges related to some samples distributions in the original spaces. The proposed Graph Convolution based Semi-supervised Embedding (GCSE) provides flexible models which can improve both the data representation and the final performance of the learning model. Experiments are run on six image datasets for comparing the proposed approach with several state-of-art semi-supervised methods. These results show the effectiveness of the proposed framework.},
journal = {Applied Intelligence},
month = nov,
pages = {7690–7704},
numpages = {15},
keywords = {Graph-based embedding, Semi-supervised learning, Graph convolutions, Discriminant embedding, Pattern recognition}
}

@article{10.1007/s11042-016-4121-8,
author = {Zhu, Yonghua and Zhang, Xuejun and Wen, Guoqiu and He, Wei and Cheng, Debo},
title = {Double sparse-representation feature selection algorithm for classification},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {16},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-016-4121-8},
doi = {10.1007/s11042-016-4121-8},
abstract = {since amount of unlabeled and high-dimensional datasets need to be preprocessed, unsupervised learning plays a more and more important role in machine learning field. This paper proposed a novel unsupervised feature selection algorithm that can select informative features from dataset without label, by mixing two sparse representation and self-representation loss function into a unified framework. That is, we use self-representation loss function to represent every feature with remainder features and achieve minimum reconstruction mirror, and then utilize l2\'{z},\'{z}1-norm regularization term and l1-norm regularization term simultaneously to enforce coefficient matrix to be sparse, such that filter redundant and irrelative features in order to conduct feature selection, where l2\'{z},\'{z}1-norm regularization can enforce group sparsity while l1-norm regularization enforce element sparsity. By this way that utilize both of sparse representation terms, we can choose representative features more accurately. At final, we feed reduced data into support vector machine (SVM) to conduct classification accuracy, which is main assessment criteria to validate performance of algorithm. Extensive experiments on synthetic datasets and real-world datasets have exhibited that our proposed method outperform most of common-used methods, such as PCA, LPP and so on.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {17525–17539},
numpages = {15},
keywords = {Feature selection, Joint sparse learning, Self-representation}
}

@article{10.1007/s11265-018-1355-x,
author = {Du, Wei and Phlypo, Ronald and Adal\i{}, T\"{u}lay},
title = {Adaptive Feature Selection and Feature Fusion for Semi-supervised Classification},
year = {2019},
issue_date = {May       2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {91},
number = {5},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-018-1355-x},
doi = {10.1007/s11265-018-1355-x},
abstract = {Labeling of data is often difficult, expensive, and time consuming since efforts of experienced human annotators are required, and often we have large number of samples and noisy data. Co-training is a practical and powerful semi-supervised learning method as it yields high classification accuracy with a training data set containing only a small set of labeled data. For successful co-training performance, two important conditions need to be satisfied for the features: diversity and sufficiency. In this paper, we propose a novel mutual information based approach inspired by the idea of dependent component analysis to achieve feature splits that are maximally independent between-subsets (diverse) or within-subsets (sufficient). In addition, we demonstrate the application of the method to a real world problem, classification of laser tread mapping tire data. We introduce several features that are designed to highlight physical characteristics of the tire data, as well as local or global descriptors, such as histograms, gradients, or representations in other domains. Results from both simulations and tire image classification confirm that co-training with the proposed feature set and feature splits consistently yields higher accuracy than supervised classification, when using only a small set of labeled training data is available. The proposed method presents a very promising complement to time consuming and subjective expert labeling of data, reducing expert efforts to a minimum. Further results show that by using a probabilistic multi-layer perceptron classifier as the base learner in co-training, our method leads to very meaningful continuous measures for the progression of irregular wear on tire surface.},
journal = {J. Signal Process. Syst.},
month = may,
pages = {521–537},
numpages = {17},
keywords = {Co-training, Feature extraction, Feature fusion, Image classification, LTM tire images, Semi-supervised learning}
}

@article{10.1016/j.patcog.2016.09.034,
author = {Wade, Benjamin S.C. and Joshi, Shantanu H. and Gutman, Boris A. and Thompson, Paul M.},
title = {Machine learning on high dimensional shape data from subcortical brain surfaces: A comparison of feature selection and classification methods},
year = {2017},
issue_date = {Mar 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.09.034},
doi = {10.1016/j.patcog.2016.09.034},
journal = {Pattern Recogn.},
month = mar,
pages = {731–739},
numpages = {9},
keywords = {HDSD, RRF, NFS, SVM, NB, AD, MCI, Feature selection, Shape analysis, Biomarker, Brain, Subcortical}
}

@article{10.1007/s10489-019-01431-6,
author = {Wang, Chenxi and Lin, Yaojin and Liu, Jinghua},
title = {Feature selection for multi-label learning with missing labels},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01431-6},
doi = {10.1007/s10489-019-01431-6},
abstract = {In multi-label learning, feature selection is a non-ignorable preprocessing step which can alleviate the negative effect of high-dimensionality. To address this problem, a number of effective information theory based feature selection algorithms for multi-label learning are proposed. However, these existing algorithms assume that the label space of multi-label training data is complete. In practice, the standpoint does not always hold true, due to the ambiguity among class labels or the cost effort to fully annotate instances. In this paper, we first define the new concepts of multi-label information entropy and multi-label mutual information. Then, feature redundancy, feature independence, and feature interaction are defined, respectively. In which, feature interaction is used to select more valuable features which may be ignored due to the incomplete label space. Moreover, a multi-label feature selection method with missing labels is proposed. Finally, extensive experiments conducted on eight publicly available data sets verify the effectiveness of the proposed algorithm via comparing it with state-of-the-art methods.},
journal = {Applied Intelligence},
month = aug,
pages = {3027–3042},
numpages = {16},
keywords = {Feature interaction, Feature selection, Missing labels, Multi-label learning, Neighborhood mutual information}
}

@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Software defect prediction, Metaheuristic feature selection, Whale optimization algorithm, Convolutional neural network, Kernel extreme learning machine}
}

@inproceedings{10.1145/3461001.3471145,
author = {Sundermann, Chico and Feichtinger, Kevin and Engelhardt, Dominik and Rabiser, Rick and Th\"{u}m, Thomas},
title = {Yet another textual variability language? a community effort towards a unified language},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471145},
doi = {10.1145/3461001.3471145},
abstract = {Variability models are commonly used to model commonalities and variability in a product line. There is a large variety of textual formats to represent and store variability models. This variety causes overhead to researchers and practitioners as they frequently need to translate models. The MODEVAR initiative consists of dozens of researchers and aims to find a unified language for variability modeling. In this work, we describe the cooperative development of a textual variability language. We evaluate preferences of the community regarding properties of existing formats and applications for an initial design of a unified variability language. Then, we examine the acceptance of the community for our proposal. The results indicate that our proposal is a promising start towards a unified variability language instead of yet another language. We envision that the community applies our language proposal in teaching, research prototypes, and industrial applications to further evolve the design and then ultimately reach a unified language.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {136–147},
numpages = {12},
keywords = {exchange format, software product lines, unified language, variability language, variability modeling},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1007/978-3-030-95388-1_37,
author = {Chen, Guo and Zheng, Junyao and Yang, Shijun and Zhou, Jieying and Wu, Weigang},
title = {FSAFA-stacking2: An Effective Ensemble Learning Model for Intrusion Detection with Firefly Algorithm Based Feature Selection},
year = {2021},
isbn = {978-3-030-95387-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-95388-1_37},
doi = {10.1007/978-3-030-95388-1_37},
abstract = {This paper presents a two-layer ensemble learning model stacking2 based on the Stacking framework to deal with the problems of lack of generalization ability and low detection rate of single model intrusion detection system. The stacking2 uses SAMME, GBDT, and RF to generate the primary learner in the first layer and constructs the meta learner using the logistic regression algorithm in the second layer. The meta learner learns from the class probability outputs produced by the primary learner. In order to solve “the curse of dimensionality” of intrusion detection dataset, this paper proposes the feature selection approach based on firefly algorithm (FSAFA), which is used to select the optimal feature subsets. Based on the selected optimal feature subsets, the training set and test set are reconstructed and then applied to stacking2. As a result, a FSAFA based stacking2 intrusion detection model is proposed. The UNSW-NB15 and NSL-KDD datasets are chosen to verify the effectiveness of the proposed model. The experiment results show that the stacking2 intrusion detection model has better generalization ability than the individual learner based intrusion detection models. Compared with other typical algorithms, the FSAFA based stacking2 intrusion detection model has good performance in detection rate.},
booktitle = {Algorithms and Architectures for Parallel Processing: 21st International Conference, ICA3PP 2021, Virtual Event, December 3–5, 2021, Proceedings, Part II},
pages = {555–570},
numpages = {16},
keywords = {Intrusion detection, Ensemble learning, Stacking, Feature selection, Firefly algorithm}
}

@inproceedings{10.1145/3387904.3389257,
author = {Raab, Markus and Denner, Bernhard and Hahnenberg, Stefan and Cito, J\"{u}rgen},
title = {Unified Configuration Setting Access in Configuration Management Systems},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389257},
doi = {10.1145/3387904.3389257},
abstract = {The behavior of software is often governed by a large set of configuration settings, distributed over several stacks in the software system. These settings are often manifested as plain text files that exhibit different formats and syntax. Configuration management systems are introduced to manage the complexity of provisioning and distributing configuration in large scale software. Globally patching configuration settings in these systems requires, however, introducing text manipulation or external templating mechanisms, that paradoxically lead to increased complexity and, eventually, to misconfigurations. These issues manifest through crashes or bugs that are often only discovered at runtime. We introduce a framework called Elektra, which integrates a centralized configuration space into configuration management systems to avoid syntax errors and avert the overriding of default values, to increase developer productivity. Elektra enables mounting different configuration files into a common, globally shared data structure to abstract away from the intricate details of file formats and configuration syntax and introduce a unified way to specify and patch configuration settings as key/value pairs. In this work, we integrate Elektra in the configuration management tool Puppet. Additionally, we present a user study with 14 developers showing that Elektra enables significant productivity improvements over existing configuration management concepts. Our study participants performed significantly faster using Elektra in solving three representative scenarios that involve configuration manipulation, compared to other general-purpose configuration manipulation methods.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {331–341},
numpages = {11},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3319619.3326897,
author = {Raju, Godwin and Zavarsky, Pavol and Makanju, Adetokunbo and Malik, Yasir},
title = {Vulnerability assessment of machine learning based malware classification models},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326897},
doi = {10.1145/3319619.3326897},
abstract = {The primary focus of the machine learning model is to train a system to achieve self-reliance. However, due to the absence of the inbuilt security functions the learning phase itself is not secured which allows attacker to exploit the security vulnerabilities in the machine learning model. When a malicious adversary manipulates the input data, it exploits vulnerabilities of machine learning algorithms which can compromise the entire system. In this research study, we are conducting a vulnerability assessment of the malware classification model by injecting the datasets with an adversarial example to degrade the quality of classification obtained currently by a trained model. The objective is to find the security gaps that are exploitable in the model. The vulnerability assessment is done by introducing the malware classification model to an AML environment using the Black-Box attack. The simulation provided an insight into the inputs injected into the classifiers and proves the inherent security vulnerability exists in the classification model.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1615–1618},
numpages = {4},
keywords = {adversarial machine learning, black-box attack, intrusion detection system, machine learning, malware classification, poisoning attack},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1016/j.infsof.2006.05.004,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {Managing the business of software product line: An empirical investigation of key business factors},
year = {2007},
issue_date = {February, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.004},
doi = {10.1016/j.infsof.2006.05.004},
abstract = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper's main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {194–208},
numpages = {15},
keywords = {Key business factor, Management, Marketing strategy, Software engineering economics, Software product line, Strategic planning}
}

@article{10.1007/s11042-020-09740-6,
author = {Rezk, Nermeen Gamal and Hemdan, Ezz El-Din and Attia, Abdel-Fattah and El-Sayed, Ayman and El-Rashidy, Mohamed A.},
title = {An efficient IoT based smart farming system using machine learning algorithms},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {1},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09740-6},
doi = {10.1007/s11042-020-09740-6},
abstract = {This paper suggests an IoT based smart farming system along with an efficient prediction method called WPART based on machine learning techniques to predict crop productivity and drought for proficient decision support making in IoT based smart farming systems. The crop productivity and drought predictions is very important to the farmers and agriculture’s executives, which greatly help agriculture-affected countries around the world. Drought prediction plays a significant role in drought early warning to mitigate its impacts on crop productivity, drought prediction research aims to enhance our understanding of the physical mechanism of drought and improve predictability skill by taking full advantage of sources of predictability. In this work, an intelligent method based on the blend of a wrapper feature selection approach, and PART classification technique is proposed for crop productivity and drought predicting. Five datasets are used for estimating the proposed method. The results indicated that the projected method is robust, accurate, and precise to classify and predict crop productivity and drought in comparison with the existing techniques. From the results, the proposed method proved to be most accurate in providing drought prediction as well as the productivity of crops like Bajra, Soybean, Jowar, and Sugarcane. The WPART method attains the maximum accuracy compared to the existing supreme standard algorithms, it is obtained up to 92.51%, 96.77%, 98.04%, 96.12%, and 98.15% for the five datasets for drought classification, and crop productivity respectively. Likewise, the proposed method outperforms existing algorithms with precision, sensitivity, and F Score metrics.},
journal = {Multimedia Tools Appl.},
month = jan,
pages = {773–797},
numpages = {25},
keywords = {Machine learning, Internet of things, Smart farming, Prediction, Drought, Crop productivity, And, Feature selection}
}

@inproceedings{10.1145/3307630.3342413,
author = {Arcaini, Paolo and Gargantini, Angelo and Radavelli, Marco},
title = {A Process for Fault-Driven Repair of Constraints Among Features},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342413},
doi = {10.1145/3307630.3342413},
abstract = {The variability of a Software Product Line is usually both described in the problem space (by using a variability model) and in the solution space (i.e., the system implementation). If the two spaces are not aligned, wrong decisions can be done regarding the system configuration. In this work, we consider the case in which the variability model is not aligned with the solution space, and we propose an approach to automatically repair (possibly) faulty constraints in variability models. The approach takes as input a variability model and a set of combinations of features that trigger conformance faults between the model and the real system, and produces the repaired set of constraints as output. The approach consists of three major phases. First, it generates a test suite and identifies the condition triggering the faults. Then, it modifies the constraints of the variability model according to the type of faults. Lastly, it uses a logic minimization method to simplify the modified constraints. We evaluate the process on variability models of 7 applications of various sizes. An empirical analysis on these models shows that our approach can effectively repair constraints among features in an automated way.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {73–81},
numpages = {9},
keywords = {automatic repair, fault, system evolution, variability model},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10846-020-01206-z,
author = {Kottath, Rahul and Poddar, Shashi and Sardana, Raghav and Bhondekar, Amol P and Karar, Vinod},
title = {Mutual Information Based Feature Selection for Stereo Visual Odometry},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {100},
number = {3–4},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-020-01206-z},
doi = {10.1007/s10846-020-01206-z},
abstract = {Visual odometry (VO) is one of the promising techniques that estimates pose using the camera and does not necessarily require other sensor aiding. With increasing automation and the use of miniaturized systems such as mobile devices, wearable gadgets, &amp; gaming consoles, demand for efficient algorithms have risen. In this paper, an attempt is made to remove the redundant features from the VO pipeline that do not have a significant effect on the estimation process. A probabilistic approach based on fast mutual information (MI) computation is suggested here as the basis for removing features. The MI value acts as a beacon for selecting distinct features while eliminating the redundant ones, thus improving the overall system speed and reducing storage requirements. The proposed MI-based feature selection framework for VO has been experimented on the KITTI vision benchmark suite and EuRoC MAV datasets available publicly. The estimated trajectory results have shown that the proposed technique is better in terms of computational efficiency and has similar accuracy as compared to the normal VO pipeline. Further investigations have also been carried out over the VSLAM framework to test its applicability in a real-time system.},
journal = {J. Intell. Robotics Syst.},
month = dec,
pages = {1559–1568},
numpages = {10},
keywords = {Visual odometry, Feature selection, Mutual information, Motion estimation}
}

@article{10.1016/j.infsof.2021.106652,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Zhang, Tao and Yang, Dan and Li, Wei},
title = {A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106652},
doi = {10.1016/j.infsof.2021.106652},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {16},
keywords = {Crash localization, Stack trace, Feature selection, Empirical study}
}

@inproceedings{10.1145/3369114.3371675,
author = {Alshawaqfeh, Mustafa and Gharaibeh, Ammar and Wajid, Bilal},
title = {A Hybrid Feature Selection Method for Classifying Metagenomic Data in Relation to Inflammatory Bowel Disease},
year = {2020},
isbn = {9781450372534},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369114.3371675},
doi = {10.1145/3369114.3371675},
abstract = {Due to the recent advances in high throughput metagenomic sequencing technologies, Microbial abundance profiles of environmental samples have become publicly available. Increasing number of metagenomic studies has associated the imbalance of bacterial abundance to health and disase state of the host. This suggests utilizing the bacterial profiles as a diagnostic tool to identify the bacterial-related disease state of individuals. However, the high dimensional nature of metagenomic datasets renders this process a challenging task. Therefore, an efficient framework that enables accurate classification of metagenomic samples belonging to different classes is of central important. In this work, a hybrid feature selection technique that combines the advantages of filter and wrapper feature selection algorithms is proposed. The experimental results demonstrate that the proposed algorithm outperforms widely used feature selection techniques in terms of classification accuracy and provide a significant reduction in the computation time.},
booktitle = {Proceedings of the 3rd International Conference on Advances in Artificial Intelligence},
pages = {86–89},
numpages = {4},
keywords = {Classification, Hybrid Feature Selection, Metagenomics},
location = {Istanbul, Turkey},
series = {ICAAI '19}
}

@inproceedings{10.5555/1885639.1885663,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara},
title = {Architecture-based unit testing of the flight software product line},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software (CFS) product line team at the NASA GSFC. The goal of the analysis is to understand, review, and recommend strategies for improving the existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other product line teams for their unit testing. The CFS unit testing framework is designed and implemented as a set of variation points, and thus testing support is built into the product line architecture. The analysis found that the CFS unit testing approach has many practical and good solutions that are worth considering when deciding how to design the testing architecture for a product line, which are documented in this paper along with some suggested improvements.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {256–270},
numpages = {15},
keywords = {coverage, flight software, function hook, implemented architecture, mock, unit testing},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.asoc.2021.107729,
author = {Manikandan, G. and Abirami, S.},
title = {An efficient feature selection framework based on information theory for high dimensional data},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {111},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107729},
doi = {10.1016/j.asoc.2021.107729},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {25},
keywords = {Feature selection, Feature fusion, Feature relevancy, Feature redundancy, Microarray, Bioinformatics, High dimensional data, Mutual information}
}

@article{10.1016/j.neunet.2019.04.015,
author = {Tang, Chang and Bian, Meiru and Liu, Xinwang and Li, Miaomiao and Zhou, Hua and Wang, Pichao and Yin, Hailin},
title = {Unsupervised feature selection via latent representation learning and manifold regularization},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {117},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.04.015},
doi = {10.1016/j.neunet.2019.04.015},
journal = {Neural Netw.},
month = sep,
pages = {163–178},
numpages = {16},
keywords = {Unsupervised feature selection, Latent representation learning, Manifold regularization, Non-negative matrix factorization, Local structure preservation}
}

@article{10.1145/3340848,
author = {Xue, Yu and Xue, Bing and Zhang, Mengjie},
title = {Self-Adaptive Particle Swarm Optimization for Large-Scale Feature Selection in Classification},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3340848},
doi = {10.1145/3340848},
abstract = {Many evolutionary computation (EC) methods have been used to solve feature selection problems and they perform well on most small-scale feature selection problems. However, as the dimensionality of feature selection problems increases, the solution space increases exponentially. Meanwhile, there are more irrelevant features than relevant features in datasets, which leads to many local optima in the huge solution space. Therefore, the existing EC methods still suffer from the problem of stagnation in local optima on large-scale feature selection problems. Furthermore, large-scale feature selection problems with different datasets may have different properties. Thus, it may be of low performance to solve different large-scale feature selection problems with an existing EC method that has only one candidate solution generation strategy (CSGS). In addition, it is time-consuming to find a suitable EC method and corresponding suitable parameter values for a given large-scale feature selection problem if we want to solve it effectively and efficiently. In this article, we propose a self-adaptive particle swarm optimization (SaPSO) algorithm for feature selection, particularly for large-scale feature selection. First, an encoding scheme for the feature selection problem is employed in the SaPSO. Second, three important issues related to self-adaptive algorithms are investigated. After that, the SaPSO algorithm with a typical self-adaptive mechanism is proposed. The experimental results on 12 datasets show that the solution size obtained by the SaPSO algorithm is smaller than its EC counterparts on all datasets. The SaPSO algorithm performs better than its non-EC and EC counterparts in terms of classification accuracy not only on most training sets but also on most test sets. Furthermore, as the dimensionality of the feature selection problem increases, the advantages of SaPSO become more prominent. This highlights that the SaPSO algorithm is suitable for solving feature selection problems, particularly large-scale feature selection problems.},
journal = {ACM Trans. Knowl. Discov. Data},
month = sep,
articleno = {50},
numpages = {27},
keywords = {Feature selection, classification, large-scale, particle swarm optimization, self-adaptive}
}

@article{10.1016/j.compbiomed.2020.104142,
author = {Salmanpour, Mohammad R. and Shamsaei, Mojtaba and Saberi, Abdollah and Hajianfar, Ghasem and Soltanian-Zadeh, Hamid and Rahmim, Arman},
title = {Robust identification of Parkinson's disease subtypes using radiomics and hybrid machine learning},
year = {2021},
issue_date = {Feb 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104142},
doi = {10.1016/j.compbiomed.2020.104142},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {14},
keywords = {Unsupervised clustering, Machine learning, Parkinson's disease (PD), Radiomics features, DAT SPECT Imaging, Disease subtypes}
}

@article{10.5555/3288251.3288289,
author = {Fang, Yue and Li, Yangding and Lei, Cong and Li, Yonggang and Deng, Xuelian},
title = {Hypergraph expressing low-rank feature selection algorithm},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Dimensionality reduction has been attracted extensive attention in machine learning. It usually includes two types: feature selection and subspace learning. Previously, many researchers have demonstrated that the dimensionality reduction is meaningful for real applications. Unfortunately, a large mass of these works utilize the feature selection and subspace learning independently. This paper explores a novel supervised feature selection algorithm by considering the subspace learning. Specifically, this paper employs an ℓ2,1?norm and an ℓ2,p?norm regularizers, respectively, to conduct sample denoising and feature selection via exploring the correlation structure of data. Then this paper uses two constraints (i.e. hypergraph and low-rank) to consider the local structure and the global structure among the data, respectively. Finally, this paper uses the optimizing framework to iteratively optimize each parameter while fixing the other parameter until the algorithm converges. A lot of experiments show that our new supervised feature selection method can get great results on the eighteen public data sets.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29551–29572},
numpages = {22},
keywords = {Feature selection, Hypergraph, LowRank}
}

@article{10.1016/j.patcog.2016.06.009,
author = {Zhang, Zhihong and Bai, Lu and Liang, Yuanheng and Hancock, Edwin},
title = {Joint hypergraph learning and sparse regression for feature selection},
year = {2017},
issue_date = {Mar 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.06.009},
doi = {10.1016/j.patcog.2016.06.009},
journal = {Pattern Recogn.},
month = mar,
pages = {291–309},
numpages = {19},
keywords = {Feature selection, Hypergraph learning, Sparse regression}
}

@article{10.1287/mnsc.2018.3255,
author = {Yoganarasimhan, Hema},
title = {Search Personalization Using Machine Learning},
year = {2020},
issue_date = {March 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {66},
number = {3},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.2018.3255},
doi = {10.1287/mnsc.2018.3255},
abstract = {Firms typically use query-based search to help consumers find information/products on their websites. We consider the problem of optimally ranking a set of results shown in response to a query. We propose a personalized ranking mechanism based on a user’s search and click history. Our machine-learning framework consists of three modules: (a) feature generation, (b) normalized discounted cumulative gain–based LambdaMART algorithm, and (c) feature selection wrapper. We deploy our framework on large-scale data from a leading search engine using Amazon EC2 servers and present results from a series of counterfactual analyses. We find that personalization improves clicks to the top position by 3.5% and reduces the average error in rank of a click by 9.43% over the baseline. Personalization based on short-term history or within-session behavior is shown to be less valuable than long-term or across-session personalization. We find that there is significant heterogeneity in returns to personalization as a function of user history and query type. The quality of personalized results increases monotonically with the length of a user’s history. Queries can be classified based on user intent as transactional, informational, or navigational, and the former two benefit more from personalization. We also find that returns to personalization are negatively correlated with a query’s past average performance. Finally, we demonstrate the scalability of our framework and derive the set of optimal features that maximizes accuracy while minimizing computing time.This paper was accepted by Juanjuan Zhang, marketing.},
journal = {Manage. Sci.},
month = mar,
pages = {1045–1070},
numpages = {26},
keywords = {marketing, online search, personalization, machine learning, search engines}
}

@article{10.1016/j.jbi.2021.103693,
author = {Li, Jun and Yuan, Pei and Hu, Xiaojuan and Huang, Jingbin and Cui, Longtao and Cui, Ji and Ma, Xuxiang and Jiang, Tao and Yao, Xinghua and Li, Jiacai and Shi, Yulin and Bi, Zijuan and Wang, Yu and Fu, Hongyuan and Wang, Jue and Lin, Yenting and Pai, ChingHsuan and Guo, Xiaojing and Zhou, Changle and Tu, Liping and Xu, Jiatuo},
title = {A tongue features fusion approach to predicting prediabetes and diabetes with machine learning},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {115},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103693},
doi = {10.1016/j.jbi.2021.103693},
journal = {J. of Biomedical Informatics},
month = mar,
numpages = {13},
keywords = {Traditional Chinese medicine, Tongue diagnosis, Diabetics, Prediabetics, Machine learning, Deep learning, Features fusion, Noninvasive, Risk prediction model, TDAS, GA, XGBT}
}

@inproceedings{10.1145/1159733.1159762,
author = {Denger, Christian and Kolb, Ronny},
title = {Testing and inspecting reusable product line components: first empirical results},
year = {2006},
isbn = {1595932186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1159733.1159762},
doi = {10.1145/1159733.1159762},
abstract = {In recent years, product line development has increasingly received attention in industry as it enables software-developing organizations to reduce both cost and time of developing and maintaining increasingly complex systems as well as to address the demands for individually customized products. Successful product line development requires high quality of reusable artifacts in order to achieve the promised benefits. The unique issues of quality assurance in the context of systematic reuse, however, have not been quantitatively investigated so far. This paper describes a first empirical study comparing the two defect detection techniques, code inspections and functional testing, in the context of product line development. The primary goal of the study was to initially investigate the defect finding potential of the techniques on reusable software components with common and variant features. The major findings of the study are that the two techniques identified different types of defects on variants of a reusable component. Inspections are on average 66.39% more effective and need on average 36.84% less effort to detect a defect We found that both the testing and inspection techniques applied in the experiment were ineffective in identifying variant-specific defects. Overall, the results indicate that the standard quality assurance techniques seem to be insufficient to address special characteristics of reusable components.},
booktitle = {Proceedings of the 2006 ACM/IEEE International Symposium on Empirical Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {controlled experiment, functional testing, inspection, quality assurance, reusable components, software product line},
location = {Rio de Janeiro, Brazil},
series = {ISESE '06}
}

@article{10.1504/ijics.2021.112209,
author = {Jayasimha, Yenumaladoddi and Reddy, R. Venkata Siva},
title = {A facial expression recognition model using hybrid feature selection and support vector machines},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {14},
number = {1},
issn = {1744-1765},
url = {https://doi.org/10.1504/ijics.2021.112209},
doi = {10.1504/ijics.2021.112209},
abstract = {Facial expression recognition is a challenging issue in the field of computer vision. Due to the limited feature extraction capability of a single feature descriptor, in this paper, a hybrid feature extraction is utilised. The proposed methodology includes local and global feature extractions that is done by local binary pattern (LBP) and histogram orientation gradient (HOG) respectively. Before applying the feature extraction process, pre-processing and face detection is applied on the face image to extract the useful features. The Viola and Jones algorithm is utilised for face detection and the hybrid Laplacian of Gaussian (HLOG) is used for pre-processing stage. The orthogonal local preserving projection (OLPP)-based dimension reduction algorithm is applied to the extracted features to minimise the computational complexity of the classification algorithm. The SVM classification algorithm is utilised for identifying the facial expression. Here, standard CK+ facial expression dataset is used for evaluating the proposed methodology. The proposed methodology performed well in terms of accuracy compared to the existing PCA + Gabor and PCA + LBP methodology.},
journal = {Int. J. Inf. Comput. Secur.},
month = jan,
pages = {79–97},
numpages = {18},
keywords = {facial expression recognition, support vector machine, local binary pattern, LBP, histogram orientation gradient, hybrid Laplacian of Gaussian, HLOG, orthogonal local preserving projection, OLPP}
}

@inproceedings{10.1145/2791060.2791095,
author = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Test control algorithms for the validation of cyber-physical systems product lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791095},
doi = {10.1145/2791060.2791095},
abstract = {Cyber-Physical Systems (CPSs) product lines appear in a wide range of applications of different domains (e.g., car's doors' windows, doors of a lift, etc.). The variability of these systems is large and as a result they can be configured into plenty of configurations. Testing each of the configurations can be time consuming as not only software has to be simulated, but also the hardware and the physical layer of the CPS, which is often modelled with complex mathematical models. Choosing the adequate test control strategy is critical when testing CPSs product lines. This paper presents a set of test control algorithms organized in an architecture of three layers (domain, application and simulation) for testing CPSs product lines. An illustrative example of a CPS product line is presented and three experiments are conducted to measure the performance of the proposed test control algorithms. We conclude that test scheduling and test suite minimization significantly help to reduce the overall test costs while preserving the test quality in CPSs product lines. In addition, we conclude that knowing the results of the previously tested configurations permits reducing the time for the detection of anomalous designs.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {273–282},
numpages = {10},
keywords = {cyber-physical systems product lines, product line engineering, testing, validation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-030-85462-1_15,
author = {Liu, Xiuyu and Li, Yanyi},
title = {Research on Classification Method of Medium Resolution Remote Sensing Image Based on Machine Learning},
year = {2021},
isbn = {978-3-030-85461-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-85462-1_15},
doi = {10.1007/978-3-030-85462-1_15},
abstract = {Surface information extraction is an important link in geographic situation monitoring and environmental protection and plays an important role in the global sustainable development strategy. In this paper, the resolution remote sensing image in Landsat8 OLI was selected as the main data source. Aiming at the problem of the lack of classified sample data, the global land cover data in 2015 and 2017 were optimized and treated as the prior knowledge of classification. The maximum likelihood method, Support Vector Machine (SVM) and Random Forest (RF) Machine learning methods, as well as deep learning methods based on VGGNET-16 and RESNET-18 models, were used to compare and study surface information extraction methods in the Yellow River Delta region. The results show that the above method is highly feasible. Based on the feature optimization, the overall classification accuracy of RF and SVM models in the machine learning method is high, and the classification accuracy of RF and SVM models is up to 87.3% and 86%. In the deep learning algorithm, the classification accuracy of VGGNET-16 and RESNET-18 models is greatly improved compared with the machine learning method. The classification accuracy of RESNET-18 is up to 94.1%, and the Kappa coefficient is 0.91. The research method in this paper has good applicability and popularization value in the classification of medium resolution remote sensing objects.},
booktitle = {Spatial Data and Intelligence: Second International Conference, SpatialDI 2021, Hangzhou, China, April 22–24, 2021, Proceedings},
pages = {164–173},
numpages = {10},
keywords = {Remote sensing classification, Surface information extraction, Machine learning, Depth learning, Landsat8 OLI},
location = {Hangzhou, China}
}

@article{10.1007/s11063-019-10144-3,
author = {Gu, Xiangyuan and Guo, Jichang and Xiao, Lijun and Ming, Tao and Li, Chongyi},
title = {A Feature Selection Algorithm Based on Equal Interval Division and Minimal-Redundancy–Maximal-Relevance},
year = {2020},
issue_date = {Apr 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {2},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-019-10144-3},
doi = {10.1007/s11063-019-10144-3},
abstract = {Minimal-redundancy–maximal-relevance (mRMR) algorithm is a typical feature selection algorithm. To select the feature which has minimal redundancy with the selected features and maximal relevance with the class label, the objective function of mRMR subtracts the average value of mutual information between features from mutual information between features and the class label, and selects the feature with the maximum difference. However, the problem is that the feature with the maximum difference is not always the feature with minimal redundancy maximal relevance. To solve the problem, the objective function of mRMR is first analyzed and a constraint condition that determines whether the objective function can guarantee the effectiveness of the selected features is achieved. Then, for the case where the objective function is not accurate, an idea of equal interval division is proposed and combined with ranking to process the interval of mutual information between features and the class label, and that of the average value of mutual information between features. Finally, a feature selection algorithm based on equal interval division and minimal-redundancy–maximal-relevance (EID–mRMR) is proposed. To validate the performance of EID–mRMR, we compare it with several incremental feature selection algorithms based on mutual information and other feature selection algorithms. Experimental results demonstrate that the EID–mRMR algorithm can achieve better feature selection performance.},
journal = {Neural Process. Lett.},
month = apr,
pages = {1237–1263},
numpages = {27},
keywords = {Minimal-redundancy–maximal-relevance, Equal interval division, Mutual information, Feature selection}
}

@article{10.3233/JIFS-189520,
author = {Guangxu, Yu and Ramachandran, Varatharajan},
title = {Research on computer network information security based on improved machine learning},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189520},
doi = {10.3233/JIFS-189520},
abstract = {The 21st century is an era of rapid development of the Internet. Internet technology is widely used in various fields. With the rapid development of network, the importance of network information security is also highlighted. The traditional network information security technology has been difficult to ensure the security of network information. Therefore, we mainly study the application of machine learning feature extraction method in situational awareness system. A feature selection method based on machine learning is proposed to extract situational features.By analyzing whether the background of network information is safe or not, and according to the current research situation at home and abroad and the trend of Internet development, this paper tries out the practical application of machine learning feature extraction method in a certain perception system. Based on the above points, a selection method based on machine learning is proposed to extract situational features. The accuracy and timeliness of situational awareness system detection are seriously affected by the high dimension, noise and redundant features of massive network traffic data.Therefore, it is of great value to further study network intrusion detection technology on the basis of machine learning.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6889–6900},
numpages = {12},
keywords = {Network information security, security potential perception, intrusion security detection, machine learning}
}

@inproceedings{10.5555/3504035.3504307,
author = {Guo, Jun and Zhu, Wenwu},
title = {Dependence guided unsupervised feature selection},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {In the past decade, various sparse learning based unsupervised feature selection methods have been developed. However, most existing studies adopt a two-step strategy. i.e., selecting the top-m features according to a calculated descending order and then performing K-means clustering, resulting in a group of sub-optimal features. To address this problem, we propose a Dependence Guided Unsupervised Feature Selection (DGUFS) method to select features and partition data in a joint manner. Our proposed method enhances the interdependence among original data, cluster labels, and selected features. In particular, a projection-free feature selection model is proposed based on l2,0-norm equality constraints. We utilize the learned cluster labels to fill in the information gap between original data and selected features. Two dependence guided terms are consequently proposed for our model. More specifically, one term increases the dependence of desired cluster labels on original data, while the other term maximizes the dependence of selected features on cluster labels to guide the process of feature selection. Last but not least, an iterative algorithm based on Alternating Direction Method of Multipliers (ADMM) is designed to solve the constrained minimization problem efficiently. Extensive experiments on different datasets consistently demonstrate that our proposed method significantly outperforms state-of-the-art baselines.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {272},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1016/j.ipl.2017.09.005,
author = {Panday, Deepak and Cordeiro de Amorim, Renato and Lane, Peter},
title = {Feature weighting as a tool for unsupervised feature selection},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0020-0190},
url = {https://doi.org/10.1016/j.ipl.2017.09.005},
doi = {10.1016/j.ipl.2017.09.005},
abstract = {Feature selection is a popular data pre-processing step. The aim is to remove some of the features in a data set with minimum information loss, leading to a number of benefits including faster running time and easier data visualisation. In this paper we introduce two unsupervised feature selection algorithms. These make use of a cluster-dependent feature-weighting mechanism reflecting the within-cluster degree of relevance of a given feature. Those features with a relatively low weight are removed from the data set. We compare our algorithms to two other popular alternatives using a number of experiments on both synthetic and real-world data sets, with and without added noisy features. These experiments demonstrate our algorithms clearly outperform the alternatives. We generate cluster-dependent feature weights reflecting the relevance of features.Features with a relatively low weight are removed from a data set.Our methods outperform other popular alternatives in synthetic and real-world data.},
journal = {Inf. Process. Lett.},
month = jan,
pages = {44–52},
numpages = {9},
keywords = {Algorithms, Clustering, Feature selection}
}

@article{10.1016/j.knosys.2018.01.009,
author = {Tang, Chang and Liu, Xinwang and Li, Miaomiao and Wang, Pichao and Chen, Jiajia and Wang, Lizhe and Li, Wanqing},
title = {Robust unsupervised feature selection via dual self-representation and manifold regularization},
year = {2018},
issue_date = {Apr 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {145},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.01.009},
doi = {10.1016/j.knosys.2018.01.009},
journal = {Know.-Based Syst.},
month = apr,
pages = {109–120},
numpages = {12},
keywords = {Unsupervised feature selection, Local geometric structure, Similarity preservation, Self-representation, Graph learning, 00-01, 99-00}
}

@article{10.1016/j.ipm.2021.102656,
author = {Zhao, Huiliang and Liu, Zhenghong and Yao, Xuemei and Yang, Qin},
title = {A machine learning-based sentiment analysis of online product reviews with a novel term weighting and feature selection approach},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {58},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2021.102656},
doi = {10.1016/j.ipm.2021.102656},
journal = {Inf. Process. Manage.},
month = sep,
numpages = {14},
keywords = {Sentiment analysis, Polarity classification, Machine learning, Sentiment analysis of online product reviews, Term weighting, Elman Neural Network (ENN), Bat algorithm (BA)}
}

@inproceedings{10.1145/3232829.3232837,
author = {Zhu, Changming},
title = {Kappa Based Weighted Multi-View Clustering with Feature Selection},
year = {2018},
isbn = {9781450364713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3232829.3232837},
doi = {10.1145/3232829.3232837},
abstract = {In recent years, multi-view clustering has been developed to a high level and widely used in many real-world applications. Since different views are variable representations of the same instance set, thus weighted multi-view clustering with feature selection (WMCFS) has been proposed to use information from multiple views simultaneously to boost the clustering results. WMCFS not only combines information from multiple views but also performs feature selection so as to solve high-dimensional data sets. Although related experiments validate the effectiveness of WMCFS, due to kappa is an index to measure the inter-rater agreement for qualitative (categorical) items, thus we introduce kappa to WMCFS and propose a kappa based WMCFS (KWMCFS) to boost the clustering performance further. Experiments on multi-view data sets Mfeat, Reuters, and Corel validate that compared with WMCFS, introducing kappa boosts the clustering and classification performances.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Pattern Recognition},
pages = {50–54},
numpages = {5},
keywords = {Feature selection, Kappa, Multi-view clustering},
location = {Shenzhen, China},
series = {ICCPR '18}
}

@article{10.5555/2188385.2343691,
author = {Song, Le and Smola, Alex and Gretton, Arthur and Bedo, Justin and Borgwardt, Karsten},
title = {Feature selection via dependence maximization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice.},
journal = {J. Mach. Learn. Res.},
month = may,
pages = {1393–1434},
numpages = {42},
keywords = {Hilbert space embedding of distribution, Hilbert-Schmidt independence criterion, feature selection, independence measure, kernel methods}
}

@article{10.1504/ijitst.2020.108133,
author = {Nguyen, Trong-Kha and Ly, Vu Duc and Hwang, Seong Oun},
title = {Effective feature selection based on MANOVA},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {4},
issn = {1748-569X},
url = {https://doi.org/10.1504/ijitst.2020.108133},
doi = {10.1504/ijitst.2020.108133},
abstract = {Effectiveness in classifying malware is a critical issue which can overheat a classifier or reduce performance in real-time malware detection systems. However, the effectiveness in feature selection stage was not studied so far. As effectiveness should be taken into account at the earliest possible stages, in this paper, we focus on the effectiveness of feature selection. Firstly, we perform an analysis on instruction levels which consists of most frequencies mnemonics. Secondly, we propose new methods to select effective features by MANOVA statistical tests. Furthermore, we use those selected features fed to a classifier. Our approach reduces significantly the number of features from 390 to 4, which explains 99.4% variation of the data. With the selected features, we classify malware samples and have achieved 96.2% of accuracy and 0.6% of false positive.},
journal = {Int. J. Internet Technol. Secur. Syst.},
month = jan,
pages = {383–395},
numpages = {12},
keywords = {malware classification, statistical analysis, security}
}

@article{10.1016/j.eswa.2021.115655,
author = {Xu, Yueting and Huang, Hui and Heidari, Ali Asghar and Gui, Wenyong and Ye, Xiaojia and Chen, Ying and Chen, Huiling and Pan, Zhifang},
title = {         MFeature: Towards high performance evolutionary tools for feature selection},
year = {2022},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {186},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115655},
doi = {10.1016/j.eswa.2021.115655},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {25},
keywords = {Moth-flame optimization algorithm, Crossover, Ensemble, Simulated annealing, Feature selection, Data classification}
}

@article{10.1007/s11063-021-10491-0,
author = {Li, Xiaohua and Zhang, Jusheng and Safara, Fatemeh},
title = {Improving the Accuracy of Diabetes Diagnosis Applications through a Hybrid Feature Selection Algorithm},
year = {2021},
issue_date = {Feb 2023},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {55},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-021-10491-0},
doi = {10.1007/s11063-021-10491-0},
abstract = {Artificial intelligence is a future and valuable tool for early disease recognition and support in patient condition monitoring. It can increase the reliability of the cure and decision making by developing useful systems and algorithms. Healthcare workers, especially nurses and physicians, are overworked due to a massive and unexpected increase in the number of patients during the coronavirus pandemic. In such situations, artificial intelligence techniques could be used to diagnose a patient with life-threatening illnesses. In particular, diseases that increase the risk of hospitalization and death in coronavirus patients, such as high blood pressure, heart disease and diabetes, should be diagnosed at an early stage. This article focuses on diagnosing a diabetic patient through data mining techniques. If we are able to diagnose diabetes in the early stages of the disease, we can force patients to stay home and care for their health, so the risk of being infected with the coronavirus would be reduced. The proposed method has three steps: preprocessing, feature selection and classification. Several combinations of Harmony search algorithm, genetic algorithm, and particle swarm optimization algorithm are examined with K-means for feature selection. The combinations have not examined before for diabetes diagnosis applications. K-nearest neighbor is used for classification of the diabetes dataset. Sensitivity, specificity, and accuracy have been measured to evaluate the results. The results achieved indicate that the proposed method with an accuracy of 91.65% outperformed the results of the earlier methods examined in this article.},
journal = {Neural Process. Lett.},
month = mar,
pages = {153–169},
numpages = {17},
keywords = {Diabetes diagnosis application, Genetic algorithm, Particle swarm optimization, Harmony search algorithm, K-means, Artificial intelligence, Coronavirus disease pandemic}
}

@article{10.5555/2503308.2343691,
author = {Song, Le and Smola, Alex and Gretton, Arthur and Bedo, Justin and Borgwardt, Karsten},
title = {Feature selection via dependence maximization},
year = {2012},
issue_date = {January 2012},
publisher = {JMLR.org},
volume = {13},
number = {1},
issn = {1532-4435},
abstract = {We introduce a framework for feature selection based on dependence maximization between the selected features and the labels of an estimation problem, using the Hilbert-Schmidt Independence Criterion. The key idea is that good features should be highly dependent on the labels. Our approach leads to a greedy procedure for feature selection. We show that a number of existing feature selectors are special cases of this framework. Experiments on both artificial and real-world data show that our feature selector works well in practice.},
journal = {J. Mach. Learn. Res.},
month = may,
pages = {1393–1434},
numpages = {42},
keywords = {Hilbert space embedding of distribution, Hilbert-Schmidt independence criterion, feature selection, independence measure, kernel methods}
}

@article{10.1016/j.knosys.2021.107223,
author = {Sang, Binbin and Chen, Hongmei and Yang, Lei and Li, Tianrui and Xu, Weihua and Luo, Chuan},
title = {Feature selection for dynamic interval-valued ordered data based on fuzzy dominance neighborhood rough set},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107223},
doi = {10.1016/j.knosys.2021.107223},
journal = {Know.-Based Syst.},
month = sep,
numpages = {20},
keywords = {Interval-valued ordered decision system, Fuzzy dominance neighborhood rough set, Conditional entropy, Incremental learning, Feature selection, Attribute reduction}
}

@article{10.1145/3422821,
author = {Chikersal, Prerna and Doryab, Afsaneh and Tumminia, Michael and Villalba, Daniella K. and Dutcher, Janine M. and Liu, Xinwen and Cohen, Sheldon and Creswell, Kasey G. and Mankoff, Jennifer and Creswell, J. David and Goel, Mayank and Dey, Anind K.},
title = {Detecting Depression and Predicting its Onset Using Longitudinal Symptoms Captured by Passive Sensing: A Machine Learning Approach With Robust Feature Selection},
year = {2021},
issue_date = {February 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1073-0516},
url = {https://doi.org/10.1145/3422821},
doi = {10.1145/3422821},
abstract = {We present a machine learning approach that uses data from smartphones and fitness trackers of 138 college students to identify students that experienced depressive symptoms at the end of the semester and students whose depressive symptoms worsened over the semester. Our novel approach is a feature extraction technique that allows us to select meaningful features indicative of depressive symptoms from longitudinal data. It allows us to detect the presence of post-semester depressive symptoms with an accuracy of 85.7% and change in symptom severity with an accuracy of 85.4%. It also predicts these outcomes with an accuracy of &gt;80%, 11–15 weeks before the end of the semester, allowing ample time for pre-emptive interventions. Our work has significant implications for the detection of health outcomes using longitudinal behavioral data and limited ground truth. By detecting change and predicting symptoms several weeks before their onset, our work also has implications for preventing depression.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jan,
articleno = {3},
numpages = {41},
keywords = {Mobile sensing, depression, feature selection, machine learning, mental health, mobile health}
}

@article{10.1016/j.ins.2018.12.074,
author = {Zhou, Peng and Hu, Xuegang and Li, Peipei and Wu, Xindong},
title = {Online streaming feature selection using adapted Neighborhood Rough Set},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {481},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.12.074},
doi = {10.1016/j.ins.2018.12.074},
journal = {Inf. Sci.},
month = may,
pages = {258–279},
numpages = {22},
keywords = {Feature selection, Online streaming feature selection, Feature streams, Neighborhood rough set, Adapted neighbors}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning, Machine Learning Platform, Machine Learning as a Service},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.3233/JIFS-210212,
author = {Ling, Jie and Xiong, Su and Luo, Yu},
title = {Semi-supervised learning approach for malicious URL detection via adversarial learning1},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {2},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210212},
doi = {10.3233/JIFS-210212},
abstract = {Uniform Resource Location (URL) is the network unified resource location system that specifies the location and access method of resources on the Internet. At present, malicious URL has become one of the main means of network attack. How to detect malicious URL timely and accurately has become an engaging research topic. The recent proposed deep learning-based detection models can achieve high accuracy in simulations, but several problems are exposed when they are used in real applications. These models need a balanced labeled dataset for training, while collecting large numbers of the latest labeled URL samples is difficult due to the rapid generation of URL in the real application environment. In addition, in most randomly collected datasets, the number of benign URL samples and malicious URL samples is extremely unbalanced, as malicious URL samples are often rare. This paper proposes a semi-supervised learning malicious URL detection method based on generative adversarial network (GAN) to solve the above two problems. By utilizing the unlabeled URLs for model training in a semi-supervised way, the requirement of large numbers of labeled samples is weakened. And the imbalance problem can be relieved with the synthetic malicious URL generated by adversarial learning. Experimental results show that the proposed method outperforms the classic SVM and LSTM based methods. Specially, the proposed method can obtain high accuracy with insufficient labeled samples and unbalanced dataset. e.g., the proposed method can achieve 87.8% /91.9% detection accuracy when the number of labeled samples is reduced to 20% /40% of that of conventional methods.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3083–3092},
numpages = {10},
keywords = {Malicious URL detection, network security, deep learning, semi-supervised learning}
}

@article{10.5555/3288251.3288291,
author = {Zheng, Wei and Zhu, Xiaofeng and Zhu, Yonghua and Hu, Rongyao and Lei, Cong},
title = {Dynamic graph learning for spectral feature selection},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Previous spectral feature selection methods generate the similarity graph via ignoring the negative effect of noise and redundancy of the original feature space, and ignoring the association between graph matrix learning and feature selection, so that easily producing suboptimal results. To address these issues, this paper joints graph learning and feature selection in a framework to obtain optimal selected performance. More specifically, we use the least square loss function and an ℓ2,1-norm regularization to remove the effect of noisy and redundancy features, and use the resulting local correlations among the features to dynamically learn a graph matrix from a low-dimensional space of original data. Experimental results on real data sets show that our method outperforms the state-of-the-art feature selection methods for classification tasks.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29739–29755},
numpages = {17},
keywords = {Graph learning, Optimization, Spectral feature selection}
}

@article{10.1145/3451179,
author = {Huang, Guyue and Hu, Jingbo and He, Yifan and Liu, Jialong and Ma, Mingyuan and Shen, Zhaoyang and Wu, Juejian and Xu, Yuanfan and Zhang, Hengrui and Zhong, Kai and Ning, Xuefei and Ma, Yuzhe and Yang, Haoyu and Yu, Bei and Yang, Huazhong and Wang, Yu},
title = {Machine Learning for Electronic Design Automation: A Survey},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3451179},
doi = {10.1145/3451179},
abstract = {With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {40},
numpages = {46},
keywords = {Electronic design automation, machine learning, neural networks}
}

@article{10.1016/j.compeleceng.2021.107329,
author = {Oprea, Simona-Vasilica and B\^{a}ra, Adela},
title = {Machine learning classification algorithms and anomaly detection in conventional meters and Tunisian electricity consumption large datasets},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107329},
doi = {10.1016/j.compeleceng.2021.107329},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {17},
keywords = {Machine learning, Fraud detection, Feature engineering, Probability density function, Conventional meter}
}

@article{10.1109/TCBB.2018.2824332,
author = {Yang, Runtao and Zhang, Chengjin and Gao, Rui and Zhang, Lina and Song, Qing},
title = {Predicting FAD Interacting Residues with Feature Selection and Comprehensive Sequence Descriptors},
year = {2019},
issue_date = {Nov.-Dec. 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2824332},
doi = {10.1109/TCBB.2018.2824332},
abstract = {The function of a flavoprotein is determined to a great extent by the binding sites on its surface that interacts with flavin adenine dinucleotide (FAD). Malfunction or dysregulation of FAD binding leads to a series of diseases. Therefore, accurately identifying FAD interacting residues (FIRs) provides insights into the molecular mechanisms of flavoprotein-related biological processes and disease progression. In this paper, a new computational method is proposed for identifying FIRs from protein sequences. Various sequence-derived discriminative features are explored. We analyze the distinctions of these features between FIRs and non-FIRs. We also investigate the predictive capabilities of both individual features and combinations of features. A relief algorithm followed by incremental feature selection (relief-IFS) is then adopted to search the optimal features. Finally, a random forest (RF) module is used to predict FIRs based on the optimal features. Using a 5-fold cross-validation test, the proposed method performs well, with a sensitivity of 0.847, a specificity of 0.933, an accuracy of 0.890, and a Matthews correlation coefficient (MCC) of 0.782, thereby outperforming previous methods. These results indicate that our method is relatively successful at predicting FIRs.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {2046–2056},
numpages = {11}
}

@article{10.1007/s42979-021-00750-1,
author = {Bakshi, Aarti and Kopparapu, Sunil Kumar},
title = {Improving Indian Spoken-Language Identification by Feature Selection in Duration Mismatch Framework},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00750-1},
doi = {10.1007/s42979-021-00750-1},
abstract = {Paper presents novel duration normalized feature selection technique and two-step modified hierarchical classifier to improve the accuracy of spoken language identification (SLID) using Indian languages for duration mismatched condition. Feature selection averages random forest-based importance vectors of open SMILE features of different duration utterances. Although it improves the SLID system’s accuracy for mismatched training and testing durations, the performance is significantly reduced for short-duration utterances. A cascade of inter-family and intra-family classifiers with an additional class to improve false language family estimation. All India Radio data set with nine Indian languages and different utterance durations was used as speech material. Experimental results showed that 150 optimal features with the proposed modified hierarchical classifier showed the highest accuracy of 96.9% and 84.4% for 30 s and 0.2 s utterances for the same train-test duration. However, we achieved an accuracy of 98.3% and 61.9% for 15 and 0.2 s test duration when trained with 30 s duration utterance. Comparative analysis showed a significant improvement in accuracy than several SLID systems in the literature.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {16},
keywords = {Spoken language identification, Indian language, Feature selection, Classifier fusion}
}

@article{10.5555/3288251.3288293,
author = {Lei, Cong and Zhu, Xiaofeng},
title = {Unsupervised feature selection via local structure learning and sparse learning},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {22},
issn = {1380-7501},
abstract = {Feature self-representation has become the backbone of unsupervised feature selection, since it is almost insensitive to noise data. However, feature selection methods based on feature self-representation have the following drawbacks: 1) The self-representation coefficient matrix is fixed and can not be fine-tuned according to the structure of data. 2) they do not consider the manifold structure of data, thus unable to further increase the performance of feature selection. To solve the above problems, this paper proposes an unsupervised feature selection algorithm that combines feature self-representation and manifold learning. Specifically, we first utilize feature self-representation to construct the model. After that, the self-representation coefficient matrix is dynamically adjusted to the optimal state based on the similarity matrix. Then, we use low-rank representation to explore the global manifold structure of the data. Finally, we combine sparse learning with feature selection. The experimental results on twelve datasets show that the proposed method outperforms all the competing methods.},
journal = {Multimedia Tools Appl.},
month = nov,
pages = {29605–29622},
numpages = {18},
keywords = {Feature selection, Hypergraph representation, Sparse feature selection, Subspace learning}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Variability management, Software product lines, Multiple-case study, Challenges}
}

@article{10.1134/S0361768818060129,
author = {Vijayashree, J. and Sultana, H. Parveen},
title = {A Machine Learning Framework for Feature Selection in Heart Disease Classification Using Improved Particle Swarm Optimization with Support Vector Machine Classifier},
year = {2018},
issue_date = {November  2018},
publisher = {Plenum Press},
address = {USA},
volume = {44},
number = {6},
issn = {0361-7688},
url = {https://doi.org/10.1134/S0361768818060129},
doi = {10.1134/S0361768818060129},
journal = {Program. Comput. Softw.},
month = nov,
pages = {388–397},
numpages = {10},
keywords = {Particle Swarm Optimization, ROC analysis, Support Vector Machine, fitness function, population diversity function, tuning function}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {DSML, SPL, methodology, software factory, variability},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1007/s00521-019-04547-5,
author = {Yadav, Anjali and Singh, Anushikha and Dutta, Malay Kishore and Travieso, Carlos M.},
title = {Machine learning-based classification of cardiac diseases from PCG recorded heart sounds},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {24},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04547-5},
doi = {10.1007/s00521-019-04547-5},
abstract = {Cardiovascular diseases are one of the most fatal diseases across the globe. Clinically, conventional stethoscope is used to check the medical condition of a human heart. Only a trained medical professional can understand and interpret the heart auscultations clinically. This paper presents a machine learning-based automatic classification system based on heart sounds to diagnose cardiac disorders. The proposed framework involves strategic processing and framing of heart sound to extract discriminatory features for machine learning. The most prominent features are selected and used to train a supervised classifier for automatic detection of cardiac diseases. The biological abnormalities disturbing the physical functioning of the heart cause variations in the auscultations, which is strategically used in terms of some discriminatory features for machine learning-based automatic classification. The proposed method achieved 97.78% accuracy with the equal error rate of 2.22% for abnormal and normal heart sound classification. The experimental results exhibit that the performance of the proposed method in proper diagnosis of the cardiac diseases is high in terms of accuracy and has low error rate which makes the proposed algorithm suitable for real-time applications.},
journal = {Neural Comput. Appl.},
month = dec,
pages = {17843–17856},
numpages = {14},
keywords = {Body auscultation, Cardiac disease, Feature extraction, p value, Machine learning, Automatic classification}
}

@article{10.1016/j.eswa.2016.02.035,
author = {Bolon-Canedo, Veronica and Fern\'{a}ndez-Francos, Diego and Peteiro-Barral, Diego and Alonso-Betanzos, Amparo and Guijarro-Berdi\~{n}as, Bertha and S\'{a}nchez-Maro\~{n}o, Noelia},
title = {A unified pipeline for online feature selection and classification},
year = {2016},
issue_date = {August 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {55},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.02.035},
doi = {10.1016/j.eswa.2016.02.035},
abstract = {A proposal for online feature selection is proposed.The proposed pipeline covers discretization, feature selection and classification.Classical algorithms were modified to make them work online.K-means discretizer, Chi-Square filter and Artificial Neural Networks were used.Results show that classification error is decreasing, adapting to the arrival of new data. With the advent of Big Data, data is being collected at an unprecedented fast pace, and it needs to be processed in a short time. To deal with data streams that flow continuously, classical batch learning algorithms cannot be applied and it is necessary to employ online approaches. Online learning consists of continuously revising and refining a model by incorporating new data as they arrive, and it allows important problems such as concept drift or management of extremely high-dimensional datasets to be solved. In this paper, we present a unified pipeline for online learning which covers online discretization, feature selection and classification. Three classical methods-the k-means discretizer, the 2 filter and a one-layer artificial neural network-have been reimplemented to be able to tackle online data, showing promising results on both synthetic and real datasets.},
journal = {Expert Syst. Appl.},
month = aug,
pages = {532–545},
numpages = {14},
keywords = {Classification, Discretization, Feature selection, Machine learning, Online learning}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-based prioritization of test cases in continuous integration of highly-configurable software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {continuous integration, family of products, software product line, test case prioritization},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.3233/JIFS-200850,
author = {Muthamil Sudar, K. and Deepalakshmi, P.},
title = {An intelligent flow-based and signature-based IDS for SDNs using ensemble feature selection and a multi-layer machine learning-based classifier},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200850},
doi = {10.3233/JIFS-200850},
abstract = {Software-defined networking is a new paradigm that overcomes problems associated with traditional network architecture by separating the control logic from data plane devices. It also enhances performance by providing a highly-programmable interface that adapts to dynamic changes in network policies. As software-defined networking controllers are prone to single-point failures, providing security is one of the biggest challenges in this framework. This paper intends to provide an intrusion detection mechanism in both the control plane and data plane to secure the controller and forwarding devices respectively. In the control plane, we imposed a flow-based intrusion detection system that inspects every new incoming flow towards the controller. In the data plane, we assigned a signature-based intrusion detection system to inspect traffic between Open Flow switches using port mirroring to analyse and detect malicious activity. Our flow-based system works with the help of trained, multi-layer machine learning-based classifier, while our signature-based system works with rule-based classifiers using the Snort intrusion detection system. The ensemble feature selection technique we adopted in the flow-based system helps to identify the prominent features and hasten the classification process. Our proposed work ensures a high level of security in the Software-defined networking environment by working simultaneously in both control plane and data plane.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {4237–4256},
numpages = {20},
keywords = {Software-defined networking (SDN), machine learning (ML), intrusion detection system (IDS), feature selection, flow-based IDS}
}

@article{10.1016/j.ins.2019.05.093,
author = {L\'{o}pez, Julio and Maldonado, Sebasti\'{a}n},
title = {Profit-based credit scoring based on robust optimization and feature selection},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {500},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.05.093},
doi = {10.1016/j.ins.2019.05.093},
journal = {Inf. Sci.},
month = oct,
pages = {190–202},
numpages = {13},
keywords = {Credit scoring, Robust optimization, Second-order cone programming, Profit metrics, Business analytics, Feature selection}
}

@inproceedings{10.1145/3297280.3297283,
author = {ShahrjooiHaghighi, AliAsghar and Frigui, Hichem and Zhang, Xiang and Wei, Xiaoli and Shi, Biyun and McClain, Craig J.},
title = {Ensemble feature selection for biomarker discovery in mass spectrometry-based metabolomics},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297283},
doi = {10.1145/3297280.3297283},
abstract = {Biomarker discovery, i.e., identifying the discriminative features that are responsible for alteration of a biological system, is often solved by feature selection implemented by machine learning approaches. While many individual feature selection methods are used in biomarker discovery, the nature of omics data (small number of samples, large number of features, and noisy data) makes each of those individual feature selection algorithms unstable. In this paper, we investigate various ensemble feature selection methods to improve the reliability of the molecular biomarker selection by combining the complementary information of multiple feature selection methods. We compare the performance of different ensemble approaches and evaluate their performances using a metabolomics dataset containing three sample groups. Our results indicate that our ensemble approach outperforms the individual feature selection algorithms and provides more stable results.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {19–24},
numpages = {6},
keywords = {biomarker discovery, ensemble feature selection, ensemble learning, feature selection, filter methods, metabolomics data},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1016/j.compbiomed.2021.104838,
author = {Khandakar, Amith and Chowdhury, Muhammad E.H. and Ibne Reaz, Mamun Bin and Md Ali, Sawal Hamid and Hasan, Md Anwarul and Kiranyaz, Serkan and Rahman, Tawsifur and Alfkey, Rashad and Bakar, Ahmad Ashrif A. and Malik, Rayaz A.},
title = {A machine learning model for early detection of diabetic foot using thermogram images},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104838},
doi = {10.1016/j.compbiomed.2021.104838},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {10},
keywords = {Thermogram, Diabetes mellitus, Diabetic foot, Convolutional neural network, Machine learning algorithms, Image enhancement techniques, Diagnostic utility}
}

@inproceedings{10.1145/3357419.3357437,
author = {Khan, Khansa and Azam, Farooque and Anwar, Muhammad Waseem and Kiran, Ayesha},
title = {A Meta-model For Software Project Change and Configuration Management},
year = {2019},
isbn = {9781450371889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357419.3357437},
doi = {10.1145/3357419.3357437},
abstract = {Project change and configuration management refers to the process of controlling and managing of change in project development process. Change can occur in any of the items and phases. This paper has focused on change management i.e. when change occurs in requirements of the software project. Several approaches have been proposed for change/configuration management. But, a metamodel is required that accommodates the tracking of change in three of the requirements i.e. Functional Requirement (FR), Non-Functional Requirements (NFR) and Domain Requirements (DR). In this paper, we have proposed a metamodel for tracking that how change in one requirement impacts change in other requirement so that it can later manage the development process of the project. It provides the identification of hierarchal relation b/w FRs, NFRs and DRs and helps to authorize the changes in order to ensure the requirements consistency in the project.},
booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
pages = {12–16},
numpages = {5},
keywords = {Metamodel, configuration management, domain requirements (DR), functional requirements (FR), non-functional requirements (NFR)},
location = {Prague, Czech Republic},
series = {ICICM '19}
}

@article{10.1007/s42979-021-00775-6,
author = {Meesad, Phayung},
title = {Thai Fake News Detection Based on Information Retrieval, Natural Language Processing and Machine Learning},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00775-6},
doi = {10.1007/s42979-021-00775-6},
abstract = {Fake news is a big problem in every society. Fake news must be detected and its sharing should be stopped before it causes further damage to the country. Spotting fake news is challenging because of its dynamics. In this research, we propose a framework for robust Thai fake news detection. The framework comprises three main modules, including information retrieval, natural language processing, and machine learning. This research has two phases: the data collection phase and the machine learning model building phase. In the data collection phase, we obtained data from Thai online news websites using web-crawler information retrieval, and we analyzed the data using natural language processing techniques to extract good features from web data. For comparison, we selected some well-known classification Machine Learning models, including Na\"{\i}ve Bayesian, Logistic Regression, K-Nearest Neighbor, Multilayer Perceptron, Support Vector Machine, Decision Tree, Random Forest, Rule-Based Classifier, and Long Short-Term Memory. The comparison study on the test set showed that Long Short-Term Memory was the best model, and we deployed an automatic online fake news detection web application.},
journal = {SN Comput. Sci.},
month = aug,
numpages = {17},
keywords = {Fake news detection, Information retrieval, Natural language processing, Machine learning}
}

@article{10.1145/3399595,
author = {Agnesina, Anthony and Lim, Sung Kyu and Lepercq, Etienne and Cid, Jose Escobedo Del},
title = {Improving FPGA-Based Logic Emulation Systems through Machine Learning},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3399595},
doi = {10.1145/3399595},
abstract = {We present a machine learning (ML) framework to improve the use of computing resources in the FPGA compilation step of a commercial FPGA-based logic emulation flow. Our ML models enable highly accurate predictability of the final place and route design qualities, runtime, and optimal mapping parameters. We identify key compilation features that may require aggressive compilation efforts using our ML models. Experiments based on our large-scale database from an industry’s emulation system show that our ML models help reduce the total number of jobs required for a given netlist by 33%. Moreover, our job scheduling algorithm based on our ML model reduces the overall time to completion of concurrent compilation runs by 24%. In addition, we propose a new method to compute “recommendations” from our ML model to perform re-partitioning of difficult partitions. Tested on a large-scale industry system on chip design, our recommendation flow provides additional 15% compile time savings for the entire system on chip. To exploit our ML model inside the time-critical multi-FPGA partitioning step, we implement it in an optimized multi-threaded representation.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jul,
articleno = {46},
numpages = {20},
keywords = {Field programmable gate array, SoC verification, emulation flow optimization with machine learning}
}

@article{10.1016/j.ijinfomgt.2018.10.006,
author = {Fernandes, Marta and Canito, Alda and Bol\'{o}n-Canedo, Ver\'{o}nica and Concei\c{c}\~{a}o, Lu\'{\i}s and Pra\c{c}a, Isabel and Marreiros, Goreti},
title = {Data analysis and feature selection for predictive maintenance: A case-study in the metallurgic industry},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2018.10.006},
doi = {10.1016/j.ijinfomgt.2018.10.006},
journal = {Int. J. Inf. Manag.},
month = jun,
pages = {252–262},
numpages = {11},
keywords = {Predictive maintenance, Data analysis, Feature selection, Rule-based model}
}

@article{10.1016/j.patcog.2017.01.016,
author = {Zhu, Pengfei and Zhu, Wencheng and Hu, Qinghua and Zhang, Changqing and Zuo, Wangmeng},
title = {Subspace clustering guided unsupervised feature selection},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.01.016},
doi = {10.1016/j.patcog.2017.01.016},
abstract = {Unsupervised feature selection (UFS) aims to reduce the time complexity and storage burden, improve the generalization ability of learning machines by removing the redundant, irrelevant and noisy features. Due to the lack of training labels, most existing UFS methods generate the pseudo labels by spectral clustering, matrix factorization or dictionary learning, and convert UFS to a supervised problem. The learned clustering labels reflect the data distribution with respect to classes and therefore are vital to the UFS performance. In this paper, we proposed a novel subspace clustering guided unsupervised feature selection (SCUFS) method. The clustering labels of the training samples are learned by representation based subspace clustering, and features that can well preserve the cluster labels are selected. SCUFS can well learn the data distribution in that it uncovers the underlying multi-subspace structure of the data and iteratively learns the similarity matrix and clustering labels. Experimental results on benchmark datasets for unsupervised feature selection show that SCUFS outperforms the state-of-the-art UFS methods. HighlightsA novel subspace clustering guided unsupervised feature selection (SCUFS) model is proposed.SCUFS learns a similarity graph by self-representation of samples and can uncover the underlying multi-subspace structure of data.The iterative updating of similarity graph and pseudo label matrix can learn a more accurate data distribution.},
journal = {Pattern Recogn.},
month = jun,
pages = {364–374},
numpages = {11},
keywords = {Group sparsity, Spectral clustering, Subspace clustering, Unsupervised feature selection}
}

@article{10.1007/s11704-016-5489-3,
author = {Hu, Xuegang and Zhou, Peng and Li, Peipei and Wang, Jing and Wu, Xindong},
title = {A survey on online feature selection with streaming features},
year = {2018},
issue_date = {June      2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-016-5489-3},
doi = {10.1007/s11704-016-5489-3},
abstract = {In the era of big data, the dimensionality of data is increasing dramatically in many domains. To deal with high dimensionality, online feature selection becomes critical in big data mining. Recently, online selection of dynamic features has received much attention. In situations where features arrive sequentially over time, we need to perform online feature selection upon feature arrivals. Meanwhile, considering grouped features, it is necessary to deal with features arriving by groups. To handle these challenges, some state-of-the-art methods for online feature selection have been proposed. In this paper, we first give a brief review of traditional feature selection approaches. Then we discuss specific problems of online feature selection with feature streams in detail. A comprehensive review of existing online feature selection methods is presented by comparing with each other. Finally, we discuss several open issues in online feature selection.},
journal = {Front. Comput. Sci.},
month = jun,
pages = {479–493},
numpages = {15},
keywords = {big data, feature selection, feature stream, online feature selection}
}

@article{10.1016/j.asoc.2021.107302,
author = {Li, An-Da and Xue, Bing and Zhang, Mengjie},
title = {Improved binary particle swarm optimization for feature selection with new initialization and search space reduction strategies},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107302},
doi = {10.1016/j.asoc.2021.107302},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {19},
keywords = {Classification, Feature selection, Particle swarm optimization, Initialization}
}

@article{10.1007/s10044-015-0524-9,
author = {Settouti, Nesma and Chikh, Mohamed Amine and Barra, Vincent},
title = {A new feature selection approach based on ensemble methods in semi-supervised classification},
year = {2017},
issue_date = {August    2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-015-0524-9},
doi = {10.1007/s10044-015-0524-9},
abstract = {In computer aided medical system, many practical classification applications are confronted to the massive multiplication of collection and storage of data, this is especially the case in areas such as the prediction of medical test efficiency, the classification of tumors and the detection of cancers. Data with known class labels (labeled data) can be limited but unlabeled data (with unknown class labels) are more readily available. Semi-supervised learning deals with methods for exploiting the unlabeled data in addition to the labeled data to improve performance on the classification task. In this paper, we consider the problem of using a large amount of unlabeled data to improve the efficiency of feature selection in large dimensional datasets, when only a small set of labeled examples is available. We propose a new semi-supervised feature evaluation method called Optimized co-Forest for Feature Selection (OFFS) that combines ideas from co-forest and the embedded principle of selecting in Random Forest based by the permutation of out-of-bag set. We provide empirical results on several medical and biological benchmark datasets, indicating an overall significant improvement of OFFS compared to four other feature selection approaches using filter, wrapper and embedded manner in semi-supervised learning. Our method proves its ability and effectiveness to select and measure importance to improve the performance of the hypothesis learned with a small amount of labeled samples by exploiting unlabeled samples.},
journal = {Pattern Anal. Appl.},
month = aug,
pages = {673–686},
numpages = {14},
keywords = {Co-forest, Ensemble methods, Feature selection, Large datasets, Medical diagnosis, Random Forest, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-86993-9_36,
author = {Akter, Tania and Ali, Mohammad Hanif and Satu, Md. Shahriare and Khan, Md. Imran and Mahmud, Mufti},
title = {Towards Autism Subtype Detection Through Identification of Discriminatory Factors Using Machine Learning},
year = {2021},
isbn = {978-3-030-86992-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86993-9_36},
doi = {10.1007/978-3-030-86993-9_36},
abstract = {Autism spectrum disorder (ASD) is a neuro-developmental disease that has a lifetime impact on a person’s ability to interact and communicate with others. Early discovery of autism can assist to prepare a plan for suitable therapy and reduce its impact on patients at an appropriate time. The aim of this work is to propose a machine learning model which generates autism subtypes and identifies discriminatory factors among them. In this work, we use Quantitative Checklist for Autism in Toddlers-10 (Q-CHAT-10) of toddler and Autism Spectrum Quotient-10 (AQ-10) datasets of child, adolescent, and adult screening datasets respectively. Then, only autism records are merged and implemented k-means algorithm to extract various autism subtypes. According to Silhoutte score, we select the best autism dataset and balance its subtypes using random oversampling (ROS) and synthetic minority oversampling technique for numeric and categorical values (SMOTENC). Afterwards, various classifiers are employed into both primary dataset and its balanced subtypes. In this work, logistic regression shows the highest result for primary dataset. Also, it achieves the greatest results for ROS and SMOTENC datasets. Hence, shapely adaptive explanation (SHAP) technique is used to rank features and scrutinized discriminatory factors of these autism subtypes.},
booktitle = {Brain Informatics: 14th International Conference, BI 2021, Virtual Event, September 17–19, 2021, Proceedings},
pages = {401–410},
numpages = {10},
keywords = {Autism, K-means clustering, Machine learning, SHAP analysis, Discriminatory factors}
}

@inproceedings{10.1145/3319619.3326771,
author = {Ayodele, Mayowa},
title = {Application of estimation of distribution algorithm for feature selection},
year = {2019},
isbn = {9781450367486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319619.3326771},
doi = {10.1145/3319619.3326771},
abstract = {Feature selection is a machine learning concept that entails selecting relevant features while eliminating irrelevant and redundant features. This process helps to speed up learning. In this paper, an Estimation of Distribution Algorithm (EDA) is applied to a feature selection problem originating from a legal business. The EDA was able to generate a realistic solution to the real-world problem.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {43–44},
numpages = {2},
keywords = {estimation of distribution algorithm, feature selection, support vector machine},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@article{10.1007/s10489-018-1313-0,
author = {Khosravi, Mohammad Hossein and Bagherzadeh, Parsa},
title = {A new method for feature selection based on intelligent water drops},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1313-0},
doi = {10.1007/s10489-018-1313-0},
abstract = {One of the trending research areas of data mining and machine learning is feature selection. Feature selection is used as a technique for improving classification accuracy of a classifier as well as a more convenient way for visualization of data. In this paper, a new method for feature subset selection, based on intelligent water drops algorithm is proposed. Intelligent water drops algorithm is a metaheuristic algorithm which is inspired from movement of water drops in nature. In the proposed method, a new objective function which is suitable for intelligent water drops algorithm is introduced. The objective function is designed such that the selected feature vector would obtain a good classification accuracy as well as providing a good generalization degree. According to the experiments, the use of proposed approach leads to more accurate results as well as significant reduction in number of features.},
journal = {Applied Intelligence},
month = mar,
pages = {1172–1184},
numpages = {13},
keywords = {Class scatter matrices, Intelligent water drops, Multi-objective optimization, Supervised feature selection}
}

@inproceedings{10.1007/978-3-030-03496-2_33,
author = {Ksieniewicz, Pawe\l{} and Wo\'{z}niak, Micha\l{}},
title = {Imbalanced Data Classification Based on Feature Selection Techniques},
year = {2018},
isbn = {978-3-030-03495-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03496-2_33},
doi = {10.1007/978-3-030-03496-2_33},
abstract = {The difficulty of the many classification tasks lies in the analyzed data nature, as disproportionate number of examples from different class in a learning set. Ignoring this characteristics causes that canonical classifiers display strongly biased performance on imbalanced datasets. In this work a novel classifier ensemble forming technique for imbalanced datasets is presented. On the one hand it takes into consideration selected features used for training individual classifiers, on the other hand it ensures an appropriate diversity of a classifier ensemble. The proposed method was tested on the basis of the computer experiments carried out on the several benchmark datasets. Their results seem to confirm the usefulness of the proposed concept.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2018: 19th International Conference,  Madrid, Spain, November 21–23, 2018, Proceedings, Part II},
pages = {296–303},
numpages = {8},
keywords = {Machine learning, Classification, Imbalanced data, Feature selection, Random search},
location = {Madrid, Spain}
}

@article{10.1016/j.ijar.2019.11.010,
author = {Liu, Ye and Zheng, Lidi and Xiu, Yeliang and Yin, Hong and Zhao, Suyun and Wang, Xizhao and Chen, Hong and Li, Cuiping},
title = {Discernibility matrix based incremental feature selection on fused decision tables},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {0888-613X},
url = {https://doi.org/10.1016/j.ijar.2019.11.010},
doi = {10.1016/j.ijar.2019.11.010},
journal = {Int. J. Approx. Reasoning},
month = mar,
pages = {1–26},
numpages = {26},
keywords = {Incremental learning, Feature selection, Fuzzy rough sets, Discernibility matrix, Fused decision table}
}

@article{10.1016/j.compbiomed.2021.104320,
author = {Le, Nguyen Quoc Khanh and Hung, Truong Nguyen Khanh and Do, Duyen Thi and Lam, Luu Ho Thanh and Dang, Luong Huu and Huynh, Tuan-Tu},
title = {Radiomics-based machine learning model for efficiently classifying transcriptome subtypes in glioblastoma patients from MRI},
year = {2021},
issue_date = {May 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104320},
doi = {10.1016/j.compbiomed.2021.104320},
journal = {Comput. Biol. Med.},
month = may,
numpages = {8},
keywords = {Radiogenomics, Glioblastoma, Neuroimaging, Transcriptome subtypes, Radiomics biomarker, XGBoost, Artificial intelligence, Magnetic resonance imaging}
}

@inproceedings{10.1007/978-3-030-72013-1_16,
author = {Scott, Joseph and Niemetz, Aina and Preiner, Mathias and Nejati, Saeed and Ganesh, Vijay},
title = {MachSMT: A Machine Learning-based Algorithm Selector for SMT Solvers},
year = {2021},
isbn = {978-3-030-72012-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-72013-1_16},
doi = {10.1007/978-3-030-72013-1_16},
abstract = {In this paper, we present MachSMT, an algorithm selection tool for Satisfiability Modulo Theories (SMT) solvers. MachSMT supports the entirety of the SMT-LIB language. It employs machine learning (ML) methods to construct both empirical hardness models (EHMs) and pairwise ranking comparators (PWCs) over state-of-the-art SMT solvers. Given an SMT formula I as input, MachSMT leverages these learnt models to output a ranking of solvers based on predicted run time on the formula I. We evaluate MachSMT on the solvers, benchmarks, and data obtained from SMT-COMP 2019 and 2020. We observe MachSMT frequently improves on competition winners, winning 54 divisions outright and up to a 198.4% improvement in PAR-2 score, notably in logics that have broad applications (e.g., BV, LIA, NRA, etc.) in verification, program analysis, and software engineering. The MachSMT tool is designed to be easily tuned and extended to any suitable solver application by users. MachSMT is not a replacement for SMT solvers by any means. Instead, it is a tool that enables users to leverage the collective strength of the diverse set of algorithms implemented as part of these sophisticated solvers.},
booktitle = {Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021,  Luxembourg City, Luxembourg, March 27 – April 1, 2021, Proceedings, Part II},
pages = {303–325},
numpages = {23},
keywords = {SMT Solvers, Machine Learning, Algorithm Selection},
location = {Luxembourg City, Luxembourg}
}

@inproceedings{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: an evaluation platform for sampling algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
doi = {10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4},
keywords = {product lines, sampling, sampling evalutaion},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.5555/1005332.1016787,
author = {Dy, Jennifer G. and Brodley, Carla E.},
title = {Feature Selection for Unsupervised Learning},
year = {2004},
issue_date = {12/1/2004},
publisher = {JMLR.org},
volume = {5},
issn = {1532-4435},
abstract = {In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {845–889},
numpages = {45}
}

@article{10.1016/j.asoc.2015.01.035,
author = {Bol\'{o}n-Canedo, V. and S\'{a}nchez-Maro\~{n}o, N. and Alonso-Betanzos, A.},
title = {Distributed feature selection},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.01.035},
doi = {10.1016/j.asoc.2015.01.035},
abstract = {Graphical abstractDisplay Omitted HighlightsFeature selection is indispensable when dealing with microarray data.A new method for distributing the filtering process is proposed.The data is distributed by features and then merged in a final subset.The method is tested on 8 microarray datasets.The classification accuracy is maintained and the time considerably shortened. Feature selection is often required as a preliminary step for many pattern recognition problems. However, most of the existing algorithms only work in a centralized fashion, i.e. using the whole dataset at once. In this research a new method for distributing the feature selection process is proposed. It distributes the data by features, i.e. according to a vertical distribution, and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy. The effectiveness of our proposal is tested on microarray data, which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size. The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non-partitioned datasets.},
journal = {Appl. Soft Comput.},
month = may,
pages = {136–150},
numpages = {15},
keywords = {Distributed learning, Feature selection, Microarray data}
}

@article{10.1016/j.procs.2021.07.023,
author = {Sarah, Neamat Al and Rifat, Fahmida Yasmin and Hossain, Md. Shohrab and Narman, Husnu S.},
title = {An Efficient Android Malware Prediction Using Ensemble machine learning algorithms},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {191},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.07.023},
doi = {10.1016/j.procs.2021.07.023},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {184–191},
numpages = {8},
keywords = {Android, Malware, Machine Learning, Feature Selection, Ensemble learning, Drebin Dataset}
}

@inproceedings{10.1007/978-3-030-87802-3_14,
author = {Dayter, Maria and Riekhakaynen, Elena},
title = {What Causes Phonetic Reduction in Russian Speech: New Evidence from Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-87801-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87802-3_14},
doi = {10.1007/978-3-030-87802-3_14},
abstract = {In this paper, we describe the second stage of the study aimed at describing the factors that influence the phonetic reduction of words in Russian speech using machine learning algorithms. We discuss the limitations of the first stage of our study and try to overcome some of them by increasing the dataset and using new algorithms such as random forest, gradient boosting, and perceptron. We used the texts from the Corpus of Russian Speech as the data. The dataset was divided into two separate datasets: one consisted of single words and the other contained multiword units from our corpus. According to the results, for single words the most important features turned out to be the number of syllables and whether the word is an adjective as they were chosen by all algorithms. For the multiword units, the main features were the number of syllables, frequency in Russian spoken texts (in ipm), and token frequency in a given text. In our further research, we are going to expand the dataset and look closer on such features as text type and token frequency in a given text.},
booktitle = {Speech and Computer: 23rd International Conference, SPECOM 2021, St. Petersburg, Russia, September 27–30, 2021, Proceedings},
pages = {146–156},
numpages = {11},
keywords = {Phonetic reduction, Speech, Machine learning, Russian},
location = {St Petersburg, Russia}
}

@inproceedings{10.1145/3336294.3336315,
author = {Wolschke, Christian and Becker, Martin and Schneickert, S\"{o}ren and Adler, Rasmus and MacGregor, John},
title = {Industrial Perspective on Reuse of Safety Artifacts in Software Product Lines},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336315},
doi = {10.1145/3336294.3336315},
abstract = {In the future, safety-critical industrial products will have to be maintained and variants will have to be produced. In order to do this economically, the safety artifacts of the components should also be reused. At present, however, it is still unclear how this reuse could take place. Moreover this reuse is complicated, by the different situations in the various industries involved and by the corresponding standards applied.Current industrial practice for certification processes relies on a component-based view of reuse. We investigate the possibilities of product lines with managed processes for reuse also across multiple domains.In order to identify the challenges and possible solutions, we conducted interviews with industry partners from the domains of ICT, Rail, Automotive, and Industrial Automation, and from small- and medium-sized enterprises to large organizations. The semi-structured interviews identified the characteristics of current safety engineering processes, the handling of general variety and reuse, the approach followed for safety artifacts, and the need for improvement.In addition, a detailed literature survey summarizes existing approaches. We investigate which modularity concepts exist for dealing with safety, how variability concepts integrate safety, by which means process models can consider safety, and how safety cases are evolved while maintenance takes place. An overview of similar research projects complements the analysis.The identified challenges and potential solution proposals show how safety is related to Software Product Lines.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {143–154},
numpages = {12},
keywords = {modular safety, open source certification, product line certification, safety reuse, safety standards},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.5555/3504035.3504465,
author = {Li, Jundong and Wu, Liang and Dani, Harsh and Liu, Huan},
title = {Unsupervised personalized feature selection},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {Feature selection is effective in preparing high-dimensional data for a variety of learning tasks such as classification, clustering and anomaly detection. A vast majority of existing feature selection methods assume that all instances share some common patterns manifested in a subset of shared features. However, this assumption is not necessarily true in many domains where data instances could show high individuality. For example, in the medical domain, we need to capture the heterogeneous nature of patients for personalized predictive modeling, which could be characterized by a subset of instance-specific features. Motivated by this, we propose to study a novel problem of personalized feature selection. In particular, we investigate the problem in an unsupervised scenario as label information is usually hard to obtain in practice. To be specific, we present a novel unsupervised personalized feature selection framework UPFS to find some shared features by all instances and instance-specific features tailored to each instance. We formulate the problem into a principled optimization framework and provide an effective algorithm to solve it. Experimental results on real-world datasets verify the effectiveness of the proposed UPFS framework.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {430},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{10.1145/3442181,
author = {Sabir, Bushra and Ullah, Faheem and Babar, M. Ali and Gaire, Raj},
title = {Machine Learning for Detecting Data Exfiltration: A Review},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3442181},
doi = {10.1145/3442181},
abstract = {Context: Research at the intersection of cybersecurity, Machine Learning (ML), and Software Engineering (SE) has recently taken significant steps in proposing countermeasures for detecting sophisticated data exfiltration attacks. It is important to systematically review and synthesize the ML-based data exfiltration countermeasures for building a body of knowledge on this important topic. Objective: This article aims at systematically reviewing ML-based data exfiltration countermeasures to identify and classify ML approaches, feature engineering techniques, evaluation datasets, and performance metrics used for these countermeasures. This review also aims at identifying gaps in research on ML-based data exfiltration countermeasures. Method: We used Systematic Literature Review (SLR) method to select and review 92 papers. Results: The review has enabled us to: (a) classify the ML approaches used in the countermeasures into data-driven, and behavior-driven approaches; (b) categorize features into six types: behavioral, content-based, statistical, syntactical, spatial, and temporal; (c) classify the evaluation datasets into simulated, synthesized, and real datasets; and (d) identify 11 performance measures used by these studies. Conclusion: We conclude that: (i) The integration of data-driven and behavior-driven approaches should be explored; (ii) There is a need of developing high quality and large size evaluation datasets; (iii) Incremental ML model training should be incorporated in countermeasures; (iv) Resilience to adversarial learning should be considered and explored during the development of countermeasures to avoid poisoning attacks; and (v) The use of automated feature engineering should be encouraged for efficiently detecting data exfiltration attacks.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {50},
numpages = {47},
keywords = {Data exfiltration, advanced persistent threat, data breach, data leakage, machine learning}
}

@article{10.1007/s00521-021-05811-3,
author = {Mehta, Sweta and Patnaik, K. Sridhar},
title = {Improved prediction of software defects using ensemble machine learning techniques},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05811-3},
doi = {10.1007/s00521-021-05811-3},
abstract = {Software testing process is a crucial part in software development. Generally the errors made by developers get fixed at a later stage of the software development process. This increases the impact of the defect. To prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. Defect prediction process involves classification of software modules into defect prone and non-defect prone. This paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. In this research work, various software metrics are evaluated using feature selection techniques such as Recursive Feature Elimination (RFE), Correlation-based feature selection, Lasso, Ridge, ElasticNet and Boruta. Logistic Regression, Decision Trees, K-nearest neighbor, Support Vector Machines and Ensemble Learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. The proposed model uses combination of Partial Least Square (PLS) Regression and RFE for dimension reduction which is further combined with Synthetic Minority Oversampling Technique due to the imbalanced nature of the used datasets. It has been observed that XGBoost and Stacking Ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {10551–10562},
numpages = {12},
keywords = {Defect prediction, Dimension reduction, Data imbalance, Machine learning algorithms, XGBoost, Stacking ensemble classifier}
}

@article{10.1007/s10115-017-1059-8,
author = {Li, Yun and Li, Tao and Liu, Huan},
title = {Recent advances in feature selection and its applications},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {53},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1059-8},
doi = {10.1007/s10115-017-1059-8},
abstract = {Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval.},
journal = {Knowl. Inf. Syst.},
month = dec,
pages = {551–577},
numpages = {27},
keywords = {Data mining, Feature selection, Survey}
}

@article{10.1016/j.eswa.2021.114774,
author = {Rasheed, Fareeha and Wahid, Abdul},
title = {Learning style detection in E-learning systems using machine learning techniques},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114774},
doi = {10.1016/j.eswa.2021.114774},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Machine learning, Classification, Learning style, E-learning}
}

@inproceedings{10.1145/3382025.3414968,
author = {Li, Yang and Schulze, Sandro and Scherrebeck, Helene Hvidegaard and Fogdal, Thomas Sorensen},
title = {Automated extraction of domain knowledge in practice: the case of feature extraction from requirements at danfoss},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414968},
doi = {10.1145/3382025.3414968},
abstract = {Software product line supports structured reuse of software artifacts in order to realize the maintenance and evolution of the typically large number of variants, which promotes the industrialization of software development, especially for software-intensive products. However, for a legacy system, it is non-trivial to gain information about commonalities and differences of the variants. Meanwhile, software requirements specifications as the initial artifacts can be used to achieve this information to generate a domain model. Unfortunately, manually analyzing these requirements is time-consuming and inefficient. To address this problem, we explored the usage of feature extraction techniques to automatically extract domain knowledge from requirements to assist domain engineers. In detail, we applied Doc2Vec and a clustering algorithm to process the requirements for achieving the initial feature tree. Moreover, we utilized key words/phrases extraction techniques to provide key information to domain engineers for further analyzing the extraction results. In particular, we developed a GUI to support the extraction process. The empirical evaluation indicates that most of the extracted features and terms are beneficial to improve the process of feature extraction.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {4},
numpages = {11},
keywords = {feature extraction, requirement documents, reverse engineering, software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1007/s11265-019-1443-6,
author = {Zhang, Zisheng and Parhi, Keshab K.},
title = {M3U: Minimum Mean Minimum Uncertainty Feature Selection for Multiclass Classification},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {92},
number = {1},
issn = {1939-8018},
url = {https://doi.org/10.1007/s11265-019-1443-6},
doi = {10.1007/s11265-019-1443-6},
abstract = {This paper presents a novel multiclass feature selection algorithm based on weighted conditional entropy, also referred to as uncertainty. The goal of the proposed algorithm is to select a feature subset such that, for each feature sample, there exists a feature that has a low uncertainty score in the selected feature subset. Features are first quantized into different bins. The proposed feature selection method first computes an uncertainty vector from weighted conditional entropy. Lower the uncertainty score for a class, better is the separability of the samples in that class. Next, an iterative feature selection method selects a feature in each iteration by (1) computing the minimum uncertainty score for each feature sample for all possible feature subset candidates, (2) computing the average minimum uncertainty score across all feature samples, and (3) selecting the feature that achieves the minimum of the mean of the minimum uncertainty score. The experimental results show that the proposed algorithm outperforms mRMR and achieves lower misclassification rates using various types of publicly available datasets. In most cases, the number of features necessary for a specified misclassification error is less than that required by traditional methods. For all datasets, the misclassification error is reduced by 5∼25% on average, compared to a traditional method.},
journal = {J. Signal Process. Syst.},
month = jan,
pages = {9–22},
numpages = {14},
keywords = {Feature selection, Mutual information, Uncertainty score, Multi-class classification, Weighted conditional entropy}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@article{10.1016/j.eswa.2020.113249,
author = {Alazzam, Hadeel and Sharieh, Ahmad and Sabri, Khair Eddin},
title = {A feature selection algorithm for intrusion detection system based on Pigeon Inspired Optimizer},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {148},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113249},
doi = {10.1016/j.eswa.2020.113249},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Feature selection, Intrusion Detection System, KDDCUP, Pigeon Inspired Optimizer, 00-01, 99-00}
}

@article{10.3233/AIC-190626,
author = {Huang, Zhi},
title = {A feature selection approach combining neural networks with genetic algorithms},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {5–6},
issn = {0921-7126},
url = {https://doi.org/10.3233/AIC-190626},
doi = {10.3233/AIC-190626},
abstract = {Value Feature selection is an effective method to solve the curse of dimensionality, which widely employs Evolutionary Computation (EC), such as Genetic Algorithms (GA), by regarding feature subsets as individuals. However, it is impossible for EC based feature selection approaches to possess big population sizes because of very long and infeasible computational time. We have proposed a method screening individuals by estimating their classification performances rapidly instead of deriving theirs with a certain classifier dilatorily. Consequently, aiming at improving classification accuracies, we propose an approach named as FS-NN-GA (Feature Selection approach based on Neural Networks and Genetic Algorithms) in this work. The proposed approach employs the neural networks trained with some randomly generated individuals, and their actual classification accuracies to estimate individuals’ classification accuracies and screens them in each round of GA. The individuals with low estimated accuracies are directly eliminated. Only a small number of individuals with high estimated accuracies are reserved, evaluated by deriving their accuracies with a certain classifier, and participate GA operations to be explored emphatically. As a result, big population sizes become feasible, and a huge number of individuals can be considered by GA in acceptable and feasible time, which improves performances of GA and derives high accuracies. We perform the experiments with 10 data sets in comparison with 11 available approaches. The experimental results show that FS-NN-GA outperforms other approaches on most data sets.},
journal = {AI Commun.},
month = jan,
pages = {361–372},
numpages = {12},
keywords = {Feature selection, classification accuracy, Genetic Algorithms, neural networks, population size, computational time}
}

@article{10.1007/s10614-018-9864-z,
author = {Zhang, Yan and Trubey, Peter},
title = {Machine Learning and Sampling Scheme: An Empirical Study of Money Laundering Detection: Machine Learning and Sampling Scheme: An Empirical Study…},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {3},
issn = {0927-7099},
url = {https://doi.org/10.1007/s10614-018-9864-z},
doi = {10.1007/s10614-018-9864-z},
abstract = {This paper studies the interplay of machine learning and sampling scheme in an empirical analysis of money laundering detection algorithms. Using actual transaction data provided by a U.S. financial institution, we study five major machine learning algorithms including Bayes logistic regression, decision tree, random forest, support vector machine, and artificial neural network. As the incidence of money laundering events is rare, we apply and compare two sampling techniques that increase the relative presence of the events. Our analysis reveals potential advantages of machine learning algorithms in modeling money laundering events. This paper provides insights into the use of machine learning and sampling schemes in money laundering detection specifically, and classification of rare events in general.},
journal = {Comput. Econ.},
month = oct,
pages = {1043–1063},
numpages = {21},
keywords = {Bootstrap, Machine learning, Money laundering, Rare event, Sampling scheme, G21, G28}
}

@article{10.1007/s10489-021-02191-y,
author = {Huang, Hui and Jia, Rong and Shi, Xiaoyu and Liang, Jun and Dang, Jian},
title = {Feature selection and hyper parameters optimization for short-term wind power forecast},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {10},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02191-y},
doi = {10.1007/s10489-021-02191-y},
abstract = {Accurate wind power forecasting plays an increasingly significant role in power grid normal operation with large-scale wind energy. The precise and stable forecasting of wind power with short computational time is still a challenge owing to various uncertainty factors. This study proposes a hybrid model based on a data prepossessing strategy, a modified Bayesian optimization algorithm, and the gradient boosted regression trees approach. More specifically, the powerful information mining ability of maximum information coefficient is used to select the important input features, and the modified Bayesian optimization algorithm is introduced to optimize the hyperparameters of the gradient boosted regression trees to acquire more satisfactory forecasting precision and computation cost. Datasets from a Chinese wind farm are used in case studies to analyze the prediction accuracy, stability, and computation efficiency of the proposed model. The point forecasting and multi-step forecasting results reveal that the performance of the hybrid forecasting model positively exceeds all the contrasted models. The developed model is extremely useful for enhancing prediction precision and is a reasonable and valid tool for online prediction with increasing data.},
journal = {Applied Intelligence},
month = oct,
pages = {6752–6770},
numpages = {19},
keywords = {Wind power forecasting, Bayesian hyperparameters optimization, Feature selection, Gradient boosted regression trees}
}

@article{10.1145/3377454,
author = {Verbraeken, Joost and Wolting, Matthijs and Katzy, Jonathan and Kloppenburg, Jeroen and Verbelen, Tim and Rellermeyer, Jan S.},
title = {A Survey on Distributed Machine Learning},
year = {2020},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3377454},
doi = {10.1145/3377454},
abstract = {The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {30},
numpages = {33},
keywords = {Distributed machine learning, distributed systems}
}

@inproceedings{10.1145/3341105.3374084,
author = {Angenent, Mitch N. and Barata, Ant\'{o}nio Pereira and Takes, Frank W.},
title = {Large-scale machine learning for business sector prediction},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374084},
doi = {10.1145/3341105.3374084},
abstract = {In this study we use machine learning to perform explainable business sector prediction from financial statements. Financial statements are a valuable source of information on the financial state and performance of firms. Recently, large-scale data on financial statements has become available in the form of open data sets. Previous work on such data mainly focused on predicting fraud and bankruptcy. In this paper we devise a model for business sector prediction, which has several valuable applications, including automated error and fraud detection. In addition, such a predictive model may help in completing similar datasets with missing sector information. The proposed method employs a supervised learning approach based on random forests that addresses business sector prediction as a classification task. Using a dataset from the Netherlands Chamber of Commerce, containing over 1.5 million financial statements from Dutch companies, we created an adequately-performing model for business sector prediction. By assessing which features are instrumental in the final classification model, we found that a small number of attributes is crucial for predicting the majority of business sectors. Interestingly, in some cases the presence or absence of a feature was more important than the value itself. The resulting insights may also prove useful in accounting, where the relation between financial statements and characteristics of the company is a frequently studied topic.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1143–1146},
numpages = {4},
keywords = {business sector prediction, data mining, explainable machine learning, financial statements},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1007/978-3-030-29959-0_2,
author = {Zheng, Yifeng and Duan, Huayi and Wang, Cong},
title = {Towards Secure and Efficient Outsourcing of Machine Learning Classification},
year = {2019},
isbn = {978-3-030-29958-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29959-0_2},
doi = {10.1007/978-3-030-29959-0_2},
abstract = {Machine learning classification has been successfully applied in numerous applications, such as healthcare, finance, and more. Outsourcing classification services to the cloud has become an intriguing practice as this brings many prominent benefits like ease of management and scalability. Such outsourcing, however, raises critical privacy concerns to both the machine learning model provider and the client interested in using the classification service. In this paper, we focus on classification outsourcing with decision trees, one of the most popular classifiers. We propose for the first time a secure framework allowing decision tree based classification outsourcing while maintaining the confidentiality of the provider’s model (parameters) and the client’s input feature vector. Our framework requires no interaction from the provider and the client—they can go offline after the initial submission of their respective encrypted inputs to the cloud. This is a distinct advantage over prior art for practical deployment, as they all work under the client-provider setting where synchronous online interactions between the provider and client is required. Leveraging the lightweight additive secret sharing technique, we build our protocol from the ground up&nbsp;to enable secure and efficient outsourcing of decision tree evaluation, tailored to address the challenges posed by secure in-the-cloud dealing with versatile components including input feature selection, decision node evaluation, path evaluation, and classification generation. Through evaluation we show the practical performance of our design, and the substantial client-side savings over prior art, say up&nbsp;to four orders of magnitude in computation and 163\texttimes{} in communication.},
booktitle = {Computer Security – ESORICS 2019: 24th European Symposium on Research in Computer Security, Luxembourg, September 23–27, 2019, Proceedings, Part I},
pages = {22–40},
numpages = {19},
keywords = {Cloud security, Machine learning, Secure outsourcing},
location = {Luxembourg, Luxembourg}
}

@inproceedings{10.1145/3377049.3377070,
author = {Islam, Ashraful and Rahman, Mohammad Masudur and Ahmed, Eshtiak and Arafat, Faisal and Rabby, Md Fazle},
title = {Adaptive Feature Selection and Classification of Colon Cancer From Gene Expression Data: an Ensemble Learning Approach},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377070},
doi = {10.1145/3377049.3377070},
abstract = {Cancer research is one of the major and significant areas in medical research. A substantial number of research has been performed in this area and several methods have been employed. However, accuracy of cancer prediction is yet to reach near perfection as the conventional classification methods have several limitations. In recent times, microarray processed gene expression data has been used to predict cancer with significant accuracy. The gene expression data are usually high dimensional and comprises of relatively small number of samples which makes them difficult to classify. In order to achieve higher accuracy, ensembles method can be deployed which combines multiple classification methods. In this study, we have used the public colon cancer gene expression data set that consists of 62 instances having 2,000 attributes. An adaptive pre-processing procedure has been conducted including Linear Discriminant Analysis (LDA) and Principle Component Analysis (PCA) to cope up with the high dimensionality of the data. This was followed by building an ensemble learning model with k-Nearest Neighbors (kNN), Random Forest (RF), Kernel Support Vector Machines (KSVM), eXtreme Gradient Boosting (XGBoost), and Bayes Generalized Linear Model (GLM). Comparing with other classifiers, this study offers a significant improvement as our ensemble learning model gives higher accuracy than previously employed classification techniques. Thus the obtained accuracy is 91.67% with the scores 0.75, 1.00 and 0.85 of precision, recall and Matthews correlation coefficient (MCC) values respectively.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {26},
numpages = {7},
keywords = {DNA micro-array, colon cancer, ensemble learning, feature selection, gene expression, machine learning},
location = {Dhaka, Bangladesh},
series = {ICCA 2020}
}

@article{10.1016/j.ins.2016.08.039,
author = {Lin, Yaojin and Hu, Qinghua and Zhang, Jia and Wu, Xindong},
title = {Multi-label feature selection with streaming labels},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {372},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.08.039},
doi = {10.1016/j.ins.2016.08.039},
abstract = {In this paper, we study a novel and challenging issue, multi-label feature selection with streaming labels, in which the number of labels is unknown in advance, and the size of the feature set is constant. In this problem, we assume that the labels arrive one at a time, and the learning task is to rank features iteratively when a new label arrives. Traditional multi-label feature selection methods cannot perform well in this scenario. Therefore, we present an optimization framework where the weight of each label's feature rank list and the final feature rank list are defined as two sets of unknown variables. The objective is to minimize the overall weighted deviation between the final feature rank list and each label's feature rank list. Extensive experiments on benchmark data sets demonstrate that the proposed method outperforms other multi-label feature selection methods.},
journal = {Inf. Sci.},
month = dec,
pages = {256–275},
numpages = {20},
keywords = {Feature selection, Multi-label learning, Streaming labels, Supervised learning}
}

@inproceedings{10.1007/978-3-031-08421-8_41,
author = {Giorgio, Lazzarinetti and Nicola, Massarenti and Fabio, Sgr\`{o} and Andrea, Salafia},
title = {Continuous Defect Prediction in CI/CD Pipelines: A Machine Learning-Based Framework},
year = {2021},
isbn = {978-3-031-08420-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-08421-8_41},
doi = {10.1007/978-3-031-08421-8_41},
abstract = {Recent advances in information technology has led to an increasing number of applications to be developed and maintained daily by product teams. Ensuring that a software application works as expected and that it is absent of bugs requires a lot of time and resources. Thanks to the recent adoption of DevOps methodologies, it is often the case where code commits and application builds are centralized and standardized. Thanks to this new approach, it is now possible to retrieve log and build data to ease the development and management operations of product teams. However, even if such approaches include code control to detect unit or integration errors, they do not check for the presence of logical bugs that can raise after code builds. For such reasons in this work we propose a framework for continuous defect prediction based on machine learning algorithms trained on a publicly available dataset. The framework is composed of a machine learning model for detecting the presence of logical bugs in code on the basis of the available data generated by DevOps tools and a dashboard to monitor the software projects status. We also describe the serverless architecture we designed for hosting the aforementioned framework.},
booktitle = {AIxIA 2021 – Advances in Artificial Intelligence: 20th International Conference of the Italian Association for Artificial Intelligence, Virtual Event, December 1–3, 2021, Revised Selected Papers},
pages = {591–606},
numpages = {16},
keywords = {Continuous defect prediction, Machine learning, DevOps, Continuous integration}
}

@article{10.1504/ijbidm.2019.101977,
author = {Keerthika, T. and Premalatha, K.},
title = {An effective feature selection for heart disease prediction with aid of hybrid kernel SVM},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2019.101977},
doi = {10.1504/ijbidm.2019.101977},
abstract = {In today's modern world cardiovascular disease is the most lethal one. This disease attacks a person so instantly that it hardly gets any time to get treated with. So, diagnosing patients correctly on timely basis is the most challenging task for the medical fraternity. In order to reduce the risk of heart disease, effective feature selection and classification based prediction system is proposed. An efficient feature selection is applied on the high dimensional medical data, for selecting the features fish swarm optimisation algorithm is used. After that, selected features from medical dataset are fed to the HKSVM for classification. The performance of the proposed technique is evaluated by accuracy, sensitivity, specificity, precision, recall and f-measure. Experimental results indicate that the proposed classification framework have outperformed by having better accuracy of 96.03% for Cleveland dataset when compared existing SVM method only achieved 91.41% and optimal rough fuzzy classifier achieved 62.25%.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {306–326},
numpages = {20},
keywords = {hybrid kernel support vector machine, HKSVM, feature selection, fish swarm optimisation, support vector machines, SVM, optimal rough fuzzy, Cleveland, Hungarian, Switzerland}
}

@inproceedings{10.1145/3185089.3185117,
author = {Fahmy, Syahrul and Deraman, Aziz and Yahaya, Jamaiah H.},
title = {The Role of Human in Software Configuration Management},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185117},
doi = {10.1145/3185089.3185117},
abstract = {Two common problems in software development projects are falling behind schedule and software that does not fulfil its purpose. These problems can be attributed to on-going changes made to software products especially during development and maintenance, leading to more work than initially anticipated, diminishing quality as new changes are implemented. One approach for addressing these problems is through a systematic Software Configuration Management (SCM) process. However, after more than 50 years after its inception, these problems are still prevalent in software development, questioning the effectiveness of SCM implementation by software organizations. Although guided by international standards, industry best practices, and array of tools to support its implementation, the role of human has received little attention (if any), in mainstream SCM research, compared to other areas in software engineering. As such, this research project challenges the traditional view of SCM and puts forth a notion of People-Centric SCM, a holistic approach for managing changes to software products, focused on human rather than tools, and that is based on existing standards and best practices. The model and assessment framework were validated by means of Subject Matter Expert reviews and 5 case studies involving 9 software practitioners from the public sector, higher education institutions and the private sector in Malaysia, in addition to 2 international experts. Results of the validation demonstrated the soundness of the model, the plausibility of the assessment framework, and the practically of the People-Centric SCM approach to software organizations.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {56–60},
numpages = {5},
keywords = {Software Configuration Management, Software Engineering, Software Quality},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@article{10.1007/s10994-017-5648-2,
author = {Sechidis, Konstantinos and Brown, Gavin},
title = {Simple strategies for semi-supervised feature selection},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {107},
number = {2},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-017-5648-2},
doi = {10.1007/s10994-017-5648-2},
abstract = {What is the simplest thing you can do to solve a problem? In the context of semi-supervised feature selection, we tackle exactly this--how much we can gain from two simple classifier-independent strategies. If we have some binary labelled data and some unlabelled, we could assume the unlabelled data are all positives, or assume them all negatives. These minimalist, seemingly naive, approaches have not previously been studied in depth. However, with theoretical and empirical studies, we show they provide powerful results for feature selection, via hypothesis testing and feature ranking. Combining them with some "soft" prior knowledge of the domain, we derive two novel algorithms (Semi-JMI, Semi-IAMB) that outperform significantly more complex competing methods, showing particularly good performance when the labels are missing-not-at-random. We conclude that simple approaches to this problem can work surprisingly well, and in many situations we can provably recover the exact feature selection dynamics, as if we had labelled the entire dataset.},
journal = {Mach. Learn.},
month = feb,
pages = {357–395},
numpages = {39},
keywords = {Feature selection, Positive unlabelled, Semi-supervised}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {software product line, systematic review, tertiary study},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3450439.3451861,
author = {Pandl, Konstantin D. and Feiland, Fabian and Thiebes, Scott and Sunyaev, Ali},
title = {Trustworthy machine learning for health care: scalable data valuation with the shapley value},
year = {2021},
isbn = {9781450383592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450439.3451861},
doi = {10.1145/3450439.3451861},
abstract = {Collecting data from many sources is an essential approach to generate large data sets required for the training of machine learning models. Trustworthy machine learning requires incentives, guarantees of data quality, and information privacy. Applying recent advancements in data valuation methods for machine learning can help to enable these. In this work, we analyze the suitability of three different data valuation methods for medical image classification tasks, specifically pleural effusion, on an extensive data set of chest X-ray scans. Our results reveal that a heuristic for calculating the Shapley valuation scheme based on a k-nearest neighbor classifier can successfully value large quantities of data instances. We also demonstrate possible applications for incentivizing data sharing, the efficient detection of mislabeled data, and summarizing data sets to exclude private information. Thereby, this work contributes to developing modern data infrastructures for trustworthy machine learning in health care.},
booktitle = {Proceedings of the Conference on Health, Inference, and Learning},
pages = {47–57},
numpages = {11},
keywords = {shapley value, medical imaging, machine learning, data valuation, computer vision},
location = {Virtual Event, USA},
series = {CHIL '21}
}

@inproceedings{10.1007/978-3-030-77102-7_6,
author = {Muniyandi, Ravie Chandren and Elkhani, Naeimeh},
title = {P System as a Computing Tool for&nbsp;Embedded Feature Selection and&nbsp;Classification Method for Microarray Cancer Data},
year = {2020},
isbn = {978-3-030-77101-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77102-7_6},
doi = {10.1007/978-3-030-77102-7_6},
abstract = {Selection of relevant genes is the crucial task for sample classification in microarray data, where researchers try to identify the smallest possible set of genes that can still achieve good predictive performance. Due to the problem of higher risk of overfitting in wrapper methods and sensitivity of the best embedded way to filter out factor that leads to unstable model and significantly different gene subsets, in this paper, we propose a novel model for evaluating and improving techniques for selecting informative genes from microarray data. This model inspired by membrane computing and used the kernel P system (kP) as the variant of the P system to improve the performance of the intelligent algorithm, multi-objective binary particle swarm optimization (MObPSO). The proposed model consists of two main parts. First, kP-MObPSO, which resembles a wrapper type feature selection, and the second part that improves the results of the first part through an embedded feature selection and classification idea based on the kP system. Division, rewriting, and input/output rules are used to make interaction among the genes inside and between the particles. The proposed model applied to the colorectal and breast dataset contains 100 genes with six attributes. The embedded part of the model extracts the marker gene sets indicate more stability and reliability based on ROC measure as well as better error rate in comparison to the wrapper part of the model. In the paper, the lowest error rate by an embedded model is displayed as 0.1111 for breast cancer and 0.0769 for colorectal data.},
booktitle = {Membrane Computing: 21st International Conference, CMC 2020, Virtual Event, September 14–18, 2020, Revised Selected Papers},
pages = {94–125},
numpages = {32},
keywords = {Multi-objective binary particle swarm optimization, Kernel P system, Microarray cancer data, Classification, Embedded feature selection}
}

@inproceedings{10.1145/3382025.3414973,
author = {Schlie, Alexander and Kn\"{u}ppel, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Incremental feature model synthesis for clone-and-own software systems in MATLAB/Simulink},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414973},
doi = {10.1145/3382025.3414973},
abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {7},
numpages = {12},
keywords = {variability, synthesis, refinement, mapping, individual, incremental, feature model, clone-and-own, MATLAB/Simulink, 150% model},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3067695.3075985,
author = {Nguyen, Hoai Bach and Xue, Bing and Ishibuchi, Hisao and Andreae, Peter and Zhang, Mengjie},
title = {Multiple reference points MOEA/D for feature selection},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3075985},
doi = {10.1145/3067695.3075985},
abstract = {Feature selection can be considered a multi-objective problem since its two main objectives usually conflict with each other. Many Pareto dominance-based algorithms have been applied to feature selection. However, feature subsets evolved by these algorithms are mostly around the center of the Pareto front. MOEA/D can avoid this issue to some extent, but still needs to be modified when applying it to solve complex feature selection problems. This paper proposes a new decomposition strategy for feature selection called MOEA/D-MRPs which uses multiple reference points instead of multiple weight vectors. The proposed algorithm, is evaluated on eight different datasets and compared with three Pareto dominance-based algorithms and the standard MOEA/D algorithm. Experimental results show that MOEA/D-MRPs can efficiently evolve a more diverse set of non-dominated solutions than three Pareto dominance-based algorithms and achieve better classification performance than the standard MOEA/D algorithm. On large datasets, MOEA/D-MRPs is also the most efficient algorithm.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {157–158},
numpages = {2},
keywords = {multi-objective, feature selection, MOEA/D},
location = {Berlin, Germany},
series = {GECCO '17}
}

@article{10.1016/j.comnet.2018.01.007,
author = {Shi, Hongtao and Li, Hongping and Zhang, Dan and Cheng, Chaqiu and Cao, Xuanxuan},
title = {An efficient feature generation approach based on deep learning and feature selection techniques for traffic classification},
year = {2018},
issue_date = {Feb 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2018.01.007},
doi = {10.1016/j.comnet.2018.01.007},
journal = {Comput. Netw.},
month = feb,
pages = {81–98},
numpages = {18},
keywords = {Traffic classification, Machine learning, Concept drift, Multi-class imbalance, Deep learning, Feature selection}
}

@article{10.1007/s10916-019-1351-0,
author = {Karthiga, R. and Mangai, S.},
title = {Feature Selection Using Multi-Objective Modified Genetic Algorithm in Multimodal Biometric System},
year = {2019},
issue_date = {Jul 2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {7},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1351-0},
doi = {10.1007/s10916-019-1351-0},
abstract = {Today the multimodal biometric system has become a major area of study that is identified with applications of a large size in a recognition system. The feature selection is probably found to be the best factor to be optimized and is an on-going challenge in the midst of the optimization problems in the human recognition system. The feature selection aspires to bring down the number of the features, remove all types of redundant data and noise which result in a very high rate of recognition. The step further effects on the human recognition system and its performance. The work further presents a newer biometric system of verification that was multimodal and based on three different features which are the face, the hand vein, and the ear. This has today emerged as an extensively researched topic which spans various disciplines like signal processing, pattern recognition, and also computer vision. The features have been extracted by making use of the Incremental Principal Component Analysis (IPCA). Further, the work presented another novel algorithm of feature selection which was based on the Multi-Objective Modified Genetic Algorithm (MOM-GA). The Genetic Algorithm (GA) had been modified by means of introducing a levy search as opposed to a process of mutation. The algorithm has also proved to be an effective method of computation in which the search space is found to be highly dimensional. A classifier that makes use of the K-Nearest Neighbour (KNN) for classifying all accurate features is used. There were some investigations that were carried out and these results proved that this MOM-GA feature selection algorithm had been found as that which can generate certain excellent results using a minimal set of chosen features.},
journal = {J. Med. Syst.},
month = jul,
pages = {1–11},
numpages = {11},
keywords = {Multimodal biometric system, Multi-objective modified using genetic algorithm (MOM-GA), Levy search and K-nearest neighbor (KNN), Incremental principal component analysis (IPCA), Genetic algorithm (GA), Feature selection}
}

@article{10.1007/s10489-019-01543-z,
author = {Shahee, Shaukat Ali and Ananthakumar, Usha},
title = {An effective distance based feature selection approach for imbalanced data},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {3},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-019-01543-z},
doi = {10.1007/s10489-019-01543-z},
abstract = {Class imbalance is one of the critical areas in classification. The challenges become more severe when the data set has a large number of features. Traditional classifiers generally favour the majority class because of skewed class distributions. In recent years, feature selection is being used to select the appropriate features for better classification of minority class. However, these studies are limited to imbalance that arise between the classes. In addition to between class imbalance, within class imbalance, along with large number of features, adds additional complexity and results in poor performance of the classifier. In the current study, we propose an effective distance based feature selection method (ED-Relief) that uses a sophisticated distance measure, in order to tackle simultaneous occurrence of between and within class imbalance. This method has been tested on a variety of simulated experiments and real life data sets and the results are compared with the traditional Relief method and some of the well known recent distance based feature selection methods. The results clearly show the superiority of the proposed effective distance based feature selection method.},
journal = {Applied Intelligence},
month = mar,
pages = {717–745},
numpages = {29},
keywords = {Jeffreys divergence, Classification, Effective distance, Feature selection, Imbalanced data}
}

@article{10.1016/j.patcog.2017.09.036,
author = {Zhu, Pengfei and Xu, Qian and Hu, Qinghua and Zhang, Changqing and Zhao, Hong},
title = {Multi-label feature selection with missing labels},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.09.036},
doi = {10.1016/j.patcog.2017.09.036},
abstract = {This is the first attempt to conduct feature selection for multi-label classification with missing labels.An embedded feature selection method is proposed with which feature selection can be conducted during the process of label recovery.The effective l_2,p-norm regularization is imposed on the feature selection matrix to select the most discriminative features and remove noisy ones at the same time.Label dependency is incorporated into the model to exploit label correlations. The consistently increasing of the feature dimension brings about great time complexity and storage burden for multi-label learning. Numerous multi-label feature selection techniques are developed to alleviate the effect of high-dimensionality. The existing multi-label feature selection algorithms assume that the labels of the training data are complete. However, this assumption does not always hold true for labeling data is costly and there is ambiguity among classes. Hence, in real-world applications, the data available usually have an incomplete set of labels. In this paper, we present a novel multi-label feature selection model under the circumstance of missing labels. With the proposed algorithm, the most discriminative features are selected and missing labels are recovered simultaneously. To remove the irrelevant and noisy features, the effective l2, p-norm (0},
journal = {Pattern Recogn.},
month = feb,
pages = {488–502},
numpages = {15},
keywords = {Multi-label learning, Missing labels, Feature selection}
}

@article{10.1016/j.comnet.2021.108591,
author = {Wazirali, Raniyah and Ahmad, Rami and Abu-Ein, Ashraf Abdel-Karim},
title = {Sustaining accurate detection of phishing URLs using SDN and feature selection approaches},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108591},
doi = {10.1016/j.comnet.2021.108591},
journal = {Comput. Netw.},
month = dec,
numpages = {13},
keywords = {Phishing detection, Deep learning, CNN, SDN, Phishing URL, Recursive feature elimination}
}

@inproceedings{10.1145/3121050.3121055,
author = {Widmann, Natalie and Verberne, Suzan},
title = {Graph-based Semi-supervised Learning for Text Classification},
year = {2017},
isbn = {9781450344906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121050.3121055},
doi = {10.1145/3121050.3121055},
abstract = {In this paper, we propose a graph-based representation of document collections in which both documents and features are represented by nodes. The nodes are connected with weights based on word order, context similarity and word frequency. Graph-based representations can overcome the limitations of bag-of-words based representations that suffer from sparseness for collections with short documents. In a series of experiments, we evaluate multiple types of graph-based text features in the context of semi-supervised text classification, and investigate the effect of the number of labeled documents in the collection. We find that graph-based semi-supervised learning outperforms bag-of-words semi-supervised learning but not bag-of-words supervised learning in 20-class text categorization. A large asset of graph-based representations is that they are flexible in the types of nodes and relations that are included.},
booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {59–66},
numpages = {8},
keywords = {text representation, text classification, semi-supervised learning, graph models},
location = {Amsterdam, The Netherlands},
series = {ICTIR '17}
}

@article{10.1007/s00500-019-04383-8,
author = {\"{O}zyurt, Fatih},
title = {A fused CNN model for WBC detection with MRMR feature selection and extreme learning machine},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {11},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04383-8},
doi = {10.1007/s00500-019-04383-8},
abstract = {White blood cell (WBC) test is used to diagnose many diseases, particularly infections, ranging from allergies to leukemia. A physician needs clinical experience to detect and classify the amount of WBCs in human blood. WBCs are divided into four subclasses: eosinophils, lymphocytes, monocytes, and neutrophils. In the present study, pre-trained architectures, namely AlexNet, VGG-16, GoogleNet, and ResNet, were used as feature extractors. The features obtained from the last fully connected layers of these architectures were combined. Efficient features were selected using the minimum redundancy maximum relevance method. Finally, unlike classical convolutional neural network (CNN) architectures, the extreme learning Machine (ELM) classifier was used in the classification stage thanks to the efficient features obtained from CNN architectures. Experimental results indicated that efficient CNN features yielded satisfactory results in a shorter execution time via ELM classification with an accuracy rate of 96.03%.},
journal = {Soft Comput.},
month = jun,
pages = {8163–8172},
numpages = {10},
keywords = {MRMR algorithm, Extreme learning machine, Convolutional neural networks, Deep learning, White blood cell detection}
}

@inproceedings{10.1145/3424311.3424330,
author = {Umar, Mubarak Albarka and Zhanfang, Chen and Liu, Yan},
title = {Network Intrusion Detection Using Wrapper-based Decision Tree for Feature Selection},
year = {2020},
isbn = {9781450377348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424311.3424330},
doi = {10.1145/3424311.3424330},
abstract = {One of the key challenges of the machine learning (ML) based intrusion detection system (IDS) is the expensive computation time which is largely caused by the redundant, incomplete, and unrelated features contain in the IDS datasets. To overcome such challenges and ensure building efficient and more accurate IDS models, many researchers utilize preprocessing techniques such as normalization and feature selection, and a hybrid modeling approach is typically used. In this work, we propose a hybrid IDS modeling approach with an algorithm for feature selection (FS) and another for building the IDS. The FS method is a wrapper-based FS with a decision tree as the feature evaluator. Five selected ML algorithms are individually used in combination with the proposed FS method to build five IDS models using the UNSW-NB15 dataset. As a baseline, five more IDS models are built, in a single modeling approach, using the full features of the datasets. We evaluate the effectiveness of our proposed method by comparing it with the baseline models and also with state-of-the-art works. Our method achieves the best DR of 97.95% and proved to be quite effective in comparison to state-of-the-art works. We, therefore, recommend its usage especially in IDS modeling with the UNSW-NB15 dataset.},
booktitle = {Proceedings of the 2020 International Conference on Internet Computing for Science and Engineering},
pages = {5–13},
numpages = {9},
keywords = {UNSW-NB15, Machine Learning Algorithms IDS Dataset, Intrusion Detection System, Intrusion Detection, Feature Selection},
location = {Male, Maldives},
series = {ICICSE '20}
}

@article{10.1007/s00521-020-05457-7,
author = {Liu, Dunnan and Xu, Xiaofeng and Liu, Mingguang and Liu, Yaling},
title = {Dynamic traffic classification algorithm and simulation of energy Internet of things based on machine learning},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {9},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05457-7},
doi = {10.1007/s00521-020-05457-7},
abstract = {With the rapid development of information technology, a large amount of traffic generated by various Internet applications occupies a large amount of network resources. It poses a huge challenge to service quality and has a negative impact on Internet security. In order to utilize network resources effectively and provide effective management and control measures for network administrators, network traffic classification technologies is a hot topic for scientists to identify application layer protocols. Today, there are more and more applications based on TCP/IP. With the emergence of various anti-surveillance applications, traditional port and application-based identification methods are difficult to meet current or future traffic identification requirements. It has become a very challenging problem to require more efficient, accurate, intelligent and real-time Internet traffic identification. The Internet of Things is a new network concept proposed by people who based on Internet prototypes. It enables the end user of the system can carry out communication and exchange of information and data between any project. In recent years, with the continuous advancement of Internet of Things technology, the coverage of the Internet of Things has become very wide, and the number of different types of networks that make up the Internet of Things is also increasing. This paper aims to find the dynamic network traffic classification problem of hybrid fixed in dynamic network and dynamic network in mobile network, and gives a reasonable mapping scheme. The dynamics of network traffic for Internet of Things are reflected fully and will not cause route flapping. The simulation results show that the decision tree classification algorithm in machine learning has higher efficiency, and improves the utilization of network resources.},
journal = {Neural Comput. Appl.},
month = may,
pages = {3967–3976},
numpages = {10},
keywords = {Traffic Classification, Network Traffic, Internet of Things, Machine Learning}
}

@inproceedings{10.1145/3457682.3457758,
author = {Deng, Ren and Liu, Ye and Luo, Liyan and Chen, DongJing and Li, Xijie},
title = {Unsupervised Feature Selection using Pseudo Label Approximation},
year = {2021},
isbn = {9781450389310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457682.3457758},
doi = {10.1145/3457682.3457758},
abstract = {Feature selection is a machine learning technique that selects a representative subset of all features available in order to reduce the time and space needed to process high-dimensional data. Traditional feature selection methods include filter, wrapper, and embedded approaches. However, many conventional methods’ performances are not suitable in many contexts. This paper proposes a new unsupervised feature selection model based on pseudo label approximation. The new derived model incorporates a projection error, a sparsity regularization, and a manifold regularization term that preserves the manifold structure of the original data. Finally, implementation of the new model onto five distinct datasets validates the effectiveness of the proposed model.},
booktitle = {Proceedings of the 2021 13th International Conference on Machine Learning and Computing},
pages = {498–502},
numpages = {5},
keywords = {Pseudo label matrix, High-dimensional data, Feature selection},
location = {Shenzhen, China},
series = {ICMLC '21}
}

@article{10.1007/s00521-020-05058-4,
author = {Borg, Anton and Boldt, Martin and Rosander, Oliver and Ahlstrand, Jim},
title = {E-mail classification with machine learning and word embeddings for improved customer support},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05058-4},
doi = {10.1007/s00521-020-05058-4},
abstract = {Classifying e-mails into distinct labels can have a great impact on customer support. By using machine learning to label e-mails, the system can set up queues containing e-mails of a specific category. This enables support personnel to handle request quicker and more easily by selecting a queue that match their expertise. This study aims to improve a manually defined rule-based algorithm, currently implemented at a large telecom company, by using machine learning. The proposed model should have higher F1-score and classification rate. Integrating or migrating from a manually defined rule-based model to a machine learning model should also reduce the administrative and maintenance work. It should also make the model more flexible. By using the frameworks, TensorFlow, Scikit-learn and Gensim, the authors conduct a number of experiments to test the performance of several common machine learning algorithms, text-representations, word embeddings to investigate how they work together. A long short-term memory network showed best classification performance with an F1-score of 0.91. The authors conclude that long short-term memory networks outperform other non-sequential models such as support vector machines and AdaBoost when predicting labels for e-mails. Further, the study also presents a Web-based interface that were implemented around the LSTM network, which can classify e-mails into 33 different labels.},
journal = {Neural Comput. Appl.},
month = mar,
pages = {1881–1902},
numpages = {22},
keywords = {Natural language processing, Long short-term memory, Machine learning, E-mail classification}
}

@article{10.1016/j.knosys.2019.105052,
author = {Gonzalez-Lopez, Jorge and Ventura, Sebasti\'{a}n and Cano, Alberto},
title = {Distributed multi-label feature selection using individual mutual information measures},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {188},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2019.105052},
doi = {10.1016/j.knosys.2019.105052},
journal = {Know.-Based Syst.},
month = jan,
numpages = {13},
keywords = {Apache spark, Distributed computing, Mutual information, Feature selection, Multi-label learning}
}

@article{10.1016/j.future.2021.06.019,
author = {Gan, Hongxiao and Zhang, Jinglan and Towsey, Michael and Truskinger, Anthony and Stark, Debra and van Rensburg, Berndt J. and Li, Yuefeng and Roe, Paul},
title = {A novel frog chorusing recognition method with acoustic indices and machine learning},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2021.06.019},
doi = {10.1016/j.future.2021.06.019},
journal = {Future Gener. Comput. Syst.},
month = dec,
pages = {485–495},
numpages = {11},
keywords = {Frog chorus recognition, Machine learning, Acoustic indices, Species recognition, Ecoacoustics}
}

@inproceedings{10.1145/2000259.2000264,
author = {Kavimandan, Amogh and Gokhale, Aniruddha and Karsai, Gabor and Gray, Jeff},
title = {Managing the quality of software product line architectures through reusable model transformations},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000264},
doi = {10.1145/2000259.2000264},
abstract = {In model-driven engineering of applications, the quality of the software architecture is realized and preserved in the successive stages of its lifecycle through model transformations. However, limited support for reuse in contemporary model transformation techniques forces developers of product line architectures to reinvent transformation rules for every variant of the product line, which can adversely impact developer productivity and in turn degrade the quality of the resulting software architecture for the variant. To overcome these challenges, this paper presents the MTS (Model-transformation Templatization and Specialization generative transformation process, which promotes reuse in model transformations through parameterization and specialization of transformation rules. MTS defines two higher order transformations to capture the variability in transformation rules and to specialize them across product variants. The core idea behind MTS is realized within a graphical model transformation tool in a way that is minimally intrusive to the underlying tool's implementation. The paper uses two product line case studies to evaluate MTS in terms of reduction in efforts to define model transformation rules as new variants are added to the product line, and the overhead in executing the higher order transformations. These metrics provide an indirect measure of how potential degradation in the quality of software architectures of product lines caused due to lack of reuse can be alleviated by MTS.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {13–22},
numpages = {10},
keywords = {software quality, reuse, model transformations},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/3453800.3453805,
author = {Ivanova, Malinka and Rozeva, Anna},
title = {Detection of XSS Attack and Defense of REST Web Service – Machine Learning Perspective},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453805},
doi = {10.1145/3453800.3453805},
abstract = {The paper presents a machine learning approach for detection of stored XSS attack and for defense of REST web service. For this purpose, a XML-based REST web service is developed in JAVA, which is tested and attacked in specially created test-bed simulation environment, consisting of IntelliJ IDEA environment, Postman and web browser. The obtained data sets are processed resulting in the selection of 30 out of 171 features for further treatment. Supervised machine learning classifiers: Random Forest, Random Tree, Decision Tree and Gradient Boosted Tree are used for the detection of known attacks and clustering algorithm k-Means for the identification of unknown threats. The efficiency of implementing machine learning algorithms is evaluated and the results confirm their high accuracy. In addition fuzzy sets and fuzzy logic theory is utilized for solving multi-criteria task in support of decision making for web service defense.},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {22–28},
numpages = {7},
keywords = {machine learning, fuzzy logic, XSS stored attack, REST web service defense},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@article{10.1016/j.jbi.2018.07.015,
author = {Urbanowicz, Ryan J. and Olson, Randal S. and Schmitt, Peter and Meeker, Melissa and Moore, Jason H.},
title = {Benchmarking relief-based feature selection methods for bioinformatics data mining},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {85},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2018.07.015},
doi = {10.1016/j.jbi.2018.07.015},
journal = {J. of Biomedical Informatics},
month = sep,
pages = {168–188},
numpages = {21},
keywords = {Regression, Classification, Genetic heterogeneity, Epistasis, ReliefF, Feature selection}
}

@article{10.1016/j.compbiomed.2020.104135,
author = {Nazari, Mostafa and Shiri, Isaac and Zaidi, Habib},
title = {Radiomics-based machine learning model to predict risk of death within 5-years in clear cell renal cell carcinoma patients},
year = {2021},
issue_date = {Feb 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2020.104135},
doi = {10.1016/j.compbiomed.2020.104135},
journal = {Comput. Biol. Med.},
month = feb,
numpages = {8},
keywords = {Renal cell carcinoma, Survival prediction, CT, Machine learning, Radiomics}
}

@inproceedings{10.1007/978-3-030-32047-8_15,
author = {Bedo, Marcos and Ciaccia, Paolo and Martinenghi, Davide and de Oliveira, Daniel},
title = {A k–Skyband Approach for Feature Selection},
year = {2019},
isbn = {978-3-030-32046-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32047-8_15},
doi = {10.1007/978-3-030-32047-8_15},
abstract = {Distance concentration is a phantom menace for the labeling of high dimensional data by distance-based classifiers. Filter methods reduce data dimensionality, but they also add their ranking bias indirectly into the classification procedure. In this study, we examine the filtering problem from another perspective, in which multiple filters are aggregated according to classifiers’ constraints. Our approach, named S-Filter, is designed as a top-k skyline (k-skyband) search over multiple rankings by relying on the concept of –dominance for weighted and monotone linear functions. Unlike existing approaches, S-Filter provides a deterministic strategy for joining multiple filters and avoids the semantic problem of breaking top-k ties. S-Filter’s first stage uses labeling-driven measures, e.g., F1-Score, for assessing the quality of each filter with regards to a particular classifier, whereas range-tolerance intervals around the initial quality measures define the partial search weights. Next, S-Filter applies the FSA instance-optimal algorithm for selecting all the dimensions that can be among the top-k for a weight within the range-tolerance intervals. Experiments on high dimensional datasets show that S-Filter outperforms state-of-the-art filters in two scenarios: (i)&nbsp;exploratory analysis on varying k and range-tolerance intervals, and (ii)&nbsp;data reduction to its intrinsic dimensionality.},
booktitle = {Similarity Search and Applications: 12th International Conference, SISAP 2019, Newark, NJ, USA, October 2–4, 2019, Proceedings},
pages = {160–168},
numpages = {9},
keywords = {Feature selection, Filters, Skyline queries, Classification},
location = {Newark, NJ, USA}
}

@article{10.1007/s11042-019-07811-x,
author = {Ghosh, Manosij and Kundu, Tuhin and Ghosh, Dipayan and Sarkar, Ram},
title = {Feature selection for facial emotion recognition using late hill-climbing based memetic algorithm},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {18},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-07811-x},
doi = {10.1007/s11042-019-07811-x},
abstract = {Facial Emotion Recognition (FER) is an important research domain which allows us to provide a better interactive environment between humans and computers. Some standard and popular features extracted from facial expression images include Uniform Local Binary Pattern (uLBP), Horizontal-Vertical Neighborhood Local Binary Pattern (hvnLBP), Gabor filters, Histogram of Oriented Gradients (HOG) and Pyramidal HOG (PHOG). However, these feature vectors may contain some features that are irrelevant or redundant in nature, thereby increasing the overall computational time as well as recognition error of a classification system. To counter this problem, we have proposed a new feature selection (FS) algorithm based on Late Hill Climbing and Memetic Algorithm (MA). A novel local search technique called Late Acceptance Hill Climbing through Redundancy and Relevancy (LAHCRR) has been used in this regard. It combines the concepts of Local Hill-Climbing and minimal-Redundancy Maximal-Relevance (mRMR) to form a more effective local search mechanism in MA. The algorithm is then evaluated on the said feature vectors extracted from the facial images of two popular FER datasets, namely RaFD and JAFFE. LAHCRR is used as local search in MA to form Late Hill Climbing based Memetic Algorithm (LHCMA). LHCMA is compared with state-of-the-art methods. The experimental outcomes show that the proposed FS algorithm reduces the feature dimension to a significant amount as well as increases the recognition accuracy as compared to other methods.},
journal = {Multimedia Tools Appl.},
month = sep,
pages = {25753–25779},
numpages = {27},
keywords = {JAFFE, RAFD, Facial Emotion Recognition, Memetic Algorithm, Late Acceptance Hill Climbing, Feature Selection}
}

@inproceedings{10.1145/2791060.2791086,
author = {Martinez, Jabier and Ziadi, Tewfik and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques and Le Traon, Yves},
title = {Bottom-up adoption of software product lines: a generic and extensible approach},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791086},
doi = {10.1145/2791060.2791086},
abstract = {Although Software Product Lines are recurrently praised as an efficient paradigm for systematic reuse, practical adoption remains challenging. For bottom-up Software Product Line adoption, where a set of artefact variants already exists, practitioners lack end-to-end support for chaining (1) feature identification, (2) feature location, (3) feature constraints discovery, as well as (4) reengineering approaches. This challenge can be overcome if there exists a set of principles for building a framework to integrate various algorithms and to support different artefact types. In this paper, we propose the principles of such a framework and we provide insights on how it can be extended with adapters, algorithms and visualisations enabling their use in different scenarios. We describe its realization in BUT4Reuse (Bottom--Up Technologies for Reuse) and we assess its generic and extensible properties by implementing a variety of extensions. We further empirically assess the complexity of integration by reproducing case studies from the literature. Finally, we present an experiment where users realize a bottom-up Software Product Line adoption building on the case study of Eclipse variants.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {101–110},
numpages = {10},
keywords = {software product line engineering, reverse engineering, mining existing assets},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1016/j.eswa.2020.114022,
author = {Rauber, Thomas Walter and da Silva Loca, Antonio Luiz and Boldt, Francisco de Assis and Rodrigues, Alexandre Loureiros and Varej\~{a}o, Fl\'{a}vio Miguel},
title = {An experimental methodology to evaluate machine learning methods for fault diagnosis based on vibration signals},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114022},
doi = {10.1016/j.eswa.2020.114022},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {18},
keywords = {Machine learning, Pattern recognition, Classification, Performance criteria, CWRU bearing fault database, Fault detection}
}

@article{10.1016/j.ins.2019.09.010,
author = {Wang, Hao and Ling, Zhaolong and Yu, Kui and Wu, Xindong},
title = {Towards efficient and effective discovery of Markov blankets for feature selection},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {509},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.09.010},
doi = {10.1016/j.ins.2019.09.010},
journal = {Inf. Sci.},
month = jan,
pages = {227–242},
numpages = {16},
keywords = {Feature selection, Bayesian network, Markov blanket}
}

@inproceedings{10.1145/3307630.3342415,
author = {Chavarriaga, Jaime and Casallas, Rubby and Parra, Carlos and Henao-Mej\'{\i}a, Martha Cecilia and Calle-Archila, Carlos Ricardo},
title = {Nine Years of Courses on Software Product Lines at Universidad de los Andes, Colombia},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342415},
doi = {10.1145/3307630.3342415},
abstract = {Software Product Lines has been taught in Universidad de los Andes, Colombia, since 2011. The content, activities and evaluation in these courses have changed during this period of time. For instance, while topics such as the processes to engineer product lines, feature models to specify domain variability, and design patterns to implement the variability are common to all these courses, other topics such as the product line maturity levels, some techniques to implement variability and recent automation practices for testing, continuous integration and delivery have varied with the time. In addition, topics and activities, such as the course project that has been present in all the courses, had also been modified. This paper (1) describes the evolution of our courses on Software Product Lines, presenting commonalities and variabilities in their topics, activities and evaluation techniques and (2) discusses some lessons learned during its recent design as a Blended Learning course.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {130–133},
numpages = {4},
keywords = {variability, teaching, software product lines},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: a benchmarking platform for uniform random sampling techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
doi = {10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5},
keywords = {variability model, software product lines, sampling, configurable systems, benchmark, SAT},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1504/ijcat.2020.107913,
author = {Samy, Ahmed and Hosny, Khalid M. and Zaied, Abdel-Naser H.},
title = {An efficient binary whale optimisation algorithm with optimum path forest for feature selection},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {63},
number = {1–2},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.107913},
doi = {10.1504/ijcat.2020.107913},
abstract = {Feature selection is an essential process which aims to find the most representative features for image processing and computer vision applications where utilising selected features reduces required time for classification and increases the classification rate. In this study, a new binary whale optimisation algorithm for feature selection is proposed. This optimisation algorithm is based on whales' behaviour. The Optimum-Path Forest (OPF) technique is used as an objective function. This function is much faster than the other classification techniques. The proposed binary whale optimisation algorithm is evaluated using five datasets of colour images. The proposed algorithm outperformed existing optimisation algorithms such as Particle Swarm Optimisation Algorithm (PSOA), Firefly Algorithm (FFA), Gravitational Search Algorithm (GSA), Binary Harmony Search (BHS), Binary Clonal Flower Pollination Algorithm (BCFA), Binary Cuckoo Search Algorithm (BCSA), and Binary Bat Algorithm (BBA) in terms of classification accuracy, number of selected features and execution times.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {41–54},
numpages = {13},
keywords = {machine learning, meta-heuristic, feature selection, optimum-path forest, OPF, whale optimisation algorithm, WOA}
}

@article{10.1007/s11042-019-7370-5,
author = {Atallah, Dalia M. and Badawy, Mohammed and El-Sayed, Ayman and Ghoneim, Mohamed A.},
title = {Predicting kidney transplantation outcome based on hybrid feature selection and KNN classifier},
year = {2019},
issue_date = {Jul 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7370-5},
doi = {10.1007/s11042-019-7370-5},
abstract = {Kidney transplantation outcome prediction is very significant and doesn't require emphasis. This will grant the selection of the best available kidney donor and the best immunosuppressive treatment for patients. Survival prediction before treatment could simplify patient's decision making and boost survival by altering clinical practice. This paper proposes a new novel prediction method based on data mining techniques to predict five-year graft survival after transplantation. This new proposed prediction method composes of three stages: data preparation stage (DPS), feature selection stage (FSS), and prediction stage (PS). The new proposed prediction method merges information gain with na\"{\i}ve Bayes and k-nearest neighbor. Initially, it uses information gain to select the essential features, uses na\"{\i}ve Bayes to select the most essential features. These two methods are combined in a new hybrid feature selection method which chooses the minimum number of features that produce highest accuracy. Finally, it uses k-nearest neighbor for graft survival prediction classification. The proposed prediction method has been evaluated against recent techniques. Experimental results have proven that the proposed prediction method outperforms the recent techniques as it attains the maximum accuracy and F-measure with minimal errors. This prediction method can also be used in other transplant datasets.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {20383–20407},
numpages = {25},
keywords = {Na\"{\i}ve Bayes, Kidney transplantation, K-nearest neighbor, Information gain, Feature selection}
}

@article{10.1007/s11280-017-0497-2,
author = {Zhu, Yonghua and Zhang, Xuejun and Wang, Ruili and Zheng, Wei and Zhu, Yingying},
title = {Self-representation and PCA embedding for unsupervised feature selection},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-017-0497-2},
doi = {10.1007/s11280-017-0497-2},
abstract = {Feature selection is an important preprocessing step for dealing with high dimensional data. In this paper, we propose a novel unsupervised feature selection method by embedding a subspace learning regularization (i.e., principal component analysis (PCA)) into the sparse feature selection framework. Specifically, we select informative features via the sparse learning framework and consider preserving the principal components (i.e., the maximal variance) of the data at the same time, such that improving the interpretable ability of the feature selection model. Furthermore, we propose an effective optimization algorithm to solve the proposed objective function which can achieve stable optimal result with fast convergence. By comparing with five state-of-the-art unsupervised feature selection methods on six benchmark and real-world datasets, our proposed method achieved the best result in terms of classification performance.},
journal = {World Wide Web},
month = nov,
pages = {1675–1688},
numpages = {14},
keywords = {Subspace learning, Sparse learning, Principal component analysis, Feature selection, Dimensionality reduction}
}

@article{10.1016/j.asoc.2019.106031,
author = {Xue, Yu and Tang, Tao and Pang, Wei and Liu, Alex X.},
title = {Self-adaptive parameter and strategy based particle swarm optimization for large-scale feature selection problems with multiple classifiers},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {88},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.106031},
doi = {10.1016/j.asoc.2019.106031},
journal = {Appl. Soft Comput.},
month = mar,
numpages = {12},
keywords = {Classification, Self-adaptive, Large-scale problems, Feature selection, Particle swarm optimization, 99-00, 00-01}
}

@article{10.1016/j.patcog.2019.04.024,
author = {Yin, Qiyue and Zhang, Junge and Wu, Shu and Li, Hexi},
title = {Multi-view clustering via joint feature selection and partially constrained cluster label learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {93},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.04.024},
doi = {10.1016/j.patcog.2019.04.024},
journal = {Pattern Recogn.},
month = sep,
pages = {380–391},
numpages = {12},
keywords = {Cluster indicator, Prior information, Feature selection, Multi-view clustering}
}

@article{10.1016/j.procs.2021.08.048,
author = {Ghosh, Pronab and Azam, Sami and Karim, Asif and Hassan, Mehedi and Roy, Kuber and Jonkman, Mirjam},
title = {A Comparative Study of Different Machine Learning Tools in Detecting Diabetes},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.08.048},
doi = {10.1016/j.procs.2021.08.048},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {467–477},
numpages = {11},
keywords = {Random Forest, AdaBoost, Support Vector Machine (RBF kernel), Gradient Boosting, MRMR}
}

@article{10.1007/s11042-021-10672-y,
author = {Nizami, Imran Fareed and Akhtar, Mehreen and Waqar, Asad and Mann, Amer Bilal and Majid, Muhammad},
title = {Multiply distorted image quality assessment based on feature level fusion and optimal feature selection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {14},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10672-y},
doi = {10.1007/s11042-021-10672-y},
abstract = {No reference image quality assessment (NR-IQA) has received considerable importance in the last decade due to a rise in the use of multimedia content in our daily lives. Due to limitations in technology, multiple distortions may be introduced in the images that need to be assessed. Recently feature selection has shown promising results for single distorted NR-IQA and their effectiveness on multiple distorted images still need to be addressed. In this paper, impact of feature level fusion and feature selection on multiple distorted image quality assessment is presented. To this end features are extracted from multiple distorted images using six NR-IQA techniques (BLIINDS-II, BRISQUE, CurveletQA, DIIVINE, GM-LOG, SSEQ) that extract features in different (discrete cosine transform, spatial, curvelet transform, wavelet transform, spatial and gradient, spatial and spectral) domains. The extracted features from different domains are fused to generate a single feature vector. All combinations of feature-level fusion from six different techniques have been evaluated. Three different feature selection algorithms (genetic search, linear forward search, particle swarm optimization) are then applied to select optimum features for NR-IQA. The selected features are then used by the support vector regression model to predict the quality score. The performance of the proposed methodology is evaluated for two multiple distorted IQA databases (LIVE multiple distorted image dataset (LIVEMD), multiply distorted image database (MDID2017)), two singly synthetically distorted IQA databases (Tampere image database (TID2013), Computational and subjective image quality database (CSIQ)), and one screen content IQA database (Screen content image quality database (SIQAD)). Experimental results show that the fusion of features from different domains gives better performance in comparison to existing multiple-distorted NR-IQA techniques with SROCC scores of 0.9555, 0.9587, 0.6892, 0.9452, and 0.7682 on the LIVEMD, MDID, TID2013, CSIQ, and SIQAD databases respectively. Moreover, the performance is further improved when the genetic search feature selection algorithm is applied to fused features to remove the redundant and irrelevant features. The SROCC scores are improved to 0.9691, 0.9723, and 0.6897 for LIVEMD, MDID, and TID2013 databases respectively.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {21843–21883},
numpages = {41},
keywords = {Feature level fusion, Feature extraction, Feature selection, Multiply distorted images, No reference image quality assessment}
}

@article{10.1016/j.procs.2019.11.159,
author = {Shan Lee, Vivian Lay and Gan, Keng Hoon and Tan, Tien Ping and Abdullah, Rosni},
title = {Semi-supervised Learning for Sentiment Classification using Small Number of Labeled Data},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {161},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.11.159},
doi = {10.1016/j.procs.2019.11.159},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {577–584},
numpages = {8},
keywords = {sentiment classification, deep learning, Semi-supervised learning}
}

@inproceedings{10.1007/978-3-030-34223-4_6,
author = {Balasubramaniam, Thirunavukarasu and Nayak, Richi and Yuen, Chau},
title = {Transfer Learning via Feature Selection Based Nonnegative Matrix Factorization},
year = {2020},
isbn = {978-3-030-34222-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-34223-4_6},
doi = {10.1007/978-3-030-34223-4_6},
abstract = {Transfer learning has been successfully used in recommender systems to deal with the data sparsity problem. Existing techniques assume that the source and target domains share the same feature space. This paper proposes a new direction in transfer learning where the source and target domains can have different feature space. The proposed technique, Feature Selection based Nonnegative Matrix Factorization (FSNMF), selects the useful features that can minimize the cost function of the target domain. The features of the source domain are learned using NMF and their importance is measured using the gradient principle. Experiments with real-world datasets show the effectiveness of FSNMF in comparison to state-of-the-art relevant transfer learning techniques.},
booktitle = {Web Information Systems Engineering – WISE 2019: 20th International Conference, Hong Kong, China, January 19–22, 2020, Proceedings},
pages = {82–97},
numpages = {16},
keywords = {Transfer learning, Feature selection, Nonnegative matrix factorization, Recommender systems},
location = {Hong Kong, China}
}

@article{10.1007/s00500-021-05874-3,
author = {Agrawal, Prachi and Ganesh, Talari and Mohamed, Ali Wagdy},
title = {Chaotic gaining sharing knowledge-based optimization algorithm: an improved metaheuristic algorithm for feature selection},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {14},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05874-3},
doi = {10.1007/s00500-021-05874-3},
abstract = {The gaining sharing knowledge based optimization algorithm (GSK) is recently developed metaheuristic algorithm, which is based on how humans acquire and share knowledge during their life-time. This paper investigates a modified version of the GSK algorithm to find the best feature subsets. Firstly, it represents a binary variant of GSK algorithm by employing a probability estimation operator (Bi-GSK) on the two main pillars of GSK algorithm. And then, the chaotic maps are used to enhance the performance of the proposed algorithm. Ten different types of chaotic maps are considered to adapt the parameters of the GSK algorithm that make a proper balance between exploration and exploitation and save the algorithm from premature convergence. To check the performance of proposed approaches of GSK algorithm, twenty-one benchmark datasets are taken from the UCI repository for feature selection. The performance is measured by calculating different type of measures, and several metaheuristic algorithms are adopted to compare the obtained results. The results indicate that Chebyshev chaotic map shows the best result among all chaotic maps which improve the performance accuracy and convergence rate of the original algorithm. Moreover, it outperforms the other metaheuristic algorithms in terms of efficiency, fitness value and the minimum number of selected features.},
journal = {Soft Comput.},
month = jul,
pages = {9505–9528},
numpages = {24},
keywords = {Binary variables, Chaos theory, Gaining sharing knowledge-based optimization algorithm, Chaotic maps, Feature selection}
}

@inproceedings{10.1007/978-3-030-64583-0_19,
author = {Bommert, Andrea and Rahnenf\"{u}hrer, J\"{o}rg},
title = {Adjusted Measures for Feature Selection Stability for Data Sets with Similar Features},
year = {2020},
isbn = {978-3-030-64582-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64583-0_19},
doi = {10.1007/978-3-030-64583-0_19},
abstract = {For data sets with similar features, for example highly correlated features, most existing stability measures behave in an undesired way: They consider features that are almost identical but have different identifiers as different features. Existing adjusted stability measures, that is, stability measures that take into account the similarities between features, have major theoretical drawbacks. We introduce new adjusted stability measures that overcome these drawbacks. We compare them to each other and to existing stability measures based on both artificial and real sets of selected features. Based on the results, we suggest using one new stability measure that considers highly similar features as exchangeable.},
booktitle = {Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020, Siena, Italy, July 19–23, 2020, Revised Selected Papers, Part I},
pages = {203–214},
numpages = {12},
keywords = {Correlated features, Similar features, Stability measures, Feature selection stability},
location = {Siena, Italy}
}

@article{10.1016/j.neucom.2017.12.055,
author = {Lan, Gongmin and Hou, Chenping and Nie, Feiping and Luo, Tingjin and Yi, Dongyun},
title = {Robust feature selection via simultaneous sapped norm and sparse regularizer minimization},
year = {2018},
issue_date = {Mar 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {283},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.12.055},
doi = {10.1016/j.neucom.2017.12.055},
journal = {Neurocomput.},
month = mar,
pages = {228–240},
numpages = {13},
keywords = {ℓ2, p -norm regularization, Capped ℓ2-norm loss, Feature selection}
}

@article{10.1016/j.infsof.2019.05.003,
author = {Espinosa, Edison and Acu\~{n}a, Silvia Teresita and Vegas, Sira and Juristo, Natalia},
title = {Adopting configuration management principles for managing experiment materials in families of experiments},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.003},
doi = {10.1016/j.infsof.2019.05.003},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {39–67},
numpages = {29},
keywords = {Experimental software configuration management, Experimental material, Experiment replication, Experimental software engineering}
}

@article{10.1007/s42979-021-00776-5,
author = {Vakadkar, Kaushik and Purkayastha, Diya and Krishnan, Deepa},
title = {Detection of Autism Spectrum Disorder in Children Using Machine Learning Techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {5},
url = {https://doi.org/10.1007/s42979-021-00776-5},
doi = {10.1007/s42979-021-00776-5},
abstract = {Autism Spectrum Disorder (ASD) is a neurological disorder which might have a lifelong impact on the language learning, speech, cognitive, and social skills of an individual. Its symptoms usually show up in the developmental stages, i.e., within the first two years after birth, and it impacts around 1% of the population globally [. Accessed 25 Dec 2019]. ASD is mainly caused by genetics or by environmental factors; however, its conditions can be improved by detecting and treating it at earlier stages. In the current times, clinical standardized tests are the only methods which are being used, to diagnose ASD. This not only requires prolonged diagnostic time but also faces a steep increase in medical costs. To improve the precision and time required for diagnosis, machine learning techniques are being used to complement the conventional methods. We have applied models such as Support Vector Machines (SVM), Random Forest Classifier (RFC), Na\"{\i}ve Bayes (NB), Logistic Regression (LR), and KNN to our dataset and constructed predictive models based on the outcome. The main objective of our paper is to thus determine if the child is susceptible to ASD in its nascent stages, which would help streamline the diagnosis process. Based on our results, Logistic Regression gives the highest accuracy for our selected dataset.},
journal = {SN Comput. Sci.},
month = jul,
numpages = {9},
keywords = {Autism spectrum disorder, Machine learning, Dataset, Preprocessing, Encoding, SVM, KNN, Random forest, Logistic regression, Confusion matrix, Precision, Recall, F1 score, Accuracy}
}

@inproceedings{10.1145/3472749.3474734,
author = {Fran\c{c}oise, Jules and Caramiaux, Baptiste and Sanchez, T\'{e}o},
title = {Marcelle: Composing Interactive Machine Learning Workflows and Interfaces},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474734},
doi = {10.1145/3472749.3474734},
abstract = {Human-centered approaches to machine learning have established theoretical foundations, design principles and interaction techniques to facilitate end-user interaction with machine learning systems. Yet, general-purpose toolkits supporting the design of interactive machine learning systems are still missing, despite their potential to foster reuse, appropriation and collaboration between different stakeholders including developers, machine learning experts, designers and end users. In this paper, we present an architectural model for toolkits dedicated to the design of human interactions with machine learning. The architecture is built upon a modular collection of interactive components that can be composed to build interactive machine learning workflows, using reactive pipelines and composable user interfaces. We introduce Marcelle, a toolkit for the design of human interactions with machine learning that implements this model. We illustrate Marcelle with two implemented case studies: (1) a HCI researcher conducts user studies to understand novice interaction with machine learning, and (2) a machine learning expert and a clinician collaborate to develop a skin cancer diagnosis system. Finally, we discuss our experience with the toolkit, along with its limitation and perspectives.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {39–53},
numpages = {15},
keywords = {Toolkit., Machine Teaching, Interactive Machine Learning, Architectural Model},
location = {Virtual Event, USA},
series = {UIST '21}
}

@article{10.5555/3319261.3319271,
title = {Hybrid feature selection technique for intrusion detection system},
year = {2019},
issue_date = {January 2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {2},
issn = {1740-0562},
abstract = {High dimensionality's problems have make feature selection as one of the most important criteria in determining the efficiency of intrusion detection systems. In this study we have selected a hybrid feature selection model that potentially combines the strengths of both the filter and the wrapper selection procedure. The potential hybrid solution is expected to effectively select the optimal set of features in detecting intrusion. The proposed hybrid model was carried out using correlation feature selection CFS together with three different search techniques known as best-first, greedy stepwise and genetic algorithm. The wrapper-based subset evaluation uses a random forest RF classifier to evaluate each of the features that were first selected by the filter method. The reduced feature selection on both KDD99 and DARPA 1999 dataset was tested using RF algorithm with ten-fold cross-validation in a supervised environment. The experimental result shows that the hybrid feature selections had produced satisfactory outcome.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jan,
pages = {232–240},
numpages = {9}
}

@article{10.1007/s00521-021-05830-0,
author = {Kumar, Saravanapriya and John, Bagyamani},
title = {A novel gaussian based particle swarm optimization gravitational search algorithm for feature selection and classification},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05830-0},
doi = {10.1007/s00521-021-05830-0},
abstract = {A Gaussian based Particle Swarm Optimization Gravitational Search Algorithm (GPSOGSA) is being proposed for extensive feature selection that serves highly in making effective predictions. GPSOGSA helps to overcome the problem of being stuck into the local optima and influences the local searching ability, thus it aims to bridge the gap of exploration and exploitation. The algorithm also limits the usage of too many parameters like acceleration factors, maximum velocity, inertia weight that plays a vital role in PSO, GSA and PSOGSA. The efficacy of the algorithm has been tested upon unimodal and multimodal benchmark functions. We have also evaluated the performance of the algorithm by applying it on various benchmark datasets. The algorithm uses a wrapper-based approach that includes Support Vector Machine as a learner algorithm, and improves both the execution time and the performance accuracy. The findings show that the proposed algorithm could escape from local optimum and converges faster than the PSO, GSA and PSOGSA algorithms.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12301–12315},
numpages = {15},
keywords = {Support Vector Machine (SVM), Particle Swarm Optimization (PSO), Nature inspired algorithm, Hybrid wrapper-based feature selection, Gaussian Particle Swarm Optimization Gravitational Search Algorithm (GPSOGSA), Gravitational Search Algorithm (GSA), Feature selection, Classification}
}

@article{10.1016/j.procs.2019.06.029,
author = {Wang, Shi Qiang and Gao, Cai Yun and Luo, Chang and Zheng, Gui Mei and Zhou, Yan Nian},
title = {Research on Feature Selection/Attribute Reduction Method Based on Rough Set Theory},
year = {2019},
issue_date = {2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {154},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2019.06.029},
doi = {10.1016/j.procs.2019.06.029},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {194–198},
numpages = {5},
keywords = {attribute reduction, feature selection, attribute set, rough set theory, radar emitter signal}
}

@inproceedings{10.1007/978-3-030-46150-8_19,
author = {Hosseini, Babak and Hammer, Barbara},
title = {Interpretable Discriminative Dimensionality Reduction and Feature Selection on the Manifold},
year = {2019},
isbn = {978-3-030-46149-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-46150-8_19},
doi = {10.1007/978-3-030-46150-8_19},
abstract = {Dimensionality reduction (DR) on the manifold includes effective methods which project the data from an implicit relational space onto a vectorial space. Regardless of the achievements in this area, these algorithms suffer from the lack of interpretation of the projection dimensions. Therefore, it is often difficult to explain the physical meaning behind the embedding dimensions. In this research, we propose the interpretable kernel DR algorithm (I-KDR) as a new algorithm which maps the data from the feature space to a lower dimensional space where the classes are more condensed with less overlapping. Besides, the algorithm creates the dimensions upon local contributions of the data samples, which makes it easier to interpret them by class labels. Additionally, we efficiently fuse the DR with feature selection task to select the most relevant features of the original space to the discriminative objective. Based on the empirical evidence, I-KDR provides better interpretations for embedding dimensions as well as higher discriminative performance in the embedded space compared to the state-of-the-art and popular DR algorithms.},
booktitle = {Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, W\"{u}rzburg, Germany, September 16–20, 2019, Proceedings, Part I},
pages = {310–326},
numpages = {17},
keywords = {Supervised, Interpretability, Dimensionality reduction},
location = {W\"{u}rzburg, Germany}
}

@article{10.1016/j.eswa.2021.114818,
author = {Lattanzi, Emanuele and Freschi, Valerio},
title = {Machine Learning Techniques to Identify Unsafe Driving Behavior by Means of In-Vehicle Sensor Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {176},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114818},
doi = {10.1016/j.eswa.2021.114818},
journal = {Expert Syst. Appl.},
month = aug,
numpages = {10},
keywords = {Road safety, Driving behavior, Machine learning, Neural networks, Support vector machines}
}

@article{10.1287/ijoc.2018.0868,
author = {Won, Daehan and Manzour, Hasan and Chaovalitwongse, Wanpracha},
title = {Convex Optimization for Group Feature Selection in Networked Data},
year = {2020},
issue_date = {Winter 2020},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {32},
number = {1},
issn = {1526-5528},
url = {https://doi.org/10.1287/ijoc.2018.0868},
doi = {10.1287/ijoc.2018.0868},
abstract = {Feature selection is at the heart of machine learning, and it is effective at facilitating data interpretability and improving prediction performance by defying the curse of dimensionality. Group feature selection is often used to reveal relationships in structured data and provide better predictive power compared with the standard feature selection methods without consideration of the grouped structure. We study a group feature selection problem in networked data in which edge weights are considered as features, while each node in the network is regarded as a group feature. This problem is particularly useful in feature selection for neuroimaging data, where the data are high dimensional and the intrinsic networked structure among the features (i.e., connectivities between regions) in brain data has to be captured properly. We propose a mathematical model based on the support vector machines (SVM), which entails the ℓ0 norm regularization to restrict the number of nodes (i.e., groups). To cope with the computational challenge of the ℓ0 norm regularization, we develop a convex relaxation reformulation of the proposed model as a convex semiinfinite programming (SIP). We then introduce a new iterative algorithm that achieves an optimal solution for this convex SIP. Experimental results for synthetic and real brain network data sets show that our approach gives better predictive performance compared with the state-of-the-art group feature selection and the standard feature selection methods. Our technique additionally yields a sparse subnetwork solution that is easier to interpret than those obtained by other methods.},
journal = {INFORMS J. on Computing},
month = jan,
pages = {182–198},
numpages = {17},
keywords = {support vector machines, convex optimization, multiple kernel learning, classification, semiinfinite programming}
}

@article{10.1007/s11280-017-0508-3,
author = {Wang, Ruili and Zong, Ming},
title = {Joint self-representation and subspace learning for unsupervised feature selection},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-017-0508-3},
doi = {10.1007/s11280-017-0508-3},
abstract = {This paper proposes a novel unsupervised feature selection method by jointing self-representation and subspace learning. In this method, we adopt the idea of self-representation and use all the features to represent each feature. A Frobenius norm regularization is used for feature selection since it can overcome the over-fitting problem. The Locality Preserving Projection (LPP) is used as a regularization term as it can maintain the local adjacent relations between data when performing feature space transformation. Further, a low-rank constraint is also introduced to find the effective low-dimensional structures of the data, which can reduce the redundancy. Experimental results on real-world datasets verify that the proposed method can select the most discriminative features and outperform the state-of-the-art unsupervised feature selection methods in terms of classification accuracy, standard deviation, and coefficient of variation.},
journal = {World Wide Web},
month = nov,
pages = {1745–1758},
numpages = {14},
keywords = {Unsupervised feature selection, Subspace learning, Self-representation}
}

@article{10.3233/JIFS-189481,
author = {Wang, Linuo and Ramachandran, Varatharajan},
title = {Simulation of sports movement training based on machine learning and brain-computer interface},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-189481},
doi = {10.3233/JIFS-189481},
abstract = {Injuries and hidden dangers in training have a greater impact on athletes ’careers. In particular, the brain function that controls the motor function area has a greater impact on the athlete ’s competitive ability. Based on this, it is necessary to adopt scientific methods to recognize brain functions. In this paper, we study the structure of motor brain-computer and improve it based on traditional methods. Moreover, supported by machine learning and SVM technology, this study uses a DSP filter to convert the preprocessed EEG signal X into a time series, and adjusts the distance between the time series to classify the data. In order to solve the inconsistency of DSP algorithms, a multi-layer joint learning framework based on logistic regression model is proposed, and a brain-machine interface system of sports based on machine learning and SVM is constructed. In addition, this study designed a control experiment to improve the performance of the method proposed by this study. The research results show that the method in this paper has a certain practical effect and can be applied to sports.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6409–6420},
numpages = {12},
keywords = {Machine learning, SVM, sports, brain-computer interface}
}

@article{10.1016/j.infsof.2021.106573,
author = {Zhang, Fanlong and Khoo, Siau-cheng},
title = {An empirical study on clone consistency prediction based on machine learning},
year = {2021},
issue_date = {Aug 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {136},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106573},
doi = {10.1016/j.infsof.2021.106573},
journal = {Inf. Softw. Technol.},
month = aug,
numpages = {16},
keywords = {Machine learning, Software maintenance, Clone consistency prediction, Clone consistent change, Code clones}
}

@inproceedings{10.1007/978-3-030-71158-0_14,
author = {Ferreira, Lu\'{\i}s and Pilastri, Andr\'{e} and Martins, Carlos and Santos, Pedro and Cortez, Paulo},
title = {A Scalable and Automated Machine Learning Framework to Support Risk Management},
year = {2020},
isbn = {978-3-030-71157-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-71158-0_14},
doi = {10.1007/978-3-030-71158-0_14},
abstract = {Due to the growth of data and widespread usage of Machine Learning (ML) by non-experts, automation and scalability are becoming key issues for ML. This paper presents an automated and scalable framework for ML that requires minimum human input. We designed the framework for the domain of telecommunications risk management. This domain often requires non-ML-experts to continuously update supervised learning models that are trained on huge amounts of data. Thus, the framework uses Automated Machine Learning (AutoML), to select and tune the ML models, and distributed ML, to deal with Big Data. The modules included in the framework are task detection (to detect classification or regression), data preprocessing, feature selection, model training, and deployment. In this paper, we focus the experiments on the model training module. We first analyze the capabilities of eight AutoML tools: Auto-Gluon, Auto-Keras, Auto-Sklearn, Auto-Weka, H2O AutoML, Rminer, TPOT, and TransmogrifAI. Then, to select the tool for model training, we performed a benchmark with the only two tools that address a distributed ML (H2O AutoML and TransmogrifAI). The experiments used three real-world datasets from the telecommunications domain (churn, event forecasting, and fraud detection), as provided by an analytics company. The experiments allowed us to measure the computational effort and predictive capability of the AutoML tools. Both tools obtained high-quality results and did not present substantial predictive differences. Nevertheless, H2O AutoML was selected by the analytics company for the model training module, since it was considered a more mature technology that presented a more interesting set of features (e.g., integration with more platforms). After choosing H2O AutoML for the ML training, we selected the technologies for the remaining components of the architecture (e.g., data preprocessing and web interface).},
booktitle = {Agents and Artificial Intelligence: 12th International Conference, ICAART 2020, Valletta, Malta, February 22–24, 2020, Revised Selected Papers},
pages = {291–307},
numpages = {17},
keywords = {Risk management, Supervised learning, Distributed machine learning, Automated machine learning},
location = {Valletta, Malta}
}

@inproceedings{10.5555/645882.672247,
author = {Muthig, Dirk and Atkinson, Colin},
title = {Model-Driven Product Line Architectures},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {It has long been recognized that successful product line engineering revolves around the creation of a coherent and flexible product line architecture that consolidates the common parts of a product family for reuse and captures the variant parts for simple adaptation. However, it has been less clear what form such architectures should take and how they should be represented. One promising approach is offered by the new Model-Driven Architecture (MDA) paradigm of the Object Management Group (OMG). This paradigm holds that an organization's key architectural assets should be represented in an abstract "platform-independent" way, in terms of Unified Modeling Language (UML) models, and thereby be shielded from the idiosyncrasies and volatility of specific implementation technologies. In this paper, we discuss the opportunities and challenges involved in using the MDA paradigm for product line engineering and explain how model-driven, product line architectures can be developed, maintained and applied. After first outlining the core concepts of product line engineering and the ad hoc strategies currently used to support it, the paper provides a detailed metamodel of the information that needs to be stored within a product line architecture.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {110–129},
numpages = {20},
series = {SPLC 2}
}

@article{10.1016/j.eswa.2020.113185,
author = {BA\c{S}, Emine and \"{U}LKER, Erkan},
title = {An efficient binary social spider algorithm for feature selection problem},
year = {2020},
issue_date = {May 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {146},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113185},
doi = {10.1016/j.eswa.2020.113185},
journal = {Expert Syst. Appl.},
month = may,
numpages = {25},
keywords = {Classifiers, Feature selection, Social spider algorithm}
}

@article{10.1007/s10489-020-01993-w,
author = {Long, Xuandong and Qian, Wenbin and Wang, Yinglong and Shu, Wenhao},
title = {Cost-sensitive feature selection on multi-label data via neighborhood granularity and label enhancement},
year = {2021},
issue_date = {Apr 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01993-w},
doi = {10.1007/s10489-020-01993-w},
abstract = {Multi-label feature selection, which is an efficient and effective pre-processing step in machine learning and data mining, can select a feature subset that contains more contributions for multi-label classification while improving the performance of the classifiers. In real-world applications, an instance may be associated with multiple related labels with different relative importances, and the process of obtaining different features usually requires different costs, containing money, and time, etc. However, most existing works with regard to multi-label feature selection do not take into consideration the above two critical issues simultaneously. Therefore, in this paper, we exploit the idea of neighborhood granularity to enhance the traditional logical labels into label distribution forms to excavate the deeper supervised information hidden in multi-label data, and further consider the effect of the test cost under three different distributions, simultaneously. Motivated by these issues, a novel test cost multi-label feature selection algorithm with label enhancement and neighborhood granularity is designed. Moreover, the proposed algorithm is tested upon ten publicly available benchmark multi-label datasets with six widely-used metrics from two different aspects. Finally, two groups of experimental results demonstrate that the proposed algorithm achieves the satisfactory and superior performance over other four state-of-the-art comparing algorithms, and it is effective for improving the learning performance and decreasing the total test costs of the selected feature subset.},
journal = {Applied Intelligence},
month = apr,
pages = {2210–2232},
numpages = {23},
keywords = {Multi-label data, Neighborhood granularity, Label enhancement, Cost-sensitive, Feature selection}
}

@inproceedings{10.1145/1404946.1404950,
author = {Hubaux, A. and Heymans, P. and Unphon, H.},
title = {Separating variability concerns in a product line re-engineering project},
year = {2008},
isbn = {9781605581439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404946.1404950},
doi = {10.1145/1404946.1404950},
abstract = {Feature diagrams have now become common variability models in software product lines engineering literature. Whereas ongoing research keeps improving their expressiveness, formalisation, and automation, open studies of their usage in real projects are still missing. This paper intends to (1) present the process we followed to elicit the variability of PloneMeeting, an Open Source project, and (2) report on the initial results obtained when applying variability modelling techniques promoting separation of concerns between software variability and product line variability.},
booktitle = {Proceedings of the 2008 AOSD Workshop on Early Aspects},
articleno = {4},
numpages = {8},
keywords = {variability model, variability management, software product lines, separation of concerns, open source, feature diagram},
location = {Brussels, Belgium},
series = {EA '08}
}

@article{10.1007/s10462-017-9541-y,
author = {Wan, Cen and Freitas, Alex A.},
title = {An empirical evaluation of hierarchical feature selection methods for classification in bioinformatics datasets with gene ontology-based features},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {50},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9541-y},
doi = {10.1007/s10462-017-9541-y},
abstract = {Hierarchical feature selection is a new research area in machine learning/data mining, which consists of performing feature selection by exploiting dependency relationships among hierarchically structured features. This paper evaluates four hierarchical feature selection methods, i.e., HIP, MR, SHSEL and GTD, used together with four types of lazy learning-based classifiers, i.e., Na\"{\i}ve Bayes, Tree Augmented Na\"{\i}ve Bayes, Bayesian Network Augmented Na\"{\i}ve Bayes and k-Nearest Neighbors classifiers. These four hierarchical feature selection methods are compared with each other and with a well-known "flat" feature selection method, i.e., Correlation-based Feature Selection. The adopted bioinformatics datasets consist of aging-related genes used as instances and Gene Ontology terms used as hierarchical features. The experimental results reveal that the HIP (Select Hierarchical Information Preserving Features) method performs best overall, in terms of predictive accuracy and robustness when coping with data where the instances' classes have a substantially imbalanced distribution. This paper also reports a list of the Gene Ontology terms that were most often selected by the HIP method.},
journal = {Artif. Intell. Rev.},
month = aug,
pages = {201–240},
numpages = {40},
keywords = {Machine learning, K-Nearest Neighbors, Hierarchical feature selection, Data mining, Classification, Biology of aging, Bayesian classifiers}
}

@article{10.1016/j.patcog.2019.107156,
author = {Gordon, Jonathan and Hern\'{a}ndez-Lobato, Jos\'{e} Miguel},
title = {Combining deep generative and discriminative models for Bayesian semi-supervised learning},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2019.107156},
doi = {10.1016/j.patcog.2019.107156},
journal = {Pattern Recogn.},
month = apr,
numpages = {10},
keywords = {Predictive uncertainty, Variational autoencoders, Semi-supervised learning, Probabilistic models}
}

@inproceedings{10.1007/978-3-030-60639-8_51,
author = {Li, Rihong and Gao, Hongxia and Luo, Jiaxiang and Liu, Haiming and Yang, Weipeng and Chen, An},
title = {Feature Selection and Classification of Texture Images Based on Local Structure and Low-Rank Constraints},
year = {2020},
isbn = {978-3-030-60638-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-60639-8_51},
doi = {10.1007/978-3-030-60639-8_51},
abstract = {Texture description is a challenging problem in computer vision and pattern recognition. The task of texture classification is to classify texture into the class it belongs to, which is influenced by variations in scale, illumination, and changes in perspective. There are many texture descriptors in the literature. In this paper, we combine five texture descriptors for texture classification, which obtained better performance than the single descriptor at the price of high dimensionality. To solve this problem, we proposed a novel unsupervised feature selection method based on local structure and low-rank constraints, which can not only reduce the dimensions but also further improve the classification accuracy. To evaluate the performance of combing multiple descriptors and the proposed feature selection method, we design a variety of experiments in two typical texture datasets, namely KTH-TIPS-2a and CURET. Finally, the result shows the proposed method outperforms the state-of-the-art methods.},
booktitle = {Pattern Recognition and Computer Vision: Third Chinese Conference, PRCV 2020, Nanjing, China, October 16–18, 2020, Proceedings, Part II},
pages = {614–625},
numpages = {12},
keywords = {Texture classification, Feature selection, Local structure learning},
location = {Nanjing, China}
}

@article{10.1007/s11042-018-6937-x,
author = {Wang, Nan and Xue, Yiming and Lin, Qiang and Zhong, Ping},
title = {Structured sparse multi-view feature selection based on weighted hinge loss},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {11},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6937-x},
doi = {10.1007/s11042-018-6937-x},
abstract = {In applications, using features obtained from multiple views to describe objects has become popular because multiple views contain much more information than the single view. As the dimensions of the data sets are high, which may cause expensive time consumption and memory space, how to identify the representative views and features becomes a crucial problem. Multi-view feature selection that can integrate multiple views to select important and relevant features to improve performance has attracted more and more attentions in recent years. Previous supervised multi-view feature selection methods usually establish the models by concatenating multiple views into long vectors. However, this concatenation is not physically meaningful and implies that different views play the similar roles for specific tasks. In this paper, we propose a novel supervised multi-view feature selection method based on the weighted hinge loss (WHMVFS) that can learn the corresponding weight for each view and implement sparsity from the group and individual point of views under the structured sparsity framework. The newly proposed multi-view weighted hinge loss penalty not only has the ability to select more discriminative features for classification, but also can make the involved optimization problem be decomposed into several small scale subproblems, which can be easily solved by an iterative algorithm, and the convergence of the iterative algorithm is also proved. Experimental results conducted on real-world data sets show the effectiveness of the proposed method.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {15455–15481},
numpages = {27},
keywords = {Weighted hinge loss, Structured sparse, Multi-view feature selection, Classification}
}

@article{10.1016/j.neucom.2017.08.047,
author = {Qi, Miao and Wang, Ting and Liu, Fucong and Zhang, Baoxue and Wang, Jianzhong and Yi, Yugen},
title = {Unsupervised feature selection by regularized matrix factorization},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {273},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.08.047},
doi = {10.1016/j.neucom.2017.08.047},
abstract = {Feature selection is an interesting and challenging task in data analysis process. In this paper, a novel algorithm named Regularized Matrix Factorization Feature Selection (RMFFS) is proposed for unsupervised feature selection. Compared with other matrix factorization based feature selection methods, a main advantage of our algorithm is that it takes the correlation among features into consideration. Through introducing an inner product regularization into our algorithm, the features selected by RMFFS would not only well represent the original high-dimensional data, but also contain low redundancy. Moreover, a simple yet efficient iteratively updating algorithm is also developed to solve the proposed RMFFS. Extensive experimental results on nine real world databases demonstrate that our proposed method can achieve better performance than some state-of-the-art unsupervised feature selection methods.},
journal = {Neurocomput.},
month = jan,
pages = {593–610},
numpages = {18},
keywords = {Sparsity and redundancy, Matrix factorization, Feature selection, Dimensionality reduction}
}

@article{10.1007/s11280-017-0502-9,
author = {Zhang, Shichao and Cheng, Debo and Hu, Rongyao and Deng, Zhenyun},
title = {Supervised feature selection algorithm via discriminative ridge regression},
year = {2018},
issue_date = {November  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1386-145X},
url = {https://doi.org/10.1007/s11280-017-0502-9},
doi = {10.1007/s11280-017-0502-9},
abstract = {This paper studies a new feature selection method for data classification that efficiently combines the discriminative capability of features with the ridge regression model. It first sets up the global structure of training data with the linear discriminant analysis that assists in identifying the discriminative features. And then, the ridge regression model is employed to assess the feature representation and the discrimination information, so as to obtain the representative coefficient matrix. The importance of features can be calculated with this representative coefficient matrix. Finally, the new subset of selected features is applied to a linear Support Vector Machine for data classification. To validate the efficiency, sets of experiments are conducted with twenty benchmark datasets. The experimental results show that the proposed approach performs much better than the state-of-the-art feature selection algorithms in terms of the evaluating indicator of classification. And the proposed feature selection algorithm possesses a competitive performance compared with existing feature selection algorithms with regard to the computational cost.},
journal = {World Wide Web},
month = nov,
pages = {1545–1562},
numpages = {18},
keywords = {Support vector machine, Ridge regression, Representative coefficient matrix, Linear discriminant analysis}
}

@inproceedings{10.1007/978-3-031-12700-7_36,
author = {Marjit, Shyam and Talukdar, Upasana and Hazarika, Shyamanta M.},
title = {Enhancing EEG-Based Emotion Recognition Using MultiDomain Features and&nbsp;Genetic Algorithm Based Feature Selection},
year = {2021},
isbn = {978-3-031-12699-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-12700-7_36},
doi = {10.1007/978-3-031-12700-7_36},
abstract = {Electroencephalography (EEG) based emotion recognition has become a subtle research area because of its promising applications. An effective emotion recognition relies on significant and stable features. In this paper, we propose an EEG based emotion recognition methodology based on a hybrid feature extraction combined with Genetic Algorithm (GA) based feature selection. The features are extracted from three domains: time, frequency and discrete wavelet The proposal is evaluated on DEAP dataset where the emotional states are classified using a GA optimized Multi-Layer Perceptron. The proposed model identifies a. two classes of emotions viz. Low/High Valence with an average accuracy of 95.96% and Low/High Arousal with an average accuracy of 95.39%, b. four classes of emotions viz. High Valence-Low Arousal, High Valence-High Arousal, Low Valence-Low Arousal and Low Valence-High Arousal with 91.88% accuracy, which are better compared to the existing results reported in the literature.},
booktitle = {Pattern Recognition and Machine Intelligence: 9th International Conference, PReMI 2021, Kolkata, India, December 15–18, 2021, Proceedings},
pages = {345–353},
numpages = {9},
keywords = {EEG, Emotions, Time Domain, Frequency Domain, Genetic Algorithm, Feature Selection, Multi-Layer Perceptron},
location = {Kolkata, India}
}

@article{10.1007/s00500-020-05349-x,
author = {kelidari, Mahsa and Hamidzadeh, Javad},
title = {Feature selection by using chaotic cuckoo optimization algorithm with levy flight, opposition-based learning and disruption operator},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05349-x},
doi = {10.1007/s00500-020-05349-x},
abstract = {Feature selection, which plays an important role in high-dimensional data analysis, is drawing increasing attention recently. Finding the most relevant and important features for classifications are one of the most important tasks of data mining and machine learning, since all of the datasets have irrelevant features that affect accuracy rate and slow down the classifier. Feature selection is an optimization process, which improves the accuracy rate of data classification and reduces the number of selected features. Applying too many features both requires a large memory capacity and leads to a slow execution speed. Feature selection algorithms are often responsible to decide which features should be selected to be used during a classification algorithm. Traditional algorithms seemed to be inefficient due to the complexity of dimensions of the problem, thus evolutionary algorithms were used to improve the problem solving process. The algorithm proposed in this paper, chaotic cuckoo optimization algorithm with levy flight, disruption operator and opposition-based learning (CCOALFDO), is applied to select the optimal feature subspace for classification. It reduces the randomization in selecting features and avoids getting stuck in local optimum solutions which lead to a more interesting feature subset. Extensive experiments are conducted on 20 high-dimensional datasets to demonstrate the effectiveness and efficiency of the proposed method. The results showed the superiority of the proposed method to state-of-the-art methods in terms of classification accuracy rate. In addition, they prove the ability of the CCOALFDO in selecting the most relevant features for classification tasks. Thus, it is a reasonable solution in handling noise and avoiding serious negative impacts on the classification accuracy rate in real world datasets.},
journal = {Soft Comput.},
month = feb,
pages = {2911–2933},
numpages = {23},
keywords = {Opposition-based learning, Disruption operator, Levy flight, Chaotic theory, Cuckoo optimization algorithm, High-dimensional data, Feature selection}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1007/s11227-018-2640-y,
author = {Zheng, Yuefeng and Li, Ying and Wang, Gang and Chen, Yupeng and Xu, Qian and Fan, Jiahao and Cui, Xueting},
title = {A hybrid feature selection algorithm for microarray data},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {5},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2640-y},
doi = {10.1007/s11227-018-2640-y},
abstract = {For each microarray data set, only a small number of genes are beneficial. Due to the high-dimensional problem, gene selection research work remains a challenge. In order to solve the high-dimensional problem, we propose a dimensionality reduction algorithm named K value maximum relevance minimum redundancy improved grey wolf optimizer (KMR2IGWO). First, in the processing of KMR2, the K genes are selected. Second, the K genes are initialized by two ways according to random selection feature and different proportions of selection feature. Finally, the IGWO algorithm selects the optimal classification accuracy and the optimal combination of gene by adjusting the parameters of fitness function. The algorithm has a significant dimensionality reduction effect and is suitable for high-dimensional data sets. Experimental results show that the proposing KMR2IGWO strategy significantly reduces the dimension of microarray data and removes the redundant features. On the 14 microarray data sets, compared with the four algorithms mRMR + PSO, mRMR + GA, mRMR + BA, mRMR + CS, the proposed algorithm has higher performance in classification accuracy and feature subset length. In five data sets, the proposed algorithm average classification accuracy is 100%. On the 14 data sets, the proposed algorithm has a very significant dimensionality reduction effect, and the dimensionality reduction range is between 0.4% and 0.04%.},
journal = {J. Supercomput.},
month = may,
pages = {3494–3526},
numpages = {33},
keywords = {Support vector machine, Minimum redundancy maximum relevance, Grey wolf optimizer, Feature selection, Classification}
}

@inproceedings{10.1145/3383219.3383229,
author = {Li, Yang and Schulze, Sandro and Xu, Jiahua},
title = {Feature Terms Prediction: A Feasible Way to Indicate the Notion of Features in Software Product Line},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383229},
doi = {10.1145/3383219.3383229},
abstract = {In Software Product Lines (SPL), feature extraction from software requirements specifications has been subject to intense research in order to assist domain analysis in a time-saving way. Although various approaches are proposed to extract features, there still exists a gap to achieve the complete view of features, that is, how to figure out the intention of a feature. Feature terms as the smallest units in a feature can be regarded as vital indicators for describing a feature. Automated feature term extraction can provide key information regarding the intention of a feature, which improves the efficiency of domain analysis. In this paper, we propose an approach to train prediction models by using machine learning techniques to identify feature terms. To this end, we extract candidate terms from requirement specifications in one domain and take six attributes of each term into account to create a labeled dataset. Subsequently, we apply seven commonly used machine algorithms to train prediction models on the labeled dataset. We then use these prediction models to predict feature terms from the requirements belonging to the other two different domains. Our results show that (1) feature terms can be predicted with high accuracy of ≈ 90% within a domain (2) prediction across domains leads to a decreased but still good accuracy (≈ 80%), and (3) machine learning algorithms perform differently.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Software Product Lines, Requirement Documents, Feature Terms Identification, Feature Extraction},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1007/978-3-030-77211-6_11,
author = {Cuadrado, David and Ria\~{n}o, David},
title = {ICU Days-to-Discharge Analysis with Machine Learning Technology},
year = {2021},
isbn = {978-3-030-77210-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-77211-6_11},
doi = {10.1007/978-3-030-77211-6_11},
abstract = {ICU management depends on the level of occupation and the length of stay of the patients. Daily prediction of the days to discharge (DTD) of ICU patients is essential to that management. Previous studies showed a low predictive capability of internists and ML-generated models. Therefore, more elaborated combinations of ML technologies are required. Here, we present four approaches to the analysis of the DTDs of ICU patients from different perspectives: heterogeneity quantification, biomarker identification, phenotype recognition, and prediction. Several ML-based methods are proposed for each approach, which were tested with the data of 3,973 patients of a Spanish ICU. Results confirm the complexity of analyzing DTDs with intelligent data analysis methods.},
booktitle = {Artificial Intelligence in Medicine: 19th International Conference on Artificial Intelligence in Medicine, AIME 2021, Virtual Event, June 15–18, 2021, Proceedings},
pages = {103–113},
numpages = {11},
keywords = {ICU, Patient phenotyping, Days-to-discharge prediction, Feature selection}
}

@article{10.1016/j.procs.2018.05.055,
author = {Tripathi, Diwakar and Edla, Damodar Reddy and Kuppili, Venkatanareshbabu and Bablani, Annushree and Dharavath, Ramesh},
title = {Credit Scoring Model based on Weighted Voting and Cluster based Feature Selection},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.055},
doi = {10.1016/j.procs.2018.05.055},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {22–31},
numpages = {10},
keywords = {Classification, Correlation coefficient, Credit scoring, Feature selection, Weighted voting}
}

@article{10.1007/s11042-021-10710-9,
author = {Sukumaran, Asha and Brindha, Thomas},
title = {Optimal feature selection with hybrid classification for automatic face shape classification using fitness sorted Grey wolf update},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {17},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-021-10710-9},
doi = {10.1007/s11042-021-10710-9},
abstract = {Depending on the prevailing researches, the classification of face shapes could be deployed for numerous applications. This paper intends to develop a new method for face shape classification using intelligent approaches. The presented method includes three stages namely, (i) Face Detection (ii) Pre-processing (iii) Feature extraction (iv) Classification. The face detection process is used for identifying the most significant objects in the face, probably eyes, nose, etc. which is done using the Viola-Jones algorithm. Moreover, the pre-processing stage includes the Histogram Equalization (HE) model for enhancing the contrast of the image. The classification of face shapes is performed by a hybrid classifier that links Convolutional Neural Network (CNN) and Neural Network (NN). For performing the CNN-based classification, the images are directly given as input. On the other hand, NN-based classification requires features as input. Hence, the pre-processed image is again subjected to the feature extraction process, where the features are extracted using the Active Appearance Model (AAM) and the Active Shape Model (ASM). For reducing the length of extracted features, the optimal feature selection process is adopted, which is done by improved Grey Wolf Optimization (GWO) algorithm. As the main contribution, the features, number of hidden neurons in the convolutional layer of CNN, and training of NN (weight update) is optimally chosen by improved GWO so-called as Fitness Sorted Grey Wolf Update (FS-GU) model. Finally, the average of two outcomes from both CNN, and NN provides the classified five categories of face shapes like heart, oblong, oval, round, and square. The performance of the proposed classification model is finally validated by comparing over the conventional models by analyzing the relevant performance metrics.},
journal = {Multimedia Tools Appl.},
month = jul,
pages = {25689–25710},
numpages = {22},
keywords = {Grey wolf optimization, Optimal feature Selecetion, Face detection, Face shape classification, Face shapes}
}

@article{10.1016/j.compbiomed.2021.104862,
author = {Maurya, Ritesh and Pathak, Vinay Kumar and Burget, Radim and Dutta, Malay Kishore},
title = {Automated detection of bioimages using novel deep feature fusion algorithm and effective high-dimensional feature selection approach},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104862},
doi = {10.1016/j.compbiomed.2021.104862},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {20},
keywords = {Pre-trained CNNs, Feature fusion, Evolutionary algorithms, Transfer learning, Bioimage classification, Convolutional neural networks}
}

@article{10.1007/s10664-021-09994-0,
author = {Vitui, Arthur and Chen, Tse-Hsun (Peter)},
title = {MLASP: Machine learning assisted capacity planning: An industrial experience report},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09994-0},
doi = {10.1007/s10664-021-09994-0},
abstract = {In industrial environments it is critical to find out the capacity of a system and plan for a deployment layout that meets the production traffic demands. The system capacity is influenced by both the performance of the system’s constituting components and the physical environment setup. In a large system, the configuration parameters of individual components give the flexibility to developers and load test engineers to tune system performance without changing the source code. However, due to the large search space, estimating the capacity of the system given different configuration values is a challenging and costly process. In this paper, we propose an approach, called MLASP, that uses machine learning models to predict the system key performance indicators (i.e., KPIs), such as throughput, given a set of features made off configuration parameter values, including server cluster setup, to help engineers in capacity planning for production environments. Under the same load, we evaluate MLASP on two large-scale mission-critical enterprise systems developed by Ericsson and on one open-source system. We find that: 1) MLASP can predict the system throughput with a very high accuracy. The difference between the predicted and the actual throughput is less than 1%; and 2) By using only a small subset of the training data (e.g., 3% of the entire data for the open-source system), MLASP can still predict the throughput accurately. We also document our experience of successfully integrating the approach into an industrial setting. In summary, this paper highlights the benefits and potential of using machine learning models to assist load test engineers in capacity planning.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {27},
keywords = {Deep learning, Machine learning, Performance testing, Capacity testing, Load testing}
}

@inproceedings{10.1007/978-3-030-89657-7_21,
author = {Gawinecki, Maciej and Szmyd, Wojciech and \.{Z}uchowicz, Urszula and Walas, Marcin},
title = {What Makes a Good Movie Recommendation? Feature Selection for Content-Based Filtering},
year = {2021},
isbn = {978-3-030-89656-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89657-7_21},
doi = {10.1007/978-3-030-89657-7_21},
abstract = {Nowadays, recommendation systems are becoming ubiquitous, especially in the entertainment industry, such as movie streaming services. In More-Like-This recommendation approach, movies are suggested based on attributes of a&nbsp;currently inspected movie. However, it is not obvious which features are the best predictors for similarity, as perceived by users. To address this problem, we developed and evaluated a&nbsp;recommendation system consisting of nine features and a&nbsp;variety of their representations. We crowdsourced relevance judgments for more than 5 thousand movie recommendations to evaluate the configurations of&nbsp;several dozen of&nbsp;movie features. From five embedding techniques for textual attributes, we selected Universal Sentence Encoder model as the best representation method for producing recommendations. Evaluation of movie features relevance showed that summary and categories extracted from Wikipedia led to the highest similarity on user perceptions in comparison to other analyzed features. We applied the feature weighting methods, commonly used in classification tasks, to determine optimal weights for a given feature set. Our results showed that we can reduce features to only genres, summary, plot, categories, and release year without losing the quality of&nbsp;recommendations.},
booktitle = {Similarity Search and Applications: 14th International Conference, SISAP 2021, Dortmund, Germany, September 29 – October 1, 2021, Proceedings},
pages = {280–294},
numpages = {15},
keywords = {Feature weighting, Feature selection, Content-based filtering, Recommender system},
location = {Dortmund, Germany}
}

@article{10.1016/j.eswa.2019.03.039,
author = {Chen, Ke and Zhou, Feng-Yu and Yuan, Xian-Feng},
title = {Hybrid particle swarm optimization with spiral-shaped mechanism for feature selection},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.039},
doi = {10.1016/j.eswa.2019.03.039},
journal = {Expert Syst. Appl.},
month = aug,
pages = {140–156},
numpages = {17},
keywords = {Optimization, Classification, Feature selection, Particle swarm optimization}
}

@article{10.1016/j.future.2019.12.033,
author = {Lu, Jiayi and Song, Enmin and Ghoneim, Ahmed and Alrashoud, Mubarak},
title = {Machine learning for assisting cervical cancer diagnosis: An ensemble approach},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {106},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.033},
doi = {10.1016/j.future.2019.12.033},
journal = {Future Gener. Comput. Syst.},
month = may,
pages = {199–205},
numpages = {7},
keywords = {Machine learning, Cervical cancer}
}

@article{10.1007/s00521-021-06099-z,
author = {Sathiyabhama, B. and Kumar, S. Udhaya and Jayanthi, J. and Sathiya, T. and Ilavarasi, A. K. and Yuvarajan, V. and Gopikrishna, Konga},
title = {A novel feature selection framework based on grey wolf optimizer for mammogram image analysis},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {21},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06099-z},
doi = {10.1007/s00521-021-06099-z},
abstract = {Breast cancer is one of the significant tumor death in women. Computer-aided diagnosis (CAD) supports the radiologists in recognizing the irregularities in an efficient manner. In this work, a novel CAD system proposed for mammogram image analysis based on grey wolf optimizer (GWO) with rough set theory. Texture, intensity, and shape-based features are extracted from mass segmented mammogram images. To derive the appropriate features from the extracted feature set, a novel dimensionality reduction algorithm is proposed based on GWO with rough set theory. GWO is a novel bio-inspired optimization algorithm, stimulated based on hunting activities and social hierarchy of the grey wolves. In this paper, a hybridization of GWO and Rough Set (GWORS) methods are used to find the significant features from the extracted mammogram images. To evaluate the effectiveness of the proposed GWORS, we compare it with other well-known rough set and bio-inspired feature selection algorithms including particle swarm optimize, genetic algorithm, Quick Reduct and Relative Reduct. From empirical results, it is observed that the proposed GWORS outperforms the other techniques in terms of accuracy, F-Measures and receiver operating characteristic curve.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {14583–14602},
numpages = {20},
keywords = {Mammogram, Feature selection, Rough set theory, Grey wolf optimizer, Breast cancer}
}

@article{10.5555/3122009.3242031,
author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
title = {On the stability of feature selection algorithms},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Feature Selection is central to modern data science, from exploratory data analysis to predictive model-building. The "stability" of a feature selection algorithm refers to the robustness of its feature preferences, with respect to data sampling and to its stochastic nature. An algorithm is 'unstable' if a small change in data leads to large changes in the chosen feature subset. Whilst the idea is simple, quantifying this has proven more challenging--we note numerous proposals in the literature, each with different motivation and justification. We present a rigorous statistical treatment for this issue. In particular, with this work we consolidate the literature and provide (1) a deeper understanding of existing work based on a small set of properties, and (2) a clearly justified statistical approach with several novel benefits. This approach serves to identify a stability measure obeying all desirable properties, and (for the first time in the literature) allowing confidence intervals and hypothesis tests on the stability, enabling rigorous experimental comparison of feature selection algorithms.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {6345–6398},
numpages = {54},
keywords = {stability, feature selection}
}

@article{10.3233/JIFS-200937,
author = {Liu, Xuning and Zhang, Guoying and Zhang, Zixian},
title = {A novel hybrid feature selection and modified KNN prediction model for coal and gas outbursts},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200937},
doi = {10.3233/JIFS-200937},
abstract = {The feature selection of influencing factors of coal and gas outbursts is of great significance for presenting the most discriminative features and improving prediction performance of a classifier, the paper presents an effective hybrid feature selection and modified outbursts classifier framework which aims at solving exiting coal and gas outbursts prediction problems. First, a measurement standard based on maximum information coefficient(MIC) is employed to identify the wide correlations between two variables; Second, based on a ranking procedure using non-dominated sorting genetic algorithm(NSGAII), maximum relevance minimum redundancy(MRMR) algorithm is subsequently performed to find out candidate feature set highly related to the class label and uncorrelated with each other; Third, random forest(RF) is employed to search the optimal feature subset from the candidate feature set, then the optimal feature subset that influences the classification performance of coal and gas outbursts is obtained; Finally, an improved classifier model has been proposed that combines gradient boosting decision tree(GBDT) and k-nearest neighbor(KNN) for outbursts prediction. In the modified classifier model, the GBDT is utilized to assign different weights to features, then the weighted features are input into the KNN to verify the effectiveness of proposed method on coal and gas outbursts dataset. The experimental results conclude that our proposed scheme is effective in the number of feature and prediction accuracy when compared with other related state-of-the-art prediction models based on feature selection for coal and gas outbursts.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {7671–7691},
numpages = {21},
keywords = {K-nearest neighbor, Gradient boosting decision tree, Random forest, Maximum relevance minimum redundancy, Non-dominated sorting genetic algorithm, Maximum information coefficient, Coal and gas outbursts}
}

@article{10.1007/s00521-017-2988-6,
author = {Sayed, Gehad Ismail and Hassanien, Aboul Ella and Azar, Ahmad Taher},
title = {Feature selection via a novel chaotic crow search algorithm},
year = {2019},
issue_date = {January   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {1},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-2988-6},
doi = {10.1007/s00521-017-2988-6},
abstract = {Crow search algorithm (CSA) is a new natural inspired algorithm proposed by Askarzadeh in 2016. The main inspiration of CSA came from crow search mechanism for hiding their food. Like most of the optimization algorithms, CSA suffers from low convergence rate and entrapment in local optima. In this paper, a novel meta-heuristic optimizer, namely chaotic crow search algorithm (CCSA), is proposed to overcome these problems. The proposed CCSA is applied to optimize feature selection problem for 20 benchmark datasets. Ten chaotic maps are employed during the optimization process of CSA. The performance of CCSA is compared with other well-known and recent optimization algorithms. Experimental results reveal the capability of CCSA to find an optimal feature subset which maximizes the classification performance and minimizes the number of selected features. Moreover, the results show that CCSA is superior compared to CSA and the other algorithms. In addition, the experiments show that sine chaotic map is the appropriate map to significantly boost the performance of CSA.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {171–188},
numpages = {18},
keywords = {Optimization algorithm, Feature selection, Crow search algorithm, Chaos theory}
}

@article{10.3233/JIFS-179967,
author = {Li, Meifang and Ruan, Binlin and Yuan, Caixing and Song, Zhishuang and Dai, Chongchong and Fu, Binghua and Qiu, Jianxing and Maseleno, Andino and Yuan, Xiaohui and Balas, Valentina E.},
title = {Intelligent system for predicting breast tumors using machine learning},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {39},
number = {4},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-179967},
doi = {10.3233/JIFS-179967},
abstract = {The early hidden characteristics of breast tumors make their features difficult to be effectively identified. In order to improve the detection accuracy of breast tumors, this study combined with computer-aided diagnosis techniques such as machine learning and computer vision and used X-ray analysis to study breast tumor diagnosis techniques. Moreover, this study combines breast tumor diagnostic images to determine various parameters of the image. At the same time, through experimental research and analysis of the region segmentation method and preprocessing method of breast detection images, the best diagnostic images are obtained, and the influence of background and other noise on the image diagnosis results is effectively proposed. In addition, this study proposes a method for detecting the distortion of the mammogram image structure, which accurately detects the structural distortion and reduces the interference of various influencing factors. Finally, this paper designs experiments to study the effects of the diagnostic method of this paper. Through comparative analysis, it can be seen that the results of this study have certain advantages in accuracy and image clarity, and have certain clinical significance, and can provide theoretical reference for subsequent related research.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {4813–4822},
numpages = {10},
keywords = {machine learning, image, diagnosis, breast neoplasms, X-ray analysis}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Thermography images, Ultrasound images, Mammogram images, Medical imaging modalities, Magnetic resonance imaging (MRI), Machine learning, Histological images, Deep learning, Computer-aided diagnosis system (CAD), Convolutional neural network, Breast cancer classification}
}

@article{10.1016/j.compeleceng.2021.107397,
author = {Jain, Praphula Kumar and Yekun, Ephrem Admasu and Pamula, Rajendra and Srivastava, Gautam},
title = {Consumer recommendation prediction in online reviews using Cuckoo optimized machine learning models},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {95},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107397},
doi = {10.1016/j.compeleceng.2021.107397},
journal = {Comput. Electr. Eng.},
month = oct,
numpages = {10},
keywords = {Extreme gradient boosting, Recommendation prediction, Sentiment analysis, Machine learning, Cuckoo Search, Online reviews}
}

@article{10.1007/s10586-020-03083-5,
author = {Taheri, Rahim and Javidan, Reza and Shojafar, Mohammad and Vinod, P. and Conti, Mauro},
title = {Can machine learning model with static features be fooled: an adversarial machine learning approach},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-020-03083-5},
doi = {10.1007/s10586-020-03083-5},
abstract = {The widespread adoption of smartphones dramatically increases the risk of attacks and the spread of mobile malware, especially on the Android platform. Machine learning-based solutions have been already used as a tool to supersede signature-based anti-malware systems. However, malware authors leverage features from malicious and legitimate samples to estimate statistical difference in-order to create adversarial examples. Hence, to evaluate the vulnerability of machine learning algorithms in malware detection, we propose five different attack scenarios to perturb malicious applications (apps). By doing this, the classification algorithm inappropriately fits the discriminant function on the set of data points, eventually yielding a higher misclassification rate. Further, to distinguish the adversarial examples from benign samples, we propose two defense mechanisms to counter attacks. To validate our attacks and solutions, we test our model on three different benchmark datasets. We also test our methods using various classifier algorithms and compare them with the state-of-the-art data poisoning method using the Jacobian matrix. Promising results show that generated adversarial samples can evade detection with a very high probability. Additionally, evasive variants generated by our attack models when used to harden the developed anti-malware system improves the detection rate up to 50% when using the generative adversarial network (GAN) method.},
journal = {Cluster Computing},
month = dec,
pages = {3233–3253},
numpages = {21},
keywords = {Jacobian algorithm, Generative adversarial network, Poison attacks, Android malware detection, Adversarial machine learning}
}

@article{10.1145/3158675,
author = {Jian, Ling and Li, Jundong and Liu, Huan},
title = {Exploiting Multilabel Information for Noise-Resilient Feature Selection},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3158675},
doi = {10.1145/3158675},
abstract = {In a conventional supervised learning paradigm, each data instance is associated with one single class label. Multilabel learning differs in the way that data instances may belong to multiple concepts simultaneously, which naturally appear in a variety of high impact domains, ranging from bioinformatics and information retrieval to multimedia analysis. It targets leveraging the multiple label information of data instances to build a predictive learning model that can classify unlabeled instances into one or multiple predefined target classes. In multilabel learning, even though each instance is associated with a rich set of class labels, the label information could be noisy and incomplete as the labeling process is both time consuming and labor expensive, leading to potential missing annotations or even erroneous annotations. The existence of noisy and missing labels could negatively affect the performance of underlying learning algorithms. More often than not, multilabeled data often has noisy, irrelevant, and redundant features of high dimensionality. The existence of these uninformative features may also deteriorate the predictive power of the learning model due to the curse of dimensionality. Feature selection, as an effective dimensionality reduction technique, has shown to be powerful in preparing high-dimensional data for numerous data mining and machine-learning tasks. However, a vast majority of existing multilabel feature selection algorithms either boil down to solving multiple single-labeled feature selection problems or directly make use of the imperfect labels to guide the selection of representative features. As a result, they may not be able to obtain discriminative features shared across multiple labels. In this article, to bridge the gap between a rich source of multilabel information and its blemish in practical usage, we propose a novel noise-resilient multilabel informed feature selection framework (MIFS) by exploiting the correlations among different labels. In particular, to reduce the negative effects of imperfect label information in obtaining label correlations, we decompose the multilabel information of data instances into a low-dimensional space and then employ the reduced label representation to guide the feature selection phase via a joint sparse regression framework. Empirical studies on both synthetic and real-world datasets demonstrate the effectiveness and efficiency of the proposed MIFS framework.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jun,
articleno = {52},
numpages = {23},
keywords = {noise resilient, label correlations, feature selection, Multilabel learning}
}

@inproceedings{10.1145/2970398.2970433,
author = {Gigli, Andrea and Lucchese, Claudio and Nardini, Franco Maria and Perego, Raffaele},
title = {Fast Feature Selection for Learning to Rank},
year = {2016},
isbn = {9781450344975},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970398.2970433},
doi = {10.1145/2970398.2970433},
abstract = {An emerging research area named Learning-to-Rank (LtR) has shown that effective solutions to the ranking problem can leverage machine learning techniques applied to a large set of features capturing the relevance of a candidate document for the user query. Large-scale search systems must however answer user queries very fast, and the computation of the features for candidate documents must comply with strict back-end latency constraints. The number of features cannot thus grow beyond a given limit, and Feature Selection (FS) techniques have to be exploited to find a subset of features that both meets latency requirements and leads to high effectiveness of the trained models. In this paper, we propose three new algorithms for FS specifically designed for the LtR context where hundreds of continuous or categorical features can be involved. We present a comprehensive experimental analysis conducted on publicly available LtR datasets and we show that the proposed strategies outperform a well-known state-of-the-art competitor.},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
pages = {167–170},
numpages = {4},
keywords = {learning to rank, feature selection},
location = {Newark, Delaware, USA},
series = {ICTIR '16}
}

@inproceedings{10.1145/3342999.3343015,
author = {Gonsalves, Amanda H. and Thabtah, Fadi and Mohammad, Rami Mustafa A. and Singh, Gurpreet},
title = {Prediction of Coronary Heart Disease using Machine Learning: An Experimental Analysis},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343015},
doi = {10.1145/3342999.3343015},
abstract = {The field of medical analysis is often referred to be a valuable source of rich information. Coronary Heart Disease (CHD) is one of the major causes of death all around the world therefore early detection of CHD can help reduce these rates. The challenge lies in the complexity of the data and correlations when it comes to prediction using conventional techniques. The aim of this research is to use the historical medical data to predict CHD using Machine Learning (ML) technology. The scope of this research is limited to using three supervised learning techniques namely Na\"{\i}ve Bayes (NB), Support Vector Machine (SVM) and Decision Tree (DT), to discover correlations in CHD data that might help improving the prediction rate. Using the South African Heart Disease dataset of 462 instances, intelligent models are derived by the considered ML techniques using 10-fold cross validation. Empirical results using different performance evaluation measures report that probabilistic models derived by NB are promising in detecting CHD.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {51–56},
numpages = {6},
keywords = {Supervised Learning, Medical Informatics, Machine Learning, Data Mining, Coronary Heart Disease},
location = {Xiamen, China},
series = {ICDLT '19}
}

@article{10.3233/JIFS-169323,
author = {Qi, Chengming and Hu, Lishuan and Yu, Xin and Guirao, Juan L.G. and Gao, Wei},
title = {A framework of multiple kernel ensemble learning for classification using two-stage feature selection method},
year = {2017},
issue_date = {2017},
publisher = {IOS Press},
address = {NLD},
volume = {33},
number = {5},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169323},
doi = {10.3233/JIFS-169323},
abstract = {Feature selection aims at selecting a feature subset that has the most discriminative information and preserve most of characteristics from original features in HyperSpectral Image (HSI) classification. This paper proposes a two-stage feature selection method based on Mutual Information (MI) and Jeffries-Matusita (J-M) measure. In first stage, we select a feature subset with minimal redundancy maximal relevance criteria. In second stage, we select further a feature subset from which obtained in first stage by maximizing J-M distance. Multiple Kernel Learning (MKL) and Ensemble Learning (EL) are promising family of machine learning algorithms and have been applied extensively in HSI classification. Many MKL methods often formulate the problem as an optimization task. To avoid solving the complicated optimization problem, this paper presents an ensemble learning framework, SMKB (Stochastic Multiple Kernel Boosting), which applies Adaptive Boosting (AdaBoost) and stochastic approach to learning multiple kernel-based classifier for multi-class classification problem. We examine empirical performance of proposed approach on benchmark hyperspectral classification data set in comparison with various state-of-the-art algorithms. Experimental results demonstrate that the proposed method obtains better feature subsets and is more effective and efficient than classical methods.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {2737–2747},
numpages = {11},
keywords = {Mutual Information, Multiple Kernel Learning, Jeffries-Matusita measure, hyperspectral classification, Feature selection}
}

@article{10.1016/j.compbiomed.2021.104324,
author = {Tohka, Jussi and van Gils, Mark},
title = {Evaluation of machine learning algorithms for health and wellness applications: A tutorial},
year = {2021},
issue_date = {May 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104324},
doi = {10.1016/j.compbiomed.2021.104324},
journal = {Comput. Biol. Med.},
month = may,
numpages = {15},
keywords = {Decision support systems, Performance assessment, Life sciences, Biomedicine, Artificial intelligence, Machine learning}
}

@inproceedings{10.1007/978-3-642-28714-5_13,
author = {Adam, Sebastian},
title = {Providing software product line knowledge to requirements engineers --- a template for elicitation instructions},
year = {2012},
isbn = {9783642287138},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-28714-5_13},
doi = {10.1007/978-3-642-28714-5_13},
abstract = {[Context &amp; Motivation] Developing new software systems based on a software product line (SPL) in so-called application engineering (AE) projects is still a time-consuming and expensive task. Especially when a large number of customer-specific requirements exists, there is still no systematic support for efficiently aligning these non-anticipated requirements with SPL characteristics early on. [Question/problem] In order to improve this process significantly, sound knowledge about an SPL must be available when guiding the requirements elicitation during AE. Thus, an appropriate reflection of SPL characteristics in process-supporting artifacts is indispensable for actually supporting a requirements engineer in this task. [Principal ideas/results] In this paper, a validated template for elicitation instructions that aims at providing a requirements engineer with knowledge about an underlying SPL in an appropriate manner is presented. This template consists of predefined text blocks and algorithms that explain how SPL-relevant product and process knowledge can be systematically reflected into capability-aware elicitation instructions. [Contribution] By using such elicitation instructions, requirements engineers are enabled to elicit requirements in an AE project more effectively.},
booktitle = {Proceedings of the 18th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {147–164},
numpages = {18},
location = {Essen, Germany},
series = {REFSQ'12}
}

@article{10.1016/j.future.2019.02.030,
author = {Cao, Bin and Zhao, Jianwei and Yang, Po and Yang, Peng and Liu, Xin and Qi, Jun and Simpson, Andrew and Elhoseny, Mohamed and Mehmood, Irfan and Muhammad, Khan},
title = {Multiobjective feature selection for microarray data via distributed parallel algorithms},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.02.030},
doi = {10.1016/j.future.2019.02.030},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {952–981},
numpages = {30},
keywords = {Feature redundancy, Distributed parallelism, Multiobjective feature selection, High dimension, Microarray dataset}
}

@article{10.1007/s10270-020-00839-w,
author = {Pol’la, Matias and Buccella, Agustina and Cechich, Alejandra},
title = {Analysis of variability models: a systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00839-w},
doi = {10.1007/s10270-020-00839-w},
abstract = {Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to develop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs. Particularly, variability management (VM) is an activity that allows flexibility and a high level of reuse during software development. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in general, and for its analysis in particular. More precisely, a specific field has emerged, named (automated) variability analysis, focusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of existing proposals (as primary studies) focused on analyzing variability models. We define a classification framework, which is composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-specific aspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models during its whole life cycle—from design to derivation—according to the activities involved during an SPL development. Also, the framework helps us answer three research questions defined for showing the state of the art and drawing challenges for the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the existence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying inconsistencies in the models.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1043–1077},
numpages = {35},
keywords = {Supporting tools, Variability management, Software Product Line, Variability analysis}
}

@article{10.1016/j.artmed.2019.101764,
author = {Hu, Yaxian and Luo, Senlin and Han, Longfei and Pan, Limin and Zhang, Tiemei},
title = {Deep supervised learning with mixture of neural networks},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers Ltd.},
address = {GBR},
volume = {102},
number = {C},
issn = {0933-3657},
url = {https://doi.org/10.1016/j.artmed.2019.101764},
doi = {10.1016/j.artmed.2019.101764},
journal = {Artif. Intell. Med.},
month = jan,
numpages = {6},
keywords = {Diabetes determination, Expectation maximization, Mixture model, Deep neural network}
}

@inproceedings{10.5555/3432601.3432605,
author = {Krishnakumar, Sanjena and Abdou, Tamer},
title = {Towards interpretable and maintainable supervised learning using shapley values in arrhythmia},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper investigates the application of a model-agnostic interpretability technique, Shapley Additive Explanations (SHAP), to understand and hence, enhance machine learning classification models using Shapley values in the prediction of arrhythmias1. Using the Arrhythmia dataset2, three different feature selection techniques, Information Gain (IG), Recursive Feature Elimination-Random Forest (RFE-RF), and AutoSpearman, were used to select features for machine learning models to predict the arrhythmia class. Four multi-class classification models, Na\"{\i}ve Bayes (NB), k-Nearest Neighbours (kNN), Random Forest (RF), and stacking heterogeneous ensemble (Ensemble) were built, evaluated, and compared. SHAP interpretation method was applied to find reliable explanations for the predictions of the classification models. Additionally, SHAP values were used to find `bellwether' instances to enhance the training of our models in order to improve their performances in the prediction of arrhythmia. The most stable and top-performing classification model was RF, followed by Ensemble in comparison to NB and kNN. SHAP provided robust and reliable explanations for the classification models. Furthermore, improving the training of our models with `bellwether' instances, found using SHAP values, enhanced the overall model performances in terms of accuracy, AUC, and F1 score. In conclusion, we recommend using SHAP value explanations as a robust and reliable method for local model-agnostic interpretability and to enhance machine learning models for arrhythmia prediction.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {23–32},
numpages = {10},
keywords = {shapley value, multi-class classification, machine learning, local model-agnostic interpretation, healthcare, bellwether, arrhythmia, SHAP, LIME},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3382025.3414964,
author = {Hoff, Adrian and Nieke, Michael and Seidl, Christoph and S\ae{}ther, Eirik Halvard and Motzfeldt, Ida Sandberg and Din, Crystal Chang and Yu, Ingrid Chieh and Schaefer, Ina},
title = {Consistency-preserving evolution planning on feature models},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414964},
doi = {10.1145/3382025.3414964},
abstract = {A software product line (SPL) enables large-scale reuse in a family of related software systems through configurable features. SPLs represent a long-term investment so that their ongoing evolution becomes paramount and requires careful planning. While existing approaches enable to create an evolution plan for an SPL on feature-model (FM) level, they assume the plan to be rigid and do not support retroactive changes. In this paper, we present a method that enables to create and retroactively adapt an FM evolution plan while preventing undesired impacts on its structural and logical consistency. This method is founded in structural operational semantics and linear temporal logic. We implement our method using rewriting logic, integrate it within an FM tool suite and perform an evaluation using a collection of existing FM evolution scenarios.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {8},
numpages = {12},
keywords = {structural operational semantics, software product lines, software evolution, rewriting logic, linear temporal logic, formal semantics, feature models, feature model evolution},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.procs.2018.05.188,
author = {Jain, Divya and Singh, Vijendra},
title = {An Efficient Hybrid Feature Selection model for Dimensionality Reduction},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.188},
doi = {10.1016/j.procs.2018.05.188},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {333–341},
numpages = {9},
keywords = {Chronic Disease Datasets, Hybrid Feature Selection, Disease Diagnosis, PCA, ReliefF}
}

@inproceedings{10.1145/3456172.3456191,
author = {Xiong, Siyu and Liu, Rengyang and Yi, Chao},
title = {Graph-AutoFS: Auto Feature Selection in Graph Neural},
year = {2021},
isbn = {9781450388450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456172.3456191},
doi = {10.1145/3456172.3456191},
abstract = {Graph embedding is an effective method to represent graph data in low-dimensional space for graph analysis. Based on the inspiration of graph convolutional network (GCN), researchers have made significant progress by learning the vector representation of graph topology and node features in this task. However, it is challenging to select the feature of nodes in the real-world data. Traditional feature selection method like one-hot encoding or description of the node brings large memory and computation cost. Even worse, useless features may introduce noise and complicate the training process. In this paper, we propose a two-stage algorithm for graph automatic feature selection (Graph-AutoFS) to improve existing models. Graph-AutoFS can automatically select important features as training inputs, and the computational cost is precisely equal to the convergence of the training target model. We did not introduce discrete candidate feature sets in the search stage but relax the choices to be continuous by introducing the architecture parameters. By implementing a regularized optimizer on the architecture parameters, the model can automatically identify and delete redundant features during the model's training process. In the re-train phase, we keep the architecture parameters serving as an attention unit to boost the performance. We use three public benchmark data sets and two popular graph embedding methods to conduct experiments to verify the performance of Graph-AutoFS in node clustering and link prediction tasks. Experimental results show that Graph-AutoFS consistently outperforms original graph embedding methods considerably on these tasks.},
booktitle = {Proceedings of the 2021 7th International Conference on Computing and Data Engineering},
pages = {41–46},
numpages = {6},
keywords = {graph embedding, graph convolutional networks, auto feature select},
location = {Phuket, Thailand},
series = {ICCDE '21}
}

@article{10.1016/j.jbi.2018.07.014,
author = {Urbanowicz, Ryan J. and Meeker, Melissa and La Cava, William and Olson, Randal S. and Moore, Jason H.},
title = {Relief-based feature selection: Introduction and review},
year = {2018},
issue_date = {Sep 2018},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {85},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2018.07.014},
doi = {10.1016/j.jbi.2018.07.014},
journal = {J. of Biomedical Informatics},
month = sep,
pages = {189–203},
numpages = {15},
keywords = {Epistasis, ReliefF, Filter, Feature weighting, Feature interaction, Feature selection}
}

@inproceedings{10.1145/3205651.3208305,
author = {Mostert, Werner and Malan, Katherine and Engelbrecht, Andries},
title = {Filter versus wrapper feature selection based on problem landscape features},
year = {2018},
isbn = {9781450357647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3205651.3208305},
doi = {10.1145/3205651.3208305},
abstract = {Feature selection is a complex problem used across many fields, such as computer vision and data mining. Feature selection algorithms extract a subset of features from a greater feature set which can improve algorithm accuracy by discarding features that are less significant in achieving the goal function. Current approaches are often computationally expensive, provide insignificant increases in predictor performance, and can lead to overfitting. This paper investigates the binary feature selection problem and the applicability of using filter and wrapper techniques guided by fitness landscape characteristics. It is shown that using filter methods are more appropriate for problems where the fitness does not provide sufficient information to guide search as needed by wrapper techniques.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1489–1496},
numpages = {8},
keywords = {neutrality, hamming distance in a level, fitness landscapes, feature selection problem},
location = {Kyoto, Japan},
series = {GECCO '18}
}

@article{10.1016/j.ipm.2016.03.007,
author = {Tutkan, Melike and Ganiz, Murat Can and Akyoku\c{s}, Selim},
title = {Helmholtz principle based supervised and unsupervised feature selection methods for text mining},
year = {2016},
issue_date = {Sep 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {52},
number = {5},
issn = {0306-4573},
url = {https://doi.org/10.1016/j.ipm.2016.03.007},
doi = {10.1016/j.ipm.2016.03.007},
journal = {Inf. Process. Manage.},
month = sep,
pages = {885–910},
numpages = {26},
keywords = {Helmholtz principle, Text classification, Text mining, Machine learning, Attribute selection, Feature selection}
}

@article{10.1007/s10796-016-9689-z,
author = {Lee, Kyuhan and Park, Jinsoo and Kim, Iljoo and Choi, Youngseok},
title = {Predicting movie success with machine learning techniques: ways to improve accuracy},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-016-9689-z},
doi = {10.1007/s10796-016-9689-z},
abstract = {Previous studies on predicting the box-office performance of a movie using machine learning techniques have shown practical levels of predictive accuracy. Their works are technically- and methodologically-oriented, focusing mainly on what algorithms are better at predicting the movie performance. However, the accuracy of prediction model can also be elevated by taking other perspectives such as introducing unexplored features that might be related to the prediction of the outcomes. In this paper, we examine multiple approaches to improve the performance of the prediction model. First, we develop and add a new feature derived from the theory of transmedia storytelling. Such theory-driven feature selection not only increases the forecast accuracy, but also enhances the interpretability of a prediction model. Second, we use an ensemble approach, which has rarely been adopted in the research on predicting box-office performance. As a result, the proposed model, Cinema Ensemble Model (CEM), outperforms the prediction models from the past studies that use machine learning algorithms. We suggest that CEM can be extensively used for industrial experts as a powerful tool for improving decision-making process.},
journal = {Information Systems Frontiers},
month = jun,
pages = {577–588},
numpages = {12},
keywords = {Transmedia storytelling, Prediction model, Movie performance, Machine learning techniques, Feature selection, Cinema ensemble model}
}

@inproceedings{10.1145/3473465.3473474,
author = {Morozov, Alexey and Angulo, Brian and Mottl, Vadim and Tatarchuk, Alexander and Krasotkina, Olga},
title = {Differential Leave-One-Out Cross-Validation for Feature Selection in Generalized Linear Dependence Models},
year = {2021},
isbn = {9781450389884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473465.3473474},
doi = {10.1145/3473465.3473474},
abstract = {Estimation of dependencies from empirical data in a growing class of models is inevitably concerned with choosing the value of a structural parameter responsible for the model’s complexity. The most popular cross-validation schemes, in particular, LOO, suffer from the necessity to multiply repeat the model estimation on different subsamples of the training set. In this paper, we propose the method of differential LOOCV for generalized linear models of arbitrary dependencies, which allows for estimation of the model only once with each tentative value of the structural parameter. The idea of the method is that, instead of complete deleting an object from the training set at a single step of the training process, we delete only an infinitesimally small part of each of them. The indicator of the model quality is computed as the average of partial derivatives of the errors at each of single objects by the weights of their occurrence in the training set. The computing of the model quality indicator does not increase the computational complexity of the estimation procedure.},
booktitle = {Proceedings of the 2021 3rd International Conference on Information Technology and Computer Communications},
pages = {47–56},
numpages = {10},
keywords = {model verification, feature selection, LOOCV, Dependence estimation},
location = {Guangzhou, China},
series = {ITCC '21}
}

@article{10.1007/s00521-020-05400-w,
author = {Liu, Liu and Wang, Rujing and Xie, Chengjun and Li, Rui and Wang, Fangyuan and Zhou, Man and Teng, Yue},
title = {Learning region-guided scale-aware feature selection for object detection},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {11},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05400-w},
doi = {10.1007/s00521-020-05400-w},
abstract = {Scale variation is one of the major challenges in object detection task. Modern region-based object detection architectures often adopt Feature Pyramid Network (FPN) as feature extraction neck to achieve multi-scale feature representation in solving scale variation problem. However, due to the rough feature selection strategy in Region of Interest (RoI) feature extraction step, these methods might not perform well on object detection under strong scale variation. In this work, we are motivated by the limitations of current FPN-based two-stage object detectors and then present a novel module, namely scale-aware feature selective (SAFS) module, that flexibly and adaptively selects feature levels in two-stage object detectors. Specifically, we firstly build the RoI Pyramid in standard FPN structure to extract RoI features from various scale levels. Next, in order to achieve scale-aware mechanism for solving scale variation issue, we develop a novel weighting gate function containing one set of trainable parameters to automatically learn the fusion weight for each RoI feature level, which relieves the limitation of hard feature selection strategy guided by online instance size. Outputs from the RoI features with the learned weights are fused for classification and bounding box regression. Furthermore, we design a multi-level SAFS architecture to obtain different types of RoI feature combinations that ensures our method is more robust to various instance scales. Experimental results show that our SAFS module is very compatible with most of two-stage object detectors and could achieve state-of-the-art results with Average Precision of 48.3 on COCO test-dev and other popular object detection benchmarks. Our code will be made publicly available.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6389–6403},
numpages = {15},
keywords = {Scale-aware feature selective, RoI Pyramid, Object detection, Scale variation}
}

@article{10.1155/2021/6715564,
author = {Chen, Bingsheng and Chen, Huijie and Li, Mengshan and Tsai, Sang-Bing},
title = {Feature Selection Based on BP Neural Network and Adaptive Particle Swarm Algorithm},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {2021},
issn = {1574-017X},
url = {https://doi.org/10.1155/2021/6715564},
doi = {10.1155/2021/6715564},
abstract = {Feature selection can classify the data with irrelevant features and improve the accuracy of data classification in pattern classification. At present, back propagation (BP) neural network and particle swarm optimization algorithm can be well combined with feature selection. On this basis, this paper adds interference factors to BP neural network and particle swarm optimization algorithm to improve the accuracy and practicability of feature selection. This paper summarizes the basic methods and requirements for feature selection and combines the benefits of global optimization with the feedback mechanism of BP neural networks to feature based on backpropagation and particle swarm optimization (BP-PSO). Firstly, a chaotic model is introduced to increase the diversity of particles in the initial process of particle swarm optimization, and an adaptive factor is introduced to enhance the global search ability of the algorithm. Then, the number of features is optimized to reduce the number of features on the basis of ensuring the accuracy of feature selection. Finally, different data sets are introduced to test the accuracy of feature selection, and the evaluation mechanisms of encapsulation mode and filtering mode are used to verify the practicability of the model. The results show that the average accuracy of BP-PSO is 8.65% higher than the suboptimal NDFs model in different data sets, and the performance of BP-PSO is 2.31% to 18.62% higher than the benchmark method in all data sets. It shows that BP-PSO can select more distinguishing feature subsets, which verifies the accuracy and practicability of this model.},
journal = {Mob. Inf. Syst.},
month = jan,
numpages = {11}
}

@inproceedings{10.1145/3382026.3431252,
author = {Michelon, Gabriela Karoline},
title = {Evolving System Families in Space and Time},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431252},
doi = {10.1145/3382026.3431252},
abstract = {Managing the evolution of system families in space and time, i.e., system variants and their revisions is still an open challenge. The software product line (SPL) approach can support the management of product variants in space by reusing a common set of features. However, feature changes over time are often necessary due to adaptations and/or bug fixes, leading to different product versions. Such changes are commonly tracked in version control systems (VCSs). However, VCSs only deal with the change history of source code, and, even though their branching mechanisms allow to develop features in isolation, VCS does not allow propagating changes across variants. Variation control systems have been developed to support more fine-grained management of variants and to allow tracking of changes at the level of files or features. However, these systems are also limited regarding the types and granularity of artifacts. Also, they are cognitively very demanding with increasing numbers of revisions and variants. Furthermore, propagating specific changes over variants of a system is still a complex task that also depends on the variability-aware change impacts. Based on these existing limitations, the goal of this doctoral work is to investigate and define a flexible and unified approach to allow an easy and scalable evolution of SPLs in space and time. The expected contributions will aid the management of SPL products and support engineers to reason about the potential impact of changes during SPL evolution. To evaluate the approach, we plan to conduct case studies with real-world SPLs.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {104–111},
numpages = {8},
keywords = {version control systems, software product lines, software evolution, feature-oriented software development},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3377930.3390192,
author = {Xu, Hang and Xue, Bing and Zhang, Mengjie},
title = {Segmented initialization and offspring modification in evolutionary algorithms for bi-objective feature selection},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390192},
doi = {10.1145/3377930.3390192},
abstract = {In classification, feature selection mainly aims at reducing the dataset dimensionality and increasing the classification accuracy, which also results in higher computational efficiency than using the original full set of features. Population-based meta-heuristic, evolutionary algorithms have been widely used to solve the bi-objective feature selection problem, which minimizes the number of selected features and the error of classification model. However, most of them are not specifically designed for feature selection, and disregard many of its complex characteristics. In this paper, we propose a generic approach that focuses on improving the initialization effectiveness and offspring quality, in order to boost the performance of existing evolutionary algorithms for bi-objective feature selection. To be more specific, a segmented initialization mechanism is used to enhance the exploration width, while an offspring modification mechanism is proposed to ensure the exploitation depth. Combining them together will make a good trade-off between the diversity and convergence. In the experiments, we plug the proposed approach into three different types of multi-objective evolutionary algorithms, and test them on 18 classification datasets with two widely-used performance metrics. The empirical results prove the significant contribution of the proposed approach on the optimization and classification performance.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {444–452},
numpages = {9},
keywords = {feature selection, evolutionary algorithm, classification},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1155/2020/2394948,
author = {Wang, Yimeng and Zhang, Yunqi and Zhang, Guangchen},
title = {Credit Risk Assessment for Small and Microsized Enterprises Using Kernel Feature Selection-Based Multiple Criteria Linear Optimization Classifier: Evidence from China},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/2394948},
doi = {10.1155/2020/2394948},
abstract = {Credit risk assessment has gained increasing marked attention in the recent years by researchers, financial institutions, and banks, especially for small and microsized enterprises. Evidence shows that the core of small and microsized enterprises’ credit risk assessment is to construct a scientific credit risk indicator system, and the key is to establish an effective credit risk prediction model. Therefore, we analyze the factors that influence the credit risk of Chinese small and microsized enterprises and then construct a comprehensive credit risk indicator system by adding behaviour information, supervision information, and policy information. Furthermore, we improve the multiple criteria linear optimization classifier (MCLOC) by introducing the one-norm kernel feature selection and thereby establish the kernel feature selection-based multiple criteria linear optimization classifier (KFS-MCLOC). As for experiments, we use real business data from a Chinese commercial bank to test the performance of these models. The results show that (1) the proposed KFS-MCLOC has greater advantages in predictive accuracy, interpretability, and stability than other models; (2) the KFS-MCLOC selects 10 features from 53 original features and gives selected features their weight automatically; (3) the features selected by the KFS-MCLOC are further verified and compared by the features selected by the logistic regression model with stepwise parameter, and the indicators of “quick ratio; net operating cash flow; enterprises’ abnormal times of water, electricity, and tax fee; overdue days of enterprises’ loans; and mortgage and pledge status” are proved to be the most influencing credit risk factors.},
journal = {Complex.},
month = jan,
numpages = {16}
}

@inproceedings{10.1145/3449639.3459374,
author = {Vatolkin, Igor and Ostermann, Fabian and M\"{u}ller, Meinard},
title = {An evolutionary multi-objective feature selection approach for detecting music segment boundaries of specific types},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459374},
doi = {10.1145/3449639.3459374},
abstract = {The goal of music segmentation is to identify boundaries between parts of music pieces which are perceived as entities. Segment boundaries often go along with a change in musical properties including instrumentation, key, and tempo (or a combination thereof). One can consider different types (or classes) of boundaries according to these musical properties. In contrast to existing datasets with missing specifications of changing properties for annotated boundaries, we have created a set of artificial music tracks with precise annotations for boundaries of different types. This allows for a profound analysis and interpretation of annotated and predicted boundaries and a more exhaustive comparison of different segmentation algorithms. For this scenario, we formulate a novel multi-objective optimisation task that identifies boundaries of only a specific type. The optimisation is conducted by means of evolutionary multi-objective feature selection and a novelty-based segmentation approach. Furthermore, we provide lists of audio features from non-dominated fronts which most significantly contribute to the estimation of given boundaries (the first objective) and most significantly reduce the performance of the prediction of other boundaries (the second objective).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1061–1069},
numpages = {9},
keywords = {music segmentation, evolutionary multi-objective feature selection},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s00521-019-04171-3,
author = {Ghosh, Manosij and Guha, Ritam and Sarkar, Ram and Abraham, Ajith},
title = {A wrapper-filter feature selection technique based on ant colony optimization},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04171-3},
doi = {10.1007/s00521-019-04171-3},
abstract = {Ant colony optimization (ACO) is a well-explored meta-heuristic algorithm, among whose many applications feature selection (FS) is an important one. Most existing versions of ACO are either wrapper based or filter based. In this paper, we propose a wrapper-filter combination of ACO, where we introduce subset evaluation using a filter method instead of using a wrapper method to reduce computational complexity. A memory to keep the best ants and feature dimension-dependent pheromone update has also been used to perform FS in a multi-objective manner. Our proposed approach has been evaluated on various real-life datasets, taken from UCI Machine Learning repository and NIPS2003 FS challenge, using K-nearest neighbors and multi-layer perceptron classifiers. The experimental outcomes have been compared to some popular FS methods. The comparison of results clearly shows that our method outperforms most of the state-of-the-art algorithms used for FS. For measuring the robustness of the proposed model, it has been additionally evaluated on facial emotion recognition and microarray datasets.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7839–7857},
numpages = {19},
keywords = {NIPS2003 challenge, Feature selection, Ant colony optimization, Wrapper-filter method}
}

@article{10.3233/IDA-150737,
author = {Peng, Yu and Xu, Yong and Liu, Datong and Li, Junbao},
title = {Locality structure preserving based feature selection for prognostics},
year = {2015},
issue_date = {Jun 2015},
publisher = {IOS Press},
address = {NLD},
volume = {19},
number = {3},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-150737},
doi = {10.3233/IDA-150737},
abstract = {Feature selection in data-driven modelling is an
important research topic for prognostics. The performance of prediction
model may vary considerably under different feature subsets. Hence it is
important to devise a systematic feature selection method, which offers the
guidance for choosing the most representative features for prognostics.
Nowadays, feature selection algorithms in the field of prognostics are
largely studied to the type of learning: supervised or unsupervised, which
leads to poor generalization between different prognostics applications. In
this paper, a unified feature selection method, called locality structure
preserving based feature selection (LSPFS), is developed to improve the
robustness and accuracy of prognostics under both unsupervised and
supervised learning conditions. In LSPFS, the local structure of original
data is constructed according to the similarity between data points, and the
representative features are selected based on their ability to preserve the
local structure. Moreover, by designing different local structure via local
information and actual degradation information of the data, the introduced
method can unify supervised and unsupervised feature selection, and enable
their joint study under a general framework. Experiments on NASA turbofan
engine simulation dataset and lithium-ion battery dataset are conducted to
test and evaluate the proposed algorithm.},
journal = {Intell. Data Anal.},
month = jun,
pages = {659–682},
numpages = {24},
keywords = {local structure preserving, feature selection, health assessment, Prognostics}
}

@inproceedings{10.1609/aaai.v33i01.33013534,
author = {Feng, Chao and Qian, Chao and Tang, Ke},
title = {Unsupervised feature selection by pareto optimization},
year = {2019},
isbn = {978-1-57735-809-1},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v33i01.33013534},
doi = {10.1609/aaai.v33i01.33013534},
abstract = {Dimensionality reduction is often employed to deal with the data with a huge number of features, which can be generally divided into two categories: feature transformation and feature selection. Due to the interpretability, the efficiency during inference and the abundance of unlabeled data, unsupervised feature selection has attracted much attention. In this paper, we consider its natural formulation, column subset selection (CSS), which is to minimize the reconstruction error of a data matrix by selecting a subset of features. We propose an anytime randomized iterative approach POCSS, which minimizes the reconstruction error and the number of selected features simultaneously. Its approximation guarantee is well bounded. Empirical results exhibit the superior performance of POCSS over the state-of-the-art algorithms.},
booktitle = {Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {434},
numpages = {8},
location = {Honolulu, Hawaii, USA},
series = {AAAI'19/IAAI'19/EAAI'19}
}

@article{10.1504/ijcat.2021.119758,
author = {Kaur, Manpreet and Rani, Shalli and Gupta, Deepali and Manocha, Amit Kumar},
title = {Prediction of diabetic patients using various machine learning techniques},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {66},
number = {2},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2021.119758},
doi = {10.1504/ijcat.2021.119758},
abstract = {Growth of technology and digitisation of several areas has made the world more successful in reaching to the solutions of the remote problems. Large amount of health records is also available in digital storage. Machine learning plays an important role for uncovering the health issues from the digital records or for diagnosis of various diseases. In this paper, we are presenting the basic introduction of Recommender System (RS) with respect to diabetic patients after the rigorous review of already present literature. An experiment analysis is performed in Python with the help of machine learning classifiers such as Logistic Regression, Averaged Perception, Bayes Point, Boosted Decision Tree, Neural Network, Decision Forest, Two-Class Support Vector Machine and Locally Deep SVM on Pima Indian Diabetes Database. We conducted an experiment on 23 K diabetic patients' data set. Based on the all classifiers results, it reveals the Logistic Regression performs best over all other classifiers with an accuracy of 78% and predicting the accurate results in specificity of 92%.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {100–106},
numpages = {6},
keywords = {machine learning, diabetic mellitus, diabetic patients, collaborative filtering, recommender system}
}

@inproceedings{10.1145/3357384.3357865,
author = {Hosseini, Babak and Hammer, Barbara},
title = {Interpretable Multiple-Kernel Prototype Learning for Discriminative Representation and Feature Selection},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357865},
doi = {10.1145/3357384.3357865},
abstract = {Prototype-based methods are of the particular interest for domain specialists and practitioners as they summarize a dataset by a small set of representatives. Therefore, in a classification setting, interpretability of the prototypes is as significant as the prediction accuracy of the algorithm. Nevertheless, the state-of-the-art methods make inefficient trade-offs between these concerns by sacrificing one in favor of the other, especially if the given data has a kernel-based (or multiple-kernel) representation. In this paper, we propose a novel interpretable multiple-kernel prototype learning (IMKPL) to construct highly interpretable prototypes in the feature space, which are also efficient for the discriminative representation of the data. Our method focuses on the local discrimination of the classes in the feature space and shaping the prototypes based on condensed class-homogeneous neighborhoods of data. Besides, IMKPL learns a combined embedding in the feature space in which the above objectives are better fulfilled. When the base kernels coincide with the data dimensions, this embedding results in a discriminative features selection. We evaluate IMKPL on several benchmarks from different domains which demonstrate its superiority to the related state-of-the-art methods regarding both interpretability and discriminative representation.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1863–1872},
numpages = {10},
keywords = {prototype learning, multiple-kernel learning, interpretation, classification},
location = {Beijing, China},
series = {CIKM '19}
}

@article{10.1007/s10462-020-09860-3,
author = {Abdel-Basset, Mohamed and Ding, Weiping and El-Shahat, Doaa},
title = {A hybrid Harris Hawks optimization algorithm with simulated annealing for feature selection},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09860-3},
doi = {10.1007/s10462-020-09860-3},
abstract = {The significant growth of modern technology and smart systems has left a massive production of big data. Not only are the dimensional problems that face the big data, but there are also other emerging problems such as redundancy, irrelevance, or noise of the features. Therefore, feature selection (FS) has become an urgent need to search for the optimal subset of features. This paper presents a hybrid version of the Harris Hawks Optimization algorithm based on Bitwise operations and Simulated Annealing (HHOBSA) to solve the FS problem for classification purposes using wrapper methods. Two bitwise operations (AND bitwise operation and OR bitwise operation) can randomly transfer the most informative features from the best solution to the others in the populations to raise their qualities. The Simulate Annealing (SA) boosts the performance of the HHOBSA algorithm and helps to flee from the local optima. A standard wrapper method K-nearest neighbors with Euclidean distance metric works as an evaluator for the new solutions. A comparison between HHOBSA and other state-of-the-art algorithms is presented based on 24 standard datasets and 19 artificial datasets and their dimension sizes can reach up to thousands. The artificial datasets help to study the effects of different dimensions of data, noise ratios, and the size of samples on the FS process. We employ several performance measures, including classification accuracy, fitness values, size of selected features, and computational time. We conduct two statistical significance tests of HHOBSA like paired-samples T and Wilcoxon signed ranks. The proposed algorithm presented superior results compared to other algorithms.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {593–637},
numpages = {45},
keywords = {Data dimensionality, Classification, k-nearest neighbor, Harris Hawks algorithm, Feature selection}
}

@inproceedings{10.1145/2939672.2939881,
author = {Yang, Haichuan and Fujimaki, Ryohei and Kusumura, Yukitaka and Liu, Ji},
title = {Online Feature Selection: A Limited-Memory Substitution Algorithm and Its Asynchronous Parallel Variation},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939881},
doi = {10.1145/2939672.2939881},
abstract = {This paper considers the feature selection scenario where only a few features are accessible at any time point. For example, features are generated sequentially and visible one by one. Therefore, one has to make an online decision to identify key features after all features are only scanned once or twice. The optimization based approach is a powerful tool for the online feature selection.However, most existing optimization based algorithms explicitly or implicitly adopt L1 norm regularization to identify important features, and suffer two main disadvantages: 1) the penalty term for L1 norm term is hard to choose; and 2) the memory usage is hard to control or predict. To overcome these two drawbacks, this paper proposes a limited-memory and model parameter free online feature selection algorithm, namely online substitution (OS) algorithm. To improve the selection efficiency, an asynchronous parallel extension for OS (Asy-OS) is proposed. Convergence guarantees are provided for both algorithms. Empirical study suggests that the performance of OS and Asy-OS is comparable to the benchmark algorithm Grafting, but requires much less memory cost and can be easily extended to the parallel implementation.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1945–1954},
numpages = {10},
keywords = {online learning, feature selection, asynchronous parallel optimization},
location = {San Francisco, California, USA},
series = {KDD '16}
}

@article{10.1016/j.aei.2019.04.007,
author = {Jung, Namcheol and Lee, Ghang},
title = {Automated classification of building information modeling (BIM) case studies by BIM use based on natural language processing (NLP) and unsupervised learning},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1474-0346},
url = {https://doi.org/10.1016/j.aei.2019.04.007},
doi = {10.1016/j.aei.2019.04.007},
journal = {Adv. Eng. Inform.},
month = aug,
numpages = {10},
keywords = {Automated document classification (ADC), Natural language processing (NLP), Unsupervised learning, Text mining, Latent Dirichlet allocation (LDA), Latent semantic analysis (LSA), BIM use, Building information modeling (BIM)}
}

@article{10.1016/j.jbi.2019.103213,
author = {Momenzadeh, Mohammadreza and Sehhati, Mohammadreza and Rabbani, Hossein},
title = {A novel feature selection method for microarray data classification based on hidden Markov model},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {95},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2019.103213},
doi = {10.1016/j.jbi.2019.103213},
journal = {J. of Biomedical Informatics},
month = jul,
numpages = {8},
keywords = {Multi-criteria ranking, Hidden Markov model (HMM), Feature selection, DNA microarray, Cancer classification}
}

@article{10.1145/3447556.3447567,
author = {Chen, Yi-Wei and Song, Qingquan and Hu, Xia},
title = {Techniques for Automated Machine Learning},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3447556.3447567},
doi = {10.1145/3447556.3447567},
abstract = {Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a problem description, its task type, and datasets. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we portray AutoML as a bi-level optimization problem, where one problem is nested within another to search the optimum in the search space, and review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter tuning (AutoMHT), and automated deep learning (AutoDL). Stateof- the-art techniques in the three categories are presented. The iterative solver is proposed to generalize AutoML techniques. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {35–50},
numpages = {16}
}

@article{10.1007/s00500-021-06086-5,
author = {Cao, Minh-Tu and Chang, Kuan-Tsung and Nguyen, Ngoc-Mai and Tran, Van-Duc and Tran, Xuan-Linh and Hoang, Nhat-Duc},
title = {Image processing-based automatic detection of asphalt pavement rutting using a novel metaheuristic optimized machine learning approach},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {20},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06086-5},
doi = {10.1007/s00500-021-06086-5},
abstract = {Pavement rutting refers to surface depression in the wheel-path along an asphalt road which causes loss of steering control and consequently leads to serious traffic accidents. Hence, it is necessary to develop powerful methods to accurately recognize pavement rutting during road condition survey. This study presents a novel computer vision-based model to automatically identify rutting on asphalt pavement road. The model is established based on a hybridization of image processing techniques (ITPs), least squares support vector classification (LSSVC), dynamic feature selection (FS) method, and forensic-based investigation (FBI). The ITPs, including Gabor filter and discrete cosine transform were employed to implement texture computation for image data. These techniques are used to generate an initial set of extracted features describing rutting and non-rutting states. The extracted features were then refined by a wrapper-based feature selection (FS) method to determine set of highly relevant features. LSSVC models were used to learn the categorization of rutting and non-rutting based on the refined features and hyper-parameters optimized by the FBI metaheuristic. The final LSSVC prediction model with the most desired prediction accuracy can be obtained once the process of the FBI’s optimization terminates. A dataset of 2000 image samples has been collected during field trip of pavement survey in Da Nang city (Vietnam) to construct and evaluate the newly developed model. The statistical results obtained from a k-fold cross-validation have demonstrated that the hybrid FBI-LSSVC-FS model can achieve the most desired rutting recognition performance with accuracy rate, precision, recall, and F1 score of 98.9%, 0.994, 0.984 and 0.989, respectively. Therefore, this paper contributes to the body of knowledge by proposing a novel AI-based prediction model to assist transportation agencies in the task of periodic asphalt pavement survey.},
journal = {Soft Comput.},
month = oct,
pages = {12839–12855},
numpages = {17},
keywords = {Image processing, Feature selection, Forensic-based investigation, Least squares support vector machine, Rutting detection}
}

@inproceedings{10.1007/978-3-030-27192-3_17,
author = {Audah, M. Z. Fatimah and Chin, Tan Saw and Zulfadzli, Y. and Lee, C. K. and Rizaluddin, K.},
title = {Towards Efficient and Scalable Machine Learning-Based QoS Traffic Classification in Software-Defined Network},
year = {2019},
isbn = {978-3-030-27191-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27192-3_17},
doi = {10.1007/978-3-030-27192-3_17},
abstract = {Internet Service Provider (ISP) has the responsibility to fulfill the Quality of Service (QoS) of various types of applications. The centralized network controller in Software Defined Networking (SDN) provides the chance to instil intelligence in managing network resources based on QoS requirements. A fined-grained QoS Traffic Engineering can be realized by identifying different traffic flow types and categorizing them according to various application/classes. Previous methods include port-based classification and Deep Packet Inspection (DPI), which have been found non-accurate and highly computational. Thus, machine learning (ML) based traffic classifier has gained much attention from the research community, which can be seen from an increase number of works being published. This paper identifies the issues in ML-based traffic classification (TC) in order to devised the best solution; i.e. the TC framework should be scalable to accommodate network expansion, can accurately identify flows according to their source applications/classes, while maintaining an efficient run-time and memory requirement. Therefore, based on these findings, this work proposed a TC engine comprises of Training and Feature Selection Module and Classifier Model, which is placed at the data plane. The training and feature selection will be done offline and regularly to keep the Classifier Model updated. In the proposed solution, the SDN switch forwards the packets the Classifier Model, which classify the packets with accurate applications and send them to the control plane. Finally, the controller will perform resource and queue management according to the labeled packets and updates the flow tables via the switch. The proposed solution will be the starting point in solving efficiency and scalability issues in SDN-ISP TC.},
booktitle = {Mobile Web and Intelligent Information Systems: 16th International Conference, MobiWIS 2019, Istanbul, Turkey, August 26–28, 2019, Proceedings},
pages = {217–229},
numpages = {13},
keywords = {Machine Learning, Software Defined Networking, Traffic Classification},
location = {Istanbul, Turkey}
}

@article{10.1016/j.sigpro.2014.12.027,
author = {Du, Xingzhong and Yan, Yan and Pan, Pingbo and Long, Guodong and Zhao, Lei},
title = {Multiple graph unsupervised feature selection},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0165-1684},
url = {https://doi.org/10.1016/j.sigpro.2014.12.027},
doi = {10.1016/j.sigpro.2014.12.027},
abstract = {Feature selection improves the quality of the model by filtering out the noisy or redundant part. In the unsupervised scenarios, the selection is challenging due to the unavailability of the labels. To overcome that, the graphs which can unfold the geometry structure on the manifold are usually used to regularize the selection process. These graphs can be constructed either in the local view or the global view. As the local graph is more discriminative, previous methods tended to use the local graph rather than the global graph. But the global graph also has useful information. In light of this, in this paper, we propose a multiple graph unsupervised feature selection method to leverage the information from both local and global graphs. Besides that, we enforce the l 2 , p norm to achieve more flexible sparse learning. The experiments which inspect the effects of multiple graph and l 2 , p norm are conducted respectively on various datasets, and the comparisons to other mainstream methods are also presented in this paper. The results support that the multiple graph could be better than the single graph in the unsupervised feature selection, and the overall performance of the proposed method is higher than the other comparisons. HighlightsA novel unsupervised feature selection algorithm is proposed which combines multiple graphs to uncover the manifold.The l 2 , p norm has more flexibility in controlling the sparse learning, thereby resulting in better performance.Combining multiple graph and l 2 , p norm results in better performance.},
journal = {Signal Process.},
month = mar,
pages = {754–760},
numpages = {7},
keywords = {p -norm, l 2, Unsupervised learning, Multiple graph, Feature selection}
}

@article{10.1613/jair.1.11854,
author = {Z\"{o}ller, Marc-Andr\'{e} and Huber, Marco F.},
title = {Benchmark and Survey of Automated Machine Learning Frameworks},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.11854},
doi = {10.1613/jair.1.11854},
abstract = {Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to build machine learning applications automatically without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 data sets from established AutoML benchmark suites.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {409–472},
numpages = {64}
}

@article{10.1007/s00521-020-05462-w,
author = {Dornaika, Fadi},
title = {Flexible data representation with graph convolution for semi-supervised learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05462-w},
doi = {10.1007/s00521-020-05462-w},
abstract = {This paper introduces a scheme for semi-supervised data representation. It proposes a flexible nonlinear embedding model that imitates the principle of spectral graph convolutions. Structured data are exploited in order to determine nonlinear and linear models. The introduced scheme takes advantage of data graphs at two different levels. First, it incorporates manifold regularization that is naturally encoded by the graph itself. Second, the regression model is built on the convolved data samples that are obtained by the joint use of the data and their associated graph. The proposed semi-supervised embedding can tackle challenges related to over-fitting in image data spaces. The proposed graph convolution-based semi-supervised embedding paves the way to new theoretical and application perspectives related to the nonlinear embedding. Indeed, building flexible models that adopt convolved data samples can enhance both the data representation and the final performance of the learning system. Several experiments are conducted on six image datasets for comparing the introduced scheme with many state-of-art semi-supervised approaches. These experimental results show the effectiveness of the introduced data representation scheme.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {6851–6863},
numpages = {13},
keywords = {Pattern recognition, Discriminant embedding, Graph convolutions, Semi-supervised learning, Graph-based embedding}
}

@phdthesis{10.5555/AAI28889421,
author = {Wissel, Benjamin D.},
title = {Generalizability of Electronic Health Record-Based Machine Learning Models},
year = {2021},
isbn = {9798492756154},
publisher = {University of Cincinnati},
address = {USA},
abstract = {Epilepsy affects over 50 million people and is responsible for 1.3% of deaths worldwide. While many people with epilepsy benefit from pharmacotherapy, approximately one-third have drug-resistant epilepsy. Clinical guidelines recommend that these patients be promptly evaluated for resective epilepsy surgery, which is associated with a 67% chance of long-term seizure freedom. Unfortunately, surgery is underutilized. The mean disease duration at the time of surgery is six years in pediatrics and 20 years in adults. Identifying surgical candidates earlier in the disease course is needed to optimize care. Machine learning models have been developed to aid in this process, and sending alerts based on these model's recommendations is associated with a three-fold increase in referrals. Electronic health record-based machine learning models have been developed to predict a variety of health outcomes. However, they are rarely implemented into clinical care. Generalizing electronic health record-based predictive models across health care systems is challenging for several reason. Electronic health record data contain process biases, health care delivery patterns change over time, patient populations vary across geographical regions, and documentation styles vary by provider. Overcoming these limitations will improve the likelihood that machine learning-based clinical decision support systems will enhance patient care.To address these challenges, this dissertation proposed a novel methodology for generalizing electronic health record-based machine learning models across health care systems. The set of procedures produced site-specific models with modifiable feature sets and parameter weights. Data preprocessing, feature selection, and model training were repeated at each institution without modification. The results showed that generalizing modelling processes, rather than the models themselves, minimized generalization error. These methods were used to identify candidates for epilepsy surgery at one adult and one pediatric epilepsy center. In a prospective validation study, model performance was robust to temporal variations in care patterns, including virtual visits that occurred during the first three months of the SARS-CoV-2 pandemic. The final chapter of the dissertation concludes by summarizing lessons learned from these experiments, the current landscape of the literature, and future avenues of research. This work provides seminal data to illustrate the effectiveness of machine learning algorithms across sites and how this could be applied to external institutions.},
note = {AAI28889421}
}

@article{10.1016/j.patcog.2018.12.020,
author = {Kashef, Shima and Nezamabadi-pour, Hossein},
title = {A label-specific multi-label feature selection algorithm based on the Pareto dominance concept},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.12.020},
doi = {10.1016/j.patcog.2018.12.020},
journal = {Pattern Recogn.},
month = apr,
pages = {654–667},
numpages = {14},
keywords = {Online feature selection, Pareto dominance, Label-specific features, Feature selection, Multi-label dataset}
}

@inproceedings{10.1145/3395245.3396432,
author = {Ai, Dan and Zhang, Tiancheng and Yu, Ge and Shao, Xinying},
title = {A Dropout Prediction Framework Combined with Ensemble Feature Selection},
year = {2020},
isbn = {9781450377058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395245.3396432},
doi = {10.1145/3395245.3396432},
abstract = {In recent years, with the rapid development of large-scale open online courses, low completion rate and high dropout rate have been important challenges for open online courses. Therefore, it is necessary to make effective prediction and timely intervention to ensure the completion of the course. Some of the traditional prediction models only use the features extracted manually from students' clickstream data, which is too subjective to guarantee the quality of features and affect the prediction accuracy. Others generate features automatically with finer granularity, but the problem of feature redundancy appears. In order to solve this problem, this paper proposes a comprehensive dropout prediction framework of MOOCs students. The framework can automatically extract features from clickstream data, and filter features with an integrated feature selection strategy based on clustering and weighted MaxDiff, and finally predict. Experiments show that the model can effectively improve the accuracy of prediction of dropout.},
booktitle = {Proceedings of the 2020 8th International Conference on Information and Education Technology},
pages = {179–185},
numpages = {7},
keywords = {ensemble feature selection, Mean-shift clustering, MOOCs, Dropout prediction},
location = {Okayama, Japan},
series = {ICIET 2020}
}

@article{10.1016/j.ins.2019.03.075,
author = {Liu, Heng and Ditzler, Gregory},
title = {A semi-parallel framework for greedy information-theoretic feature selection},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {492},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.03.075},
doi = {10.1016/j.ins.2019.03.075},
journal = {Inf. Sci.},
month = aug,
pages = {13–28},
numpages = {16},
keywords = {Parallel computing, Information theory, Feature selection}
}

@article{10.1016/j.patcog.2017.09.033,
author = {Xia, Xinghua and Song, Xiaoyu and Luan, Fangun and Zheng, Jungang and Chen, Zhili and Ma, Xiaofu},
title = {Discriminative feature selection for on-line signature verification},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {74},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.09.033},
doi = {10.1016/j.patcog.2017.09.033},
abstract = {On-line test signatures are aligned effectively to reference templates based on Gaussian mixture model before verification.Discriminative features are selected based on full factorial experiment design among consistent feature candidates.An alternative method of discriminative feature selection based on optimal orthogonal experiment design is presented to improve the efficiency.Features are not matched by DTW directly, but they are matched with the location constraints instead, which are inherent in two matching signature curves. On-line handwritten signatures are collected as real-time dynamical signals which are written on collective devices by users. Since individuals have different writing habits, consistent and discriminative features should be selected to distinguish genuine signatures from forged signatures. In this paper, two methods, which are based on full factorial experiment design and optimal orthogonal experiment design, are proposed for selecting discriminative features among candidates. To improve the robustness, consistency of feature is analyzed at first, and more consistent features are selected as candidates for discriminative feature selection. To reduce the influences of fluctuations caused by internal and external writing environments changes before verification, signatures are effectively aligned to their reference templates based on Gaussian mixture model. A modified dynamic time warping with signature curve constraint is presented for verification to improve the efficiency. Comprehensive experiments are implemented based on the data of the open access databases MCYT and SVC2004 Task2. Experimental results verify the effectiveness and robustness of our proposed methods.},
journal = {Pattern Recogn.},
month = feb,
pages = {422–433},
numpages = {12},
keywords = {Signature curve constraint, Signature alignment, Orthogonal experiment design, On-line signature verification, Factorial experiment design, Discriminative feature selection}
}

@article{10.1007/s00521-020-05560-9,
author = {Ghosh, Kushal Kanti and Guha, Ritam and Bera, Suman Kumar and Kumar, Neeraj and Sarkar, Ram},
title = {S-shaped versus V-shaped transfer functions for binary Manta ray foraging optimization in feature selection problem},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {17},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05560-9},
doi = {10.1007/s00521-020-05560-9},
abstract = {Feature selection (FS) is considered as one of the core concepts in the areas of machine learning and data mining which immensely impacts the performance of classification model. Through FS, irrelevant or partially relevant features can be eliminated which in turn helps in enhancing the performance of the model. Over the years, researchers have applied different meta-heuristic optimization techniques for the purpose of FS as these overcome the limitations of traditional optimization approaches. Going by the trend, we introduce a new FS approach based on a recently proposed meta-heuristic algorithm called Manta ray foraging optimization (MRFO) which is developed following the food foraging nature of the Manta rays, one of the largest known marine creatures. As MRFO is apposite for continuous search space problems, we have adapted a binary version of MRFO to fit it into the problem of FS by applying eight different transfer functions belonging to two different families: S-shaped and V-shaped. We have evaluated the eight binary versions of MRFO on 18 standard UCI datasets. Of these, the best one is considered for comparison with 16 recently proposed meta-heuristic FS approaches. The results show that MRFO outperforms the state-of-the-art methods in terms of both classification accuracy and number of features selected. The source code of this work is available in .},
journal = {Neural Comput. Appl.},
month = sep,
pages = {11027–11041},
numpages = {15},
keywords = {Metaheuristic, Transfer functions, Classification, Optimization, Feature selection, Manta ray foraging optimization}
}

@article{10.1016/j.asoc.2019.105545,
author = {Fatemi Bushehri, S.M.M. and Zarchi, Mohsen Sardari},
title = {An expert model for self-care problems classification using probabilistic neural network and feature selection approach},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105545},
doi = {10.1016/j.asoc.2019.105545},
journal = {Appl. Soft Comput.},
month = sep,
numpages = {11},
keywords = {Genetic algorithm, ICF-CY, Rule extraction, Feature selection, PNN classifier, Self-care classification, Soft computing}
}

@article{10.1016/j.inffus.2016.10.001,
author = {Pes, Barbara and Dess, Nicoletta and Angioni, Marta},
title = {Exploiting the ensemble paradigm for stable feature selection},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {C},
issn = {1566-2535},
url = {https://doi.org/10.1016/j.inffus.2016.10.001},
doi = {10.1016/j.inffus.2016.10.001},
abstract = {We discuss the rationale of ensemble feature selection.We empirically evaluate the effectiveness of a data perturbation ensemble approach.Our study involves both univariate and multivariate selection algorithms.A special emphasis is given to the stability level of the selected feature subsets.Useful insight is gained from the analysis of high-dimensional genomic datasets. Ensemble classification is a well-established approach that involves fusing the decisions of multiple predictive models. A similar ensemble logic has been recently applied to challenging feature selection tasks aimed at identifying the most informative variables (or features) for a given domain of interest. In this work, we discuss the rationale of ensemble feature selection and evaluate the effects and the implications of a specific ensemble approach, namely the data perturbation strategy. Basically, it consists in combining multiple selectors that exploit the same core algorithm but are trained on different perturbed versions of the original data. The real potential of this approach, still object of debate in the feature selection literature, is here investigated in conjunction with different kinds of core selection algorithms (both univariate and multivariate). In particular, we evaluate the extent to which the ensemble implementation improves the overall performance of the selection process, in terms of predictive accuracy and stability (i.e., robustness with respect to changes in the training data). Furthermore, we measure the impact of the ensemble approach on the final selection outcome, i.e. on the composition of the selected feature subsets. The results obtained on ten public genomic benchmarks provide useful insight on both the benefits and the limitations of such ensemble approach, paving the way to the exploration of new and wider ensemble schemes.},
journal = {Inf. Fusion},
month = may,
pages = {132–147},
numpages = {16},
keywords = {Selection stability, High-dimensional genomic data, Feature selection, Ensemble paradigm, Data perturbation}
}

@article{10.1016/j.compbiomed.2021.104450,
author = {Sharma, Samriti and Singh, Gurvinder and Sharma, Manik},
title = {A comprehensive review and analysis of supervised-learning and soft computing techniques for stress diagnosis in humans},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104450},
doi = {10.1016/j.compbiomed.2021.104450},
journal = {Comput. Biol. Med.},
month = jul,
numpages = {19},
keywords = {Deep learning techniques, Fuzzy logic, Nature-inspired methods, Soft computing, Supervised learning, Stress}
}

@inproceedings{10.1145/2019136.2019161,
author = {Pleuss, Andreas and Rabiser, Rick and Botterweck, Goetz},
title = {Visualization techniques for application in interactive product configuration},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019161},
doi = {10.1145/2019136.2019161},
abstract = {In product line engineering (PLE) a major challenge is the complexity of artifacts that have to be handled. In real-world product lines, variability models can become large and complex comprising thousands of elements with hundreds of non-trivial dependencies. Visual and interactive techniques aim to reduce the (cognitive) complexity and support the user during challenging PLE tasks like product configuration. There are many visualization techniques described in the literature -- e.g., in Software Visualization -- and some isolated techniques have been applied in PLE tools. Nevertheless, the full potential of visualization in the context of PLE has not been exploited so far. This paper provides an overview of (1) available visualization techniques and criteria to judge their benefits and drawbacks for product configuration, (2) which have been applied in product configuration in PLE, and (3) which could be beneficial to support product configuration. We propose a research agenda for future work in visual and interactive PLE techniques.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {22},
numpages = {8},
keywords = {software visualization, product line engineering, product configuration},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1016/j.neucom.2017.02.034,
author = {Du, Shiqiang and Ma, Yide and Li, Shouliang and Ma, Yurun},
title = {Robust unsupervised feature selection via matrix factorization},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {241},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2017.02.034},
doi = {10.1016/j.neucom.2017.02.034},
abstract = {We proposed a robust unsupervised method to remove redundant and irrelevant features.Both the cluster centers and the sparse representation are predicted.The feature selection and clustering are performed simultaneously.An efficient iterative update algorithm based on ADMM is proposed.Superiority over seven existing methods is established for five data sets. Dimensionality reduction is a challenging task for high-dimensional data processing in machine learning and data mining. It can help to reduce computation time, save storage space and improve the performance of learning algorithms. As an effective dimension reduction technique, unsupervised feature selection aims at finding a subset of features to retain the most relevant information. In this paper, we propose a novel unsupervised feature selection method, called Robust Unsupervised Feature Selection via Matrix Factorization (RUFSM), in which robust discriminative feature selection and robust clustering are performed simultaneously under l2, 1-norm while the local manifold structures of data are preserved. The advantages of this work are three-fold. Firstly, both the latent orthogonal cluster centers and the sparse representation of the projected data points based on matrix factorization are predicted for selecting robust discriminative features. Secondly, the feature selection and the clustering are performed simultaneously to guarantee an overall optimum. Thirdly, an efficient iterative update algorithm, which is based on Alternating Direction Method of Multipliers (ADMM), is used for RUFSM optimization. Compared with several state-of-the-art unsupervised feature selection methods, the proposed algorithm comes with better clustering performance for almost all datasets we have experimented with here.},
journal = {Neurocomput.},
month = jun,
pages = {115–127},
numpages = {13},
keywords = {l2, Unsupervised feature selection, Matrix factorization, Manifold regularization, 1-norm}
}

@article{10.1007/s00521-017-3131-4,
author = {Ewees, Ahmed A. and El Aziz, Mohamed Abd and Hassanien, Aboul Ella},
title = {Chaotic multi-verse optimizer-based feature selection},
year = {2019},
issue_date = {April     2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {4},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-017-3131-4},
doi = {10.1007/s00521-017-3131-4},
abstract = {The multi-verse optimizer (MVO) is a new evolutionary algorithm inspired by the concepts of multi-verse theory namely, the white/black holes, which represents the interaction between the universes. However, the MVO has some drawbacks, like any other evolutionary algorithms, such as slow convergence and getting stuck in local optima (maximum or minimum). This paper provides a novel chaotic MVO algorithm (CMVO) to avoid these drawbacks, where chaotic maps are used to improve the performance of MVO algorithm. The CMVO algorithm is applied to solve the feature selection problem, in which five benchmark datasets are used to evaluate the performance of CMVO algorithm. The results of CMVO is compared with standard MVO and two other swarm algorithms. The experimental results show that logistic chaotic map is the best chaotic map that increases the performance of MVO, and also the MVO is better than other swarm algorithms.},
journal = {Neural Comput. Appl.},
month = apr,
pages = {991–1006},
numpages = {16},
keywords = {Multi-verse optimizer, Feature selection, Dimensionality reduction, Chaotic maps}
}

@article{10.1007/s10796-016-9724-0,
author = {Liu, Jun and Timsina, Prem and El-Gayar, Omar},
title = {A comparative analysis of semi-supervised learning: The case of article selection for medical systematic reviews},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-016-9724-0},
doi = {10.1007/s10796-016-9724-0},
abstract = {While systematic reviews are positioned as an essential element of modern evidence-based medical practice, the creation of these reviews is resource intensive. To mitigate this problem, there have been some attempts to leverage supervised machine learning to automate the article triage procedure. This approach has been proved to be helpful for updating existing systematic reviews. However, this technique holds very little promise for creating new reviews because training data is rarely available when it comes to systematic creation. In this research we assess and compare the applicability of semi-supervised learning to overcome this labeling bottleneck and support the creation of systematic reviews. The results indicated that semi-supervised learning could significantly reduce the human effort and is a viable technique for automating medical systematic review creation with a small-sized training dataset.},
journal = {Information Systems Frontiers},
month = apr,
pages = {195–207},
numpages = {13},
keywords = {Text mining, Text analytics, Semi-supervised learning, Self-training, Medical systematic reviews, Active learning}
}

@article{10.1016/j.eswa.2017.03.057,
author = {Agnihotri, Deepak and Verma, Kesari and Tripathi, Priyanka},
title = {Variable Global Feature Selection Scheme for automatic classification of text documents},
year = {2017},
issue_date = {September 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {81},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.03.057},
doi = {10.1016/j.eswa.2017.03.057},
abstract = {A novel Variable Global Feature Selection Scheme (VGFSS) is proposed.VGFSS selects variable number of features from each class instead of equal features.The selection of features in VGFSS is based on distribution of terms in the classes.The methods are evaluated using Macro_F1 and Micro_F1 measure followed by Z-test.The VGFSS algorithm outperforms among seven competing methods in benchmark datasets. The feature selection is important to speed up the process of Automatic Text Document Classification (ATDC). At present, the most common method for discriminating feature selection is based on Global Filter-based Feature Selection Scheme (GFSS). The GFSS assigns a score to each feature based on its discriminating power and selects the top-N features from the feature set, where N is an empirically determined number. As a result, it may be possible that the features of a few classes are discarded either partially or completely. The Improved Global Feature Selection Scheme (IGFSS) solves this issue by selecting an equal number of representative features from all the classes. However, it suffers in dealing with an unbalanced dataset having large number of classes. The distribution of features in these classes are highly variable. In this case, if an equal number of features are chosen from each class, it may exclude some important features from the class containing a higher number of features. To overcome this problem, we propose a novel Variable Global Feature Selection Scheme (VGFSS) to select a variable number of features from each class based on the distribution of terms in the classes. It ensures that, a minimum number of terms are selected from each class. The numerical results on benchmark datasets show the effectiveness of the proposed algorithm VGFSS over classical information science methods and IGFSS.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {268–281},
numpages = {14},
keywords = {Text mining, Text document classification, Text analysis, Feature selection}
}

@article{10.1007/s11704-014-3359-4,
author = {Ziani, Djamal},
title = {Feature selection on probabilistic symbolic objects},
year = {2014},
issue_date = {Dec 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-014-3359-4},
doi = {10.1007/s11704-014-3359-4},
abstract = {In data analysis tasks, we are often confronted to very high dimensional data. Based on the purpose of a data analysis study, feature selection will find and select the relevant subset of features from the original features. Many feature selection algorithms have been proposed in classical data analysis, but very few in symbolic data analysis (SDA) which is an extension of the classical data analysis, since it uses rich objects instead to simple matrices. A symbolic object, compared to the data used in classical data analysis can describe not only individuals, but also most of the time a cluster of individuals. In this paper we present an unsupervised feature selection algorithm on probabilistic symbolic objects (PSOs), with the purpose of discrimination. A PSO is a symbolic object that describes a cluster of individuals by modal variables using relative frequency distribution associated with each value. This paper presents new dissimilarity measures between PSOs, which are used as feature selection criteria, and explains how to reduce the complexity of the algorithm by using the discrimination matrix.},
journal = {Front. Comput. Sci.},
month = dec,
pages = {933–947},
numpages = {15},
keywords = {symbolic data analysis, probabilistic symbolic object, feature selection, discrimination criteria, data and knowledge visualization}
}

@article{10.1007/s10115-017-1145-y,
author = {Palma-Mendoza, Raul-Jose and Rodriguez, Daniel and De-Marcos, Luis},
title = {Distributed ReliefF-based feature selection in Spark},
year = {2018},
issue_date = {October   2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1145-y},
doi = {10.1007/s10115-017-1145-y},
abstract = {Feature selection (FS) is a key research area in the machine learning and data mining fields; removing irrelevant and redundant features usually helps to reduce the effort required to process a dataset while maintaining or even improving the processing algorithm's accuracy. However, traditional algorithms designed for executing on a single machine lack scalability to deal with the increasing amount of data that have become available in the current Big Data era. ReliefF is one of the most important algorithms successfully implemented in many FS applications. In this paper, we present a completely redesigned distributed version of the popular ReliefF algorithm based on the novel Spark cluster computing model that we have called DiReliefF. The effectiveness of our proposal is tested on four publicly available datasets, all of them with a large number of instances and two of them with also a large number of features. Subsets of these datasets were also used to compare the results to a non-distributed implementation of the algorithm. The results show that the non-distributed implementation is unable to handle such large volumes of data without specialized hardware, while our design can process them in a scalable way with much better processing times and memory usage.},
journal = {Knowl. Inf. Syst.},
month = oct,
pages = {1–20},
numpages = {20},
keywords = {ReliefF, Feature selection, Distributed algorithm, Big Data, Apache Spark}
}

@article{10.1016/j.eswa.2019.112961,
author = {Hameed, Nazia and Shabut, Antesar M. and Ghosh, Miltu K. and Hossain, M.A.},
title = {Multi-class multi-level classification algorithm for skin lesions classification using machine learning techniques},
year = {2020},
issue_date = {Mar 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {141},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.112961},
doi = {10.1016/j.eswa.2019.112961},
journal = {Expert Syst. Appl.},
month = mar,
numpages = {18},
keywords = {Eczema classification, Melanoma classification, Texture &amp; colour features, Deep learning, Machine learning, Computer-aided diagnosise, Skin lesion classification}
}

@article{10.1016/j.compbiomed.2019.02.017,
author = {Zhang, Zhenwei and Sejdi\'{c}, Ervin},
title = {Radiological images and machine learning: Trends, perspectives, and prospects},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.02.017},
doi = {10.1016/j.compbiomed.2019.02.017},
journal = {Comput. Biol. Med.},
month = may,
pages = {354–370},
numpages = {17},
keywords = {Deep neural network, Imaging modalities, Machine learning, Deep learning}
}

@article{10.1016/j.procs.2018.05.127,
author = {Lamba, Monika and Munjal, Geetika and Gigras, Yogita},
title = {Feature Selection of Micro-array expression data (FSM) - A Review},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.127},
doi = {10.1016/j.procs.2018.05.127},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1619–1625},
numpages = {7},
keywords = {Feature Selection, Gene expression, Filter method, Wrapper method}
}

@inproceedings{10.1145/3465332.3470875,
author = {Akgun, Ibrahim Umit and Aydin, Ali Selman and Shaikh, Aadil and Velikov, Lukas and Zadok, Erez},
title = {A Machine Learning Framework to Improve Storage System Performance},
year = {2021},
isbn = {9781450385503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465332.3470875},
doi = {10.1145/3465332.3470875},
abstract = {Storage systems and their OS components are designed to accommodate a wide variety of applications and dynamic workloads. Storage components inside the OS contain various heuristic algorithms to provide high performance and adaptability for different workloads. These heuristics may be tunable via parameters, and some system calls allow users to optimize their system performance. These parameters are often predetermined based on experiments with limited applications and hardware. Thus, storage systems often run with these predetermined and possibly suboptimal values. Tuning these parameters manually is impractical: one needs an adaptive, intelligent system to handle dynamic and complex workloads. Machine learning (ML) techniques are capable of recognizing patterns, abstracting them, and making predictions on new data. ML can be a key component to optimize and adapt storage systems. In this position paper, we propose KML, an ML framework for storage systems. We implemented a prototype and demonstrated its capabilities on the well-known problem of tuning optimal readahead values. Our results show that KML has a small memory footprint, introduces negligible overhead, and yet enhances throughput by as much as 2.3x.},
booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {94–102},
numpages = {9},
keywords = {storage systems, storage performance optimization, operating systems, machine learning},
location = {Virtual, USA},
series = {HotStorage '21}
}

@article{10.1016/j.eswa.2015.01.069,
author = {Dess\`{\i}, Nicoletta and Pes, Barbara},
title = {Similarity of feature selection methods},
year = {2015},
issue_date = {June 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {10},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.01.069},
doi = {10.1016/j.eswa.2015.01.069},
abstract = {We empirically investigated the similarity among feature selection methods.Extensive experiments were carried out across high dimensional classification tasks.We obtained useful insight into the pattern of agreement of eight popular methods. In the past two decades, the dimensionality of datasets involved in machine learning and data mining applications has increased explosively. Therefore, feature selection has become a necessary step to make the analysis more manageable and to extract useful knowledge about a given domain. A large variety of feature selection techniques are available in literature, and their comparative analysis is a very difficult task. So far, few studies have investigated, from a theoretical and/or experimental point of view, the degree of similarity/dissimilarity among the available techniques, namely the extent to which they tend to produce similar results within specific application contexts. This kind of similarity analysis is of crucial importance when two or more methods are combined in an ensemble fashion: indeed the ensemble paradigm is beneficial only if the involved methods are capable of giving different and complementary representations of the considered domain. This paper gives a contribution in this direction by proposing an empirical approach to evaluate the degree of consistency among the outputs of different selection algorithms in the context of high dimensional classification tasks. Leveraging on a proper similarity index, we systematically compared the feature subsets selected by eight popular selection methods, representatives of different selection approaches, and derived a similarity trend for feature subsets of increasing size. Through an extensive experimentation involving sixteen datasets from three challenging domains (Internet advertisements, text categorization and micro-array data classification), we obtained useful insight into the pattern of agreement of the considered methods. In particular, our results revealed how multivariate selection approaches systematically produce feature subsets that overlap to a small extent with those selected by the other methods.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {4632–4642},
numpages = {11},
keywords = {Similarity measures, Knowledge discovery, Feature selection, Data mining}
}

@article{10.1007/s11277-018-5309-1,
author = {Xu, Huali and Yu, Shuhao and Chen, Jiajun and Zuo, Xukun},
title = {An Improved Firefly Algorithm for Feature Selection in Classification},
year = {2018},
issue_date = {Oct 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {4},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-018-5309-1},
doi = {10.1007/s11277-018-5309-1},
abstract = {Feature selection functions as an important method of receiving data so as to make the amount of features decrease. While solving the issue of classifying there exists numerous features having no relevance and being unnecessary which have the potential of making classification performance decrease. Firefly algorithm (FA) functions as an efficient method to make computation which is efficient and progressive. Nevertheless, the conventional FA is easily fallen into the local optima which imposes unsatisfactory practice on feature selection. In this research, one proposal was put forward, the firefly algorithm that combines the binary firefly algorithm with opposition-based learning to select features in classification. Experiment outcomes indicate the fact that the means put forward surpasses PSO and the conventional firefly algorithm.},
journal = {Wirel. Pers. Commun.},
month = oct,
pages = {2823–2834},
numpages = {12},
keywords = {Opposition-based learning, Firefly algorithm, Feature selection, Evolutionary computation, Classification}
}

@article{10.1287/mnsc.1090.1058,
author = {Wang, Xinfang (Jocelyn) and Camm, Jeffrey D. and Curry, David J.},
title = {A Branch-and-Price Approach to the Share-of-Choice Product Line Design Problem},
year = {2009},
issue_date = {October 2009},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {55},
number = {10},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1090.1058},
doi = {10.1287/mnsc.1090.1058},
abstract = {We develop a branch-and-price algorithm for constructing an optimal product line using partworth estimates from choice-based conjoint analysis. The algorithm determines the specific attribute levels for each multiattribute product in a set of products to maximize the resulting product line's share of choice, i.e., the number of respondents for whom at least one new product's utility exceeds the respondent's reservation utility. Computational results using large commercial and simulated data sets demonstrate that the algorithm can identify provably optimal, robust solutions to realistically sized problems.},
journal = {Manage. Sci.},
month = oct,
pages = {1718–1728},
numpages = {11},
keywords = {share of choice, product line design, optimization, marketing, integer programming, conjoint analysis, combinatorial optimization, column generation, branch and price}
}

@inproceedings{10.1007/978-3-030-79457-6_44,
author = {Yilmaz, Ibrahim and Baza, Mohamed and Amer, Ramy and Rasheed, Amar and Amsaad, Fathi and Morsi, Rasha},
title = {On the Assessment of Robustness of Telemedicine Applications against Adversarial Machine Learning Attacks},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_44},
doi = {10.1007/978-3-030-79457-6_44},
abstract = {Telemedicine applications have been recently evolved to allow patients in underdeveloped areas to receive medical services. Meanwhile, machine learning (ML) techniques have been widely adopted in such telemedicine applications to help in disease diagnosis. The performance of these ML techniques, however, are limited by the fact that attackers can manipulate clean data to fool the model classifier and break the truthfulness and robustness of these models. For instance, due to attacks, a benign sample can be treated as a malicious one by the classifier and vice versa. Motivated by this, this paper aims at exploring this issue for telemedicine applications. Particularly, this paper studies the impact of adversarial attacks on mammographic image classifier. First, mamographic images are used to train and evaluate the accuracy of our proposed model. The original dataset is then poisoned to generate adversarial samples that can mislead the model. For this, structural similarity index is used to determine similarity between clean images and adversarial images. Results show that adversarial attacks can severely fool the ML model.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {519–529},
numpages = {11},
keywords = {Structural similarity index, Convolutional neural network, Machine learning, Telemedicine applications},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1007/s10586-018-2550-z,
author = {Joseph Manoj, R. and Anto Praveena, M. D. and Vijayakumar, K.},
title = {An ACO–ANN based feature selection algorithm for big data},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-2550-z},
doi = {10.1007/s10586-018-2550-z},
abstract = {Feature selection is the approach of choosing subset of given dataset based on some feature. It can be used to minimize dimensions of the huge data set. So that it removes unnecessary data in the data source and produces prediction or output accurately in big data analytics. In the proposed work, feature selection algorithm process is implemented for text categorization using the algorithms ant colony optimization (ACO) and artificial neural network (ANN). This hybrid approach simulated using Reuter’s data set and proved its efficiency.},
journal = {Cluster Computing},
month = mar,
pages = {3953–3960},
numpages = {8},
keywords = {Ant colony optimization, Feature selection, Artificial neural network}
}

@inproceedings{10.1145/3233347.3233369,
author = {Harish, B. S. and Manju, N.},
title = {Hybrid Feature Selection Method using Fisher's Discriminate Ratio to Classify Internet Traffic Data},
year = {2018},
isbn = {9781450364720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233347.3233369},
doi = {10.1145/3233347.3233369},
abstract = {Identification and classification of network applications is a key area of network management and network security. This is due to exponential growth of the Internet users, which in turn increases the growth of the internet traffic. As Internet grows, different types of Internet traffic generated over the network also grow. This recommends proposing new methods to identify and classify the network traffic. In this paper, we proposed to embed Fishers's Discriminate Ratio (FDR) with Sequential Forward Selection (SFS), Sequential Backward Selection (SBS) and Plus L Minus R feature selection methods to analyze and classify the Internet traffic. To evaluate the proposed method, we used publicly available KDDcup99 dataset. Experimental results proved that the proposed embedding method will outperform compare to the existing methods.},
booktitle = {Proceedings of the 4th International Conference on Frontiers of Educational Technologies},
pages = {75–79},
numpages = {5},
keywords = {Internet Traffic, Feature Selection, Classification},
location = {Moscow, Russian Federation},
series = {ICFET '18}
}

@article{10.4018/IJAEC.2016100102,
author = {Biswas, Saroj and Bordoloi, Monali and Purkayastha, Biswajit},
title = {Review on Feature Selection and Classification using Neuro-Fuzzy Approaches},
year = {2016},
issue_date = {October 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {4},
issn = {1942-3594},
url = {https://doi.org/10.4018/IJAEC.2016100102},
doi = {10.4018/IJAEC.2016100102},
abstract = {This research article attempts to provide a recent survey on neuro-fuzzy approaches for feature selection and classification. Feature selection acts as a catalyst in reducing computation time and dimensionality, enhancing prediction performance or accuracy and curtailing irrelevant or redundant data. The neuro-fuzzy approach is used for feature selection and for providing some insight to the user about the symbolic knowledge embedded within the network. The neuro-fuzzy approach combines the merits of neural network and fuzzy logic to solve many complex machine learning problems. The objective of this article is to provide a generic introduction and a recent survey to neuro-fuzzy approaches for feature selection and classification in a wide area of machine learning problems. Some of the existing neuro-fuzzy models are also applied on standard datasets to demonstrate the applicability of neuro-fuzzy approaches.},
journal = {Int. J. Appl. Evol. Comput.},
month = oct,
pages = {28–44},
numpages = {17},
keywords = {Neuro-Fuzzy System, Fuzzy Logic, Feature Selection, Data Mining, Artificial Neural Networks}
}

@inproceedings{10.1145/3341162.3349333,
author = {He, Miao and Gu, Weixi and Zhou, Yuxun and Kong, Ying and Zhang, Lin},
title = {Causal feature selection for physical sensing data: a case study on power events prediction},
year = {2019},
isbn = {9781450368698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341162.3349333},
doi = {10.1145/3341162.3349333},
abstract = {Identifying the causal features from multi-dimensional physical data streams is one of the underpinnings for the success of the data-driven inference tasks. Prior research utilizes the 'correlation' or 'mutual information' to select features, which ignored the crucial inherent causality behind the data. In this work, we consider the problem of selecting causal features from streaming physical sensing data. Inspired by a metric from information theory which calibrates both instantaneous and temporal relations, we formulate the causal feature selection as a cardinality constrained joint directed information maximization (i.e., Max-JDI) problem. Then we propose a near optimal greedy algorithm for streaming feature selection and present an information-interpretive solution for the cardinality constraint presetting. The proposed method is evaluated on a real-world case study involving feature selection for the power distribution network event detection. Compared with other selection baselines, the proposed method increases the detection accuracy by around 5%, while concurrently reduces the computation time from several weeks to within a minute. The promising results demonstrate that it can be applied to optimize the energy operation and enhance the resilience of power buildings.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {565–570},
numpages = {6},
keywords = {power events prediction, multi-dimensional physical data streams, feature selection},
location = {London, United Kingdom},
series = {UbiComp/ISWC '19 Adjunct}
}

@inproceedings{10.1145/3421937.3421970,
author = {Enayati, Moein and Farahani, Nasibeh Zanjirani and Skubic, Marjorie},
title = {Machine Learning Approach for Motion Artifact Detection in Ballistocardiogram Signals},
year = {2021},
isbn = {9781450375320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421937.3421970},
doi = {10.1145/3421937.3421970},
abstract = {With the current increase in cardiovascular disease and the complexities they create, especially for aging seniors, we are working on in-home and non-invasive techniques to monitor vital signs for early detection of health conditions. Ballistocardiography has shown to be useful for long-term evaluation of myocardial strength. We have previously reported the successful utilization of our hydraulic bed sensor in the estimation of heart rate, sleep posture, and blood pressure. However, bed sensors used in naturalistic settings such as the home are known to be highly susceptible to motion artifacts.In this paper, the state-of-the-art methods for motion artifact detection and reduction are reviewed, and a new sequential machine learning approach is proposed. The proposed method is based on 53 novel features extracted jointly from time and frequency domains for noise detection. Our experiments show detection accuracy and sensitivities as high as 99%. Data were collected in two separate IRB approved data collections, one with 16-minute sequences from 25 subjects in the lab and the other with 5 sets of overnight data collected at a sleep center.},
booktitle = {Proceedings of the 14th EAI International Conference on Pervasive Computing Technologies for Healthcare},
pages = {406–410},
numpages = {5},
keywords = {Motion Artifact Detection, Machine Learning, Feature Engineering, Ballistocardiography},
location = {Atlanta, GA, USA},
series = {PervasiveHealth '20}
}

@inproceedings{10.1145/2145204.2145402,
author = {Liu, Xiaoqing (Frank) and Barnes, Eric Christopher and Savolainen, Juha Erik},
title = {Conflict detection and resolution for product line design in a collaborative decision making environment},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145402},
doi = {10.1145/2145204.2145402},
abstract = {Ensuring that the non-functional requirements (NFRs), of a system are satisfied is an essential task in software development. However, this task is complicated by the fact that many NFRs conflict with each other from multiple perspectives. It is essential to resolve conflicts collectively in a collaborative decision making process since stakeholders often disagree on how conflicts should be resolved. In this paper, we describe a method for dividing high-level NFR conflicts within a product line into more manageable sub-problems. Stakeholders make use of an argumentation based collaborative decision support system to determine which design alternatives provide the best trade-offs between NFRs. Finally, we present an empirical study in which the aforementioned system was used to resolve a single instance of an NFR conflict across 3 members of a product line. It shows that the system is effective in resolving conflicts in a collaborative decision process.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1327–1336},
numpages = {10},
keywords = {participatory/cooperative design, computer-mediated communication, collaborative software development, Collaboration architectures},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@article{10.1016/j.cogsys.2018.09.009,
author = {Chu, Yongjie and Zhao, Yong},
title = {Bidirectional feature selection with global and local structure preservation for small size samples},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2018.09.009},
doi = {10.1016/j.cogsys.2018.09.009},
journal = {Cogn. Syst. Res.},
month = dec,
pages = {756–764},
numpages = {9},
keywords = {Small size samples, Local structure preservation, Bidirectional feature selection}
}

@inproceedings{10.1145/3461002.3473066,
author = {Fortz, Sophie},
title = {LIFTS: learning featured transition systems},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473066},
doi = {10.1145/3461002.3473066},
abstract = {This PhD project aims to automatically learn transition systems capturing the behaviour of a whole family of software-based systems. Reasoning at the family level yields important economies of scale and quality improvements for a broad range of systems such as software product lines, adaptive and configurable systems. Yet, to fully benefit from the above advantages, a model of the system family's behaviour is necessary. Such a model is often prohibitively expensive to create manually due to the number of variants. For large long-lived systems with outdated specifications or for systems that continuously adapt, the modelling cost is even higher. Therefore, this PhD proposes to automate the learning of such models from existing artefacts. To advance research at a fundamental level, our learning target are Featured Transition Systems (FTS), an abstract formalism that can be used to provide a pivot semantics to a range of variability-aware state-based modelling languages. The main research questions addressed by this PhD project are: (1) Can we learn variability-aware models efficiently? (2) Can we learn FTS in a black-box fashion? (i.e., with access to execution logs but not to source code); (3) Can we learn FTS in a white/grey-box testing fashion? (i.e., with access to source code); and (4) How do the proposed techniques scale in practice?},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–6},
numpages = {6},
keywords = {variability mining, software product lines, model learning, featured transition systems, active automata learning},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@article{10.1016/j.neucom.2015.06.068,
author = {Kamyab, Shima and Eftekhari, Mahdi},
title = {Feature selection using multimodal optimization techniques},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {171},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2015.06.068},
doi = {10.1016/j.neucom.2015.06.068},
abstract = {This paper investigates the effect of using Multimodal Optimization (MO) techniques on solving the Feature Selection (FSel) problem. The FSel problem is a high-dimensional optimization problem in the nature and thus needs a solver with high exploration power. On the other hand, if alternative optimal solutions could be provided for a problem, the implementation phase may become more selective depending on the cost and limitations of domain of the problem. The high exploration power and solution conservation capability of MO methods make them able to find multiple suitable solutions in a single run. Therefore, MO methods can be considered as a powerful tool of finding suitable feature subsets for FSel problem. In this paper, we made a special study on the use of MO methods in the feature selection problem. The binary versions of some existing Evolutionary Algorithm (EA) based MO methods like Dynamic Fitness Sharing (DFS), local Best PSO variants and GA_SN_CM, are proposed and used for selection of suitable features from several benchmark datasets. The results obtained by the MO methods are compared to some well-known heuristic approaches for FSel problem from the literature. The obtained results and their statistical analyses indicate the effectiveness of MO methods in finding multiple accurate feature subsets compared to existing powerful methods.},
journal = {Neurocomput.},
month = jan,
pages = {586–597},
numpages = {12},
keywords = {Multimodal optimization, Feature selection, Evolutionary feature selection}
}

@article{10.1016/j.procs.2020.03.438,
author = {Almasoudy, Faezah Hamad and Al-Yaseen, Wathiq Laftah and Idrees, Ali Kadhum},
title = {Differential Evolution Wrapper Feature Selection for Intrusion Detection System},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.438},
doi = {10.1016/j.procs.2020.03.438},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1230–1239},
numpages = {10},
keywords = {Intrusion Detection System, Feature Selection, Differential Evolution, Extreme Learning Machine, NSL-KDD}
}

@inproceedings{10.1007/978-3-030-67731-2_37,
author = {Bayram, Firas and Garbarino, Davide and Barla, Annalisa},
title = {Predicting Tennis Match Outcomes with Network Analysis and Machine Learning},
year = {2021},
isbn = {978-3-030-67730-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-67731-2_37},
doi = {10.1007/978-3-030-67731-2_37},
abstract = {Singles tennis is one of the most popular individual sports in the world. Many researchers have embarked on a wide range of approaches to model a tennis match, using probabilistic modeling, or applying machine learning models to predict the outcome of matches. In this paper, we propose a novel approach based on network analysis to infer a surface-specific and time-varying score for professional tennis players and use it in addition to players’ statistics of previous matches to represent tennis match data. Using the resulting features, we apply advanced machine learning paradigms such as Multi-Output Regression and Learning Using Privileged Information, and compare the results with standard machine learning approaches. The models are trained and tested on more than 83,000 men’s singles tennis matches between the years 1991 and 2020. Evaluating the results shows the proposed methods provide more accurate predictions of tennis match outcome than classical approaches and outperform the existing methods in the literature and the current state-of-the-art models in tennis.},
booktitle = {SOFSEM 2021: Theory and Practice of Computer Science: 47th International Conference on Current Trends in Theory and Practice of Computer Science, SOFSEM 2021, Bolzano-Bozen, Italy, January 25–29, 2021, Proceedings},
pages = {505–518},
numpages = {14},
keywords = {Tennis outcome prediction, Multi-Output Regression, Learning Using Privileged Information, Network analysis, Machine learning},
location = {Bolzano-Bozen, Italy}
}

@inproceedings{10.1007/978-3-030-97549-4_24,
author = {Alihod\v{z}i\'{c}, Adis and Zvorni\v{c}anin, Enes and \v{C}unjalo, Fikret},
title = {A Comparison of Machine Learning Methods for Forecasting Dow Jones Stock Index},
year = {2021},
isbn = {978-3-030-97548-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-97549-4_24},
doi = {10.1007/978-3-030-97549-4_24},
abstract = {Stock market forecasting is a challenging and attractive topic for researchers and investors, helping them test their new methods and improve stock returns. Especially in the time of financial crisis, these methods gain popularity. The algorithmic solutions based on machine learning are used widely among investors, starting from amateur ones up&nbsp;to leading hedge funds, improving their investment strategies. This paper made an extensive analysis and comparison of several machine learning algorithms to predict the Dow Jones stock index movement. The input features for the algorithms will be some other financial indices, commodity prices and technical indicators. The algorithms such as decision tree, logistic regression, neural networks, support vector machine, random forest, and AdaBoost have exploited for comparison purposes. The data preprocessing step used a few normalization and data transformation techniques to investigate their influence on the predictions. In the end, we presented a few ways of tuning hyperparameters by metaheuristics such as genetic algorithm, differential evolution, and immunological algorithm.},
booktitle = {Large-Scale Scientific Computing: 13th International Conference, LSSC 2021, Sozopol, Bulgaria, June 7–11, 2021, Revised Selected Papers},
pages = {209–216},
numpages = {8},
keywords = {Data preprocessing, Metaheuristics, Classification, Financial time series, Machine learning},
location = {Sozopol, Bulgaria}
}

@inproceedings{10.1145/2815782.2815799,
author = {Schaefer, Ina and Seidl, Christoph and Cleophas, Loek and Watson, Bruce W.},
title = {SPLicing TABASCO: Custom-Tailored Software Product Line Variants from Taxonomy-Based Toolkits},
year = {2015},
isbn = {9781450336833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2815782.2815799},
doi = {10.1145/2815782.2815799},
abstract = {Taxonomy-Based Software Construction (TABASCO) applies extensive domain analyses to create conceptual hierarchies of algorithmic domains. Those are used as basis for the implementation of software toolkits. The monolithic structure of TABASCO-based toolkits restricts their adoption on resource-constrained or special-purpose devices. In this paper, we address this problem by applying Software Product Line (SPL) techniques to TABASCO-based toolkits: We use software taxonomies as input to creating a conceptual representation of variability as feature models of an SPL. We apply the variability realization mechanism delta modeling to transform realization artifacts, such as source code, to only contain elements for a particular selection of features. Our method is suitable for proactive, reactive and extractive SPL development so that it supports a seamless adoption and evolution of an SPL approach for TABASCO-based toolkits. We demonstrate the feasibility of the method with three case studies by proactively, reactively and extractively transforming TABASCO-based toolkits to SPLs, which allow derivation of variants with custom-tailored functionality.},
booktitle = {Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists},
articleno = {34},
numpages = {10},
keywords = {Taxonomy-Based Software Construction (TABASCO) toolkit, Software Product Line (SPL) adoption},
location = {Stellenbosch, South Africa},
series = {SAICSIT '15}
}

@article{10.1016/j.compbiomed.2021.104558,
author = {Piri, Jayashree and Mohapatra, Puspanjali},
title = {An analytical study of modified multi-objective Harris Hawk Optimizer towards medical data feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104558},
doi = {10.1016/j.compbiomed.2021.104558},
journal = {Comput. Biol. Med.},
month = aug,
numpages = {21},
keywords = {Medical data, Feature selection, Multi-objective optimization, Harris hawk algorithm, Pareto front}
}

@article{10.1007/s11045-019-00651-w,
author = {Mohanty, Monalisa and Biswal, Pradyut and Sabut, Sukanta},
title = {Machine learning approach to recognize ventricular arrhythmias using VMD based features},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {31},
number = {1},
issn = {0923-6082},
url = {https://doi.org/10.1007/s11045-019-00651-w},
doi = {10.1007/s11045-019-00651-w},
abstract = {The occurrence of life-threatening ventricular arrhythmias (VAs) such as Ventricular tachycardia (VT) and Ventricular fibrillation (VF) leads to sudden cardiac death which requires detection at an early stage. The main aim of this work is to develop an automated system using machine learning tool for accurate prediction of VAs that may reduce the mortality rate. In this paper, a novel method using variational mode decomposition (VMD) based features and C4.5 classifier for detection of ventricular arrhythmias is presented. The VMD model was used to decompose the electrocardiography (ECG) signals to extract useful informative features. The method was tested for ECG signals obtained from PhysioNet database. Two standard databases i.e. CUDB (Creighton University Ventricular Tachyarrhythmia Database) and VFDB (MIT-BIH Malignant Ventricular Ectopy Database) were considered for this work. A set of time–frequency features were extracted and ranked by the gain ratio attribute evaluation method. The ranked features are subjected to support vector machine (SVM) and C4.5 classifier for classification of normal, VT and VF classes. The best detection was obtained with sensitivity of 97.97%, specificity of 99.15%, and accuracy of 99.18% for C4.5 classifier with a 5&nbsp;s data analysis window. These results were better than SVM classifier result having an average accuracy of 86.87%. Hence, the proposed method demonstrates the efficiency in detecting the life-threatening VAs and can serve as an assistive tool to clinicians in the diagnosis process.},
journal = {Multidimensional Syst. Signal Process.},
month = jan,
pages = {49–71},
numpages = {23},
keywords = {Classification, Machine learning, Features, VMD, ECG, Ventricular arrhythmias}
}

@article{10.1007/s10846-021-01327-z,
author = {Roy, Priya and Chowdhury, Chandreyee},
title = {A Survey of Machine Learning Techniques for Indoor Localization and Navigation Systems},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {101},
number = {3},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-021-01327-z},
doi = {10.1007/s10846-021-01327-z},
abstract = {In the recent past, we have witnessed the adoption of different machine learning techniques for indoor positioning applications using WiFi, Bluetooth and other technologies. The techniques range from heuristically derived hand-crafted feature-based traditional machine learning algorithms, feature selection algorithms to the hierarchically self-evolving feature-based Deep Learning algorithms. The transient and chaotic nature of the WiFi/Bluetooth fingerprint data along with different signal sensitivity of different device configurations presents numerous challenges that influence the performance of the indoor localization system in the wild. This article is intended to offer a comprehensive state-of-the-art survey on machine learning techniques that have recently been adopted for localization purposes. Hence, we review the applicability of machine learning techniques in this domain along with basic localization principles, applications, and the underlying problems and challenges associated with the existing systems. We also articulate the recent advances and state-of-the-art machine learning techniques to visualize the possible future directions in the research field of indoor localization.},
journal = {J. Intell. Robotics Syst.},
month = mar,
numpages = {34},
keywords = {SLAM, Mobile robot, Deep learning, Extreme learning machine, Transfer learning, Supervised learning, Fingerprinting, Indoor localization}
}

@article{10.1007/s00521-020-05109-w,
author = {Jirak, Doreen and Biertimpel, David and Kerzel, Matthias and Wermter, Stefan},
title = {Solving visual object ambiguities when pointing: an unsupervised learning approach},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {7},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-05109-w},
doi = {10.1007/s00521-020-05109-w},
abstract = {Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Particularly, pointing gestures are necessary to dissolve ambiguities in a scene and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domestic domains, the development of gesture interfaces complementing human–robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the corresponding object have to be correctly recognized in real time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be captured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects and use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised learning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define the so-called ambiguity classes. Our implementation for the hand and object detection is independent of any markers or skeleton models; thus, it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities resulting from close object proximity.},
journal = {Neural Comput. Appl.},
month = apr,
pages = {2297–2319},
numpages = {23},
keywords = {Human–robot interaction, Grow-when-required networks, Object ambiguities, Pointing intention, Pointing gestures}
}

@article{10.1504/ijbidm.2020.106132,
author = {Sivasubramanian, Sivasankari and Jacob, Shomona Gracia},
title = {An automated ontology learning for benchmarking classifier models through gain-based relative-non-redundant feature selection: a case-study with erythemato-squamous disease},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {3},
issn = {1743-8195},
url = {https://doi.org/10.1504/ijbidm.2020.106132},
doi = {10.1504/ijbidm.2020.106132},
abstract = {Erythemato-squamous disease (ESD) is one of the complex diseases in the dermatology field, the diagnosis of which is challenging, due to common morphological features and often leads to inconsistent results. Besides, diagnosis has been done on the basis of inculcated visible symptoms pertinent with the expertise of the physician. Hence, ontology construction for prediction of erythemato-squamous disease through data mining techniques was believed to yield a clear representation of the relationships between the disease, symptoms and course of treatment. However, the classification accuracy required to be high in order to obtain a precise ontology. This required identifying the correct set of optimal features required to predict ESD. This paper proposes the Gain based Relative-Non-Redundant Attribute selection approach for diagnosis of ESD. This methodology yielded 98.1% classification accuracy with Adaboost algorithm that executed J48 as the base classifier. The feature selection approach revealed an optimal feature set comprising of 19 selected features.},
journal = {Int. J. Bus. Intell. Data Min.},
month = jan,
pages = {261–278},
numpages = {17},
keywords = {erythemato-squamous, gain base, web ontology language, classifier, feature selection, ontology}
}

@article{10.1016/j.cmpb.2020.105551,
author = {Hashem, Somaya and ElHefnawi, Mahmoud and Habashy, Shahira and El-Adawy, Mohamed and Esmat, Gamal and Elakel, Wafaa and Abdelazziz, Ashraf Omar and Nabeel, Mohamed Mahmoud and Abdelmaksoud, Ahmed Hosni and Elbaz, Tamer Mahmoud and Shousha, Hend Ibrahim},
title = {Machine Learning Prediction Models for Diagnosing Hepatocellular Carcinoma with HCV-related Chronic Liver Disease},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {196},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2020.105551},
doi = {10.1016/j.cmpb.2020.105551},
journal = {Comput. Methods Prog. Biomed.},
month = nov,
numpages = {8},
keywords = {Hepatocellular Carcinoma prediction, Machine Learning Algorithm, Decision Learning Tree, Serum marker, Hepatitis C virus}
}

@article{10.1016/j.patcog.2021.108079,
author = {Meenachi, L. and Ramakrishnan, S.},
title = {Metaheuristic Search Based Feature Selection Methods for Classification of Cancer},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {119},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2021.108079},
doi = {10.1016/j.patcog.2021.108079},
journal = {Pattern Recogn.},
month = nov,
numpages = {14},
keywords = {Optimal feature selection, Fuzzy Rough set, Tabu Search, Genetic Algorithm, Ant Colony Optimization}
}

@inproceedings{10.1007/978-3-030-27192-3_9,
author = {Ever, Yoney Kirsal and Sekeroglu, Boran and Dimililer, Kamil},
title = {Classification Analysis of Intrusion Detection on NSL-KDD Using Machine Learning Algorithms},
year = {2019},
isbn = {978-3-030-27191-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27192-3_9},
doi = {10.1007/978-3-030-27192-3_9},
abstract = {Since three decades, artificial intelligence has been evolved in order to outperform the tasks that human beings are not capable. These tasks can be any problem from our lives and one of these problems is computer networks-related tasks which huge number of privacy data is transferred even a second. Within last two decades, machine learning techniques with capabilities for prediction, optimisation, and as well as classification are developed for using to solve the real-life problems. In this paper, challenging and popular NSL-KDD dataset for intrusion detection is chosen for performed experiments, where classification and three benchmark machine learning techniques are used in order to determine optimum technique for classification domain. Experiments are performed by implementing 3-layered Back-propagation Neural Network, Support Vector Machine and Decision Tree. Thirty percent (30%) of instances of NSL-KDD Dataset were considered that causes 25193 of total instances in experiments. Each experiment is repeated for two times by using 60% and 70% of instances for training and the rest for testing. Increment of training patterns or instances caused little fluctuations on accuracy rates in Decision Tree and Back-propagation but it causes more effect in Support Vector Machine which is about 1% decrement in accuracy rate. It is seen from the performed experiments’ results that, increment or degradation of training ratio of instances in dataset does not affect the performance of the techniques directly.},
booktitle = {Mobile Web and Intelligent Information Systems: 16th International Conference, MobiWIS 2019, Istanbul, Turkey, August 26–28, 2019, Proceedings},
pages = {111–122},
numpages = {12},
keywords = {Decision Tree, SVM, BPNN, NSL-KDD dataset, Intrusion detection system, Machine learning, Classification},
location = {Istanbul, Turkey}
}

@article{10.1016/j.cmpb.2021.106248,
author = {Roldan-Vasco, Sebastian and Orozco-Duque, Andres and Suarez-Escudero, Juan Camilo and Orozco-Arroyave, Juan Rafael},
title = {Machine learning based analysis of speech dimensions in functional oropharyngeal dysphagia},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {208},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106248},
doi = {10.1016/j.cmpb.2021.106248},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {22},
keywords = {Feature extraction, Machine learning, Voice changes, Speech processing, Speech analysis, Dysphagia}
}

@article{10.1016/j.advengsoft.2021.103029,
author = {Nagy, Enik\H{o} and Lovas, R\'{o}bert and Pintye, Istv\'{a}n and Hajnal, \'{A}kos and Kacsuk, P\'{e}ter},
title = {Cloud-agnostic architectures for machine learning based on Apache Spark},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {159},
number = {C},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2021.103029},
doi = {10.1016/j.advengsoft.2021.103029},
journal = {Adv. Eng. Softw.},
month = sep,
numpages = {9},
keywords = {Spark, Stream processing, Distributed computing, Orchestration, Cloud computing, Machine learning, Artificial intelligence, Big data, Reference architectures}
}

@inproceedings{10.1145/1176887.1176897,
author = {Yoshimura, Kentaro and Ganesan, Dharmalingam and Muthig, Dirk},
title = {Defining a strategy to introduce a software product line using existing embedded systems},
year = {2006},
isbn = {1595935428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1176887.1176897},
doi = {10.1145/1176887.1176897},
abstract = {Engine Control Systems (ECS) for automobiles have numerous variants for many manufactures and different markets. To improve development efficiency, exploiting ECS commonalities and predicting their variability are mandatory. The concept of software product line engineering meets the business background of ECS. However, we should carefully investigate the expected technical, economical, and organizational effects of introducing this strategy into existing products.This paper explains an approach for assessing the potential of merging existing embedded software into a product line approach. The definition of an economically useful product line approach requires two things: analyzing return on investment (ROI) expectations of a product line and understanding the effort required for building reusable assets. We did a clone analysis to provide the basis for effort estimation for merge potential assessment of existing variants. We also report on a case study with ECS. We package the lessons learned and open issues that arose during the case study.},
booktitle = {Proceedings of the 6th ACM &amp; IEEE International Conference on Embedded Software},
pages = {63–72},
numpages = {10},
keywords = {software product line, software, reverse rngineering, engine control systems, economics, clone detection and classification},
location = {Seoul, Korea},
series = {EMSOFT '06}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {variability modeling, product line, model-based engineering, derivation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3231053.3231071,
author = {Mafarja, Majdi and Jarrar, Radi and Ahmad, Sobhi and Abusnaina, Ahmed A.},
title = {Feature selection using binary particle swarm optimization with time varying inertia weight strategies},
year = {2018},
isbn = {9781450364287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231053.3231071},
doi = {10.1145/3231053.3231071},
abstract = {In this paper, a feature selection approach that based on Binary Particle Swarm Optimization (PSO) with time varying inertia weight strategies is proposed. Feature Selection is an important preprocessing technique that aims to enhance the learning algorithm (e.g., classification) by improving its performance or reducing the processing time or both of them. Searching for the best feature set is a challenging problem in feature selection process, metaheuristics algorithms have proved a good performance in finding the (near) optimal solution for this problem. PSO algorithm is considered a primary Swarm Intelligence technique that showed a good performance in solving different optimization problems. A key component that highly affect the performance of PSO is the updating strategy of the inertia weight that controls the balance between exploration and exploitation. This paper studies the effect of different time varying inertia weight updating strategies on the performance of BPSO in tackling feature selection problem. To assess the performance of the proposed approach, 18 standard UCI datasets were used. The proposed approach is compared with well regarded metaheuristics based feature selection approaches, and the results proved the superiority of the proposed approach.},
booktitle = {Proceedings of the 2nd International Conference on Future Networks and Distributed Systems},
articleno = {18},
numpages = {9},
keywords = {optimization, inertia weight, feature selection, classification, binary particle swarm optimization, PSO},
location = {Amman, Jordan},
series = {ICFNDS '18}
}

@inproceedings{10.1007/978-3-030-26766-7_61,
author = {Niu, Ben and Yang, Xuesen and Wang, Hong},
title = {Feature Selection Using a Reinforcement-Behaved Brain Storm Optimization},
year = {2019},
isbn = {978-3-030-26765-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26766-7_61},
doi = {10.1007/978-3-030-26766-7_61},
abstract = {In this era of data explosion, feature selection has received sustained attention to remove the large amounts of meaningless data and improve the classification ac-curacy rate. In this paper, a feature selection method based on reinforcement-behaved strategy is proposed, which identifies the most important features by embedding the Brain Storm Optimization (BSO) algorithm into the classifier. The ideas of the BSO are mapped to feature subsets, and the importance of the feature is evaluated through some indicators, i.e. the validity of the feature migration. In the migration of each feature, the feature is updated to a new feature in the same position between the two generations. The feedback of each action is used as the basis for the ordering of feature importance. An updating strategy is presented to modify the actions based on the current state to improve the feature set. The effectiveness of the proposed algorithm has been demonstrated on six different binary classification datasets (e.g., biometrics, geography, etc.) in comparison to several embedded methods. The results show that our proposed method is superior in high performance, stability and low computing costs.},
booktitle = {Intelligent Computing Methodologies: 15th International Conference, ICIC 2019, Nanchang, China, August 3–6, 2019, Proceedings, Part III},
pages = {672–681},
numpages = {10},
keywords = {Feature selection, Brain storm optimization, Wrapper method, Binary classification},
location = {Nanchang, China}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Systematic literature review, Deep learning, Machine learning, Software process, Software development, Software engineering}
}

@inproceedings{10.1145/3469678.3469682,
author = {Xie, Minzhu and Liu, Fang},
title = {Overview of Machine Learning Methods for Genome-Wide Association Analysis},
year = {2021},
isbn = {9781450389297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469678.3469682},
doi = {10.1145/3469678.3469682},
abstract = {Genome-wide association studies (GWAS) is an effective way to reveal the pathogenic genes of complex diseases by analyzing the genotype information and related disease phenotype information on the SNP loci of the whole genome of a large number of living organisms. Machine learning (ML) is a method that allows computers to simulate human cognitive processes to solve problems. The advantage of using machine learning methods to carry out genome-wide association analysis research is that it does not require false anchor points or gene-gene interaction models in advance Instead of exhaustive search, computer algorithms that simulate human cognitive processes can learn from a large amount of data to discover the ability of nonlinear high-dimensional gene-gene interactions. In recent years, a large number of machine learning methods have been used in the study of genome-wide association analysis. This article will briefly introduct these methods.},
booktitle = {The Fifth International Conference on Biological Information and Biomedical Engineering},
articleno = {4},
numpages = {7},
keywords = {Machine learning methods, Genome-wide association, Gene-gene interaction},
location = {Hangzhou, China},
series = {BIBE2021}
}

@article{10.4018/IJCVIP.2017040105,
author = {Guru, D. S. and Kumar, N. Vinay and Suhil, Mahamad},
title = {Feature Selection of Interval Valued Data Through Interval K-Means Clustering},
year = {2017},
issue_date = {April 2017},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {2},
issn = {2155-6997},
url = {https://doi.org/10.4018/IJCVIP.2017040105},
doi = {10.4018/IJCVIP.2017040105},
abstract = {This paper introduces a novel feature selection model for supervised interval valued data based on interval K-Means clustering. The proposed model explores two kinds of feature selection through feature clustering viz., class independent feature selection and class dependent feature selection. The former one clusters the features spread across all the samples belonging to all the classes, whereas the latter one clusters the features spread across only the samples belonging to the respective classes. Both feature selection models are demonstrated to explore the generosity of clustering in selecting the interval valued features. For clustering, the kernel of the K-means clustering has been altered to operate on interval valued data. For experimentation purpose four standard benchmarking datasets and three symbolic classifiers have been used. To corroborate the effectiveness of the proposed model, a comparative analysis against the state-of-the-art models is given and results show the superiority of the proposed model.},
journal = {Int. J. Comput. Vis. Image Process.},
month = apr,
pages = {64–80},
numpages = {17},
keywords = {Symbolic Similarity Measure, Symbolic Classification, Interval K-Means Clustering, Interval Data, Feature Selection}
}

@article{10.1016/j.asoc.2020.106092,
author = {Agrawal, R.K. and Kaur, Baljeet and Sharma, Surbhi},
title = {Quantum based Whale Optimization Algorithm for wrapper feature selection},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {89},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106092},
doi = {10.1016/j.asoc.2020.106092},
journal = {Appl. Soft Comput.},
month = apr,
numpages = {14},
keywords = {Feature selection, Swarm based techniques, Evolutionary techniques, Bio-inspired technique, Whale Optimization Algorithm, Quantum}
}

@article{10.1007/s10115-015-0841-8,
author = {Alalga, Abdelouahid and Benabdeslem, Khalid and Taleb, Nora},
title = {Soft-constrained Laplacian score for semi-supervised multi-label feature selection},
year = {2016},
issue_date = {April     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {47},
number = {1},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-015-0841-8},
doi = {10.1007/s10115-015-0841-8},
abstract = {Feature selection, semi-supervised learning and multi-label classification are different challenges for machine learning and data mining communities. While other works have addressed each of these problems separately, in this paper we show how they can be addressed together. We propose a unified framework for semi-supervised multi-label feature selection, based on Laplacian score. In particular, we show how to constrain the function of this score, when data are partially labeled and each instance is associated with a set of labels. We transform the labeled part of data into soft constraints and show how to integrate them in a measure of feature relevance, according to the available labels. Experiments on benchmark data sets are provided for validating the proposed approach and comparing it with some other state-of-the-art feature selection methods in a multi-label context.},
journal = {Knowl. Inf. Syst.},
month = apr,
pages = {75–98},
numpages = {24},
keywords = {Semi-supervised context, Multi-label learning, Feature selection, Constraints}
}

@article{10.1016/j.cosrev.2021.100414,
author = {Micol Policarpo, Lucas and da Silveira, Di\'{o}rgenes Eug\^{e}nio and da Rosa Righi, Rodrigo and Antunes Stoffel, Rodolfo and da Costa, Cristiano Andr\'{e} and Vict\'{o}ria Barbosa, Jorge Luis and Scorsatto, Rodrigo and Arcot, Tanuj},
title = {Machine learning through the lens of e-commerce initiatives: An up-to-date systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {41},
number = {C},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2021.100414},
doi = {10.1016/j.cosrev.2021.100414},
journal = {Comput. Sci. Rev.},
month = aug,
numpages = {17},
keywords = {Conversion rate, Machine learning, User behavior, E-commerce}
}

@article{10.1016/j.compbiomed.2021.104805,
author = {Beheshti, Iman and Sone, Daichi and Maikusa, Norihide and Kimura, Yukio and Shigemoto, Yoko and Sato, Noriko and Matsuda, Hiroshi},
title = {Accurate lateralization and classification of MRI-negative 18F-FDG-PET-positive temporal lobe epilepsy using double inversion recovery and machine-learning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104805},
doi = {10.1016/j.compbiomed.2021.104805},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {11},
keywords = {MRI-negative focal epilepsy, Temporal lobe epilepsy, Support vector machine, Prediction, Double inversion recovery, Biomarker}
}

@inproceedings{10.1145/3449726.3461415,
author = {Xue, Bing and Zhang, Mengjie},
title = {Evolutionary computation for feature selection and feature construction},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3461415},
doi = {10.1145/3449726.3461415},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1141–1168},
numpages = {28},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1007/s11042-020-10067-5,
author = {\"{O}zer, \c{C}a\u{g}da\c{s} and \c{C}evik, Taner and G\"{u}rhanl\i{}, Ahmet},
title = {A machine learning-based framework for predicting game server load},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {80},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-10067-5},
doi = {10.1007/s11042-020-10067-5},
abstract = {Server load prediction can be utilized for load-balancing and load-sharing in distributed systems. The use of machine learning (ML) algorithms for load estimation in distributed system applications can increase the availability and performance of servers. Hence, a number of machine learning algorithms have been applied thus far for server load estimation. This study focuses on increasing the performance of game servers by accurately predicting the workload of game servers in short, medium and long term prediction situations. While doing this, various machine learning techniques have been applied and the algorithms that give the best results are presented. In terms of implementation, companies using their servers and data centers can try to increase their level of satisfaction by using these algorithms. A prediction model is developed and the estimation performances of a number of fundamental ML methods i.e., Na\"{\i}ve Bayes (NB), Generalized Linear Model (GLM), Logistic Regression (LR), Decision Tree (DT), Random Forest (RF), Gradient Boosted Trees (GBT), Support Vector Machine (SVM), Fast Large Margin (FLM), Convolutional Neural Network CNN are analyzed. The data used during the training stage is obtained by listening to the TCP/IP packet traffic and the real-data is extracted by performing an extensive analysis of the total transferred-data that includes also the payload. In the analysis phase, the goodput is considered in order to reveal exact resource requirements. Comprehensive simulations are performed under various conditions for high accuracy performance analysis. Experimental results indicate that the proposed ML-based prediction shows promising performance in terms of load prediction when compared to the common approaches present in the literature.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {9527–9546},
numpages = {20},
keywords = {Game server, Load prediction, Machine learning}
}

@article{10.1016/j.cmpb.2019.104992,
author = {Abdar, Moloud and Ksi\k{a}\.{z}ek, Wojciech and Acharya, U Rajendra and Tan, Ru-San and Makarenkov, Vladimir and P\l{}awiak, Pawe\l{}},
title = {A new machine learning technique for an accurate diagnosis of coronary artery disease},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {179},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.104992},
doi = {10.1016/j.cmpb.2019.104992},
journal = {Comput. Methods Prog. Biomed.},
month = oct,
numpages = {11},
keywords = {Classification, Feature selection, Particle swarm optimization, Genetic algorithm, Normalization, Machine learning, Coronary artery disease (CAD)}
}

@article{10.4018/IJIIT.289971,
author = {Yaokumah, Winfred and Clottey, Richard Nunoo and Appati, Justice Kwame},
title = {Modelling and Evaluation of Network Intrusion Detection Systems Using Machine Learning Techniques},
year = {2021},
issue_date = {Oct 2021},
publisher = {IGI Global},
address = {USA},
volume = {17},
number = {4},
issn = {1548-3657},
url = {https://doi.org/10.4018/IJIIT.289971},
doi = {10.4018/IJIIT.289971},
abstract = {This study aims at modelling and evaluating the performance of machine learning techniques on a recent network intrusion dataset. Five machine learning algorithms, which include k-nearest neighbour (KNN), support vector machines (SVM), voting ensemble, random forest, and XGBoost, have been utilized in the development of the network intrusion detection models. The proposed models are tested using the UNSW_NB15 dataset. Three different K values are used for model with KNN algorithm and two different kernels are utilized in the development of the model with SVM. The best detection accuracy of the model developed with KNN was 84.9% with a K value of 9; the SVM model with the best accuracy is developed with the Gaussian kernel and obtained an accuracy of 83%, and the Voting Ensemble achieved 83.4% accuracy. Random forest model achieved accuracies of 90.2% and 70.8% for binary classification and multiclass classification respectively. Finally, XGBoost model also achieves accuracies of 85% and 51.77% for binary and multiclass classification respectively.},
journal = {Int. J. Intell. Inf. Technol.},
month = oct,
pages = {1–19},
numpages = {19},
keywords = {Cyber-Attacks, K-Nearest Neighbour, Machine Learning Algorithms, Modelling, Network Intrusion Detection, Random Forest, Support Vector Machines, UNSW-NB15 Dataset, Voting Ensemble, XGBoost}
}

@article{10.1016/j.eswa.2018.01.054,
author = {Sanchez, A. and Soguero-Ruiz, C. and Mora-Jimnez, I. and Rivas-Flores, F.J. and Lehmann, D.J. and Rubio-Snchez, M.},
title = {Scaled radial axes for interactive visual feature selection},
year = {2018},
issue_date = {June 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {100},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.01.054},
doi = {10.1016/j.eswa.2018.01.054},
abstract = {We propose a new radial axes method to help visual backward feature selection.Experts can incorporate domain knowledge to analyze classes through LMNN, NCA, etc.The method reduces clutter in visualization compared to other radial axes plots.We conducted different experiments with several public data sets.We present a case study using high dimensional data of chronic medical conditions. In statistics, machine learning, and related fields, feature selection is the process of choosing a smaller subset of features to work with. This is an important topic since selecting a subset of features can help analysts to interpret models and data, and to decrease computational runtimes. While many techniques are purely automatic, the data visualization community has produced a number of interactive approaches where users can make decisions taking into account their domain knowledge. In this paper we propose a new visualization technique based on radial axes that allows analysts to perform feature selection effectively, in contrast to previous radial axes methods. This is achieved by employing alternative scaled axes that provide insight regarding the features that have a smaller contribution to the visualizations. Therefore, analysts can use the technique to carry out interactive backwards feature elimination, by discarding the least relevant features according to the information on the plots and their expertise. Our approach can be coupled with any linear dimensionality reduction method, and can be used when performing analyses of cluster structure, correlations, class separability, etc. Specifically, in this paper we focus on combining the proposed technique with methods designed for classification. Lastly, we illustrate the effectiveness of our proposal through a case study analyzing high-dimensional medical chronic conditions data. In particular, clinicians have used the technique for determining the most important features that discriminate between patients with diabetes and high blood pressure.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {182–196},
numpages = {15},
keywords = {Visual analytics, Medical chronic conditions, Interactive feature selection, High-dimensional data visualization, Exploratory data analysis}
}

@article{10.1016/j.cose.2021.102414,
author = {Alhogail, Areej and Alsabih, Afrah},
title = {Applying machine learning and natural language processing to detect phishing email},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {110},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102414},
doi = {10.1016/j.cose.2021.102414},
journal = {Comput. Secur.},
month = nov,
numpages = {11},
keywords = {Information security, Graph conventional network, Natural language processinyg, Deep learning, Phishing Email detection}
}

@article{10.1016/j.patrec.2021.07.004,
author = {Briguglio, William and Moghaddam, Parisa and Yousef, Waleed A. and Traor\'{e}, Issa and Mamun, Mohammad},
title = {Machine learning in precision medicine to preserve privacy via encryption},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.07.004},
doi = {10.1016/j.patrec.2021.07.004},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {148–154},
numpages = {7},
keywords = {Privacy, Precision medicine, Homomorphic encryption, Encryption, Machine learning}
}

@inproceedings{10.1007/978-3-030-79457-6_42,
author = {Quang, Do Nguyet and Selamat, Ali and Krejcar, Ondrej},
title = {Recent Research on Phishing Detection Through Machine Learning Algorithm},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_42},
doi = {10.1007/978-3-030-79457-6_42},
abstract = {The rapid growth of emerging technologies, smart devices, 5G communication, etc. have contributed to the accumulation of data, hence introducing the big data era. Big data imposes a variety of challenges associated with machine learning, especially in phishing detection. Therefore, this paper aims to provide an analysis and summary of current research in phishing detection through machine learning for big data. To achieve this goal, this study adopted a systematic literature review (SLR) technique and critically analyzed a total of 30 papers from various journals and conference proceedings. These papers were selected from previous studies in five different databases on content published between 2018 and January 2021. The results obtained from this study reveal a limited number of research works that comprehensively reviewed the feasibility of applying both machine learning and big data technologies in the context of phishing detection.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {495–508},
numpages = {14},
keywords = {Big data, Machine learning (ML), Phishing detection, Cybersecurity},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1007/s00500-020-05226-7,
author = {Khuat, Thanh Tung and Ruta, Dymitr and Gabrys, Bogdan},
title = {Hyperbox-based machine learning algorithms: a comprehensive survey},
year = {2021},
issue_date = {Jan 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05226-7},
doi = {10.1007/s00500-020-05226-7},
abstract = {With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min–max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.},
journal = {Soft Comput.},
month = jan,
pages = {1325–1363},
numpages = {39},
keywords = {Online learning, Clustering, Data classification, Hybrid classifiers, Fuzzy min–max neural network, Membership function, Hyperboxes}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-28377-3_28,
author = {Gwetu, Mandlenkosi Victor and Tapamo, Jules-Raymond and Viriri, Serestina},
title = {Exploring the Impact of Purity Gap Gain on the Efficiency and Effectiveness of Random Forest Feature Selection},
year = {2019},
isbn = {978-3-030-28376-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-28377-3_28},
doi = {10.1007/978-3-030-28377-3_28},
abstract = {The Random Forest (RF) classifier has the capacity to facilitate both wrapper and embedded feature selection through the Mean Decrease Accuracy (MDA) and Mean Decrease Impurity (MDI) methods, respectively. MDI is known to be biased towards predictor variables with multiple values whilst MDA is stable in this regard. As such, MDA is the predominantly preferred option for RF-based feature selection, despite its higher computational overhead in comparison to MDI. This research seeks to simultaneously reduce the computational overhead and improve the effectiveness of RF feature selection. We propose two improvements to the MDI method to overcome its shortcomings. The first is using our proposed Purity Gap Gain (PGG) measure which has an emphasis on computational efficiency, as an alternative to the Gini Importance (GI) metric. The second is incorporating a Relative Mean Decrease Impurity (RMDI) score, which aims to offset the bias towards multi-valued predictor variables through random feature value permutations. Experiments are conducted on UCI datasets to establish the impact of PGG and RMDI on RF performance.},
booktitle = {Computational Collective Intelligence: 11th International Conference, ICCCI 2019, Hendaye, France, September 4–6, 2019, Proceedings, Part I},
pages = {340–352},
numpages = {13},
keywords = {Relative Mean Decrease Impurity, Purity Gap Gain, Mean Decrease Impurity, Mean Decrease Accuracy, Random Forest},
location = {Hendaye, France}
}

@inproceedings{10.1007/978-3-030-91100-3_15,
author = {\'{A}lvarez-Chaves, Hugo and Barrero, David F. and Cobos, Mario and R-Moreno, Maria D.},
title = {Patients Forecasting in Emergency Services by Using Machine Learning and Exogenous Variables},
year = {2021},
isbn = {978-3-030-91099-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91100-3_15},
doi = {10.1007/978-3-030-91100-3_15},
abstract = {Emergency Departments (ED) in hospitals around the world are a critical service in medical care of patients. Being able to predict the number of patients every day, week or month could be of great help to the healthcare system, and especially in the current situation.In this paper we present some results in forecasting the admissions, the inpatients and the discharges series in EDs by using Machine Learning algorithms. We have considered different time aggregations (specifically: eight hours, twelve hours, one day and the official shifts of the workers) and exogenous variables (in particular, the Spanish public holiday calendar, the academic calendar, the phase of the lunar cycle and national football matches).Results show that the best performance is obtained for the admissions series using eight hours aggregations and the biggest improvement is with the daily aggregation in the admission series. The academic calendar and public holidays were the most selected variables.},
booktitle = {Artificial Intelligence XXXVIII: 41st SGAI International Conference on Artificial Intelligence, AI 2021, Cambridge, UK, December 14–16, 2021, Proceedings},
pages = {167–180},
numpages = {14},
keywords = {Machine learning, ED forecasting, e-Health},
location = {Cambridge, United Kingdom}
}

@article{10.1007/s11390-020-0323-7,
author = {Mhawish, Mohammad Y. and Gupta, Manjari},
title = {Predicting Code Smells and Analysis of Predictions: Using Machine Learning Techniques and Software Metrics},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0323-7},
doi = {10.1007/s11390-020-0323-7},
abstract = {Code smell detection is essential to improve software quality, enhancing software maintainability, and decrease the risk of faults and failures in the software system. In this paper, we proposed a code smell prediction approach based on machine learning techniques and software metrics. The local interpretable model-agnostic explanations (LIME) algorithm was further used to explain the machine learning model’s predictions and interpretability. The datasets obtained from Fontana et al. were reformed and used to build binary-label and multi-label datasets. The results of 10-fold cross-validation show that the performance of tree-based algorithms (mainly Random Forest) is higher compared with kernel-based and network-based algorithms. The genetic algorithm based feature selection methods enhance the accuracy of these machine learning algorithms by selecting the most relevant features in each dataset. Moreover, the parameter optimization techniques based on the grid search algorithm significantly enhance the accuracy of all these algorithms. Finally, machine learning techniques have high potential in predicting the code smells, which contribute to detect these smells and enhance the software’s quality.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1428–1445},
numpages = {18},
keywords = {parameter optimization, prediction explanation, feature selection, code smell detection, code smell}
}

@inproceedings{10.1145/3307630.3342704,
author = {Ca\~{n}ete, Angel},
title = {Energy Efficient Assignment and Deployment of Tasks in Structurally Variable Infrastructures},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342704},
doi = {10.1145/3307630.3342704},
abstract = {The importance of cyber-physical systems is growing very fast, being part of the Internet of Things vision. These devices generate data that could collapse the network and can not be assumed by the cloud. New technologies like Mobile Cloud Computing and Mobile Edge Computing are taking importance as solution for this issue. The idea is offloading some tasks to devices situated closer to the user device, reducing network congestion and improving applications performance (e.g., in terms of latency and energy). However, the variability of the target devices' features and processing tasks' requirements is very diverse, being difficult to decide which device is more adequate to deploy and run such processing tasks. Once decided, task offloading used to be done manually. Then, it is necessary a method to automatize the task assignation and deployment process. In this thesis we propose to model the structural variability of the deployment infrastructure and applications using feature models, on the basis of a SPL engineering process. Combining SPL methodology with Edge Computing, the deployment of applications is addressed as the derivation of a product. The data of the valid configurations is used by a task assignment framework, which determines the optimal tasks offloading solution in different network devices, and the resources of them that should be assigned to each task/user. Our solution provides the most energy and latency efficient deployment solution, accomplishing the QoS requirements of the application in the process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {222–229},
numpages = {8},
keywords = {software product line, optimisation, mobile edge computing, mobile cloud computing, latency, energy efficiency},
location = {Paris, France},
series = {SPLC '19}
}

@book{10.5555/2728712,
author = {Stanczyk, Urszula and Jain, Lakhmi C.},
title = {Feature Selection for Data and Pattern Recognition},
year = {2014},
isbn = {3662456192},
publisher = {Springer Publishing Company, Incorporated},
abstract = {This research book provides the reader with a selection of high-quality texts dedicated to current progress, new developments and research trends in feature selection for data and pattern recognition. Even though it has been the subject of interest for some time, feature selection remains one of actively pursued avenues of investigations due to its importance and bearing upon other problems and tasks. This volume points to a number of advances topically subdivided into four parts: estimation of importance of characteristic features, their relevance, dependencies, weighting and ranking; rough set approach to attribute reduction with focus on relative reducts; construction of rules and their evaluation; and data- and domain-oriented methodologies.}
}

@article{10.1007/s00530-014-0390-0,
author = {Song, Xiaonan and Zhang, Jianguang and Han, Yahong and Jiang, Jianmin},
title = {Semi-supervised feature selection via hierarchical regression for web image classification},
year = {2016},
issue_date = {February  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {1},
issn = {0942-4962},
url = {https://doi.org/10.1007/s00530-014-0390-0},
doi = {10.1007/s00530-014-0390-0},
abstract = {Feature selection is an important step for large-scale image data analysis, which has been proved to be difficult due to large size in both dimensions and samples. Feature selection firstly eliminates redundant and irrelevant features and then chooses a subset of features that performs as efficient as the complete set. Generally, supervised feature selection yields better performance than unsupervised feature selection because of the utilization of labeled information. However, labeled data samples are always expensive to obtain, which constraints the performance of supervised feature selection, especially for the large web image datasets. In this paper, we propose a semi-supervised feature selection algorithm that is based on a hierarchical regression model. Our contribution can be highlighted as: (1) Our algorithm utilizes a statistical approach to exploit both labeled and unlabeled data, which preserves the manifold structure of each feature type. (2) The predicted label matrix of the training data and the feature selection matrix are learned simultaneously, making the two aspects mutually benefited. Extensive experiments are performed on three large-scale image datasets. Experimental results demonstrate the better performance of our algorithm, compared with the state-of-the-art algorithms.},
journal = {Multimedia Syst.},
month = feb,
pages = {41–49},
numpages = {9},
keywords = {Semi-supervised learning, Multi-class classification, Feature selection}
}

@article{10.1016/j.eswa.2017.11.053,
author = {Tang, Chang and Zhu, Xinzhong and Chen, Jiajia and Wang, Pichao and Liu, Xinwang and Tian, Jie},
title = {Robust graph regularized unsupervised feature selection},
year = {2018},
issue_date = {April 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {96},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.11.053},
doi = {10.1016/j.eswa.2017.11.053},
abstract = {We propose a robust graph regularized unsupervised feature selection model.We introduce an l1-norm based graph to preserve the local structure of data.Feature reconstruction term is regularized by l2, 1-norm for robustness to outliers.An efficient solver is developed to solve the proposed optimization problem.Experiments confirmed our method outperforms other state-of-the-art methods. Recent research indicates the critical importance of preserving local geometric structure of data in unsupervised feature selection (UFS), and the well studied graph Laplacian is usually deployed to capture this property. By using a squared l2-norm, we observe that conventional graph Laplacian is sensitive to noisy data, leading to unsatisfying data processing performance. To address this issue, we propose a unified UFS framework via feature self-representation and robust graph regularization, with the aim at reducing the sensitivity to outliers from the following two aspects: i) an l2, 1-norm is used to characterize the feature representation residual matrix; and ii) an l1-norm based graph Laplacian regularization term is adopted to preserve the local geometric structure of data. By this way, the proposed framework is able to reduce the effect of noisy data on feature selection. Furthermore, the proposed l1-norm based graph Laplacian is readily extendible, which can be easily integrated into other UFS methods and machine learning tasks with local geometrical structure of data being preserved. As demonstrated on ten challenging benchmark data sets, our algorithm significantly and consistently outperforms state-of-the-art UFS methods in the literature, suggesting the effectiveness of the proposed UFS framework.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {64–76},
numpages = {13},
keywords = {Unsupervised feature selection, Similarity preservation, Local geometric structure, Graph regularization, 99-00, 00-01}
}

@inproceedings{10.1007/978-3-030-82472-3_14,
author = {Chakrabarti, Arnab and Das, Abhijeet and Cochez, Michael and Quix, Christoph},
title = {Unsupervised Feature Selection for Efficient Exploration of High Dimensional Data},
year = {2021},
isbn = {978-3-030-82471-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-82472-3_14},
doi = {10.1007/978-3-030-82472-3_14},
abstract = {The exponential growth in the ability to generate, capture, and store high dimensional data has driven sophisticated machine learning applications. However, high dimensionality often poses a challenge for analysts to effectively identify and extract relevant features from datasets. Though many feature selection methods have shown good results in supervised learning, the major challenge lies in the area of unsupervised feature selection. For example, in the domain of data visualization, high-dimensional data is difficult to visualize and interpret due to the limitations of the screen, resulting in visual clutter. Visualizations are more interpretable when visualized in a low dimensional feature space. To mitigate these challenges, we present an approach to perform unsupervised feature clustering and selection using our novel graph clustering algorithm based on Clique-Cover Theory. We implemented our approach in an interactive data exploration tool which facilitates the exploration of relationships between features and generates interpretable visualizations.},
booktitle = {Advances in Databases and Information Systems: 25th European Conference, ADBIS 2021, Tartu, Estonia, August 24–26, 2021, Proceedings},
pages = {183–197},
numpages = {15},
location = {Tartu, Estonia}
}

@article{10.1504/ijdmb.2021.116881,
author = {Dagasso, Gabrielle and Yan, Yan and Wang, Lipu and Li, Longhai and Kutcher, Randy and Zhang, Wentao and Jin, Lingling},
title = {Leveraging machine learning to advance genome-wide association studies},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {25},
number = {1–2},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2021.116881},
doi = {10.1504/ijdmb.2021.116881},
abstract = {Genome-Wide Association Studies (GWAS) has demonstrated its power in discovering genetic variations to particular traits related to agronomically important features in crops. The typical output of a GWAS program includes a series of Single Nucleotide Polymorphisms (SNPs) and their significance. Currently, there is no standard way to compare results across different programs or to select the most 'significant' results uniformly and consistently. To obtain a comprehensive and accurate set of SNPs associated with a trait of interest, we present a novel automated pipeline that leverages machine learning for GWAS discoveries. The pipeline first performs population structure analysis, then executes multiple GWAS software and combines their results into a single SNP set. After that, it selects SNPs from the set with high individual and/or joint effects with the Least Absolute Shrinkage and Selection Operator analysis. Finally, the predictivity of the model is assessed using cross-validation.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {17–36},
numpages = {19},
keywords = {fusarium head blight, LASSO, cross-validation, population structure analysis, machine learning, genome-wide association studies}
}

@article{10.1016/j.eswa.2020.113237,
author = {Niu, Tong and Wang, Jianzhou and Lu, Haiyan and Yang, Wendong and Du, Pei},
title = {Developing a deep learning framework with two-stage feature selection for multivariate financial time series forecasting},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {148},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113237},
doi = {10.1016/j.eswa.2020.113237},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {17},
keywords = {Multi-objective optimization, Feature selection, Forecasting, Multivariate financial time series, Deep learning}
}

@inproceedings{10.1007/978-3-030-31624-2_8,
author = {Xu, Jin and Zhang, Chengzhi and Ma, Shutian},
title = {Ensemble System for Identification of Cited Text Spans: Based on Two Steps of Feature Selection},
year = {2019},
isbn = {978-3-030-31623-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31624-2_8},
doi = {10.1007/978-3-030-31624-2_8},
abstract = {CL-SciSumm Shared Task proposed a novel approach which is to generate scientific summary based on cited text spans (CTS) in target paper. This mechanism requires identifying CTS from reference paper according to citation sentence (citance) firstly. Therefore, CTS identification has then arisen the attention of many scholars since identified sentences will finally be aggregated for summary generation. Prior studies viewed this task as a text classification problem and feature selection is one key step for modeling the linkage between CTS and citance. Since most studies have paved the work by building features arbitrarily and applying them directly to model training. There is a lack of investigation to evaluate the effectiveness of features. Performance variation caused by different classifiers are barely taken into consideration as well. To further improve the performance of CTS identification, this paper builds an ensemble system based on two steps of feature selection. In the first step, we construct a set of features and do correlation analysis to select those which are higher-correlated with CTS. The second step is responsible for assigning several basic classifiers (SVM, Decision Tree and Logistic Regression) with their best performing feature sets. Experimental results demonstrate that our proposed systems can surpass the previous best performing one.},
booktitle = {Information Retrieval: 25th China Conference, CCIR 2019, Fuzhou, China, September 20–22, 2019, Proceedings},
pages = {95–107},
numpages = {13},
keywords = {Ensemble system, Text classification, Negative sampling, Feature selection, Cited text spans},
location = {Fuzhou, China}
}

@article{10.1016/j.neunet.2019.04.011,
author = {Ma, Jianghong and Chow, Tommy W.S.},
title = {Label-specific feature selection and two-level label recovery for multi-label classification with missing labels},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {118},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.04.011},
doi = {10.1016/j.neunet.2019.04.011},
journal = {Neural Netw.},
month = oct,
pages = {110–126},
numpages = {17},
keywords = {Missing label, Label-specific feature selection, Two-level semantic correlations, Label recovery, Multi-label learning}
}

@article{10.1016/j.patcog.2017.11.027,
author = {Hafiz, Faizal and Swain, Akshya and Patel, Nitish and Naik, Chirag},
title = {A two-dimensional (2-D) learning framework for Particle Swarm based feature selection},
year = {2018},
issue_date = {April 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {76},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.11.027},
doi = {10.1016/j.patcog.2017.11.027},
abstract = {A new learning method is proposed for PSO based feature selection.Subset cardinality is used as the extra learning dimension to improve the search.Position update process through inclusive learning on cardinality and feature.Key elements of canonical PSO are retained despite the extra learning dimension. This paper proposes a new generalized two dimensional learning approach for particle swarm based feature selection. The core idea of the proposed approach is to include the information about the subset cardinality into the learning framework by extending the dimension of the velocity. The 2D-learning framework retains all the key features of the original PSO, despite the extra learning dimension. Most of the popular variants of PSO can easily be adapted into this 2D learning framework for feature selection problems. The efficacy of the proposed learning approach has been evaluated considering several benchmark data and two induction algorithms: NaiveBayes and k-Nearest Neighbor. The results of the comparative investigation including the time-complexity analysis with GA, ACO and five other PSO variants illustrate that the proposed 2D learning approach gives feature subset with relatively smaller cardinality and better classification performance with shorter run times.},
journal = {Pattern Recogn.},
month = apr,
pages = {416–433},
numpages = {18},
keywords = {Particle Swarm Optimization, Machine learning, Feature selection, Dimensionality reduction, Classification}
}

@article{10.1007/s10916-016-0600-8,
author = {Somu, Nivethitha and Raman, M. R. and Kirthivasan, Kannan and Sriram, V. S.},
title = {Hypergraph Based Feature Selection Technique for Medical Diagnosis},
year = {2016},
issue_date = {November  2016},
publisher = {Plenum Press},
address = {USA},
volume = {40},
number = {11},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-016-0600-8},
doi = {10.1007/s10916-016-0600-8},
abstract = {The impact of internet and information systems across various domains have resulted in substantial generation of multidimensional datasets. The use of data mining and knowledge discovery techniques to extract the original information contained in the multidimensional datasets play a significant role in the exploitation of complete benefit provided by them. The presence of large number of features in the high dimensional datasets incurs high computational cost in terms of computing power and time. Hence, feature selection technique has been commonly used to build robust machine learning models to select a subset of relevant features which projects the maximal information content of the original dataset. In this paper, a novel Rough Set based K --- Helly feature selection technique (RSKHT) which hybridize Rough Set Theory (RST) and K --- Helly property of hypergraph representation had been designed to identify the optimal feature subset or reduct for medical diagnostic applications. Experiments carried out using the medical datasets from the UCI repository proves the dominance of the RSKHT over other feature selection techniques with respect to the reduct size, classification accuracy and time complexity. The performance of the RSKHT had been validated using WEKA tool, which shows that RSKHT had been computationally attractive and flexible over massive datasets.},
journal = {J. Med. Syst.},
month = nov,
pages = {1–16},
numpages = {16},
keywords = {Rough set theory (RST), Medical diagnosis, K --- Helly property, Hypergraph, High dimensional datasets, Feature selection}
}

@inproceedings{10.1007/978-3-030-45778-5_4,
author = {Das, Anurag and Ajila, Samuel A. and Lung, Chung-Horng},
title = {A Comprehensive Analysis of Accuracies of Machine Learning Algorithms for Network Intrusion Detection},
year = {2019},
isbn = {978-3-030-45777-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45778-5_4},
doi = {10.1007/978-3-030-45778-5_4},
abstract = {Intrusion and anomaly detection are particularly important in the time of increased vulnerability in computer networks and communication. Therefore, this research aims to detect network intrusion with the highest accuracy and fastest time. To achieve this, nine supervised machine learning algorithms were first applied to the UNSW-NB15 dataset for network anomaly detection. In addition, different attacks are investigated with different mitigation techniques that help determine the types of attacks. Once detection was done, the feature set was reduced according to existing research work to increase the speed of the model without compromising accuracy. Furthermore, seven supervised machine learning algorithms were also applied to the newly released BoT-IoT dataset with around three million network flows. The results show that the Random Forest is the best in terms of accuracy (97.9121%) and Na\"{\i}ve Bayes the fastest algorithm with 0.69&nbsp;s for the UNSW-NB15 dataset. C4.5 is the most accurate one (87.66%), with all the features considered to identify the types of anomalies. For BoT-IoT, six of the seven algorithms have a close to 100% detection rate, except Na\"{\i}ve Bayes.},
booktitle = {Machine Learning for Networking: Second IFIP TC 6 International Conference, MLN 2019, Paris, France, December 3–5, 2019, Revised Selected Papers},
pages = {40–57},
numpages = {18},
keywords = {Network intrusion detection, Supervised learning, UNSW-NB15 dataset, BoT-IoT dataset},
location = {Paris, France}
}

@inproceedings{10.1145/3479162.3479185,
author = {Choti, Chaiyos and Hnoohom, Narit and Tritilanunt, Suratose and Yuenyong, Sumeth},
title = {Prediction of Intrusion Detection in Voice over Internet Protocol System using Machine Learning},
year = {2021},
isbn = {9781450390071},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3479162.3479185},
doi = {10.1145/3479162.3479185},
abstract = {Currently, the threats of the internet and computer network have still at a high level. The firewall installation does not enough to detect intrusion of any attempts into the network. This research developed an intrusion detection that corresponded to the attack behavior by inspecting the components of SIP messages from the network traffic. The rules determination applied to detect intrusion related to unauthorized access, sending fraudulent SIP messages for denial of service, and flooding with malformed packets to reduce the performance of the network operation. The data collection would be from actual operation of the system in scenarios of attack and normal activity. The performance measure of data classification using machine learning algorithms shows that the support vector machine had the lowest accuracy of 90.81%, artificial neural network had an accuracy of 96.92%, decision tree had an accuracy of 99.29%, k-nearest neighbor had the highest accuracy of 99.81%. The evaluation of time consumed for training a dataset and detection per a data item shows that the decision tree spent the least time of 0.014s and 0.000ms, support vector machine spent 1.276s and 0.000ms, k-nearest neighbor spent 2.535s and 0.001ms, whereas the artificial neural network spent the most time of 7.425s, and 0.005ms, respectively.},
booktitle = {Proceedings of the 9th International Conference on Computer and Communications Management},
pages = {149–155},
numpages = {7},
keywords = {Voice over Internet Protocol, Network Security, Machine Learning},
location = {Singapore, Singapore},
series = {ICCCM '21}
}

@article{10.1016/j.ins.2019.05.041,
author = {Grzegorowski, Marek and undefinedlundefinedzak, Dominik},
title = {On resilient feature selection: Computational foundations of r - C-reducts},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {499},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.05.041},
doi = {10.1016/j.ins.2019.05.041},
journal = {Inf. Sci.},
month = oct,
pages = {25–44},
numpages = {20},
keywords = {Heuristic search, NP-hardness, Rough-set-based approximate reducts, Multivariate feature selection, Resilient feature selection}
}

@article{10.1016/j.patcog.2014.08.004,
author = {Wang, Shiping and Pedrycz, Witold and Zhu, Qingxin and Zhu, William},
title = {Subspace learning for unsupervised feature selection via matrix factorization},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {1},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.08.004},
doi = {10.1016/j.patcog.2014.08.004},
abstract = {Dimensionality reduction is an important and challenging task in machine learning and data mining. Feature selection and feature extraction are two commonly used techniques for decreasing dimensionality of the data and increasing efficiency of learning algorithms. Specifically, feature selection realized in the absence of class labels, namely unsupervised feature selection, is challenging and interesting. In this paper, we propose a new unsupervised feature selection criterion developed from the viewpoint of subspace learning, which is treated as a matrix factorization problem. The advantages of this work are four-fold. First, dwelling on the technique of matrix factorization, a unified framework is established for feature selection, feature extraction and clustering. Second, an iterative update algorithm is provided via matrix factorization, which is an efficient technique to deal with high-dimensional data. Third, an effective method for feature selection with numeric data is put forward, instead of drawing support from the discretization process. Fourth, this new criterion provides a sound foundation for embedding kernel tricks into feature selection. With this regard, an algorithm based on kernel methods is also proposed. The algorithms are compared with four state-of-the-art feature selection methods using six publicly available datasets. Experimental results demonstrate that in terms of clustering results, the proposed two algorithms come with better performance than the others for almost all datasets we experimented with here. HighlightsPropose a new feature selection based on matrix factorization.Present a fast convergent algorithm for matrix factorization on certain constraints.Incorporate kernel tricks into feature selection problems.Construct a unified framework for feature extraction, feature selection and clustering.},
journal = {Pattern Recogn.},
month = jan,
pages = {10–19},
numpages = {10},
keywords = {Unsupervised learning, Subspace distance, Matrix factorization, Machine learning, Kernel method, Feature selection}
}

@inproceedings{10.5555/3172077.3172188,
author = {Li, Jundong and Tang, Jiliang and Liu, Huan},
title = {Reconstruction-based unsupervised feature selection: an embedded approach},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Feature selection has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. Since real-world data is usually unlabeled, unsupervised feature selection has received increasing attention in recent years. Without label information, unsupervised feature selection needs alternative criteria to define feature relevance. Recently, data reconstruction error emerged as a new criterion for unsupervised feature selection, which defines feature relevance as the capability of features to approximate original data via a reconstruction function. Most existing algorithms in this family assume predefined, linear reconstruction functions. However, the reconstruction function should be data dependent and may not always be linear especially when the original data is high-dimensional. In this paper, we investigate how to learn the reconstruction function from the data automatically for unsupervised feature selection, and propose a novel reconstruction-based unsupervised feature selection framework REFS, which embeds the reconstruction function learning process into feature selection. Experiments on various types of real-world datasets demonstrate the effectiveness of the proposed framework REFS.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {2159–2165},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@article{10.1016/j.cose.2020.102164,
author = {Nazir, Anjum and Khan, Rizwan Ahmed},
title = {A novel combinatorial optimization based feature selection method for network intrusion detection},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {102},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102164},
doi = {10.1016/j.cose.2020.102164},
journal = {Comput. Secur.},
month = mar,
numpages = {12},
keywords = {Metaheuristics, Feature selection, Machine learning, Intrusion detection}
}

@article{10.1007/s00521-018-3880-8,
author = {Das, Priyanka and Das, Asit Kumar and Nayak, Janmenjoy},
title = {Feature selection generating directed rough-spanning tree for crime pattern analysis},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3880-8},
doi = {10.1007/s00521-018-3880-8},
abstract = {Nowadays, crime is a major threat to the society that affects the normal life of human beings all over the world. It is very important to make the world free from all aspects of crime activities. The main motivation of this work is to understand various crime patterns for avoiding and preventing the crime events to occur in future and save the world from such curse. Though research is going on for solving such problems, no work is noticed to handle the roughness or ambiguity that exists in the crime reports. The present work extracts all possible crime features from the crime reports and selects only the important features required for crime pattern analysis. For this purpose, it develops a purely supervised feature selection model integrating rough set theory and graph theory (spanning tree of a directed weighted graph). The crime reports are preprocessed, and crime features are extracted to represent each report as a feature vector (i.e., a set of distinct crime features). For crime pattern analysis, the main objective of our work, all extracted features are not necessarily essential, rather a minimal subset of relevant features are sufficient. Thus, feature selection is the main contribution in the paper that not only enhances the efficiency of subsequent mining process but also increases its correctness. The rough set theory-based relative indiscernibility relation is defined to measure the similarity between two features relative to the crime type. Based on the similarity score, a weighted and directed graph has been constructed that comprises the features as nodes and the inverse of the similarity score representing the similarity of feature v to u as the weight of the corresponding edge. Then, a minimal spanning tree (termed as rough-spanning tree) is generated using Edmond/Chu–Liu algorithm from the constructed directed graph and the importance of the nodes in the spanning tree is measured using the weights of the edges and the degrees (in-degrees and out-degrees) of the nodes in the spanning tree. Finally, a feature selection algorithm has been proposed that selects the most important node and remove it from the spanning tree iteratively until the modified graph (not necessarily a tree) becomes a null graph. The selected nodes are considered as the important feature subset sufficient for crime pattern analysis. The method is evaluated using various statistical measures and compared with related state-of-the-art methods to express its effectiveness in crime pattern analysis. The Wilcoxon rank-sum test, a popular nonparametric version of the two-sample t test, is done to express that the proposed supervised model is statistically significant.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7623–7639},
numpages = {17},
keywords = {Crime pattern analysis, Classification, Minimal spanning tree, Relative indiscernibility relation, Rough set theory, Feature selection, Data mining}
}

@inproceedings{10.1145/2623330.2623635,
author = {Xu, Zhixiang and Huang, Gao and Weinberger, Kilian Q. and Zheng, Alice X.},
title = {Gradient boosted feature selection},
year = {2014},
isbn = {9781450329569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2623330.2623635},
doi = {10.1145/2623330.2623635},
abstract = {A feature selection algorithm should ideally satisfy four conditions: reliably extract relevant features; be able to identify non-linear feature interactions; scale linearly with the number of features and dimensions; allow the incorporation of known sparsity structure. In this work we propose a novel feature selection algorithm, Gradient Boosted Feature Selection (GBFS), which satisfies all four of these requirements. The algorithm is flexible, scalable, and surprisingly straight-forward to implement as it is based on a modification of Gradient Boosted Trees. We evaluate GBFS on several real world data sets and show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-specific side information.},
booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {522–531},
numpages = {10},
keywords = {large-scale, gradient boosting, feature selection},
location = {New York, New York, USA},
series = {KDD '14}
}

@inproceedings{10.1145/3306618.3314293,
author = {Teso, Stefano and Kersting, Kristian},
title = {Explanatory Interactive Machine Learning},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314293},
doi = {10.1145/3306618.3314293},
abstract = {Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {239–245},
numpages = {7},
keywords = {machine learning, interpretability, explainable artificial intelligence, active learning},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@article{10.1016/j.future.2018.11.021,
author = {P., Vinod and Zemmari, Akka and Conti, Mauro},
title = {A machine learning based approach to detect malicious android apps using discriminant system calls},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {94},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.11.021},
doi = {10.1016/j.future.2018.11.021},
journal = {Future Gener. Comput. Syst.},
month = may,
pages = {333–350},
numpages = {18},
keywords = {Adversarial machine learning, Data poisoning, Adversarial attacks, Feature selection, Classifier, Machine learning, Malware detection}
}

@article{10.1016/j.knosys.2016.09.006,
author = {Shang, Ronghua and Wang, Wenbing and Stolkin, Rustam and Jiao, Licheng},
title = {Subspace learning-based graph regularized feature selection},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.09.006},
doi = {10.1016/j.knosys.2016.09.006},
abstract = {In recent years, a variety of feature selection algorithms based on subspace learning have been proposed. However, such methods typically do not exploit information about the underlying geometry of the data. To overcome this shortcoming, we propose a novel algorithm called subspace learning-based graph regularized feature selection (SGFS). SGFS builds on the feature selection framework of subspace learning, but extends it by incorporating the idea of graph regularization, in which a feature map is constructed on the feature space in order to preserve geometric structure information on the feature manifold. Additionally, the L2,1-norm is used to constrain the feature selection matrix to ensure the sparsity of the feature array and avoid trivial solutions. The resulting method can provide more accurate discrimination information for feature selection. We evaluate SGFS by comparing it against five other state-of-the-art algorithms from the literature, on twelve publicly available benchmark data sets. Empirical results suggest that SGFS is more effective than the other five feature selection algorithms.},
journal = {Know.-Based Syst.},
month = nov,
pages = {152–165},
numpages = {14},
keywords = {Subspace learning, Sparse constraint, Graph regularized, Feature selection, Feature manifold}
}

@article{10.1007/s00500-019-04167-0,
author = {Sivasankar, E. and Selvi, C. and Mahalakshmi, S.},
title = {Rough set-based feature selection for credit risk prediction using weight-adjusted boosting ensemble method},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-019-04167-0},
doi = {10.1007/s00500-019-04167-0},
abstract = {With the tremendous development of financial institutions, credit risk prediction (CRP) plays an essential role in granting loans to customers and helps them to minimize their loss because credit approval sometimes results in massive financial loss. So extra attention is needed to identify risky customer. Researchers have designed complex CRP models using artificial intelligence (AI) and statistical techniques to support the financial institutions to take correct business decisions. Though there are various statistical and AI methods available, the recent literature shows that the ensemble-based CRP model provides improved prediction results than single classifier system. The small increase in the performance of CRP model could result in a significant improvement in the profit of financial institutions and banks. This work proposes a weight-adjusted boosting ensemble method (WABEM) using rough set (RS)-based feature selection (FS) technique with the balancing and regression-based preprocessing called RS_RFS-WABEM. Regression is used to fill missing value in the records to improve the performance of CRP. Three credit datasets (Australia, German and Japanese) are chosen to validate the feasibility and effectiveness of the proposed ensemble method. The trade-off between the uncertainty and imprecise probability of the proposed classifier model is evaluated using the performance measures such as accuracy and area under the curve. Experimental results show that the proposed ensemble method performs better than other base and ensemble classifier methods.},
journal = {Soft Comput.},
month = mar,
pages = {3975–3988},
numpages = {14},
keywords = {Friedman Test, Ensemble method, Regression, Missing value, Feature selection (FS), Preprocessing, Credit risk prediction (CRP)}
}

@inproceedings{10.1007/978-3-030-87013-3_20,
author = {Aliyu, Garba and Haruna, Usman and Abdulmumin, Idris and Isma’il, Murtala and Umar, Ibrahim Enesi and Adamu, Shehu},
title = {Machine Learning Model for Recommending Suitable Courses of Study to Candidates in Nigerian Universities},
year = {2021},
isbn = {978-3-030-87012-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87013-3_20},
doi = {10.1007/978-3-030-87013-3_20},
abstract = {The diversity of courses and complications of admission requirements are complex tasks particularly in Nigerian Universities where a number of parameters are used during the admission process. These courses may be wrongly assigned to applicants who have not met the minimum requirements. In a previous related work, a model was developed to address this issue. However, the model considered only seven subjects out of the mandatory nine subjects required of every senior secondary school student to register (O’Level). Such a decision may be to the detriment to the candidates because credits may be required from those subjects that were not considered. This paper tends to enhance the existing model to address all these issues. Grade of nine Secondary school subjects, the aggregate score of Unified Tertiary Matriculation Examination (UTME) and post-UTME, and catchment area are used as parameters in this study. The results were obtained when various reference classifiers were trained and tested using the processed dataset of the O’Level and JAMB results of candidates seeking admission into the university. Individual classifiers namely, Logistic Regression, Naive Bayes, Decision Tree, K-Nearest Neighbor, and Random Forest were trained and evaluated using reference performance metrics namely precision, recall, and f1-score. The resulting best classifier, the Random Forest, has shown to be correct 94.94% of the time and is capable of detecting correctly 94.17% of the classes. Since the precision and recall are similar in value, the f1-score tends to favor this classifier also with a value of 93.19%.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part IX},
pages = {257–271},
numpages = {15},
keywords = {Machine learning, Classifiers, Prediction, Recommender system, Admission},
location = {Cagliari, Italy}
}

@article{10.1016/j.jnca.2019.102526,
author = {Gibert, Daniel and Mateu, Carles and Planes, Jordi},
title = {The rise of machine learning for detection and classification of malware: Research developments, trends and challenges},
year = {2020},
issue_date = {Mar 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {153},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2019.102526},
doi = {10.1016/j.jnca.2019.102526},
journal = {J. Netw. Comput. Appl.},
month = mar,
numpages = {22},
keywords = {Multimodal learning, Deep learning, Machine learning, Feature engineering, Malware detection}
}

@inproceedings{10.1007/978-3-030-61527-7_14,
author = {Ruiz, Saulo and Gomes, Pedro and Rodrigues, Lu\'{\i}s and Gama, Jo\~{a}o},
title = {Assembled Feature Selection for Credit Scoring in Microfinance with Non-traditional Features},
year = {2020},
isbn = {978-3-030-61526-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-61527-7_14},
doi = {10.1007/978-3-030-61527-7_14},
abstract = {Since early 2000, Microfinance Institutions (MFI) have been using credit scoring for their risk assessment. However, one of the main problems of credit scoring in microfinance is the lack of structured financial data. To address this problem, MFI have started using non-traditional data which can be extracted from the digital footprint of their users. The non-traditional data can be used to build algorithms that can identify good borrowers as in traditional banking. This paper proposes an assembled method to evaluate the predictive power of the non-traditional method. By using the Weight of Evidence (WoE), a transformation based on the distribution within the feature, as feature transformation method, and then applying extremely randomized trees for feature selection, we were able to improve the accuracy of the credit scoring model by 20.20% when compared to the credit scoring model built with the traditional implementation of WoE. This paper shows how the assembling of WoE with different feature selection criteria can result in more robust credit scoring models in microfinance.},
booktitle = {Discovery Science: 23rd International Conference, DS 2020, Thessaloniki, Greece, October 19–21, 2020, Proceedings},
pages = {207–216},
numpages = {10},
keywords = {Credit scoring, Microfinance, Logistic regression, Weight of Evidence, Emerging markets, Feature selection},
location = {Thessaloniki, Greece}
}

@article{10.1016/j.asoc.2019.105777,
author = {Sabando, Mar\'{\i}a Virginia and Ponzoni, Ignacio and Soto, Axel J.},
title = {Neural-based approaches to overcome feature selection and applicability domain in drug-related property prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {85},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105777},
doi = {10.1016/j.asoc.2019.105777},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {14},
keywords = {Feature selection, Applicability domain, Model interpretability, QSAR modeling, Neural networks}
}

@article{10.1007/s10207-019-00434-1,
author = {Aamir, Muhammad and Zaidi, Syed Mustafa Ali},
title = {DDoS attack detection with feature engineering and machine learning: the framework and performance evaluation},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {6},
issn = {1615-5262},
url = {https://doi.org/10.1007/s10207-019-00434-1},
doi = {10.1007/s10207-019-00434-1},
abstract = {This paper applies an organized flow of feature engineering and machine learning to detect distributed denial-of-service (DDoS) attacks. Feature engineering has a focus to obtain the datasets of different dimensions with significant features, using feature selection methods of backward elimination, chi2, and information gain scores. Different supervised machine learning models are applied on the feature-engineered datasets to demonstrate the adaptability of datasets for machine learning under optimal tuning of parameters within given sets of values. The results show that substantial feature reduction is possible to make DDoS detection faster and optimized with minimal performance hit. The paper proposes a strategic-level framework which incorporates the necessary elements of feature engineering and machine learning with a defined flow of experimentation. The models are also validated with cross-validation and evaluated for area-under-curve analyses. It provides comprehensive solutions which can be trusted to avoid the overfitting and collinearity problems of data while detecting DDoS attacks. In the case study of DDoS datasets, K-nearest neighbors algorithm overall exhibits the best performance followed by support vector machine, whereas low-dimensional datasets of discrete feature types perform better under the Random Forest model as compared to high dimensions with numerical features. The accuracy scores of dataset with the lowest number of features remain competitive with other datasets under all machine learning models, leading to a substantially reduced processing overhead. The experiments show that approximately 68% reduction in the feature space is possible with an impact of only about 0.03% on accuracy.},
journal = {Int. J. Inf. Secur.},
month = dec,
pages = {761–785},
numpages = {25},
keywords = {Neural network, Machine learning, Feature selection, Feature engineering, denial-of-service, DDoS attacks, Cyber security}
}

@article{10.1016/j.ins.2016.04.040,
author = {Forestier, Germain and Wemmert, C\'{e}dric},
title = {Semi-supervised learning using multiple clusterings with limited labeled data},
year = {2016},
issue_date = {September 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {361},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2016.04.040},
doi = {10.1016/j.ins.2016.04.040},
abstract = {Supervised classification consists in learning a predictive model using a set of labeled samples. It is accepted that predictive models accuracy usually increases as more labeled samples are available. Labeled samples are generally difficult to obtain as the labeling step if often performed manually. On the contrary, unlabeled samples are easily available. As the labeling task is tedious and time consuming, users generally provide a very limited number of labeled objects. However, designing approaches able to work efficiently with a very limited number of labeled samples is highly challenging. In this context, semi-supervised approaches have been proposed to leverage from both labeled and unlabeled data.In this paper, we focus on cases where the number of labeled samples is very limited. We review and formalize eight semi-supervised learning algorithms and introduce a new method that combine supervised and unsupervised learning in order to use both labeled and unlabeled data. The main idea of this method is to produce new features derived from a first step of data clustering. These features are then used to enrich the description of the input data leading to a better use of the data distribution. The efficiency of all the methods is compared on various artificial, UCI datasets, and on the classification of a very high resolution remote sensing image. The experiments reveal that our method shows good results, especially when the number of labeled sample is very limited. It also confirms that combining labeled and unlabeled data is very useful in pattern recognition.},
journal = {Inf. Sci.},
month = sep,
pages = {48–65},
numpages = {18},
keywords = {Semi-supervised learning, Remote sensing, Pattern recognition, Classification}
}

@article{10.14778/3467861.3467872,
author = {Li, Yifan and Yu, Xiaohui and Koudas, Nick},
title = {Data acquisition for improving machine learning models},
year = {2021},
issue_date = {June 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3467861.3467872},
doi = {10.14778/3467861.3467872},
abstract = {The vast advances in Machine Learning (ML) over the last ten years have been powered by the availability of suitably prepared data for training purposes. The future of ML-enabled enterprise hinges on data. As such, there is already a vibrant market offering data annotation services to tailor sophisticated ML models.In this paper, inspired by the recent vision of online data markets and associated market designs, we present research on the practical problem of obtaining data in order to improve the accuracy of ML models. We consider an environment in which consumers query for data to enhance the accuracy of their models and data providers who possess data make them available for training purposes. We first formalize this interaction process laying out the suitable framework and associated parameters for data exchange. We then propose two data acquisition strategies that consider a trade-off between exploration during which we obtain data to learn about the distribution of a provider's data and exploitation during which we optimize our data inquiries utilizing the gained knowledge. In the first strategy, Estimation and Allocation (EA), we utilize queries to estimate the utilities of various predicates while learning about the distribution of the provider's data; then we proceed to the allocation stage in which we utilize those learned utility estimates to inform our data acquisition decisions. The second algorithmic proposal, named Sequential Predicate Selection (SPS), utilizes a sampling strategy to explore the distribution of the provider's data, adaptively investing more resources to parts of the data space that are statistically more promising to improve overall model accuracy.We present a detailed experimental evaluation of our proposals utilizing a variety of ML models and associated real data sets exploring all applicable parameters of interest. Our results demonstrate the relative benefits of the proposed algorithms. Depending on the models trained and the associated learning tasks we identify trade-offs and highlight the relative benefits of each algorithm to further optimize model accuracy.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1832–1844},
numpages = {13}
}

@article{10.1186/s13640-021-00558-2,
author = {Khan, Muhammad Attique and Akram, Tallha and Sharif, Muhammad and Alhaisoni, Majed and Saba, Tanzila and Nawaz, Nadia},
title = {A probabilistic segmentation and entropy-rank correlation-based feature selection approach for the recognition of fruit diseases},
year = {2021},
issue_date = {Nov 2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
number = {1},
issn = {1687-5176},
url = {https://doi.org/10.1186/s13640-021-00558-2},
doi = {10.1186/s13640-021-00558-2},
abstract = {Agriculture plays a critical role in the economy of several countries, by providing the main sources of income, employment, and food to their rural population. However, in recent years, it has been observed that plants and fruits are widely damaged by different diseases which cause a huge loss to the farmers, although this loss can be minimized by detecting plants’ diseases at their earlier stages using pattern recognition (PR) and machine learning (ML) techniques. In this article, an automated system is proposed for the identification and recognition of fruit diseases. Our approach is distinctive in a way, it overcomes the challenges like convex edges, inconsistency between colors, irregularity, visibility, scale, and origin. The proposed approach incorporates five primary steps including preprocessing,Standard instruction requires city and country for affiliations. Hence, please check if the provided information for each affiliation with missing data is correct and amend if deemed necessary. disease identification through segmentation, feature extraction and fusion, feature selection, and classification. The infection regions are extracted using the proposed adaptive and quartile deviation-based segmentation approach and fused resultant binary images by employing the weighted coefficient of correlation (CoC). Then the most appropriate features are selected using a novel framework of entropy and rank-based correlation (EaRbC). Finally, selected features are classified using multi-class support vector machine (MC-SCM). A PlantVillage dataset is utilized for the evaluation of the proposed system to achieving an average segmentation and classification accuracy of 93.74% and 97.7%, respectively. From the set of statistical measure, we sincerely believe that our proposed method outperforms existing method with greater accuracy.},
journal = {J. Image Video Process.},
month = may,
numpages = {28},
keywords = {Classification, Feature selection, Feature extraction, Fusion, Segmentation, Contrast stretching}
}

@article{10.3233/KES-140293,
author = {Liu, Shuang and Zhao, Qiang and Wu, Xiang},
title = {Feature selection based on partition clustering},
year = {2014},
issue_date = {April 2014},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-140293},
doi = {10.3233/KES-140293},
abstract = {Feature selection plays an important role in data mining, machine learning and pattern recognition, especially for large scale data with high dimensions. Many selection techniques have been proposed during past years. Their general purposes are to exploit certain metric to measure the relevance or irrelevance between different features of data for certain task, and then select fewer features without deteriorating discriminative capability. Each technique, however, has not absolutely better performance than others' for all kinds of data, due to the data characterized by incorrectness, incompleteness, inconsistency, and diversity. Based on this fact, this paper put forward to a new scheme based on partition clustering for feature selection, which is a special preprocessing procedure and independent of selection techniques. Experimental results carried out on UCI data sets show that the performance achieved by our proposed scheme is better than selection techniques without using this scheme in most cases.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = apr,
pages = {135–142},
numpages = {8},
keywords = {Partition Clustering, Methodology, Feature Selection, Data Preprocessing, Clustering}
}

@article{10.1016/j.jvcir.2017.09.016,
author = {Hou, Xiaodan and Zhang, Tao and Ji, Lei and Wu, Yunda},
title = {Combating highly imbalanced steganalysis with small training samples using feature selection,},
year = {2017},
issue_date = {November 2017},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {49},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2017.09.016},
doi = {10.1016/j.jvcir.2017.09.016},
abstract = {Considering a particular highly imbalanced steganalysis with small training samples.Providing a systematic comparison of eight feature selection metrics.Evaluating the efficiency of each metric.Examining the performance of three types of methods and their combinations.Investigating the effect of several factors on the performance of feature selection. We consider a particular paradigm of steganalysis, namely, highly imbalanced steganalysis with small training samples, in which the cover images always significantly outnumber the stego ones. Researchers have rigorously studied sampling and learning algorithms as well as feature selection approaches to the class imbalance problem, but the research in the steganalysis domain is rare. This study provides a systematic comparison of eight feature selection metrics and of three types of methods developed for the imbalanced data classification problem in the steganalysis domain. Each metric is compared across three different classifiers and four steganalytic features. The efficiency of the metrics is evaluated to determine which performs best with minimal features selected. The performance of the three types of methods and their combinations is examined. Moreover, we also investigate the effect of feature dimensionality, sample number and imbalance degree on the performance of feature selection inresolving imbalanced image steganalysis.},
journal = {J. Vis. Comun. Image Represent.},
month = nov,
pages = {243–256},
numpages = {14},
keywords = {Steganalysis, Sampling, Learning algorithms, Feature selection, Class imbalance}
}

@article{10.1145/3408063,
author = {Xama, Nektar and Andraud, Martin and Gomez, Jhon and Esen, Baris and Dobbelaere, Wim and Vanhooren, Ronny and Coyette, Anthony and Gielen, Georges},
title = {Machine Learning-based Defect Coverage Boosting of Analog Circuits under Measurement Variations},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {5},
issn = {1084-4309},
url = {https://doi.org/10.1145/3408063},
doi = {10.1145/3408063},
abstract = {Safety-critical and mission-critical systems, such as airplanes or (semi-)autonomous cars, are relying on an ever-increasing number of embedded integrated circuits. Consequently, there is a need for complete defect coverage during the testing of these circuits to guarantee their functionality in the field. In this context, reducing the escape rate of defects during production testing is crucial, and significant progress has been made to this end. However, production testing using automatic test equipment is subject to various measurement parasitic variations, which may have a negative impact on the testing procedure and therefore limit the final defect coverage. To tackle this issue, this article proposes an improved test flow targeting increased analog defect coverage, both at the system and block levels, by analyzing and improving the coverage of typical functional and structural tests under these measurement variations. To illustrate the flow, the technique of inserting a pseudo-random signal at available circuit nodes and applying machine learning techniques to its response is presented. A DC-DC converter, derived from an industrial product, is used as a case study to validate the flow. In short, results show that system-level tests for the converter suffer strongly from the measurement variations and are limited to just under 80% coverage, even when applying the proposed test flow. Block-level testing, however, can achieve only 70% fault coverage without improvements but is able to consistently achieve 98% of fault coverage at a cost of at most 2% yield loss with the proposed machine learning–based boosting technique.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = aug,
articleno = {47},
numpages = {27},
keywords = {test under measurements variations, Machine learning for test, AMS IC test}
}

@article{10.1016/j.neunet.2015.08.004,
author = {Yang, Haiqin and Xu, Zenglin and Lyu, Michael R. and King, Irwin},
title = {Budget constrained non-monotonic feature selection},
year = {2015},
issue_date = {November 2015},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {71},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2015.08.004},
doi = {10.1016/j.neunet.2015.08.004},
abstract = {Feature selection is an important problem in machine learning and data mining. We consider the problem of selecting features under the budget constraint on the feature subset size. Traditional feature selection methods suffer from the "monotonic" property. That is, if a feature is selected when the number of specified features is set, it will always be chosen when the number of specified feature is larger than the previous setting. This sacrifices the effectiveness of the non-monotonic feature selection methods. Hence, in this paper, we develop an algorithm for non-monotonic feature selection that approximates the related combinatorial optimization problem by a Multiple Kernel Learning (MKL) problem. We justify the performance guarantee for the derived solution when compared to the global optimal solution for the related combinatorial optimization problem. Finally, we conduct a series of empirical evaluation on both synthetic and real-world benchmark datasets for the classification and regression tasks to demonstrate the promising performance of the proposed framework compared with the baseline feature selection approaches.},
journal = {Neural Netw.},
month = nov,
pages = {214–224},
numpages = {11},
keywords = {Non-monotonic, Multiple kernel learning, Feature selection, Budget constraint}
}

@inproceedings{10.1145/3387168.3387219,
author = {Mohammad, Nurul Izzati and Ismail, Saiful Adli and Kama, Mohd Nazri and Yusop, Othman Mohd and Azmi, Azri},
title = {Customer Churn Prediction In Telecommunication Industry Using Machine Learning Classifiers},
year = {2020},
isbn = {9781450376259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387168.3387219},
doi = {10.1145/3387168.3387219},
abstract = {Customer churn is one of the main problems in telecommunication industry. This study aims to identify the factors that influence customer churn and develop an effective churn prediction model as well as provide best analysis of data visualization results. The dataset has been collected from Kaggle open data website. The proposed methodology for analysis of churn prediction covers several phases: data pre-processing, analysis, implementing machine learning algorithms, evaluation of the classifiers and choose the best one for prediction. Data preprocessing process involved three major action, which are data cleaning, data transformation and feature selection. Machine learning classifiers was chosen are Logistic Regression, Artificial Neural Network and Random Forest. Then, classifiers were evaluated by using performance measurement which are accuracy, precision, recall and error rate in order to find the best classifier. Based on this study, the output shows that logistic regression outperform compared to artificial neural network and random forest.},
booktitle = {Proceedings of the 3rd International Conference on Vision, Image and Signal Processing},
articleno = {34},
numpages = {7},
keywords = {Telecommunication Industry, Prediction, Machine Learning, Customer Churn},
location = {Vancouver, BC, Canada},
series = {ICVISP 2019}
}

@inproceedings{10.1007/978-3-030-33617-2_21,
author = {Taha, Ahmed K. and Kholief, Mohamed H. and AbdelMoez, Walid},
title = {Adaptive Machine Learning-Based Stock Prediction Using Financial Time Series Technical Indicators},
year = {2019},
isbn = {978-3-030-33616-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33617-2_21},
doi = {10.1007/978-3-030-33617-2_21},
abstract = {Stock market prediction is a hard task even with the help of advanced machine learning algorithms and computational power. Although much research has been conducted in the field, the results often are not reproducible. That is the reason why the proposed workflow is publicly available on GitHub [1] as a continuous effort to help improve the research in the field. This study explores in detail the importance of financial time series technical indicators. Exploring new approaches and technical indicators, targets, feature selection techniques, and machine learning algorithms. Using data from multiple assets and periods, the proposed model adapts to market patterns to predict the future and using multiple supervised learning algorithms to ensure the adoption of different markets. The lack of research focusing on feature importance and the premise that technical indicators can improve prediction accuracy directed this research. The proposed approach highest accuracy reaches 75% with an area under the curve (AUC) of 0.82, using historical data up to 2019 to ensure the applicability for today’s market, with more than a hundred experiments on a diverse set of assets publicly available.},
booktitle = {Intelligent Data Engineering and Automated Learning – IDEAL 2019: 20th International Conference, Manchester, UK, November 14–16, 2019, Proceedings, Part II},
pages = {191–199},
numpages = {9},
keywords = {Stock price prediction, Technical indicators feature importance, Adaptive stock prediction, Machine learning, Feature selection},
location = {Manchester, United Kingdom}
}

@inproceedings{10.1007/978-3-030-79457-6_52,
author = {Dedabrishvili, Mariam and Dundua, Besik and Mamaiashvili, Natia},
title = {Smartphone Sensor-Based Fall Detection Using Machine Learning Algorithms},
year = {2021},
isbn = {978-3-030-79456-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79457-6_52},
doi = {10.1007/978-3-030-79457-6_52},
abstract = {Human Activity Recognition and particularly detection of abnormal activities such as falls have become a point of interest to many researchers worldwide since falls are considered to be one of the leading causes of injury and death, especially in the elderly population. The prompt intervention of caregivers in critical situations can significantly improve the autonomy and well-being of individuals living alone and those who require remote monitoring. This paper presents a study of accelerometer and gyroscope data retrieved from smartphone embedded sensors, using iOS-based devices. In the project framework there was developed a mobile application for data collection with the following fall type and fall-like activities: Falling Right, Falling Left, Falling Forward, Falling Backward, Sitting Fast, and Jumping. The collected dataset has passed the preprocessing phase and afterward was classified using different Machine Learning algorithms, namely, by Decision Trees, Random Forest, Logistic Regression, k-Nearest Neighbour, XGBoost, LightGBM, and Pytorch Neural Network. Unlike other similar studies, during the experimental setting, volunteers were asked to have smartphones freely in their pockets without tightening and fixing them on the body. This natural way of keeping a mobile device is quite challenging in terms of noisiness however it is more comfortable to wearers and causes fewer constraints. The obtained results are promising that encourages us to continue working with the aim to reach sufficient accuracy along with building a real-time application for potential users.},
booktitle = {Advances and Trends in Artificial Intelligence. Artificial Intelligence Practices: 34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021, Kuala Lumpur, Malaysia, July 26–29, 2021, Proceedings, Part I},
pages = {609–620},
numpages = {12},
keywords = {Mobile applications, Smartphone embedded sensors, Data classification, Data preprocessing, Fall detection},
location = {Kuala Lumpur, Malaysia}
}

@article{10.1155/2020/1064934,
author = {Fahad, Labiba Gillani and Tahir, Syed Fahad and Shahzad, Waseem and Hassan, Mehdi and Alquhayz, Hani and Hassan, Rabia and Acacio Sanchez, Manuel E.},
title = {Ant Colony Optimization-Based Streaming Feature Selection: An Application to the Medical Image Diagnosis},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1058-9244},
url = {https://doi.org/10.1155/2020/1064934},
doi = {10.1155/2020/1064934},
abstract = {Irrelevant and redundant features increase the computation and storage requirements, and the extraction of required information becomes challenging. Feature selection enables us to extract the useful information from the given data. Streaming feature selection is an emerging field for the processing of high-dimensional data, where the total number of attributes may be infinite or unknown while the number of data instances is fixed. We propose a hybrid feature selection approach for streaming features using ant colony optimization with symmetric uncertainty (ACO-SU). The proposed approach tests the usefulness of the incoming features and removes the redundant features. The algorithm updates the obtained feature set when a new feature arrives. We evaluate our approach on fourteen datasets from the UCI repository. The results show that our approach achieves better accuracy with a minimal number of features compared with the existing methods.},
journal = {Sci. Program.},
month = jan,
numpages = {10}
}

@inproceedings{10.1145/3195106.3195113,
author = {Lotfabadi, Maryam Shahabi and Zhan, Yongzhao and Tabrizi, Amir Bashirzadeh},
title = {A Review of Wrapper Feature Selection in Content Based Image Retrieval Systems},
year = {2018},
isbn = {9781450363532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195106.3195113},
doi = {10.1145/3195106.3195113},
abstract = {In Content Based Image Retrieval systems, feature selection methods have been used for reducing the semantic gap between the visual features and richness of human semantics. The main aim of feature selection is to determine a minimal feature subset from an image, which can be used to represent the original image features. In many real world problems like content based image retrieval systems, feature selection is an important method that helps to remove noisy, irrelevant or misleading features. For example, by removing these features, learning techniques can improve their accuracy. This paper provides a review of the different wrapper feature selection methods used in content based image retrieval systems.},
booktitle = {Proceedings of the 2018 10th International Conference on Machine Learning and Computing},
pages = {178–183},
numpages = {6},
keywords = {Wrapper model, Feature selection, Content based image retrieval systems},
location = {Macau, China},
series = {ICMLC '18}
}

@article{10.3233/JIFS-200258,
author = {Mandal, Ashis Kumar and Sen, Rikta and Chakraborty, Basabi},
title = {Feature selection in classification using self-adaptive owl search optimization algorithm with elitism and mutation strategies},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {40},
number = {1},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-200258},
doi = {10.3233/JIFS-200258},
abstract = {The fundamental aim of feature selection is to reduce the dimensionality of data by removing irrelevant and redundant features. As finding out the best subset of features from all possible subsets is computationally expensive, especially for high dimensional data sets, meta-heuristic algorithms are often used as a promising method for addressing the task. In this paper, a variant of recent meta-heuristic approach Owl Search Optimization algorithm (OSA) has been proposed for solving the feature selection problem within a wrapper-based framework. Several strategies are incorporated with an aim to strengthen BOSA (binary version of OSA) in searching the global best solution. The meta-parameter of BOSA is initialized dynamically and then adjusted using a self-adaptive mechanism during the search process. Besides, elitism and mutation operations are combined with BOSA to control the exploitation and exploration better. This improved BOSA is named in this paper as Modified Binary Owl Search Algorithm (MBOSA). Decision Tree (DT) classifier is used for wrapper based fitness function, and the final classification performance of the selected feature subset is evaluated by Support Vector Machine (SVM) classifier. Simulation experiments are conducted on twenty well-known benchmark datasets from UCI for the evaluation of the proposed algorithm, and the results are reported based on classification accuracy, the number of selected features, and execution time. In addition, BOSA along with three common meta-heuristic algorithms Binary Bat Algorithm (BBA), Binary Particle Swarm Optimization (BPSO), and Binary Genetic Algorithm (BGA) are used for comparison. Simulation results show that the proposed approach outperforms similar methods by reducing the number of features significantly while maintaining a comparable level of classification accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {535–550},
numpages = {16},
keywords = {self adaptive mechanism, optimization, meta-heuristic, binary owl search algorithm, Feature subset selection}
}

@article{10.1016/j.knosys.2021.107435,
author = {Li, Qiwei and Wang, Jianzhou and Zhang, Haipeng},
title = {A wind speed interval forecasting system based on constrained lower upper bound estimation and parallel feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {231},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107435},
doi = {10.1016/j.knosys.2021.107435},
journal = {Know.-Based Syst.},
month = nov,
numpages = {19},
keywords = {Metaheuristics, Decomposition, Grasshopper optimization algorithm, Feature selection, Wind speed interval forecasting, WSRT, PFS, PSO, GOA, CWC, PINAW, PICP, FI, IMF, EMD, CEEMDAN, SSA, LUBE}
}

@article{10.1007/s11063-020-10383-9,
author = {Zhu, Jinting and Jang-Jaccard, Julian and Liu, Tong and Zhou, Jukai},
title = {Joint Spectral Clustering based on Optimal Graph and Feature Selection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10383-9},
doi = {10.1007/s11063-020-10383-9},
abstract = {Redundant features and outliers (noise) included in the data points for a machine learning clustering model heavily influences the discovery of more distinguished features for clustering. To solve this issue, we propose a spectral new clustering method to consider the feature selection with the L2,1-norm regularization as well as simultaneously learns orthogonal representations for each sample to preserve the local structures of data points. Our model also solves the issue of out-of-sample, where the training process does not output an explicit model to predict unseen data points, along with providing an efficient optimization method for the proposed objective function. Experimental results showed that our method on twelve data sets achieves the best performance compared with other similar models.},
journal = {Neural Process. Lett.},
month = feb,
pages = {257–273},
numpages = {17},
keywords = {subspace learning, dimensionality reduction, Graph matrix, Clustering, Feature selection}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {software product lines, product-line analysis, PLA model},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1007/s11042-018-6348-z,
author = {Srinivasan, A. and Gnanavel, V. K.},
title = {Multiple feature set with feature selection for anomaly search in videos using hybrid classification},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {6},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6348-z},
doi = {10.1007/s11042-018-6348-z},
abstract = {An examination of abnormal activities in video scenes is a very difficult task in computer vision community. An efficient anomaly detection technique to detect anomalies in crowded scenes is presented in this paper. It uses Multiple Feature Set (MFS) to represent a piece of rectangular region of predefined size in a video frame called as patch with Hybrid Classification (HC) using Gaussian Mixture Model (GMM) and Support Vector Machine (SVM) classifiers for anomaly detection. The MFS contains a combination of the following types of features; gray intensity values, gradient edge features and texture energy map. The predominant features are selected from MFS by using a model of t-test feature selection method and are classified by HC model made up of GMM and SVM classifiers. The UCSD video clip database is used for performance analysis of MFS-HC system and compared with other approaches. Results show that MFS-HC provides better results than other approaches.},
journal = {Multimedia Tools Appl.},
month = mar,
pages = {7713–7725},
numpages = {13},
keywords = {SVM, Multiple features, Hybrid classification, GMM, Feature selection, Anomaly detection}
}

@article{10.1016/j.neucom.2019.02.035,
author = {Zhang, Jiashuai and Miao, Jianyu and Zhao, Kun and Tian, Yingjie},
title = {Multi-task feature selection with sparse regularization to extract common and task-specific features},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {340},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.02.035},
doi = {10.1016/j.neucom.2019.02.035},
journal = {Neurocomput.},
month = may,
pages = {76–89},
numpages = {14},
keywords = {ADMM, Non-convex, Sparse regularization, Multi-task feature learning}
}

@article{10.1016/j.compbiomed.2019.02.009,
author = {Malan, Nitesh Singh and Sharma, Shiru},
title = {Feature selection using regularized neighbourhood component analysis to enhance the classification performance of motor imagery signals},
year = {2019},
issue_date = {Apr 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {107},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2019.02.009},
doi = {10.1016/j.compbiomed.2019.02.009},
journal = {Comput. Biol. Med.},
month = apr,
pages = {118–126},
numpages = {9},
keywords = {Support vector machine, Principal component analysis, Neighbourhood component analysis, Motor imagery, Genetic algorithm, Brain-computer interface}
}

@article{10.1016/j.compbiomed.2021.104798,
author = {Torres, Noelia and Trujillo, Leonardo and Maldonado, Yazmin and Vera, Carlos},
title = {Correction of the travel time estimation for ambulances of the red cross Tijuana using machine learning},
year = {2021},
issue_date = {Oct 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {137},
number = {C},
issn = {0010-4825},
url = {https://doi.org/10.1016/j.compbiomed.2021.104798},
doi = {10.1016/j.compbiomed.2021.104798},
journal = {Comput. Biol. Med.},
month = oct,
numpages = {13},
keywords = {Machine learning, Ambulance travel time, Mapping systems, Emergency medical services}
}

@article{10.1145/3070646,
author = {Wu, Yue and Hoi, Steven C. H. and Mei, Tao and Yu, Nenghai},
title = {Large-Scale Online Feature Selection for Ultra-High Dimensional Sparse Data},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3070646},
doi = {10.1145/3070646},
abstract = {Feature selection (FS) is an important technique in machine learning and data mining, especially for large-scale high-dimensional data. Most existing studies have been restricted to batch learning, which is often inefficient and poorly scalable when handling big data in real world. As real data may arrive sequentially and continuously, batch learning has to retrain the model for the new coming data, which is very computationally intensive. Online feature selection (OFS) is a promising new paradigm that is more efficient and scalable than batch learning algorithms. However, existing online algorithms usually fall short in their inferior efficacy. In this article, we present a novel second-order OFS algorithm that is simple yet effective, very fast and extremely scalable to deal with large-scale ultra-high dimensional sparse data streams. The basic idea is to exploit the second-order information to choose the subset of important features with high confidence weights. Unlike existing OFS methods that often suffer from extra high computational cost, we devise a novel algorithm with a MaxHeap-based approach, which is not only more effective than the existing first-order algorithms, but also significantly more efficient and scalable. Our extensive experiments validated that the proposed technique achieves highly competitive accuracy as compared with state-of-the-art batch FS methods, meanwhile it consumes significantly less computational cost that is orders of magnitude lower. Impressively, on a billion-scale synthetic dataset (1-billion dimensions, 1-billion non-zero features, and 1-million samples), the proposed algorithm takes less than 3 minutes to run on a single PC.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {48},
numpages = {22},
keywords = {ultra-high dimensionality, sparsity, second-order online learning, Feature selection}
}

@article{10.1504/ijguc.2020.108475,
author = {Arias-Torres, Dante and Jos\'{e} and Hern\'{a}ndez-Nolasco;, Ad\'{a}n and Wister, Miguel A.},
title = {Detection of fatigue on gait using accelerometer data and supervised machine learning},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1741-847X},
url = {https://doi.org/10.1504/ijguc.2020.108475},
doi = {10.1504/ijguc.2020.108475},
abstract = {In this paper, we aim to detect the fatigue based on accelerometer data from human gait using traditional classifiers from machine learning. First, we compare widely used machine learning classifiers to know which classifier can detect fatigue with the fewest errors. We observe that the best results were obtained with a Support Vector Machine (SVM) classifier. Later, we propose a new approach to solve the feature selection problem to know which features are more relevant to detect fatigue in healthy people based on their gait patterns. Finally, we used relevant gait features discovered in a previous step as input in classifiers used previously to know its impact on the classification process. Our results indicate that using only some gait features selected by our proposed feature selection method it is possible to improve fatigue detection based on data from human gait. We conclude that it is possible to distinguish between a normal gait person and a fatigued gait person with high accuracy.},
journal = {Int. J. Grid Util. Comput.},
month = jan,
pages = {474–485},
numpages = {11},
keywords = {supervised learning, accelerometer, detection, fatigue, gait}
}

@article{10.1016/j.ins.2015.08.022,
author = {Hern\'{a}ndez-Pereira, Elena and Bol\'{o}n-Canedo, Veronica and S\'{a}nchez-Maro\~{n}o, Noelia and \'{A}lvarez-Est\'{e}vez, Diego and Moret-Bonillo, Vicente and Alonso-Betanzos, Amparo},
title = {A comparison of performance of K-complex classification methods using feature selection},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {328},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.08.022},
doi = {10.1016/j.ins.2015.08.022},
abstract = {The main objective of this work is to obtain a method that achieves the best accuracy results with a low false positive rate in the classification of K-complexes, a kind of transient waveform found in the Electroencephalogram. With this in mind, the capabilities of several machine learning techniques were tried. The inputs for the models were a set of features based on amplitude and duration measurements obtained from waveforms to be classified. Among all the classifiers tested, the Support Vector Machine obtained the best results with an accuracy of 88.69%. Finally, to enhance the generalization capabilities of the classifiers, while at the same time discarding the existing irrelevant features, feature selection methods were employed. After this process, the classification performance was significantly improved. The best result was obtained applying a correlation-based filter, achieving a 91.40% of accuracy using only 36% of the total input features.},
journal = {Inf. Sci.},
month = jan,
pages = {1–14},
numpages = {14},
keywords = {Machine learning, K-complex classification, Feature selection}
}

@article{10.1007/s00500-020-05363-z,
author = {Jain, Pankhuri and Tiwari, Anoop Kumar and Som, Tanmoy},
title = {Enhanced prediction of anti-tubercular peptides from sequence information using divergence measure-based intuitionistic fuzzy-rough feature selection},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {4},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05363-z},
doi = {10.1007/s00500-020-05363-z},
abstract = {Tuberculosis is one of the leading causes of millions of deaths across the world, mainly due to growth of drug-resistant strains. Anti-tubercular peptides may facilitate an alternate way to combat antibiotic tolerance. This study describes a novel approach for enhancing the prediction of anti-tubercular peptides by feature extraction from sequence of the peptides, selection of optimal features from the extracted features, and selection of suitable learning algorithm. Firstly, we extract different sequence features by using iFeature web server. Then, the optimal features are obtained by using a novel divergence measure-based intuitionistic fuzzy rough sets-assisted feature selection technique. Furthermore, an attempt has been made to develop models using different machine learning techniques for enhancing the prediction of anti-tubercular (or anti-mycobacterial peptides) with other antibacterial peptides (ABP) as well non-antibacterial peptides (non-ABP). Moreover, the best prediction result is obtained by vote-based classifier. Using 80:20 percentage split, the proposed method performs well, with sensitivity of 92.0%, 96.4%, specificity of 83.3%, 88.4%, overall accuracy of 87.80%, 92.90%, Mathews correlation coefficient of 0.757, 0.857, AUC of 0.922, 0.914, and g-means of 87.5%, 92.3% for anti-tubercular and ABP (primary dataset), anti-tubercular and non-ABP (secondary dataset), respectively. Finally, we have evaluated the performances of different machine learning algorithms by using the reduced training sets as produced by our proposed feature selection technique as well as already existing intuitionistic fuzzy rough set based and ensemble feature selection technique. Moreover, the performance of our proposed approach is evaluated on few benchmark and AMP datasets. From the experimental results, it can be observed that our proposed method is outperforming the previous methods.},
journal = {Soft Comput.},
month = feb,
pages = {3065–3086},
numpages = {22},
keywords = {Intuitionistic fuzzy set, Intuitionistic fuzzy divergence measure, Feature selection, Feature extraction, Dependency function}
}

@article{10.1016/j.jvcir.2018.09.020,
author = {Su, Yuting and Bai, Xu and Li, Wu and Jing, Peiguang and Zhang, Jing and Liu, Jing},
title = {Graph regularized low-rank tensor representation for feature selection},
year = {2018},
issue_date = {Oct 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {56},
number = {C},
issn = {1047-3203},
url = {https://doi.org/10.1016/j.jvcir.2018.09.020},
doi = {10.1016/j.jvcir.2018.09.020},
journal = {J. Vis. Comun. Image Represent.},
month = oct,
pages = {234–244},
numpages = {11},
keywords = {Subspace clustering, Graph embedding, Low-rank tensor representation, Unsupervised feature selection}
}

@article{10.1016/j.procs.2017.09.077,
author = {Gopalapillai, Radhakrishnan and Gupta, Deepa and Tsb, Sudarshan},
title = {Pattern Identification of Robotic Environments using Machine Learning Techniques},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {115},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.09.077},
doi = {10.1016/j.procs.2017.09.077},
abstract = {Analysis of time series data collected from mobile robots is getting more attention in many application areas. When multiple robots move through an environment to perform certain actions, an understanding of the environment viewed by each robot is essential. This paper presents analysis of robotic data using machine learning techniques when the data consist of multiple views of the environment. Robotic environments have been classified using the data captured by onboard sensors of mobile robots using a set of machine learning algorithms and their performances have been compared The machine learning model is validated using a test environment where some of the objects are displaced or removed from their designated position.},
journal = {Procedia Comput. Sci.},
month = nov,
pages = {63–71},
numpages = {9},
keywords = {feature reduction, Time series data, Object displacement, Machine Learning, Feature selection, Classification}
}

@article{10.1007/s10462-018-09677-1,
author = {Kadhim, Ammar Ismael},
title = {Survey on supervised machine learning techniques for automatic text classification},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-09677-1},
doi = {10.1007/s10462-018-09677-1},
abstract = {Supervised machine learning studies are gaining more significant recently because of the availability of the increasing number of the electronic documents from different resources. Text classification can be defined that the task was automatically categorized a group documents into one or more predefined classes according to their subjects. Thereby, the major objective of text classification is to enable users for extracting information from textual resource and deals with process such as retrieval, classification, and machine learning techniques together in order to classify different pattern. In text classification technique, term weighting methods design suitable weights to the specific terms to enhance the text classification performance. This paper surveys of text classification, process of different term weighing methods and comparison between different classification techniques.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {273–292},
numpages = {20},
keywords = {Text classification, Term weighting, Supervised machine learning, Classification techniques}
}

@article{10.5555/3288065.3288137,
author = {Lausser, Ludwig and Szekely, Robin and Schirra, Lyn-Rouven and Kestler, Hans A.},
title = {The Influence of Multi-class Feature Selection on the Prediction of Diagnostic Phenotypes},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {48},
number = {2},
issn = {1370-4621},
abstract = {In this work, we evaluate two schemes for incorporating feature selection processes in multi-class classifier systems on high-dimensional data of low cardinality. These schemes operate on the level of the systems' individual base classifiers and therefore do not perfectly fit in the traditional categories of filter, wrapper and embedded feature selection strategies. They can be seen as two examples of feature selection networks that are only loosely related to the structure of the multi-class classifier system. The architectures are tested for their application in predicting diagnostic phenotypes from gene expression profiles. Their selection stability and the overall generalization ability are evaluated in $$10 times 10$$10 10 cross-validation experiments with support vector machines, random forests and nearest neighbor classifiers on eight publicly available multi-class microarray datasets. Overall the feature selecting multi-class classifier systems were able to outperform their counterparts on at least five of eight datasets.},
journal = {Neural Process. Lett.},
month = oct,
pages = {863–880},
numpages = {18},
keywords = {Multi-class classification, Low cardinality, High-dimensional data, Feature selection, Classifier fusion}
}

@inproceedings{10.1145/3321408.3323928,
author = {Feng, Hongwei and Li, Shuang and He, Dianyuan and Feng, Jun},
title = {A novel feature selection approach based on multiple filters and new separable degree index for credit scoring},
year = {2019},
isbn = {9781450371582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321408.3323928},
doi = {10.1145/3321408.3323928},
abstract = {With the rapid development of Internet finance, credit scoring has played a significant role in peer to peer lending platforms. However, the massive and high-dimensional characteristics of the credit data make it difficult to directly build the credit scoring model. Therefore, the feature selection is attracting more and more attention, which can be used for processing the complicated credit data. In this paper, we proposed a novel feature selection approach, which is specifically designed for analyzing the customer data in credit scoring. Firstly, we proposed a strategy that combining multiple filters to select the different candidate feature subsets from customer data. Then, a New Separable Degree index is proposed, which can select the optimal feature subset for credit scoring. Experimental results indicated that the performance of the approach we proposed is superior to other single filters, and the computational cost is greatly reduced compared to the traditional wrappers.},
booktitle = {Proceedings of the ACM Turing Celebration Conference - China},
articleno = {97},
numpages = {5},
keywords = {optimal feature subsets, filters, feature selection, credit scoring},
location = {Chengdu, China},
series = {ACM TURC '19}
}

@inproceedings{10.1145/3180445.3180452,
author = {Anandan, Balamurugan and Clifton, Chris},
title = {Differentially Private Feature Selection for Data Mining},
year = {2018},
isbn = {9781450356343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180445.3180452},
doi = {10.1145/3180445.3180452},
abstract = {One approach to analysis of private data is ε-differential privacy, a randomization-based approach that protects individual data items by injecting carefully limited noise into results. A challenge in applying this to private data analysis is that the noise added to the feature parameters is directly proportional to the number of parameters learned. While careful feature selection would alleviate this problem, the process of feature selection itself can reveal private information, requiring the application of differential privacy to the feature selection process. In this paper, we analyze the sensitivity of various feature selection techniques used in data mining and show that some of them are not suitable for differentially private analysis due to high sensitivity. We give experimental results showing the value of using low sensitivity feature selection techniques. We also show that the same concepts can be used to improve differentially private decision trees.},
booktitle = {Proceedings of the Fourth ACM International Workshop on Security and Privacy Analytics},
pages = {43–53},
numpages = {11},
keywords = {sensitivity, privacy preserving data mining, naive Bayes, feature selection, differential privacy, decision trees, data mining, classification},
location = {Tempe, AZ, USA},
series = {IWSPA '18}
}

@article{10.1016/j.knosys.2018.05.020,
author = {Liao, Shujiao and Zhu, Qingxin and Qian, Yuhua and Lin, Guoping},
title = {Multi-granularity feature selection on cost-sensitive data with measurement errors and variable costs},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {158},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2018.05.020},
doi = {10.1016/j.knosys.2018.05.020},
journal = {Know.-Based Syst.},
month = oct,
pages = {25–42},
numpages = {18},
keywords = {Variable costs, Rough sets, Neighborhood, Multi-granularity, Measurement errors, Feature-granularity selection}
}

@article{10.1162/neco_a_01163,
author = {Hu, Haojie and Wang, Rong and Yang, Xiaojun and Nie, Feiping},
title = {Scalable and flexible unsupervised feature selection},
year = {2019},
issue_date = {March 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {31},
number = {3},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco_a_01163},
doi = {10.1162/neco_a_01163},
abstract = {Recently, graph-based unsupervised feature selection algorithms GUFS have been shown to efficiently handle prevalent high-dimensional unlabeled data. One common drawback associated with existing graph-based approaches is that they tend to be time-consuming and in need of large storage, especially when faced with the increasing size of data. Research has started using anchors to accelerate graph-based learning model for feature selection, while the hard linear constraint between the data matrix and the lower-dimensional representation is usually overstrict in many applications. In this letter, we propose a flexible linearization model with anchor graph and \'{y}21-norm regularization, which can deal with large-scale data sets and improve the performance of the existing anchor-based method. In addition, the anchor-based graph Laplacian is constructed to characterize the manifold embedding structure by means of a parameter-free adaptive neighbor assignment strategy. An efficient iterative algorithm is developed to address the optimization problem, and we also prove the convergence of the algorithm. Experiments on several public data sets demonstrate the effectiveness and efficiency of the method we propose. p&gt;},
journal = {Neural Comput.},
month = mar,
pages = {517–537},
numpages = {21}
}

@article{10.1016/j.patcog.2018.01.016,
author = {Shao, Yuan-Hai and Li, Chun-Na and Liu, Ming-Zeng and Wang, Zhen and Deng, Nai-Yang},
title = {Sparse Lq-norm least squares support vector machine with feature selection},
year = {2018},
issue_date = {June 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2018.01.016},
doi = {10.1016/j.patcog.2018.01.016},
abstract = {We propose an Lq-norm LS-SVM with feature selection for small size samples.Feature selection is achieved effectively by minimizing the Lq-norm of weight.The number of selected features can be adjusted by choosing the parameters.An efficient iterative global convergent algorithm is introduced to solve the primal problem.Experimental results show its feasibility and efficiency. Least squares support vector machine (LS-SVM) is a popular hyperplane-based classifier and has attracted many attentions. However, it may suffer from singularity or ill-condition issue for the small sample size (SSS) problem where the sample size is much smaller than the number of features of a data set. Feature selection is an effective way to solve this problem. Motivated by this, in the paper, we propose a sparse Lq-norm least squares support vector machine (Lq-norm LS-SVM) with 0},
journal = {Pattern Recogn.},
month = jun,
pages = {167–181},
numpages = {15},
keywords = {Sparse approximation, Lq-norm, Least squares support vector machine (LS-SVM), Global optimality, Feature selection}
}

@article{10.1007/s10766-017-0493-2,
author = {Silva, Jorge and Aguiar, Ana and Silva, Fernando},
title = {Parallel Asynchronous Strategies for the Execution of Feature Selection Algorithms},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {46},
number = {2},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-017-0493-2},
doi = {10.1007/s10766-017-0493-2},
abstract = {Reducing the dimensionality of datasets is a fundamental step in the task of building a classification model. Feature selection is the process of selecting a smaller subset of features from the original one in order to enhance the performance of the classification model. The problem is known to be NP-hard, and despite the existence of several algorithms there is not one that outperforms the others in all scenarios. Due to the complexity of the problem usually feature selection algorithms have to compromise the quality of their solutions in order to execute in a practicable amount of time. Parallel computing techniques emerge as a potential solution to tackle this problem. There are several approaches that already execute feature selection in parallel resorting to synchronous models. These are preferred due to their simplicity and capability to use with any feature selection algorithm. However, synchronous models implement pausing points during the execution flow, which decrease the parallel performance. In this paper, we discuss the challenges of executing feature selection algorithms in parallel using asynchronous models, and present a feature selection algorithm that favours these models. Furthermore, we present two strategies for an asynchronous parallel execution not only of our algorithm but of any other feature selection approach. The first strategy solves the problem using the distributed memory paradigm, while the second exploits the use of shared memory. We evaluate the parallel performance of our strategies using up to 32 cores. The results show near linear speedups for both strategies, with the shared memory strategy outperforming the distributed one. Additionally, we provide an example of adapting our strategies to execute the Sequential forward Search asynchronously. We further test this version versus a synchronous one. Our results revealed that, by using an asynchronous strategy, we are able to save an average of 7.5% of the execution time.},
journal = {Int. J. Parallel Program.},
month = apr,
pages = {252–283},
numpages = {32},
keywords = {Parallel computing, Machine learning, Feature selection, Asynchronous model}
}

@article{10.1016/S0164-1212(03)00013-X,
author = {Zelkowitz, Marvin V. and Rus, Ioana},
title = {Defect evolution in a product line environment},
year = {2004},
issue_date = {February, 2004},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {70},
number = {1–2},
issn = {0164-1212},
url = {https://doi.org/10.1016/S0164-1212(03)00013-X},
doi = {10.1016/S0164-1212(03)00013-X},
abstract = {One mechanism used for monitoring the development of the Space Shuttle flight control software, in order to minimize any risks to the missions, is the independent verification and validation (IV&amp;V) process. Using data provided by both the Shuttle software developer and the IV&amp;V contractor, in this paper we describe the overall IV&amp;V process as used on the Space Shuttle program and provide an analysis of the use of metrics to document and control this process over multiple releases of this software. Our findings reaffirm the value of IV&amp;V, show the impact of IV&amp;V on multiple releases of a large complex software system, and indicate that some of the traditional measures of defect detection and repair are not applicable in a multiple-release environment such as this one.},
journal = {J. Syst. Softw.},
month = feb,
pages = {143–154},
numpages = {12},
keywords = {Space Shuttle program, Software safety and reliability, Software independent verification and validation, Product line development, Process characterization, Metrics, Life and mission critical software, Evolutionary software}
}

@inproceedings{10.1145/3383783.3383789,
author = {Lim, Tito C. and Torregosa, Jaedy O. and Pescadero, Aubrey Rose A. and Pangantihon, Rodrigo S.},
title = {De-husked Coconut Quality Evaluation using Image Processing and Machine Learning Techniques},
year = {2020},
isbn = {9781450372183},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383783.3383789},
doi = {10.1145/3383783.3383789},
abstract = {Qualitative evaluation provides the basis for determining if the quality of products meets the target specifications. Manual evaluation of de-husked coconuts is still being performed by coconut farmers, however, it is time consuming and costly. Ergo this study aiming to replace the manual inspection, a prototype was developed for objective and automated quality evaluation of de-husked coconuts through the application of computer vision and machine learning, identifying good-quality de-husked coconuts from defective ones with respect to its RGB color space. JavaFX platform was utilized to create the system performing K-Nearest Neighbor and Arduino technology played a significant role in the hardware control of the device. The image samples were captured by a CMOS camera in an imaging chamber with invariant illumination on top of a conveyor belt. Image processing is done to get the required features of the sample and by comparing the average RGB value from the custom dataset, then the maturity level of the coconut is determined. With the accuracy of 86.67%, the system is able to evaluate de-husked coconuts which are good for further processing used in export and premature coconuts that are to be rejected.},
booktitle = {Proceedings of the 6th International Conference on Bioinformatics Research and Applications},
pages = {28–33},
numpages = {6},
keywords = {k-NN, OpenCV, Machine learning, JavaFX, Coconut quality},
location = {Seoul, Republic of Korea},
series = {ICBRA '19}
}

@article{10.1016/j.eswa.2021.114758,
author = {Perboli, Guido and Arabnezhad, Ehsan},
title = {A Machine Learning-based DSS for mid and long-term company crisis prediction},
year = {2021},
issue_date = {Jul 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {174},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.114758},
doi = {10.1016/j.eswa.2021.114758},
journal = {Expert Syst. Appl.},
month = jul,
numpages = {12},
keywords = {Machine learning, Data analysis, Bankruptcy prediction}
}

@article{10.1007/s00521-021-05838-6,
author = {Chandra, Subhash and Singh, Koushlendra Kumar and Kumar, Sanjay and Ganesh, K. V. K. S. and Sravya, Lavu and Kumar, B. Phani},
title = {A novel approach to validate online signature using machine learning based on dynamic features},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {19},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05838-6},
doi = {10.1007/s00521-021-05838-6},
abstract = {This paper presents a new and mathematical method for online signature validation based on machine learning. In this way, the average values of the factors are taken into account to ensure validity. Here, seven different types of features used are x coordinates, y coordinates, time stamp, pen up and down, azimuth, height and pressure. Three new features are extracted from it, i.e., (displacement, velocity and acceleration) using the correlated extraction process to obtain dynamic feature of signature. These features are extracted from the popular dataset SVC2004. The extracted feature is then passed to various classifiers named as Naive Bayes, random forest, J48, MLP, logistic regression and PART. The result of genuine and forge signatures is obtained in terms of precision, true positive rate, false positive rate, F-score, etc. The obtained result is then compared with the existing method with respect to false acceptance rate and false rejection rate.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {12347–12366},
numpages = {20},
keywords = {Precision, Fallout, Threshold value, False rejection rate (FRR), False acceptance rate (FAR), Classifiers}
}

@article{10.1016/j.jbi.2021.103763,
author = {Speiser, Jaime Lynn},
title = {A random forest method with feature selection for developing medical prediction models with clustered and longitudinal data},
year = {2021},
issue_date = {May 2021},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {117},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2021.103763},
doi = {10.1016/j.jbi.2021.103763},
journal = {J. of Biomedical Informatics},
month = may,
numpages = {11},
keywords = {ROC, MCC, DGP, LASSO, AUC, BiMM, GLMM, Health ABC, Binary mixed model forest, Clustered outcomes, Longitudinal outcomes, Variable selection, Feature selection, Random forest}
}

@article{10.1007/s10710-019-09358-0,
author = {Qiu, Chenye},
title = {A novel multi-swarm particle swarm optimization for feature selection},
year = {2019},
issue_date = {Dec 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1389-2576},
url = {https://doi.org/10.1007/s10710-019-09358-0},
doi = {10.1007/s10710-019-09358-0},
abstract = {A novel feature selection method based on a multi-swarm particle swarm optimization (MSPSO) is proposed in this paper. The canonical particle swarm optimization (PSO) has been widely used for&nbsp;feature selection problems. However, PSO suffers from stagnation in local optimal solutions and premature convergence in complex feature selection problems. This paper employs the multi-swarm topology in which the population is split into several small-sized sub-swarms. Particles in each sub-swarm update their positions with the guidance of the local best particle in its own sub-swarm. In order to promote information exchange among the sub-swarms, an elite learning strategy is introduced in which the elite particles in each sub-swarm learn from the useful information found by other sub-swarms. Moreover, a local search operator is proposed to improve the exploitation ability of each sub-swarm. MSPSO is able to improve the population diversity and better explore the entire feature space. The performance of the proposed method is compared with six PSO based wrappers, three traditional wrappers, and three popular filters on eleven datasets. Experimental results verify that MSPSO can find feature subsets with high classification accuracies and smaller numbers of features. The analysis of&nbsp;the search behavior of MSPSO demonstrates its effectiveness on maintaining population diversity and finding better feature subsets. The statistical test demonstrates that the superiority of MSPSO over other methods is significant.},
journal = {Genetic Programming and Evolvable Machines},
month = dec,
pages = {503–529},
numpages = {27},
keywords = {Local search operator, Elite learning strategy, Multi-swarm topology, Particle swarm optimization, Feature selection}
}

@article{10.1007/s00521-018-3455-8,
author = {Mostafa, Sheikh Shanawaz and Morgado-Dias, Fernando and Ravelo-Garc\'{\i}a, Antonio G.},
title = {Comparison of SFS and mRMR for oximetry feature selection in obstructive sleep apnea detection},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {20},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3455-8},
doi = {10.1007/s00521-018-3455-8},
abstract = {Obstructive sleep apnea is a disorder characterized by pauses in respiration during sleep. Due to this disturbance in breathing, there is a decrease in the oxygen saturation (SpO2) level. Thus, SpO2 can be used as a source of information for the automatic detection of apnea. Several solutions exist in the literature where different features are used. To find a better discriminant capacity, a subset of few features that obtains higher accuracy with the proper classifier is needed. To face this challenge, this work compares two different feature selection methods. The first one is a filter method named minimum redundancy maximum relevance, and the other one is called sequential forward search. These methods are tested with different classifiers. Two public datasets with 8 and 25 subjects are used to test and compare the performances of the different feature selection methods. A set of features for each classifier is obtained, and the results are compared with the previous work. The results found in this work show a good performance with respect to the state of the art and present a good option for apnea screening with low resources.},
journal = {Neural Comput. Appl.},
month = oct,
pages = {15711–15731},
numpages = {21},
keywords = {SpO2, Sleep apnea, SFS, mRMR, Feature section, Classification}
}

@inproceedings{10.1145/1143997.1144248,
author = {Mierswa, Ingo and Wurst, Michael},
title = {Information preserving multi-objective feature selection for unsupervised learning},
year = {2006},
isbn = {1595931864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143997.1144248},
doi = {10.1145/1143997.1144248},
abstract = {In this work we propose a novel, sound framework for evolutionary feature selection in unsupervised machine learning problems. We show that unsupervised feature selection is inherently multi-objective and behaves differently from supervised feature selection in that the number of features must be maximized instead of being minimized. Although this might sound surprising from a supervised learning point of view, we exemplify this relationship on the problem of data clustering and show that existing approaches do not pose the optimization problem in an appropriate way. Another important consequence of this paradigm change is a method which segments the Pareto sets produced by our approach. Inspecting only prototypical points from these segments drastically reduces the amount of work for selecting a final solution. We compare our methods against existing approaches on eight data sets.},
booktitle = {Proceedings of the 8th Annual Conference on Genetic and Evolutionary Computation},
pages = {1545–1552},
numpages = {8},
keywords = {unsupervised learning, multi-objective feature selection, Pareto front segmentation},
location = {Seattle, Washington, USA},
series = {GECCO '06}
}

@article{10.1016/j.asoc.2019.105866,
author = {Aladeemy, Mohammed and Adwan, Linda and Booth, Amy and Khasawneh, Mohammad T. and Poranki, Srikanth},
title = {New feature selection methods based on opposition-based learning and self-adaptive cohort intelligence for predicting patient no-shows},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {86},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105866},
doi = {10.1016/j.asoc.2019.105866},
journal = {Appl. Soft Comput.},
month = jan,
numpages = {19},
keywords = {Classification, Health care, Cohort intelligence, Metaheuristic, Opposition-based learning, Feature selection, Machine learning, Artificial intelligence}
}

@article{10.1016/j.procs.2021.07.038,
author = {Bakshi, Siddak Singh and Jaiswal, Raj K and Jaiswal, Ritika},
title = {Efficiency Check Using Cointegration and Machine Learning Approach: Crude Oil Futures Markets},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {191},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2021.07.038},
doi = {10.1016/j.procs.2021.07.038},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {304–311},
numpages = {8},
keywords = {XGboost, Support Vector Machine, Machine Learning, Market Efficiency, Crude Oil Futures}
}

@article{10.1016/j.eswa.2021.115497,
author = {Barucci, Emilio and Bonollo, Michele and Poli, Federico and Rroji, Edit},
title = {A machine learning algorithm for stock picking built on information based outliers},
year = {2021},
issue_date = {Dec 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {184},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2021.115497},
doi = {10.1016/j.eswa.2021.115497},
journal = {Expert Syst. Appl.},
month = dec,
numpages = {14},
keywords = {Classification algorithm, Private information, Technical analysis, Stock picking, Finance}
}

@article{10.1007/s10462-020-09948-w,
author = {Injadat, MohammadNoor and Moubayed, Abdallah and Nassif, Ali Bou and Shami, Abdallah},
title = {Machine learning towards intelligent systems: applications, challenges, and opportunities},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09948-w},
doi = {10.1007/s10462-020-09948-w},
abstract = {The emergence and continued reliance on the Internet and related technologies has resulted in the generation of large amounts of data that can be made available for analyses. However, humans do not possess the cognitive capabilities to understand such large amounts of data. Machine learning (ML) provides a mechanism for humans to process large amounts of data, gain insights about the behavior of the data, and make more informed decision based on the resulting analysis. ML has applications in various fields. This review focuses on some of the fields and applications such as education, healthcare, network security, banking and finance, and social media. Within these fields, there are multiple unique challenges that exist. However, ML can provide solutions to these challenges, as well as create further research opportunities. Accordingly, this work surveys some of the challenges facing the aforementioned fields and presents some of the previous literature works that tackled them. Moreover, it suggests several research opportunities that benefit from the use of ML to address these challenges.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {3299–3348},
numpages = {50},
keywords = {Research opportunities, Application fields, Data analytics, Machine learning}
}

@article{10.5555/3143203.3143248,
author = {Gaudioso, M. and Gorgone, E. and Labb, M. and Rodrguez-Cha, A.M.},
title = {Lagrangian relaxation for SVM feature selection},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {87},
number = {C},
issn = {0305-0548},
abstract = {Feature selection for SVM is stated as a Mixed Binary Linear Programming problem.Lagrangian relaxation and dual ascent are applied to the model.Both a lower and an upper bound available at each iteration.Numerical results on benchmark datasets are given. We discuss a Lagrangian-relaxation-based heuristics for dealing with feature selection in the Support Vector Machine (SVM) framework for binary classification. In particular we embed into our objective function a weighted combination of the L1 and L0 norm of the normal to the separating hyperplane. We come out with a Mixed Binary Linear Programming problem which is suitable for a Lagrangian relaxation approach.Based on a property of the optimal multiplier setting, we apply a consolidated nonsmooth optimization ascent algorithm to solve the resulting Lagrangian dual. In the proposed approach we get, at every ascent step, both a lower bound on the optimal solution as well as a feasible solution at low computational cost.We present the results of our numerical experiments on some benchmark datasets.},
journal = {Comput. Oper. Res.},
month = nov,
pages = {137–145},
numpages = {9},
keywords = {SVM classification, Nonsmooth optimization, Lagrangian relaxation, Feature selection}
}

@article{10.1016/j.dss.2019.03.011,
author = {Kozodoi, Nikita and Lessmann, Stefan and Papakonstantinou, Konstantinos and Gatsoulis, Yiannis and Baesens, Bart},
title = {A multi-objective approach for profit-driven feature selection in credit scoring},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {120},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2019.03.011},
doi = {10.1016/j.dss.2019.03.011},
journal = {Decis. Support Syst.},
month = may,
pages = {106–117},
numpages = {12},
keywords = {Genetic algorithm, Profit maximization, Credit scoring, Multi-objective optimization, Feature selection}
}

@article{10.1016/j.neucom.2012.10.028,
author = {Doquire, Gauthier and Verleysen, Michel},
title = {A graph Laplacian based approach to semi-supervised feature selection for regression problems},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {121},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2012.10.028},
doi = {10.1016/j.neucom.2012.10.028},
abstract = {Feature selection is a task of fundamental importance for many data mining or machine learning applications, including regression. Surprisingly, most of the existing feature selection algorithms assume the problems to address are either supervised or unsupervised, while supervised and unsupervised samples are often simultaneously available in real-world applications. Semi-supervised feature selection methods are thus necessary, and many solutions have been proposed recently. However, almost all of them exclusively tackle classification problems. This paper introduces a semi-supervised feature selection algorithm which is specifically designed for regression problems. It relies on the notion of Laplacian score, a quantity recently introduced in the unsupervised framework. Experimental results demonstrate the efficiency of the proposed algorithm.},
journal = {Neurocomput.},
month = dec,
pages = {5–13},
numpages = {9},
keywords = {Semi-supervised learning, Graph Laplacian, Feature selection}
}

@article{10.1016/j.ins.2019.07.096,
author = {Zhang, Yun and Kwong, Sam and Wang, Shiqi},
title = {Machine learning based video coding optimizations: A survey},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {506},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.07.096},
doi = {10.1016/j.ins.2019.07.096},
journal = {Inf. Sci.},
month = jan,
pages = {395–423},
numpages = {29},
keywords = {Versatile video coding, Deep learning, Convolutional neural network, Visual quality assessment, Mode decision, Machine learning, High efficiency video coding, Video coding}
}

@inproceedings{10.1007/978-3-030-89394-1_9,
author = {Porcino, Thiago and Rodrigues, Erick O. and Bernardini, Flavia and Trevisan, Daniela and Clua, Esteban},
title = {A Symbolic Machine Learning Approach for Cybersickness Potential-Cause Estimation},
year = {2021},
isbn = {978-3-030-89393-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89394-1_9},
doi = {10.1007/978-3-030-89394-1_9},
abstract = {Virtual reality (VR) and head-mounted displays are constantly gaining popularity in various fields such as education, military, entertainment, and bio/medical informatics. Although such technologies provide a high sense of immersion, they can also trigger symptoms of discomfort. This condition is called cybersickness (CS) and is quite popular in recent publications in the virtual reality context. This work proposes a novel experimental analysis using symbolic machine learning that ranks potential causes for CS. We estimate the CS causes and rank them according to their impact on the classification capabilities of CS. The experiments are performed using two distinct virtual reality games. We were able to identify that acceleration triggered cybersickness more frequently in a race game in contrast to a flight game. Furthermore, participants less experienced with VR are more prone to feel discomfort and this variable has a greater impact in the race game in contrast to the flight game, where the acceleration is not controlled by the user.},
booktitle = {Entertainment Computing – ICEC 2021: 20th IFIP TC 14 International Conference, ICEC 2021, Coimbra, Portugal, November 2–5, 2021, Proceedings},
pages = {115–126},
numpages = {12},
keywords = {Virtual reality, Cybersickness, Machine learning, Games},
location = {Coimbra, Portugal}
}

@article{10.1016/j.eswa.2016.01.021,
author = {Dadaneh, Behrouz Zamani and Markid, Hossein Yeganeh and Zakerolhosseini, Ali},
title = {Unsupervised probabilistic feature selection using ant colony optimization},
year = {2016},
issue_date = {July 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {53},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.01.021},
doi = {10.1016/j.eswa.2016.01.021},
abstract = {We proposed an unsupervised method to remove redundant and irrelevant features.The algorithm needs no learning algorithms and class label to select features.Similarity between features will be considered in computation of feature relevance. Feature selection (FS) is one of the most important fields in pattern recognition, which aims to pick a subset of relevant and informative features from an original feature set. There are two kinds of FS algorithms depending on the presence of information about dataset class labels: supervised and unsupervised algorithms. Supervised approaches utilize class labels of dataset in the process of feature selection. On the other hand, unsupervised algorithms act in the absence of class labels, which makes their process more difficult. In this paper, we propose unsupervised probabilistic feature selection using ant colony optimization (UPFS). The algorithm looks for the optimal feature subset in an iterative process. In this algorithm, we utilize inter-feature information which shows the similarity between the features that leads the algorithm to decreased redundancy in the final set. In each step of the ACO algorithm, to select the next potential feature, we calculate the amount of redundancy between current feature and all those which have been selected thus far. In addition, we utilize a matrix to hold ant related pheromone which shows the rate of the co-presence of every pair of features in solutions. Afterwards, features are ranked based on a probability function extracted from the matrix; then, their m-top is returned as the final solution. We compare the performance of UPFS with 15 well-known supervised and unsupervised feature selection methods using different classifiers (support vector machine, naive Bayes, and k-nearest neighbor) on 10 well-known datasets. The experimental results show the efficiency of the proposed method compared to the previous related methods.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {27–42},
numpages = {16},
keywords = {Unsupervised methods, Filter approaches, Feature selection, Classification accuracy, Ant colony optimization}
}

@article{10.1016/j.comnet.2019.04.027,
author = {Hosseini, Soodeh and Azizi, Mehrdad},
title = {The hybrid technique for DDoS detection with supervised learning algorithms},
year = {2019},
issue_date = {Jul 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.04.027},
doi = {10.1016/j.comnet.2019.04.027},
journal = {Comput. Netw.},
month = jul,
pages = {35–45},
numpages = {11},
keywords = {Hybrid mechanism, Incremental learning, Machine learning, DDoS}
}

@article{10.1007/s10916-019-1200-1,
author = {Venkatasalam, K. and Rajendran, P. and Thangavel, M.},
title = {Improving the Accuracy of Feature Selection in Big Data Mining Using Accelerated Flower Pollination (AFP) Algorithm},
year = {2019},
issue_date = {April     2019},
publisher = {Plenum Press},
address = {USA},
volume = {43},
number = {4},
issn = {0148-5598},
url = {https://doi.org/10.1007/s10916-019-1200-1},
doi = {10.1007/s10916-019-1200-1},
abstract = {In recent times, the main problem associated with big data analytics is its high dimensional data over the search space. Such data gathers continuously in search space making traditional algorithms infeasible for data mining in real time environment. Hence, feature selection is an important method to lighten the load during processing while inducing a model for mining. However, mining over such high dimensional data leads to formulation of optimal feature subset, which grows exponentially and leads to intractable computational demand. In this paper, a novel lightweight mechanism is used as a feature selection method, which solves the after effects arising with optimal feature selection. The feature selection in big data mining is done using accelerated flower pollination (AFP) algorithm. This method improves the accuracy of feature selection with reduced processing time. The proposed method is tested under larger set of data with high dimensionality to test the performance of proposed method.},
journal = {J. Med. Syst.},
month = apr,
pages = {1–11},
numpages = {11},
keywords = {Feature selection, Data mining, Big data mining, Accelerated flower pollination (AFP) algorithm}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.eswa.2018.12.033,
author = {Turabieh, Hamza and Mafarja, Majdi and Li, Xiaodong},
title = {Iterated feature selection algorithms with layered recurrent neural network for software fault prediction},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.12.033},
doi = {10.1016/j.eswa.2018.12.033},
journal = {Expert Syst. Appl.},
month = may,
pages = {27–42},
numpages = {16},
keywords = {Layered recurrent neural network, Feature selection, Software fault prediction}
}

@article{10.1007/s10115-018-1226-6,
author = {Malar, B. and Nadarajan, R. and Gowri Thangam, J.},
title = {A hybrid isotonic separation training algorithm with correlation-based isotonic feature selection for binary classification},
year = {2019},
issue_date = {Jun 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {59},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-018-1226-6},
doi = {10.1007/s10115-018-1226-6},
abstract = {Isotonic separation is a classification technique which constructs a model by transforming the training set into a linear programming problem (LPP). It is computationally expensive to solve large-scale LPPs using traditional methods when data set grows. This paper proposes a hybrid binary classification algorithm, meta-heuristic isotonic separation with particle swarm optimization and convergence criterion (MeHeIS---CPSO), in which a particle swarm optimization-based meta-heuristic is embedded in the training phase to find a solution for LPP. The proposed framework formulates the LPP as a directed acyclic graph (DAG) and arranges decision variables using topological sort. It obtains a new threshold value from training set and sets up a convergence criterion using this threshold. It also deploys a new correlation coefficient-based supervised feature selection technique to select isotonic features and improves predictive accuracy of the classifier. Experiments are conducted on publicly available data sets and synthetic data set. Theoretical, empirical, and statistical analyses show that MeHeIS---CPSO is superior to its predecessors in terms of training time and predictive ability on large data sets. It also outperforms state-of-the-art machine learning and isotonic classification techniques in terms of predictive performance on small- and large-scale data sets.},
journal = {Knowl. Inf. Syst.},
month = jun,
pages = {651–683},
numpages = {33},
keywords = {Particle swarm optimization, Meta-heuristic isotonic separation, Isotonic separation, Correlation coefficient-based feature selection}
}

@article{10.5555/3319200.3319207,
title = {A hybridised feature selection approach in molecular classification using CSO and GA},
year = {2019},
issue_date = {January 2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {59},
number = {2},
issn = {0952-8091},
abstract = {Feature selection in molecular classification is a basic area of research in chemoinformatics field. This paper introduces a hybrid approach that investigates the performances of chicken swarm optimisation CSO algorithm with genetic algorithms GA for feature selection and support vector machine SVM for classification. The purpose of this paper is to test the effect of elimination of the inconsequential and redundant features in chemical datasets to realise the success of the classification. The proposed algorithm was applied to four chemical datasets and proved superiority in achieving minimum classification error rate in comparison with different feature selection algorithms for molecular classification.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {165–174},
numpages = {10}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2934466.2962727,
author = {Zhang, Bo and Becker, Martin},
title = {Supporting product configuration in application engineering using EXConfig},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962727},
doi = {10.1145/2934466.2962727},
abstract = {Nowadays systems are often developed following the product line approach in order to reduce time to market, achieve lower cost, and ensure high quality. To this end, common and variable requirements of product variants are specified as features in a feature model, so that each product variant can be configured and customized along the development lifecycle. While developers in family engineering tend to use variability management tools (e.g., pure::variants) for feature modeling and developing core assets for reuse, such sophisticated tools might be too complicated and inappropriate for product configuration in application engineering. In order to solve this challenge, this paper introduces an Excel-based product configurator called EXConfig, which focuses on product line features in the problem space and supports staged product configuration in application engineering. This tool can be easily customized and integrated with other system design tools or variability management tools, which connects application engineering and family engineering in development. The usage of this tool and its integration have been validated several times in industry.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {324–327},
numpages = {4},
keywords = {variability configuration, feature model, excel, enterprise architect},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.jbi.2014.11.013,
author = {Kamkar, Iman and Gupta, Sunil Kumar and Phung, Dinh and Venkatesh, Svetha},
title = {Stable feature selection for clinical prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
volume = {53},
number = {C},
issn = {1532-0464},
url = {https://doi.org/10.1016/j.jbi.2014.11.013},
doi = {10.1016/j.jbi.2014.11.013},
abstract = {Display Omitted We model new application of Tree-Lasso for stable feature selection in healthcare.Tree-Lasso finds more stable features compared to other feature selection methods.Tree-Lasso results in better prediction accuracy compared to other methods.The features selected by Tree-Lasso are consistent with those used by clinicians. Modern healthcare is getting reshaped by growing Electronic Medical Records (EMR). Recently, these records have been shown of great value towards building clinical prediction models. In EMR data, patients' diseases and hospital interventions are captured through a set of diagnoses and procedures codes. These codes are usually represented in a tree form (e.g. ICD-10 tree) and the codes within a tree branch may be highly correlated. These codes can be used as features to build a prediction model and an appropriate feature selection can inform a clinician about important risk factors for a disease. Traditional feature selection methods (e.g. Information Gain, T-test, etc.) consider each variable independently and usually end up having a long feature list. Recently, Lasso and related l 1 -penalty based feature selection methods have become popular due to their joint feature selection property. However, Lasso is known to have problems of selecting one feature of many correlated features randomly. This hinders the clinicians to arrive at a stable feature set, which is crucial for clinical decision making process. In this paper, we solve this problem by using a recently proposed Tree-Lasso model. Since, the stability behavior of Tree-Lasso is not well understood, we study the stability behavior of Tree-Lasso and compare it with other feature selection methods. Using a synthetic and two real-world datasets (Cancer and Acute Myocardial Infarction), we show that Tree-Lasso based feature selection is significantly more stable than Lasso and comparable to other methods e.g. Information Gain, ReliefF and T-test. We further show that, using different types of classifiers such as logistic regression, naive Bayes, support vector machines, decision trees and Random Forest, the classification performance of Tree-Lasso is comparable to Lasso and better than other methods. Our result has implications in identifying stable risk factors for many healthcare problems and therefore can potentially assist clinical decision making for accurate medical prognosis.},
journal = {J. of Biomedical Informatics},
month = feb,
pages = {277–290},
numpages = {14},
keywords = {Tree-Lasso, Lasso, Feature stability, Feature selection, Classification}
}

@article{10.1007/s10462-020-09814-9,
author = {Gangavarapu, Tushaar and Jaidhar, C. D. and Chanduka, Bhabesh},
title = {Applicability of machine learning in spam and phishing email filtering: review and approaches},
year = {2020},
issue_date = {Oct 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {7},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09814-9},
doi = {10.1007/s10462-020-09814-9},
abstract = {With the influx of technological advancements and the increased simplicity in communication, especially through emails, the upsurge in the volume of unsolicited bulk emails (UBEs) has become a severe threat to global security and economy. Spam emails not only waste users’ time, but also consume a lot of network bandwidth, and may also include malware as executable files. Alternatively, phishing emails falsely claim users’ personal information to facilitate identity theft and are comparatively more dangerous. Thus, there is an intrinsic need for the development of more robust and dependable UBE filters that facilitate automatic detection of such emails. There are several countermeasures to spam and phishing, including blacklisting and content-based filtering. However, in addition to content-based features, behavior-based features are well-suited in the detection of UBEs. Machine learning models are being extensively used by leading internet service providers like Yahoo, Gmail, and Outlook, to filter and classify UBEs successfully. There are far too many options to consider, owing to the need to facilitate UBE detection and the recent advances in this domain. In this paper, we aim at elucidating on the way of extracting email content and behavior-based features, what features are appropriate in the detection of UBEs, and the selection of the most discriminating feature set. Furthermore, to accurately handle the menace of UBEs, we facilitate an exhaustive comparative study using several state-of-the-art machine learning algorithms. Our proposed models resulted in an overall accuracy of 99% in the classification of UBEs. The text is accompanied by snippets of Python code, to enable the reader to implement the approaches elucidated in this paper.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {5019–5081},
numpages = {63},
keywords = {Spam, Python, Phishing, Machine learning, Feature engineering}
}

@inproceedings{10.1007/978-3-030-17935-9_49,
author = {Adriano Gon\c{c}alves, Carlos and Lorenzo Iglesias, Eva and Borrajo, Lourdes and Camacho, Rui and Seara Vieira, Adri\'{a}n and Talma Gon\c{c}alves, C\'{e}lia},
title = {Comparative Study of Feature Selection Methods for Medical Full Text Classification},
year = {2019},
isbn = {978-3-030-17934-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-17935-9_49},
doi = {10.1007/978-3-030-17935-9_49},
abstract = {There is a lot of work in text categorization using only the title and abstract of the papers. However, in a full paper there is a much larger amount of information that could be used to improve the text classification performance. The potential benefits of using full texts come with an additional problem: the increased size of the data sets.To overcome the increased the size of full text data sets we performed an assessment study on the use of feature selection methods for full text classification. We have compared two existing feature selection methods (Information Gain and Correlation) and a novel method called k-Best-Discriminative-Terms. The assessment was conducted using the Ohsumed corpora. We have made two sets of experiments: using title and abstract only; and full text.The results achieved by the novel method show that the novel method does not perform well in small amounts of text like title and abstract but performs much better for the full text data sets and requires a much smaller number of attributes.},
booktitle = {Bioinformatics and Biomedical Engineering: 7th International Work-Conference, IWBBIO 2019, Granada, Spain, May 8-10, 2019, Proceedings, Part II},
pages = {550–560},
numpages = {11},
keywords = {Text classification, Feature selection, Medical texts corpus},
location = {Granada, Spain}
}

@article{10.1016/j.patcog.2017.02.025,
author = {Li, Feng and Miao, Duoqian and Pedrycz, Witold},
title = {Granular multi-label feature selection based on mutual information},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {67},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2017.02.025},
doi = {10.1016/j.patcog.2017.02.025},
abstract = {We granulate the label space into information granules to exploit label dependency.We present a multi-label maximal correlation minimal redundancy criterion.The proposed method can select compact and specific feature subsets.The proposed method can significantly improve the algorithm performance. Like the traditional machine learning, the multi-label learning is faced with the curse of dimensionality. Some feature selection algorithms have been proposed for multi-label learning, which either convert the multi-label feature selection problem into numerous single-label feature selection problems, or directly select features from the multi-label data set. However, the former omit the label dependency, or produce too many new labels leading to learning with significant difficulties; the latter, taking the global label dependency into consideration, usually select a few redundant or irrelevant features, because actually not all labels depend on each other, which may confuse the algorithm and degrade its classification performance. To select a more relevant and compact feature subset as well as explore the label dependency, a granular feature selection method for multi-label learning is proposed with a maximal correlation minimal redundancy criterion based on mutual information. The maximal correlation minimal redundancy criterion makes sure that the selected feature subset contains the most class-discriminative information, while in the meantime exhibits the least intra-redundancy. Granulation can help explore the label dependency. We study the relation of the label granularity and the performance on four data sets, and compare the proposed method with other three multi-label feature selection methods. The experimental results demonstrate that the proposed method can select compact and specific feature subsets, improve the classification performance and performs better than other three methods on the widely-used multi-label learning evaluation criteria.},
journal = {Pattern Recogn.},
month = jul,
pages = {410–423},
numpages = {14},
keywords = {Mutual information, Multi-label learning, Granular computing, Feature selection}
}

@article{10.1007/s11227-018-2618-9,
author = {Chen, Mu-Song and Hwang, Chi-Pan and Ho, Tze-Yee and Wang, Hsuan-Fu and Shih, Chih-Min and Chen, Hsing-Yu and Liu, Wen Kai},
title = {Driving behaviors analysis based on feature selection and statistical approach: a preliminary study},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {75},
number = {4},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2618-9},
doi = {10.1007/s11227-018-2618-9},
abstract = {Due to the prevalence of IoV technology, big data has increasingly been promoted as a revolutionary development in a variety of applications. Indeed, the received big data from IoV is valuable particularly for those involved in analyzing driver's behaviors. For instance, in the fleet management domain, fleet administrators are interested in fine-grained information about fleet usage, which is influenced by different driver usage patterns. In the vehicle insurance market, usage-based insurance or pay-as-you-drive schemes aim to adapt the insurance premium to individual driver behavior or even to provide various value-added services to policy holders. These applications can be expected to improve and to make safer the driving style of various individuals. Nowadays, big data analysis is becoming indispensable for automatic discovering of intelligence that is involved in the frequently occurring patterns and hidden rules. It is essential and necessary to study how to utilize these large-scale data. Regarding driving behaviors analysis, this paper presents a preliminary study based on feature selection and statistical approach. Feature selection is one of the important and frequently used techniques in data preprocessing for big data mining. Feature selection, as a dimensionality reduction technique, focuses on choosing a small subset of the significant features from the original data by removing irrelevant or redundant features. According to selection process, the most significant feature is vehicle speed for the collected vehicular data. Afterward, the statistical approach calculates skewness and dispersion in speed distribution as the statistical features for driving behaviors analysis. Finally, the established classification rules not only provide data-driven services and big data analytics but also offer training data samples for supervised machine learning algorithms. To validate the feasibility of the proposed method, over 150 drivers and more than 200,000 trips are verified in the simulation. As expected, experimental results are well matched with our observations.},
journal = {J. Supercomput.},
month = apr,
pages = {2007–2026},
numpages = {20},
keywords = {Statistical approach, IoV, Feature selection, Driving behaviors, Big data}
}

@article{10.1007/s42979-019-0037-5,
author = {Niedzielewski, Karol and Marchwiany, Maciej E. and Piliszek, Radoslaw and Michalewicz, Marek and Rudnicki, Witold},
title = {Multidimensional Feature Selection and High Performance ParalleX: A tool for detection of informative variables for big data},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {1},
url = {https://doi.org/10.1007/s42979-019-0037-5},
doi = {10.1007/s42979-019-0037-5},
abstract = {Great amount of stored information used in connection with Machine Learning and statistical methods enables high quality insight and analysis of data that leads to design of high precision predictive and classification systems. In the process of analysis, selection of most informative features is crucial for later quality of the designed system. In this report, we propose two implementations of multidimensional feature selection (MDFS) algorithm (Piliszek et&nbsp;al. in Mdfs-multidimensional feature selection. arXiv preprint. , 2018) that can be used in distributed environments for detection of all-relevant variables in data sets with discrete decision variable. While most methods discard information about interactions between features, MDFS is designed towards identification of informative variables that are not relevant when considered alone but are relevant in groups. We have developed software using C++ and High Performance ParalleX (HPX) (Kaiser et&nbsp;al. in STEllAR-GROUP/hpx: HPX V1.3.0: the C++ Standards library for parallelism and concurrency. 2019. https://doi.org/10.5281/zenodo.3189323, 2019) to achieve best performance, great scalability and portability. HPX is a library that uses lightweight threads, asynchronous communication, and asynchronous task submission based on the declarative criteria of work. These features enabled us to deeply explore granularity and parallelism of the MDFS algorithm. Software is prepared entirely in C++; therefore, calculations can be performed using CPUs on desktops, distributed systems, and any system with C++ compiler support. During testing on Cray XC40 (Okeanos) using artificially prepared data, we achieved 196 times acceleration on 256 nodes compared to a single node. From this point, ICM computing facility is capable of massively parallel feature engineering. The main purpose of the software is to enable researchers for more accurate genomics data analysis in search for multiple correlations in potential sources of the diseases.},
journal = {SN Comput. Sci.},
month = oct,
numpages = {7},
keywords = {Genomics, Big data, Distributed systems, HPX, Mutual information, Multidimensional feature selection}
}

@inproceedings{10.1145/3377929.3389857,
author = {Xue, Bing and Zhang, Mengjie},
title = {Evolutionary computation for feature selection and feature construction},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3389857},
doi = {10.1145/3377929.3389857},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {1283–1312},
numpages = {30},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@article{10.1155/2021/7890923,
author = {Pan, Xiaohui and Yau, Wei-Chuen},
title = {Quantitative Analysis and Prediction of Global Terrorist Attacks Based on Machine Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/7890923},
doi = {10.1155/2021/7890923},
abstract = {Terrorist attacks pose a great threat to global security, and their analysis and prediction are imperative. Considering the high frequency of terrorist attacks and the inherent difficulty in finding related terrorist organizations, we propose a classification framework based on ensemble learning for classifying and predicting terrorist organizations. The framework includes data preprocessing, data splitting, five classifier prediction models, and model evaluation. Based on a quantitative statistical analysis of terrorist organization activities in GTD from 1970 to 2017 and feature selection using the SelectKBest method in scikit learn, we constructed five classification and prediction models of terrorist organizations, namely, decision tree, bagging, random forest, extra tree, and XGBoost, and utilized a 10-fold cross-validation method to verify the performance and stability of the proposed model. Experimental results showed that the five models achieved excellent performance. The XGBoost and random forest models achieved the best accuracies (97.16% and 96.82%, respectively) of predicting 32 terrorist organizations with the highest attack frequencies. The proposed classifier framework is useful for the accurate and efficient prediction of terrorist organizations responsible for attacks and can be extended to predict all terrorist organizations.},
journal = {Sci. Program.},
month = jan,
numpages = {15}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.4018/IJIIT.2021070105,
author = {Adel, Alti and Farid, Ayeche},
title = {Performance Evaluation of Machine Learning for Recognizing Human Facial Emotions},
year = {2021},
issue_date = {Jul 2021},
publisher = {IGI Global},
address = {USA},
volume = {17},
number = {3},
issn = {1548-3657},
url = {https://doi.org/10.4018/IJIIT.2021070105},
doi = {10.4018/IJIIT.2021070105},
abstract = {Facial expression recognition is a human emotion classification problem attracting much attention from scientific research. Classifying human emotions can be a challenging task for machines. However, more accurate results and less execution time are still the issues when extracting features of human emotions. To cope with these challenges, the authors propose an automatic system that provides users with a well-adopted classifier for recognizing facial expressions in a more accurate manner. The system is based on two fundamental machine learning stages, namely feature selection and feature classification. Feature selection is realized by active shape model (ASM) composed of landmarks while the feature classification algorithm is based on seven well-known classifiers. The authors have used CK+ dataset, implemented and tested seven classifiers to find the best classifier. The experimental results show that quadratic classifier (DA) provides excellent performance, and it outperforms the other classifiers with the highest recognition rate of 100% for the same dataset.},
journal = {Int. J. Intell. Inf. Technol.},
month = jul,
pages = {1–17},
numpages = {17},
keywords = {Active Shape Model, Generalized Procrust Analysis, Human Facial Emotions, Machine Learning, Quadratic Classifier}
}

@article{10.1016/j.ins.2017.09.028,
author = {Hancer, Emrah and Xue, Bing and Zhang, Mengjie and Karaboga, Dervis and Akay, Bahriye},
title = {Pareto front feature selection based on artificial bee colony optimization},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {422},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2017.09.028},
doi = {10.1016/j.ins.2017.09.028},
abstract = {Feature selection has two major conflicting aims, i.e., to maximize the classification performance and to minimize the number of selected features to overcome the curse of dimensionality. To balance their trade-off, feature selection can be handled as a multi-objective problem. In this paper, a feature selection approach is proposed based on a new multi-objective artificial bee colony algorithm integrated with non-dominated sorting procedure and genetic operators. Two different implementations of the proposed approach are developed: ABC with binary representation and ABC with continuous representation. Their performance are examined on 12 benchmark datasets and the results are compared with those of linear forward selection, greedy stepwise backward selection, two single objective ABC algorithms and three well-known multi-objective evolutionary computation algorithms. The results show that the proposed approach with the binary representation outperformed the other methods in terms of both the dimensionality reduction and the classification accuracy.},
journal = {Inf. Sci.},
month = jan,
pages = {462–479},
numpages = {18},
keywords = {Multi-objective optimization, Feature selection, Classification, Artificial bee colony}
}

@article{10.1016/j.jnca.2020.102577,
author = {Rovetta, Stefano and Suchacka, Gra\.{z}yna and Masulli, Francesco},
title = {Bot recognition in a Web store: An approach based on unsupervised learning},
year = {2020},
issue_date = {May 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {157},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2020.102577},
doi = {10.1016/j.jnca.2020.102577},
journal = {J. Netw. Comput. Appl.},
month = may,
numpages = {15},
keywords = {Web server, Machine learning, Unsupervised classification, Supervised classification, Web bot detection, Internet robot, Web bot}
}

@article{10.1145/3470974,
author = {Park, Jurn-Gyu and Dutt, Nikil and Lim, Sung-Soo},
title = {An Interpretable Machine Learning Model Enhanced Integrated CPU-GPU DVFS Governor},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3470974},
doi = {10.1145/3470974},
abstract = {Modern heterogeneous CPU-GPU-based mobile architectures, which execute intensive mobile gaming/graphics applications, use software governors to achieve high performance with energy-efficiency. However, existing governors typically utilize simple statistical or heuristic models, assuming linear relationships using a small unbalanced dataset of mobile games; and the limitations result in high prediction errors for dynamic and diverse gaming workloads on heterogeneous platforms. To overcome these limitations, we propose an interpretable machine learning (ML) model enhanced integrated CPU-GPU governor: (1) It builds tree-based piecewise linear models (i.e., model trees) offline considering both high accuracy (low error) and interpretable ML models based on mathematical formulas using a simulatability operation counts quantitative metric. And then (2) it deploys the selected models for online estimation into an integrated CPU-GPU Dynamic Voltage Frequency Scaling governor. Our experiments on a test set of 20 mobile games exhibiting diverse characteristics show that our governor achieved significant energy efficiency gains of over 10% (up to 38%) improvements on average in energy-per-frame with a surprising-but-modest 3% improvement in Frames-per-Second performance, compared to a typical state-of-the-art governor that employs simple linear regression models.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {108},
numpages = {28},
keywords = {interpretable machine learning models, model-based design, integrated GPU, dynamic voltage and frequency scaling (DVFS), power management policies, Machine learning techniques}
}

@article{10.1007/s00521-019-04477-2,
author = {Velliangiri, S. and Karthikeyan, P.},
title = {Hybrid optimization scheme for intrusion detection using considerable feature selection},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {12},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-019-04477-2},
doi = {10.1007/s00521-019-04477-2},
abstract = {The intrusion detection is an essential section in network security because of its immense volume of threats which bothers the computing systems. The real-time intrusion detection dataset comprises redundant or irrelevant features. The duplicate features make it quite challenging to locate the patterns for intrusion detection. Hybrid optimization scheme (HOS) is designed for combining adaptive artificial bee colony (AABC) with adaptive particle swarm optimization (APSO) for detecting intrusive activities. The schemes are aggregated for locating improved optimization-based outcomes, and the precision during categorization is acquired using tenfold cross-validation scheme. The main objective of the proposed method is to improve the rate of precision in intrusion activities in internetwork by choosing the relevant features. Effectiveness of the hybrid categorization scheme is accessed using an NSL-KDD dataset. Single feature selection method and random feature selection method are used to assess the proposed HOS intrusion detection approaches. The effectiveness of the designed scheme is evaluated with existing machine learning schemes such as Naive Bayes, AABC, APSO, and support vector machine, which outperform the HOS.},
journal = {Neural Comput. Appl.},
month = jun,
pages = {7925–7939},
numpages = {15},
keywords = {Hybrid optimization scheme, Support vector machine, APSO, AABC, Intrusion detection}
}

@article{10.1007/s00521-018-3655-2,
author = {Veredas, Francisco J. and Urda, Daniel and Subirats, Jos\'{e} L. and Cant\'{o}n, Francisco R. and Aledo, Juan C.},
title = {Combining feature engineering and feature selection to improve the prediction of methionine oxidation sites in proteins},
year = {2020},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {32},
number = {2},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-018-3655-2},
doi = {10.1007/s00521-018-3655-2},
abstract = {Methionine is a proteinogenic amino acid that can be post-translationally modified. It is now well established that reactive oxygen species can oxidise methionine residues within living cells. For a long time, it has been thought that such a modification represents merely an inevitable damage derived from aerobic metabolism. However, several authors have begun to contemplate a possible role for this methionine modification in cell signalling. During the last years, a number of proteomic studies have been carried out with the purpose of detecting proteins containing oxidised methionines. Although these proteomic works allow to pinpoint those methionines being oxidised, they are also arduous, expensive and time-consuming. For these reasons, computational approaches aimed at predicting methionine oxidation sites in proteins become an appealing alternative. In the current work, we address methionine oxidation prediction by combining computational intelligence methods with feature engineering and feature selection techniques to improve the efficacy of several machine learning models, while reducing the number of input characteristics needed to get high accuracy rates. We compare random forests, support vector machines, neural networks and flexible discriminant analysis models. Random forests give the best AUC (0.8124±0.0334) and accuracy rates (0.7590±0.0551) by using only a reduced set of 16 characteristics. These results surpass the outcomes of previous works. In addition, we present an end-user script that has been developed to take a protein ID as an input and return a list with the oxidation state of all the methionine residues found in the analysed protein. Finally, to illustrate the applicability of this tool, we have selected the human α1-antitrypsin protein as a case study. This protein was selected because it was not present among the set of proteins used to build up the predictive models but the protein has been well characterised experimentally in terms of methionine oxidation. The prediction returned by our script fully matches the empirical evidence. Out of the nine methionine residues found in this protein, our model predicts the oxidation of only two of them, M351 and M358, which have been reported, on the base of mass spectrometry analyses, to be particularly susceptible to oxidation.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {323–334},
numpages = {12},
keywords = {Predictive computational model, Methionine oxidation, Post-translational modification, Protein prediction}
}

@article{10.1007/s10044-017-0668-x,
author = {Goswami, Saptarsi and Das, Amit Kumar and Guha, Priyanka and Tarafdar, Arunabha and Chakraborty, Sanjay and Chakrabarti, Amlan and Chakraborty, Basabi},
title = {An approach of feature selection using graph-theoretic heuristic and hill climbing},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {1433-7541},
url = {https://doi.org/10.1007/s10044-017-0668-x},
doi = {10.1007/s10044-017-0668-x},
abstract = {Search-based methods that use matrix- or vector-based representations of the dataset are commonly employed to solve the problem of feature selection. These methods are more generalized and easy to apply. Recently, a set of algorithms have started using graph-based representation of the dataset instead of the traditional representations. These methods require additional modelling as the dataset needs to be represented as a graph. However, graph-based methods help in visualizing inter-feature relationship based on which graph-theoretic principles can be applied to identify good-quality feature subsets. A combination of the graph-based representation with traditional search techniques has the potential to increase model performance as well as interpretability. As per literature study, there is hardly any method which combines these approaches. In this paper, we have proposed a feature selection algorithm, which represents the dataset as a graph and then uses maximal independent sets and minimal vertex covers to improve traditional hill climbing search. The proposed method produces statistically significant improvement over (i) hill climbing, (ii) standard search-based methods and (iii) pure graph-based methods.},
journal = {Pattern Anal. Appl.},
month = may,
pages = {615–631},
numpages = {17},
keywords = {Hill climbing, Graph-theoretic approach, Feature selection, Feature Association Map}
}

@inproceedings{10.1007/978-3-030-85613-7_19,
author = {Desolda, Giuseppe and Esposito, Andrea and Lanzilotti, Rosa and Costabile, Maria F.},
title = {Detecting Emotions Through Machine Learning for Automatic UX Evaluation},
year = {2021},
isbn = {978-3-030-85612-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-85613-7_19},
doi = {10.1007/978-3-030-85613-7_19},
abstract = {Although User eXperience (UX) is widely acknowledged as an important aspect of software products, its evaluation is often neglected during the development of most software products, primarily because developers think that it is resource-demanding and complain about the fact that is scarcely automated. Various attempts have been made to develop tools that support and automate the execution of tests with users. This paper is about an ongoing research work that exploits Machine Learning (ML) for automatic UX evaluation, specifically for understanding users’ emotions by analyzing the log data of the users’ interactions with websites. The approach described aims at overcoming some limitations of existing proposals based on ML.},
booktitle = {Human-Computer Interaction – INTERACT 2021: 18th IFIP TC 13 International Conference, Bari, Italy, August 30 – September 3, 2021, Proceedings, Part III},
pages = {270–279},
numpages = {10},
keywords = {Automatic UX evaluation, User eXperience, Usability},
location = {Bari, Italy}
}

@article{10.1504/ijdmb.2021.122862,
author = {Sarkhosh, Seyyed Mahdi Hosseini and Taghvaei, Mahdieh},
title = {Identification and prioritisation of diabetic nephropathy risk factors in diabetes patients using machine learning approach},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {25},
number = {3–4},
issn = {1748-5673},
url = {https://doi.org/10.1504/ijdmb.2021.122862},
doi = {10.1504/ijdmb.2021.122862},
abstract = {A significant microvascular complication in diabetic patients is diabetic nephropathy (DN). Despite the fact that different risk factors were previously identified for DN, machine learning (ML) techniques can confirm the importance of the predictive factors and determine their priority. Hence, this research is primarily aimed to identify and prioritise DN predictive risk factors among patients suffering from type 2 diabetes mellitus (T2DM) using the ML techniques. The characteristics of 2703 patients with T2DM are obtained from the dataset of the National Health and Nutrition Examination Survey (NHANES). Then, the recursive feature elimination using the cross-validation (RFECV) technique is used to select the essential factors. Next, five classification algorithms are utilised to construct the predictive model. The results confirm the known predictors for DN and emphasise the importance of controlling hypertension, lipid, weight, glucose, and uric acid in patients suffering from T2DM.},
journal = {Int. J. Data Min. Bioinformatics},
month = jan,
pages = {216–233},
numpages = {17},
keywords = {NHANES, risk factors, machine learning, diabetic nephropathy}
}

@article{10.1016/j.patcog.2016.08.011,
author = {Wang, Yintong and Wang, Jiandong and Liao, Hao and Chen, Haiyan},
title = {An efficient semi-supervised representatives feature selection algorithm based on information theory},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {61},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.08.011},
doi = {10.1016/j.patcog.2016.08.011},
abstract = {Feature selection (FS) plays an important role in data mining and recognition, especially regarding large scale text, images and biological data. The Markov blanket provides a complete and sound solution to the selection of optimal features in supervised feature selection, and investigates thoroughly the relevance of features relating to class and the conditional independence relationship between features. However, incomplete label information makes it particularly difficult to acquire the optimal feature subset. In this paper, we propose a novel algorithm called the Semi-supervised Representatives Feature Selection algorithm based on information theory (SRFS), which is independent of any algorithm used for classification learning, and can rapidly and effectively identify and remove non-essential information and irrelevant and redundant features. More importantly, the unlabeled data are utilized in the Markov blanket as the labeled data through the relevance gain. Our results on several benchmark datasets demonstrate that SRFS can significantly improve upon state of the art supervised and semi-supervised algorithms. HighlightsA relevance gain framework by which the relevance of features can be measured in the unlabeled data.The partition of the directed acyclic graph to cluster the redundant features.Extend the existing Markov blanket algorithms to exploit the information of the unlabeled data.},
journal = {Pattern Recogn.},
month = jan,
pages = {511–523},
numpages = {13},
keywords = {Semi-supervised learning, Representative features, Markov blanket, Information theory, Feature selection}
}

@inproceedings{10.1145/2505515.2505542,
author = {Hindawi, Mohammed and Benabdeslem, Khalid},
title = {Local-to-global semi-supervised feature selection},
year = {2013},
isbn = {9781450322638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2505515.2505542},
doi = {10.1145/2505515.2505542},
abstract = {Variable-weighting approaches are well-known in the context of embedded feature selection. Generally, this task is performed in a global way, when the algorithm selects a single cluster-independent subset of features (global feature selection). However, there exist other approaches that aim to select cluster-specific subsets of features (local feature selection). Global and local feature selection have different objectives, nevertheless, in this paper we propose a novel embedded approach which locally weights the variables towards a global feature selection. The proposed approach is presented in the semi-supervised paradigm. Experiments on some known data sets are presented to validate our model and compare it with some representative methods.},
booktitle = {Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management},
pages = {2159–2168},
numpages = {10},
keywords = {constraints, feature selection, semi-supervised learning, variable weighting},
location = {San Francisco, California, USA},
series = {CIKM '13}
}

@article{10.1016/j.comnet.2021.108474,
author = {Ara\'{u}jo, Samuel M.A. and de Souza, Fernanda S.H. and Mateus, Geraldo R.},
title = {A hybrid optimization-Machine Learning approach for the VNF placement and chaining problem},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {199},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2021.108474},
doi = {10.1016/j.comnet.2021.108474},
journal = {Comput. Netw.},
month = nov,
numpages = {18},
keywords = {Online, Chaining, Placement, Machine Learning, Optimization, NFV, VNF}
}

@article{10.1145/3465171,
author = {Miao, Yuantian and Chen, Chao and Pan, Lei and Han, Qing-Long and Zhang, Jun and Xiang, Yang},
title = {Machine Learning–based Cyber Attacks Targeting on Controlled Information: A Survey},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3465171},
doi = {10.1145/3465171},
abstract = {Stealing attack against controlled information, along with the increasing number of information leakage incidents, has become an emerging cyber security threat in recent years. Due to the booming development and deployment of advanced analytics solutions, novel stealing attacks utilize machine learning (ML) algorithms to achieve high success rate and cause a lot of damage. Detecting and defending against such attacks is challenging and urgent so governments, organizations, and individuals should attach great importance to the ML-based stealing attacks. This survey presents the recent advances in this new type of attack and corresponding countermeasures. The ML-based stealing attack is reviewed in perspectives of three categories of targeted controlled information, including controlled user activities, controlled ML model-related information, and controlled authentication information. Recent publications are summarized to generalize an overarching attack methodology and to derive the limitations and future directions of ML-based stealing attacks. Furthermore, countermeasures are proposed towards developing effective protections from three aspects—detection, disruption, and isolation.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {139},
numpages = {36},
keywords = {machine learning, information leakage, cyber security, controlled information, Cyber attacks}
}

@article{10.1007/s00180-020-00970-8,
author = {Sambasivan, Rajiv and Das, Sourish and Sahu, Sujit K.},
title = {A Bayesian perspective of statistical machine learning for big data},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {3},
issn = {0943-4062},
url = {https://doi.org/10.1007/s00180-020-00970-8},
doi = {10.1007/s00180-020-00970-8},
abstract = {Statistical Machine Learning (SML) refers to a body of algorithms and methods by which computers are allowed to discover important features of input data sets which are often very large in size. The very task of feature discovery from data is essentially the meaning of the keyword ‘learning’ in SML. Theoretical justifications for the effectiveness of the SML algorithms are underpinned by sound principles from different disciplines, such as Computer Science and Statistics. The theoretical underpinnings particularly justified by statistical inference methods are together termed as statistical learning theory. This paper provides a review of SML from a Bayesian decision theoretic point of view—where we argue that many SML techniques are closely connected to making inference by using the so called Bayesian paradigm. We discuss many important SML techniques such as supervised and unsupervised learning, deep learning, online learning and Gaussian processes especially in the context of very large data sets where these are often employed. We present a dictionary which maps the key concepts of SML from Computer Science and Statistics. We illustrate the SML techniques with three moderately large data sets where we also discuss many practical implementation issues. Thus the review is especially targeted at statisticians and computer scientists who are aspiring to understand and apply SML for moderately large to big data sets.},
journal = {Comput. Stat.},
month = sep,
pages = {893–930},
numpages = {38},
keywords = {Statistical learning, Machine learning, Big data, Bayesian methods}
}

@article{10.3233/IDA-205302,
author = {Singla, Sandeep Kumar and Garg, Rahul Dev and Dubey, Om Prakash},
title = {Ensemble machine learning methods for spatio-temporal data analysis of plant and ratoon sugarcane},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {5},
issn = {1088-467X},
url = {https://doi.org/10.3233/IDA-205302},
doi = {10.3233/IDA-205302},
abstract = {Recent technological enhancements in the field of information technology and statistical techniques allowed the sophisticated and reliable analysis based on machine learning methods. A number of machine learning data analytical tools may be exploited for the classification and regression problems. These tools and techniques can be effectively used for the highly data-intensive operations such as agricultural and meteorological applications, bioinformatics and stock market analysis based on the daily prices of the market. Machine learning ensemble methods such as Decision Tree (C5.0), Classification and Regression (CART), Gradient Boosting Machine (GBM) and Random Forest (RF) has been investigated in the proposed work. The proposed work demonstrates that temporal variations in the spectral data and computational efficiency of machine learning methods may be effectively used for the discrimination of types of sugarcane. The discrimination has been considered as a binary classification problem to segregate ratoon from plantation sugarcane. Variable importance selection based on Mean Decrease in Accuracy (MDA) and Mean Decrease in Gini (MDG) have been used to create the appropriate dataset for the classification. The performance of the binary classification model based on RF is the best in all the possible combination of input images. Feature selection based on MDA and MDG measures of RF is also important for the dimensionality reduction. It has been observed that RF model performed best with 97% accuracy, whereas the performance of GBM method is the lowest. Binary classification based on the remotely sensed data can be effectively handled using random forest method.},
journal = {Intell. Data Anal.},
month = jan,
pages = {1291–1322},
numpages = {32},
keywords = {Landsat-8, NDVI, KNN, GBM, CART, Random forest}
}

@inproceedings{10.1145/1842752.1842773,
author = {McGregor, John D.},
title = {A method for analyzing software product line ecosystems},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842773},
doi = {10.1145/1842752.1842773},
abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {73–80},
numpages = {8},
keywords = {software product line, software ecosystem},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1016/j.neucom.2016.10.062,
author = {Wang, Shuqin and Wei, Jinmao},
title = {Feature selection based on measurement of ability to classify subproblems},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {224},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.10.062},
doi = {10.1016/j.neucom.2016.10.062},
abstract = {Feature selection is important and necessary especially for processing large scale data. Existing feature selection methods generally compute a discriminant value with respect to class variable for a feature to indicate its classification ability. Such a scalar value can hardly reveal the multi-faceted classification abilities of a feature for the different subproblems in a classification task. In this paper, an effective way is proposed for feature selection based on measurement of ability to classify subproblems and discrimination structure complementarity of features. The classification abilities of a feature for different subproblems are calculated respectively. Hence for the feature, a discrimination structure vector representing its classification abilities for all subproblems can be obtained. In feature selection, the features, which can individually classify as many subproblems as possible, are firstly evaluated and selected. Subsequently, their complementary features are selectively chosen, which can complementarily classify the subproblems that the selected features cannot classify. Two algorithms are designed for progressively selecting features, by firstly eliminating irrelevant features and then abandoning redundant features based on discrimination structure complementarity. The proposed algorithms are compared with some related methods for feature selection on some open gene expression datasets and UCI datasets. Experimental results demonstrate the effectiveness of the proposed method.},
journal = {Neurocomput.},
month = feb,
pages = {155–165},
numpages = {11},
keywords = {Fisher's discriminant ratio, Feature selection, Discrimination structure complementarity}
}

@article{10.1016/j.ins.2015.07.041,
author = {Garc\'{\i}a-Torres, Miguel and G\'{o}mez-Vela, Francisco and Meli\'{a}n-Batista, Bel\'{e}n and Moreno-Vega, J. Marcos},
title = {High-dimensional feature selection via feature grouping},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {326},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2015.07.041},
doi = {10.1016/j.ins.2015.07.041},
abstract = {We introduce the concept of predominant group based on the idea of Markov blanket to identify groups of correlated features.We propose a greedy strategy (GreedyPGG) that groups features based on the concept of predominant groups.We propose a VNS metaheuristic that uses the GreedyPGG strategy to reduce the dimensionality in high-dimensional data.Results show that VNS finds smaller subsets of features without degrading the predictive model. In recent years, advances in technology have led to increasingly high-dimensional datasets. This increase of dimensionality along with the presence of irrelevant and redundant features make the feature selection process challenging with respect to efficiency and effectiveness. In this context, approximate algorithms are typically applied since they provide good solutions in a reasonable time. On the other hand, feature grouping has arisen as a powerful approach to reduce dimensionality in high-dimensional data. Recently, some authors have focused their attention on developing methods that combine feature grouping and feature selection to improve the model. In this paper, we propose a feature selection strategy that utilizes feature grouping to increase the effectiveness of the search. As feature selection strategy, we propose a Variable Neighborhood Search (VNS) metaheuristic. Then, we propose to group the input space into subsets of features by using the concept of Markov blankets. To the best of our knowledge, this is the first time in which the Markov blanket is used for grouping features. We test the performance of VNS by conducting experiments on several high-dimensional datasets from two different domains: microarray and text mining. We compare VNS with popular and competitive techniques. Results show that VNS is a competitive strategy capable of finding a small size of features with similar predictive power than that obtained with other algorithms used in this study.},
journal = {Inf. Sci.},
month = jan,
pages = {102–118},
numpages = {17},
keywords = {Metaheuristic, High dimensionality, Feature selection, Feature grouping}
}

@article{10.1016/j.neucom.2016.12.045,
author = {Jimnez, F. and Snchez, G. and Garca, J.M. and Sciavicco, G. and Miralles, L.},
title = {Multi-objective evolutionary feature selection for online sales forecasting},
year = {2017},
issue_date = {April 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {234},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.12.045},
doi = {10.1016/j.neucom.2016.12.045},
abstract = {Sales forecasting uses historical sales figures, in association with products characteristics and peculiarities, to predict short-term or long-term future performance in a business, and it can be used to derive sound financial and business plans. By using publicly available data, we build an accurate regression model for online sales forecasting obtained via a novel feature selection methodology composed by the application of the multi-objective evolutionary algorithm ENORA (Evolutionary NOn-dominated Radial slots based Algorithm) as search strategy in a wrapper method driven by the well-known regression model learner Random Forest. Our proposal integrates feature selection for regression, model evaluation, and decision making, in order to choose the most satisfactory model according to an a posteriori process in a multi-objective context. We test and compare the performances of ENORA as multi-objective evolutionary search strategy against a standard multi-objective evolutionary search strategy such as NSGA-II (Non-dominated Sorted Genetic Algorithm), against a classical backward search strategy such as RFE (Recursive Feature Elimination), and against the original data set.},
journal = {Neurocomput.},
month = apr,
pages = {75–92},
numpages = {18},
keywords = {Regression model, Random forest, Online sales forecasting, Multi-objective evolutionary algorithms, Feature selection}
}

@article{10.1155/2021/6685396,
author = {Li, Yuwen and Wei, Shoushui and Liu, Xing and Zhang, Zhimin and Li, Jianxin},
title = {A Novel Robust Fuzzy Rough Set Model for Feature Selection},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/6685396},
doi = {10.1155/2021/6685396},
abstract = {The existing fuzzy rough set (FRS) models all believe that the decision attribute divides the sample set into several “clear” decision classes, and this data processing method makes the model sensitive to noise information when conducting feature selection. To solve this problem, this paper proposes a robust fuzzy rough set model (RS-FRS) based on representative samples. Firstly, the fuzzy membership degree of the samples is defined to reflect its fuzziness and uncertainty, and RS-FRS model is constructed to reduce the influence of the noise samples. RS-FRS model does not need to set parameters for the model in advance and can effectively reduce the complexity of the model and human intervention. On this basis, the related properties of RS-FRS model are studied, and the sample pair selection algorithm (SPS) based on RS-FRS is used for feature selection. In this paper, RS-FRS is tested and analysed on the open 12 datasets. The experimental results show that RS-FRS model proposed can effectively select the most relevant features and has certain robustness to the noise information. The proposed model has a good applicability for data processing and can effectively improve the performance of feature selection.},
journal = {Complex.},
month = jan,
numpages = {12}
}

@article{10.3233/JIFS-181665,
author = {Nagpal, Arpita and Singh, Vijendra},
title = {Feature selection from high dimensional data based on iterative qualitative mutual information},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-181665},
doi = {10.3233/JIFS-181665},
abstract = {High Dimensional cancer microarray is devilishly challenging while finding the best features for classification. In this paper a new algorithm is proposed based on iterative qualitative mutual information to choose the features that can provide optimal feature set with reliability, stability, and best classification results. It finds the qualitative (i.e. utility) score of each feature with the help of Random Forest algorithm and combines it with mutual information of each feature with its class variable. Adding a qualitative measure along with mutual information can improve the robustness and find redundant features in data. The proposed algorithm has been compared with other representative methods through the ten microarray based cancer datasets in terms of number of features and classification accuracy of three well-known classifiers: Na\"{\i}ve Bayes, IB1 and C4.5. Experimental results show that the proposed approach is effective in producing an optimal feature subset and improves the accuracy of these datasets.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {5845–5856},
numpages = {12},
keywords = {mutual information, random forest, filter model, wrapper, classification, microarray, Feature selection}
}

@article{10.1016/j.engappai.2021.104460,
author = {He, Jingyi and Zheng, Hao},
title = {Prediction of crime rate in urban neighborhoods based on machine learning},
year = {2021},
issue_date = {Nov 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {106},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2021.104460},
doi = {10.1016/j.engappai.2021.104460},
journal = {Eng. Appl. Artif. Intell.},
month = nov,
numpages = {13},
keywords = {Crime rate, Urban design, Big data analysis, Machine learning}
}

@article{10.1016/j.micpro.2021.103830,
author = {Mittal, Mamta and Satapathy, Suresh Chandra and Pal, Vaibhav and Agarwal, Basant and Goyal, Lalit Mohan and Parwekar, Pritee},
title = {Prediction of coefficient of consolidation in soil using machine learning techniques},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103830},
doi = {10.1016/j.micpro.2021.103830},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {15},
keywords = {Geotechnical engineering, Coefficient of consolidation, Machine learning, Prediction}
}

@article{10.1016/j.knosys.2017.03.020,
author = {Akhtar, Md Shad and Gupta, Deepak and Ekbal, Asif and Bhattacharyya, Pushpak},
title = {Feature selection and ensemble construction},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.03.020},
doi = {10.1016/j.knosys.2017.03.020},
abstract = {In this paper we present a cascaded framework of feature selection and classifier ensemble using particle swarm optimization (PSO) for aspect based sentiment analysis. Aspect based sentiment analysis is performed in two steps, viz. aspect term extraction and sentiment classification. The pruned, compact set of features performs better compared to the baseline model that makes use of the complete set of features for aspect term extraction and sentiment classification. We further construct an ensemble based on PSO, and put it in cascade after the feature selection module. We use the features that are identified based on the properties of different classifiers and domains. As base learning algorithms we use three classifiers, namely Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM). Experiments for aspect term extraction and sentiment analysis on two different kinds of domains show the effectiveness of our proposed approach.},
journal = {Know.-Based Syst.},
month = jun,
pages = {116–135},
numpages = {20},
keywords = {Support vector machine, Sentiment analysis, Particle swarm optimization, Maximum entropy, Feature selection, Ensemble, Conditional random field, Aspect term extraction}
}

@article{10.1016/j.cose.2020.102159,
author = {Gibert, Daniel and Mateu, Carles and Planes, Jordi and Marques-Silva, Joao},
title = {Auditing static machine learning anti-Malware tools against metamorphic attacks},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {102},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102159},
doi = {10.1016/j.cose.2020.102159},
journal = {Comput. Secur.},
month = mar,
numpages = {23},
keywords = {Deep learning, Machine learning, N-Gram extraction, Software obfuscation, Malware classification, Malware analysis}
}

@inproceedings{10.5555/3367471.3367617,
author = {Ye, Xiucai and Li, Hongmin and Imakura, Akira and Sakurai, Tetsuya},
title = {Distributed collaborative feature selection based on intermediate representation},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Feature selection is an efficient dimensionality reduction technique for artificial intelligence and machine learning. Many feature selection methods learn the data structure to select the most discriminative features for distinguishing different classes. However, the data is sometimes distributed in multiple parties and sharing the original data is difficult due to the privacy requirement. As a result, the data in one party may be lack of useful information to learn the most discriminative features. In this paper, we propose a novel distributed method which allows collaborative feature selection for multiple parties without revealing their original data. In the proposed method, each party finds the intermediate representations from the original data, and shares the intermediate representations for collaborative feature selection. Based on the shared intermediate representations, the original data from multiple parties are transformed to the same low dimensional space. The feature ranking of the original data is learned by imposing row sparsity on the transformation matrix simultaneously. Experimental results on real-world datasets demonstrate the effectiveness of the proposed method.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {4142–4149},
numpages = {8},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1007/s00521-021-05704-5,
author = {Rathore, Heena and Mohamed, Amr and Guizani, Mohsen and Rathore, Shailendra},
title = {Neuro-fuzzy analytics in athlete development (NueroFATH): a machine learning approach},
year = {2021},
issue_date = {Nov 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {33},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05704-5},
doi = {10.1007/s00521-021-05704-5},
abstract = {Athletes represent the apex of physical capacity filling in a social picture of performance and build. In light of the fundamental contrasts in athletic capacities required for different games, each game demands an alternate body type standard. Because of the decent variety of these body types, each can have an altogether different body standard. Nowadays, a large number of athletes participate in assessments and a large number of human hours are spent on playing out these assessments every year. These assessments are performed to check the physical strength of athletes and evaluate them for different games. This paper presents a machine learning approach to the physical assessment of athletes known as NueroFATH. The proposed NueroFATH approach relies on neuro-fuzzy analytics that involves the deployment of neural networks and fuzzy c-means techniques to predict the athletes for the potential of winning medals. This can be achieved using athletes’ physical assessment parameters. The goal of this study is not only to identify the athletes based on which group they fall into (gold/silver/bronze), but also to understand which physical characteristic is important to identify them and categorize them in a medal group. It was determined that features, namely height, body mass, body mass index, 40&nbsp;m and vertical jump are the most important for achieving 98.40% accuracy for athletes to classify them in the gold category when they are in the bronze category. Unsupervised learning showed that features, namely body mass, body mass index, vertical jump, med ball, 40&nbsp;m, peak oxygen content, peak height velocity have the highest variability. We can achieve upto 97.06% accuracy when features, i.e., body mass, body mass index, vertical jump, med ball, 40&nbsp;m, peak oxygen content, peak height velocity were used.},
journal = {Neural Comput. Appl.},
month = feb,
pages = {23697–23710},
numpages = {14},
keywords = {Athletes, Fuzzy c-means, Multilayer perceptron model, Machine learning, Neuro-fuzzy analytics}
}

@article{10.1016/j.cose.2020.102092,
author = {Nowroozi, Ehsan and Dehghantanha, Ali and Parizi, Reza M. and Choo, Kim-Kwang Raymond},
title = {A survey of machine learning techniques in adversarial image forensics},
year = {2021},
issue_date = {Jan 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {100},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102092},
doi = {10.1016/j.cose.2020.102092},
journal = {Comput. Secur.},
month = jan,
numpages = {25},
keywords = {Cyber security, Image manipulation detection, Adversarial setting, Adversarial learning, Adversarial machine learning, Image forensics}
}

@inproceedings{10.1145/3236405.3236425,
author = {Hinterreiter, Daniel},
title = {Supporting feature-oriented development and evolution in industrial software ecosystems},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236425},
doi = {10.1145/3236405.3236425},
abstract = {Companies nowadays need to serve a mass market while at the same time customers request highly individual solutions. To handle this problem, development is frequently organized in software ecosystems (SECOs), i.e., interrelated software product lines involving internal and external developers. Individual products for customers are derived and adapted by adding new features or creating new versions of existing features to meet the customer-specific requirements. Development teams typically use version control systems to track fine-grained, implementation-level changes to product lines and products. However, it is difficult to relate such low-level changes to features and their evolution in the SECO. State-of-the-art approaches addressing this issue are variation control systems, which allow tracking of changes at the level of features. However, these systems have not found their way into mainstream development so far. In this thesis we will describe which workflows and additions to variation control systems are required to support feature-oriented development in an industrial SECO environment. We will further investigate mechanisms that support feature-based monitoring to guide the evolution in SECOs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {79–86},
numpages = {8},
keywords = {variation control systems, software product lines, software evolution, configuration management},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-030-58666-9_2,
author = {Akkiraju, Rama and Sinha, Vibha and Xu, Anbang and Mahmud, Jalal and Gundecha, Pritam and Liu, Zhe and Liu, Xiaotong and Schumacher, John},
title = {Characterizing Machine Learning Processes: A Maturity Framework},
year = {2020},
isbn = {978-3-030-58665-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58666-9_2},
doi = {10.1007/978-3-030-58666-9_2},
abstract = {Academic literature on machine learning modeling fails to address how to make machine learning models work for enterprises. For example, existing machine learning processes cannot address how to define business use cases for an AI application, how to convert business requirements from product managers into data requirements for data scientists, and how to continuously improve AI applications in term of accuracy and fairness, how to customize general purpose machine learning models with industry, domain, and use case specific data to make them more accurate for specific situations etc. Making AI work for enterprises requires special considerations, tools, methods and processes. In this paper we present a maturity framework for machine learning model lifecycle management for enterprises. Our framework is a re-interpretation of the software Capability Maturity Model (CMM) for machine learning model development process. We present a set of best practices from authors’ personal experience of building large scale real-world machine learning models to help organizations achieve higher levels of maturity independent of their starting point.},
booktitle = {Business Process Management: 18th International Conference, BPM 2020, Seville, Spain, September 13–18, 2020, Proceedings},
pages = {17–31},
numpages = {15},
keywords = {Machine learning models, Maturity model, Maturity framework, AI model life cycle management},
location = {Seville, Spain}
}

@article{10.1016/j.cmpb.2021.106320,
author = {Ferjaoui, Radhia and Cherni, Mohamed Ali and Boujnah, Sana and Kraiem, Nour El Houda and Kraiem, Tarek},
title = {Machine learning for evolutive lymphoma and residual masses recognition in whole body diffusion weighted magnetic resonance images},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {209},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106320},
doi = {10.1016/j.cmpb.2021.106320},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {18},
keywords = {Convolutional neural network (CNN), Relevance vectors machine (RVM), Random forest, K-nearest neighbours, Support vector machine, Back-propagation artificial neural network, Deep learning, Machine learning, Residual masses, Evolutive lymphoma}
}

@inproceedings{10.1145/375212.375271,
author = {Niemel\"{a}, Eila and Ihme, Tuomas},
title = {Product line software engineering of embedded systems},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375271},
doi = {10.1145/375212.375271},
abstract = {In order to be able to determine whether the product line approach is suitable, a company needs to analyse its business drivers, commonality of existing products, domain knowledge owned by the engineering staff, and quality of the representations of existing software artefacts. In this paper we present evaluation criteria for the development of a product line and give an overview of the current state of practices in the embedded software area. Evaluation criteria are divided into three classes. Business drivers of a product line are defined by analysing product assortment and business manners. Domains and personnel are considered in the analysis of the preconditions and targets of a product line. In the development of core assets, elements that affect assets engineering are considered as  well as the mechanisms needed in their maintenance. A product line architecture that brings about a balance between sub- domains and their most important properties is an investment that must be looked after. However, the subdomains need flexibility to use, change and manage their own technologies, and evolve separately, but in a controlled way.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {118–125},
numpages = {8},
keywords = {quality attributes, product line architecture, product features, domain engineering},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@article{10.1016/j.cmpb.2021.106222,
author = {Mej\'{\i}a-Mej\'{\i}a, Elisa and May, James M. and Elgendi, Mohamed and Kyriacou, Panayiotis A.},
title = {Classification of blood pressure in critically ill patients using photoplethysmography and machine learning},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {208},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106222},
doi = {10.1016/j.cmpb.2021.106222},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {27},
keywords = {Hypotension, Hypertension, Blood pressure, Pulse rate variability, Photoplethysmography}
}

@inproceedings{10.1007/978-3-030-89817-5_7,
author = {Valdez-Valenzuela, Eric and Kuri-Morales, Angel and Gomez-Adorno, Helena},
title = {Measuring the Effect of Categorical Encoders in Machine Learning Tasks Using Synthetic Data},
year = {2021},
isbn = {978-3-030-89816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-89817-5_7},
doi = {10.1007/978-3-030-89817-5_7},
abstract = {Most of the datasets used in Machine Learning (ML) tasks contain categorical attributes. In practice, these attributes must be numerically encoded for their use in supervised learning algorithms. Although there are several encoding techniques, the most commonly used ones do not necessarily preserve possible patterns embedded in the data when they are applied inappropriately. This potential loss of information affects the performance of ML algorithms in automated learning tasks. In this paper, a comparative study is presented to measure how the different encoding techniques affect the performance of machine learning models. We test 10 encoding methods, using 5 ML algorithms on real and synthetic data. Furthermore, we propose a novel approach that uses synthetically created datasets that allows us to know a priori the relationship between the independent and the dependent variables, which implies a more precise measurement of the encoding techniques’ impact. We show that some ML models are affected negatively or positively depending on the encoding technique used. We also show that the proposed approach is more easily controlled and faster when performing experiments on categorical encoders.},
booktitle = {Advances in Computational Intelligence: 20th Mexican International Conference on Artificial Intelligence, MICAI 2021, Mexico City, Mexico, October 25–30, 2021, Proceedings, Part I},
pages = {92–107},
numpages = {16},
keywords = {Supervised machine learning, Data preprocessing, Categorical encoding, Synthetic data}
}

@article{10.1007/s10115-017-1131-4,
author = {Xue, Xiaowei and Yao, Min and Wu, Zhaohui},
title = {A novel ensemble-based wrapper method for feature selection using extreme learning machine and genetic algorithm},
year = {2018},
issue_date = {November  2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {57},
number = {2},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-017-1131-4},
doi = {10.1007/s10115-017-1131-4},
abstract = {This paper presents a novel wrapper feature selection algorithm for classification problems, namely hybrid genetic algorithm (GA)- and extreme learning machine (ELM)-based feature selection algorithm (HGEFS). It utilizes GA to wrap ELM to search for the optimum subsets in the huge feature space, and then, a set of subsets are selected to make ensemble to improve the final prediction accuracy. To prevent GA from being trapped in the local optimum, we propose a novel and efficient mechanism specifically designed for feature selection problems to maintain GA's diversity. To measure each subset's quality fairly and efficiently, we adopt a modified ELM called error-minimized extreme learning machine (EM-ELM) which automatically determines an appropriate network architecture for each feature subsets. Moreover, EM-ELM has good generalization ability and extreme learning speed which allows us to perform wrapper feature selection processes in an affordable time. In other words, we simultaneously optimize feature subset and classifiers' parameters. After finishing the search process of GA, to further promote the prediction accuracy and get a stable result, we select a set of EM-ELMs from the obtained population to make the final ensemble according to a specific ranking and selecting strategy. To verify the performance of HGEFS, empirical comparisons are carried out on different feature selection methods and HGEFS with benchmark datasets. The results reveal that HGEFS is a useful method for feature selection problems and always outperforms other algorithms in comparison.},
journal = {Knowl. Inf. Syst.},
month = nov,
pages = {389–412},
numpages = {24},
keywords = {Genetic algorithm, Feature selection, Extreme learning machine, Ensemble learning}
}

@article{10.15388/21-INFOR468,
author = {Kovalev, Maxim and Utkin, Lev and Coolen, Frank and Konstantinov, Andrei},
title = {Counterfactual Explanation of Machine Learning Survival Models},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {32},
number = {4},
issn = {0868-4952},
url = {https://doi.org/10.15388/21-INFOR468},
doi = {10.15388/21-INFOR468},
abstract = {A method for counterfactual explanation of machine learning survival models is proposed. One of the difficulties of solving the counterfactual explanation problem is that the classes of examples are implicitly defined through outcomes of a machine learning survival model in the form of survival functions. A condition that establishes the difference between survival functions of the original example and the counterfactual is introduced. This condition is based on using a distance between mean times to event. It is shown that the counterfactual explanation problem can be reduced to a standard convex optimization problem with linear constraints when the explained black-box model is the Cox model. For other black-box models, it is proposed to apply the well-known Particle Swarm Optimization algorithm. Numerical experiments with real and synthetic data demonstrate the proposed method.},
journal = {Informatica},
month = jan,
pages = {817–847},
numpages = {31},
keywords = {Particle Swarm Optimization, Cox model, counterfactual explanation, convex optimization, censored data, survival analysis, explainable AI, interpretable model}
}

@article{10.1016/j.cviu.2021.103273,
author = {Chen, Xiuhong and Lu, Yun and Zhang, Jun and Zhu, Xingyu},
title = {Margin-based discriminant embedding guided sparse matrix regression for image supervised feature selection},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {1077-3142},
url = {https://doi.org/10.1016/j.cviu.2021.103273},
doi = {10.1016/j.cviu.2021.103273},
journal = {Comput. Vis. Image Underst.},
month = nov,
numpages = {12},
keywords = {Classification, Discriminant embedding, Margin, Sparse matrix regression, Supervised feature selection, Two dimensional image}
}

@article{10.1186/s13638-020-01709-1,
author = {Shinkuma, Ryoichi and Nishio, Takayuki and Inagaki, Yuichi and Oki, Eiji},
title = {Data assessment and prioritization in mobile networks for real-time prediction of spatial information using machine learning},
year = {2020},
issue_date = {Nov 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
number = {1},
issn = {1687-1472},
url = {https://doi.org/10.1186/s13638-020-01709-1},
doi = {10.1186/s13638-020-01709-1},
abstract = {A new framework of data assessment and prioritization for real-time prediction of spatial information is presented. The real-time prediction of spatial information is promising for next-generation mobile networks. Recent developments in machine learning technology have enabled prediction of spatial information, which will be quite useful for smart mobility services including navigation, driving assistance, and self-driving. Other key enablers for forming spatial information are image sensors in mobile devices like smartphones and tablets and in vehicles such as cars and drones and real-time cognitive computing like automatic number/license plate recognition systems and object recognition systems. However, since image data collected by mobile devices and vehicles need to be delivered to the server in real time to extract input data for real-time prediction, the uplink transmission speed of mobile networks is a major impediment. This paper proposes a framework of data assessment and prioritization that reduces the uplink traffic volume while maintaining the prediction accuracy of spatial information. In our framework, machine learning is used to estimate the importance of each data element and to predict spatial information under the limitation of available data. A numerical evaluation using an actual vehicle mobility dataset demonstrated the validity of the proposed framework. Two extension schemes in our framework, which use the ensemble of importance scores obtained from multiple feature selection methods, are also presented to improve its robustness against various machine learning and feature selection methods. We discuss the performance of those schemes through numerical evaluation.},
journal = {EURASIP J. Wirel. Commun. Netw.},
month = may,
numpages = {19},
keywords = {Feature selection, Machine learning, Data assessment, Mobile crowdsensing, Real-time prediction, Spatial information}
}

@inproceedings{10.1007/978-3-030-58799-4_38,
author = {Nucci, Higor Henrique Picoli and Ishii, Renato Porfirio and da Costa&nbsp;Gomes, Rodrigo and Costa, Celso Soares and Feij\'{o}, Gelson Lu\'{\i}s Dias},
title = {Classification of Carcass Fatness Degree in Finishing Cattle Using Machine Learning},
year = {2020},
isbn = {978-3-030-58798-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58799-4_38},
doi = {10.1007/978-3-030-58799-4_38},
abstract = {Nowadays, there is an increase in world demand for quality beef. In this way, the Government of the State of Mato Grosso do Sul has created an incentive program (Precoce MS) that stimulates producers to fit into production systems that lead to the slaughter of animals at young ages and superior carcass quality, towards a more sustainable production model. This work aims to build a classification model of carcass fatness degree using machine learning algorithms and to provide the cattle ranchers with indicators that help them to early finishing cattle with better carcass finishing. The dataset from Precoce MS contains twenty-nine different features with categorical and discrete data and size of 1.05 million cattle slaughter records. In the data mining process, the data were cleaned, transformed and reduced in order to extract patterns more efficiently. In the model selection step, the data was divided into five different datasets for performing cross-validation. The training set received 80% of the data and the test set received the other 20%, emphasizing that both had their data stratified respecting the percentage of each target class. The algorithms analyzed and tested in this work were Support Vector Machines, K-Nearest Neighbors, AdaBoost, Multilayer Perceptron, Naive Bayes and Random Forest Classifier. In order to obtain a better classification, the recursive feature elimination and grid search techniques were used in the models with the objective of selecting better characteristics and obtaining better hyperparameters, respectively. The precision, recall and f1 score metrics were applied in the test set to confirm the choice of the model. Finally, analysis of variance ANOVA indicated that there are no significant differences between the models. Therefore, all these classifiers can be used for the construction of a final model without prejudice in the classification performance.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part I},
pages = {519–535},
numpages = {17},
keywords = {Precision livestock, Precoce MS, Early calf, Data mining},
location = {Cagliari, Italy}
}

@article{10.1016/j.future.2019.04.017,
author = {Din, Ikram Ud and Guizani, Mohsen and Rodrigues, Joel J.P.C. and Hassan, Suhaidi and Korotaev, Valery V.},
title = {Machine learning in the Internet of Things: Designed techniques for smart cities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {100},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.017},
doi = {10.1016/j.future.2019.04.017},
journal = {Future Gener. Comput. Syst.},
month = nov,
pages = {826–843},
numpages = {18},
keywords = {VANET, Smart grid, Medical, Machine learning, Internet of Things}
}

@inproceedings{10.1007/978-3-319-46128-1_34,
author = {Yan, Peng and Li, Yun},
title = {Graph-Margin Based Multi-label Feature Selection},
year = {2016},
isbn = {9783319461274},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-46128-1_34},
doi = {10.1007/978-3-319-46128-1_34},
abstract = {Since instances in multi-label problems are associated with several labels simultaneously, most traditional feature selection algorithms for single label problems are inapplicable. Therefore, new criteria to evaluate features and new methods to model label correlations are needed. In this paper, we adopt the graph model to capture the label correlation, and propose a feature selection algorithm for multi-label problems according to the graph combining with the large margin theory. The proposed multi-label feature selection algorithm GMBA can efficiently utilize the high order label correlation. Experiments on real world data sets demonstrate the effectiveness of the proposed method. The codes of the experiment of this paper are available at https://github.com/Faustus-/ECML2016-GMBA.},
booktitle = {European Conference on Machine Learning and Knowledge Discovery in Databases - Volume 9851},
pages = {540–555},
numpages = {16},
keywords = {Multi-label learning, Margin, Graph, Feature selection},
location = {Riva del Garda, Italy},
series = {ECML PKDD 2016}
}

@inproceedings{10.1145/3379597.3387461,
author = {Chen, Yang and Santosa, Andrew E. and Yi, Ang Ming and Sharma, Abhishek and Sharma, Asankhaya and Lo, David},
title = {A Machine Learning Approach for Vulnerability Curation},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387461},
doi = {10.1145/3379597.3387461},
abstract = {Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {32–42},
numpages = {11},
keywords = {self-training, open-source software, machine learning, classifiers ensemble, application security},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1016/j.knosys.2021.106855,
author = {Kim, Hansu and Lee, Tae Hee and Kwon, Taejoon},
title = {Normalized neighborhood component feature selection and feasible-improved weight allocation for input variable selection},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {218},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.106855},
doi = {10.1016/j.knosys.2021.106855},
journal = {Know.-Based Syst.},
month = apr,
numpages = {14},
keywords = {Body-in-white, Design optimization, Multi-response system, Input variable selection, Feasible-improved weight allocation, Normalized neighborhood component feature selection}
}

@article{10.1504/ijsn.2020.106509,
author = {Farahmand-Nejad, Akram and Noferesti, Samira},
title = {A real-time botnet detection model based on an efficient wrapper feature selection method},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {1},
issn = {1747-8405},
url = {https://doi.org/10.1504/ijsn.2020.106509},
doi = {10.1504/ijsn.2020.106509},
abstract = {Botnets are one of the most widespread and serious threats of cybersecurity that have infected millions of computers around the world over the past few years. Previous research has shown that machine learning methods can accurately detect botnet attacks. However, these methods often do not address the problem of real-time botnet detection, which is one of the main challenges in this area and is essential to prevent the damage caused by botnet attacks. This paper aims to present an efficient real-time model for botnet detection. In the proposed method, a subset of the effective features in detecting the bot traffic is initially selected using the world competitive contests algorithm. Then, based on the selected features, a support vector machine model is created offline to detect real-time bot traffic from the normal one. The test results show that the proposed method can detect botnets with 95% accuracy and outperforms other methods.},
journal = {Int. J. Secur. Netw.},
month = jan,
pages = {36–45},
numpages = {9},
keywords = {botnet attacks, wrapper methods, WCC, world competitive contests algorithm, feature selection, SVM, support vector machine, machine learning, real-time, botnets, network security}
}

@article{10.1007/s10489-020-01981-0,
author = {Alweshah, Mohammed},
title = {Solving feature selection problems by combining mutation and crossover operations with the monarch butterfly optimization algorithm},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01981-0},
doi = {10.1007/s10489-020-01981-0},
abstract = {Feature selection (FS) is used to solve hard optimization problems in artificial intelligence and data mining. In the FS process, some, rather than all of the features of a dataset are selected in order to both maximize classification accuracy and minimize the time required for computation. In this paper a FS wrapper method that uses K-nearest Neighbor (KSN) classification is subjected to two modifications using a current improvement algorithm, the Monarch Butterfly Optimization (MBO) algorithm. The first modification, named MBOICO, involves the utilization of an enhanced crossover operator to improve FS. The second, named MBOLF, integrates the L\'{e}vy flight distribution into the MBO to improve convergence speed. Experiments are carried out on 25 benchmark data sets using the original MBO, MBOICO and MBOLF. The results show that MBOICO is superior, so its performance is also compared against that of four metaheuristic algorithms (PSO, ALO, WOASAT, and GA). The results indicate that it has a high classification accuracy rate of 93% on average for all datasets and significantly reduces the selection size. Hence, the findings demonstrate that the MBOICO outperforms the other algorithms in terms of classification accuracy and number of features chosen (selection size).},
journal = {Applied Intelligence},
month = jun,
pages = {4058–4081},
numpages = {24},
keywords = {L\'{e}vy flight. Monarch butterfly optimization, Crossover operator, Mutation operator, Wrapper approach, Classification, Feature selection}
}

@inproceedings{10.1145/3463274.3463325,
author = {Fu, Liming and Liang, Peng and Li, Xueying and Yang, Chen},
title = {A Machine Learning Based Ensemble Method for Automatic Multiclass Classification of Decisions},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463325},
doi = {10.1145/3463274.3463325},
abstract = {Stakeholders make various types of decisions with respect to requirements, design, management, and so on during the software development life cycle. Nevertheless, these decisions are typically not well documented and classified due to limited human resources, time, and budget. To this end, automatic approaches provide a promising way. In this paper, we aimed at automatically classifying decisions into five types to help stakeholders better document and understand decisions. First, we collected a dataset from the Hibernate developer mailing list. We then experimented and evaluated 270 configurations regarding feature selection, feature extraction techniques, and machine learning classifiers to seek the best configuration for classifying decisions. Especially, we applied an ensemble learning method and constructed ensemble classifiers to compare the performance between ensemble classifiers and base classifiers. Our experiment results show that (1) feature selection can decently improve the classification results; (2) ensemble classifiers can outperform base classifiers provided that ensemble classifiers are well constructed; (3) BoW + 50% features selected by feature selection with an ensemble classifier that combines Na\"{\i}ve Bayes (NB), Logistic Regression (LR), and Support Vector Machine (SVM) achieves the best classification result (with a weighted precision of 0.750, a weighted recall of 0.739, and a weighted F1-score of 0.727) among all the configurations. Our work can benefit various types of stakeholders in software development through providing an automatic approach for effectively classifying decisions into specific types that are relevant to their interests.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {40–49},
numpages = {10},
keywords = {Software Development, Hibernate, Ensemble Classifier, Decision, Automatic Classification},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.1016/j.cam.2019.112395,
author = {Chen, Zheshi and Li, Chunhong and Sun, Wenjun},
title = {Bitcoin price prediction using machine learning: An approach to sample dimension engineering},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {365},
number = {C},
issn = {0377-0427},
url = {https://doi.org/10.1016/j.cam.2019.112395},
doi = {10.1016/j.cam.2019.112395},
journal = {J. Comput. Appl. Math.},
month = feb,
numpages = {13},
keywords = {Machine learning algorithms, Bitcoin price prediction, Occam’s Razor principle, Sample dimension engineering}
}

@inproceedings{10.1007/978-3-319-93040-4_5,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {Feature Selection for Multiclass Binary Data},
year = {2018},
isbn = {978-3-319-93039-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-93040-4_5},
doi = {10.1007/978-3-319-93040-4_5},
abstract = {Feature selection in binary datasets is an important task in many real world machine learning applications such as document classification, genomic data analysis, and image recognition. Despite many algorithms available, selecting features that distinguish all classes from one another in a multiclass binary dataset remains a challenge. Furthermore, many existing feature selection methods incur unnecessary computation costs for binary data, as they are not specifically designed for binary data. We show that exploiting the symmetry and feature value imbalance of binary datasets, more efficient feature selection measures that can better distinguish the classes in multiclass binary datasets can be developed. Using these measures, we propose a greedy feature selection algorithm, CovSkew, for multiclass binary data. We show that CovSkew achieves high accuracy gain over baseline methods, upto ∼40%, especially when the selected feature subset is small. We also show that CovSkew has low computational costs compared with most of the baselines.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 22nd Pacific-Asia Conference, PAKDD 2018, Melbourne, VIC, Australia, June 3-6, 2018, Proceedings, Part III},
pages = {52–63},
numpages = {12},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.patrec.2018.02.021,
author = {Li, Xiangrui and Zhu, Dongxiao},
title = {Robust feature selection via l         2,1-norm in finite mixture of regression},
year = {2018},
issue_date = {Jun 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {108},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2018.02.021},
doi = {10.1016/j.patrec.2018.02.021},
journal = {Pattern Recogn. Lett.},
month = jun,
pages = {15–22},
numpages = {8},
keywords = {65D17, 65D05, 41A10, 41A05, Non-convex optimization, Feature selection, Finite mixture of regression}
}

@inproceedings{10.1145/2401603.2401605,
author = {Sharma, Anuj and Dey, Shubhamoy},
title = {A comparative study of feature selection and machine learning techniques for sentiment analysis},
year = {2012},
isbn = {9781450314923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2401603.2401605},
doi = {10.1145/2401603.2401605},
abstract = {Sentiment analysis is performed to extract opinion and subjectivity knowledge from user generated text content. This is contextually different from traditional topic based text classification since it involves classifying opinionated text according to the sentiment conveyed by it. Feature selection is a critical task in sentiment analysis and effectively selected representative features from subjective text can improve sentiment based classification. This paper explores the applicability of five commonly used feature selection methods in data mining research (DF, IG, GR, CHI and Relief-F) and seven machine learning based classification techniques (Na\"{\i}ve Bayes, Support Vector Machine, Maximum Entropy, Decision Tree, K-Nearest Neighbor, Winnow, Adaboost) for sentiment analysis on online movie reviews dataset. The paper demonstrates that feature selection does improve the performance of sentiment based classification, but it depends on the method adopted and the number of feature selected. The experimental results presented in this paper show that Gain Ratio gives the best performance for sentimental feature selection, and SVM performs better than other techniques for sentiment based classification.},
booktitle = {Proceedings of the 2012 ACM Research in Applied Computation Symposium},
pages = {1–7},
numpages = {7},
keywords = {text classification, sentiment analysis, feature selection},
location = {San Antonio, Texas},
series = {RACS '12}
}

@inproceedings{10.1145/3468264.3468536,
author = {Biswas, Sumon and Rajan, Hridesh},
title = {Fair preprocessing: towards understanding compositional fairness of data transformers in machine learning pipeline},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468536},
doi = {10.1145/3468264.3468536},
abstract = {In recent years, many incidents have been reported where machine learning models exhibited discrimination among people based on race, sex, age, etc. Research has been conducted to measure and mitigate unfairness in machine learning models. For a machine learning task, it is a common practice to build a pipeline that includes an ordered set of data preprocessing stages followed by a classifier. However, most of the research on fairness has considered a single classifier based prediction task. What are the fairness impacts of the preprocessing stages in machine learning pipeline? Furthermore, studies showed that often the root cause of unfairness is ingrained in the data itself, rather than the model. But no research has been conducted to measure the unfairness caused by a specific transformation made in the data preprocessing stage. In this paper, we introduced the causal method of fairness to reason about the fairness impact of data preprocessing stages in ML pipeline. We leveraged existing metrics to define the fairness measures of the stages. Then we conducted a detailed fairness evaluation of the preprocessing stages in 37 pipelines collected from three different sources. Our results show that certain data transformers are causing the model to exhibit unfairness. We identified a number of fairness patterns in several categories of data transformers. Finally, we showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline. We used the fairness composition to choose appropriate downstream transformer that mitigates unfairness in the machine learning pipeline.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {981–993},
numpages = {13},
keywords = {preprocessing, pipeline, models, machine learning, fairness},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1007/s10115-015-0901-0,
author = {Benabdeslem, Khalid and Elghazel, Haytham and Hindawi, Mohammed},
title = {Ensemble constrained Laplacian score for efficient and robust semi-supervised feature selection},
year = {2016},
issue_date = {December  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {49},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-015-0901-0},
doi = {10.1007/s10115-015-0901-0},
abstract = {In this paper, we propose an efficient and robust approach for semi-supervised feature selection, based on the constrained Laplacian score. The main drawback of this method is the choice of the scant supervision information, represented by pairwise constraints. In fact, constraints are proven to have some noise which may deteriorate learning performance. In this work, we try to override any negative effects of constraint set by the variation of their sources. This is achieved by an ensemble technique using both a resampling of data (bagging) and a random subspace strategy. Experiments on high-dimensional datasets are provided for validating the proposed approach and comparing it with other representative feature selection methods.},
journal = {Knowl. Inf. Syst.},
month = dec,
pages = {1161–1185},
numpages = {25},
keywords = {Semi-supervised context, Feature selection, Ensemble methods, Constraints}
}

@article{10.1016/j.cor.2018.05.005,
author = {Agor, Joseph and \"{O}zalt\i{}n, Osman Y.},
title = {Feature selection for classification models via bilevel optimization},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {106},
number = {C},
issn = {0305-0548},
url = {https://doi.org/10.1016/j.cor.2018.05.005},
doi = {10.1016/j.cor.2018.05.005},
journal = {Comput. Oper. Res.},
month = jun,
pages = {156–168},
numpages = {13},
keywords = {Cross validation, Bilevel programming, Classification, Feature selection}
}

@inproceedings{10.1145/3409334.3452065,
author = {Rabby, Md Khurram Monir and Islam, A. K. M. Kamrul and Belkasim, Saeid and Bikdash, Marwan U},
title = {Epileptic seizures classification in EEG using PCA based genetic algorithm through machine learning},
year = {2021},
isbn = {9781450380683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409334.3452065},
doi = {10.1145/3409334.3452065},
abstract = {In this research, a Principal Component Analysis (PCA) with Genetic Algorithm based Machine Learning (ML) approach is developed for the binary classification of epileptic seizures from the EEG dataset. The proposed approach utilizes PCA to reduce the number of features for binary classification of epileptic seizures and is applied to the existing machine learning models to evaluate the model performance in comparison to the higher number of features. Here, Genetic Algorithm (GA) is employed to tune the hyperparameters of the machine learning models for identifying the best ML model. The proposed approach is applied to the UCI epileptic seizure recognition dataset, which is originated from the EEG dataset of Bonn University. As a preliminary analysis of the proposed approach, the data analysis result shows a significant reduction in the number of features but has minimal impact on the ML performance parameters in comparison to the existing ML method.},
booktitle = {Proceedings of the 2021 ACM Southeast Conference},
pages = {17–24},
numpages = {8},
keywords = {electroencephalogram (EEG), epileptic seizure (ES), genetic algorithm (GA), machine learning (ML), principal component analysis (PCA)},
location = {Virtual Event, USA},
series = {ACMSE '21}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1007/s10489-018-1349-1,
author = {Agnihotri, Deepak and Verma, Kesari and Tripathi, Priyanka and Singh, Bikesh Kumar},
title = {Soft voting technique to improve the performance of global filter based feature selection in text corpus},
year = {2019},
issue_date = {April     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {4},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-018-1349-1},
doi = {10.1007/s10489-018-1349-1},
abstract = {In text classification, the Global Filter-based Feature Selection Scheme (GFSS) selects the top-N ranked words as features. It discards the low ranked features from some classes either partially or completely. The low rank is usually due to varying occurrence of the words (terms) in the classes. The Latent Semantic Analysis (LSA) can be used to address this issue as it eliminates the redundant terms. It assigns an equal rank to the terms that represent similar concepts or meanings, e.g. four terms "carcinoma", "sarcoma", "melanoma", and "cancer" represent a similar concept, i.e. "cancer". Thus, any selected term by the algorithms from these four terms doesn't affect the classifier performance. However, it does not guarantee that the selection of top-N LSA ranked terms by GFSS are the representative terms of each class. An Improved Global Feature Selection Scheme (IGFSS) solves this issue by selecting an equal number of representative terms from all the classes. However, it has two issues, first, it assigns the class label and membership of each term on the basis of an individual vote of the Odds Ratio (OR) method thereby limiting the decision making capability. Second, the ratio of selected terms is determined empirically by the IGFSS and a common ratio is applied to all the classes to assign the positive and negative membership of the terms. However, the ratio of positive and negative nature terms varies from one class to another and it may be very less for one class, whereas high for other classes. Thus, one common negative features ratio used by the IGFSS affects those classes of a dataset in which there is an imbalance between positive and negative nature words. To address these issues of IGFSS, a new Soft Voting Technique (SVT) is proposed to improve the performance of GFSS. There are two main contributions in this paper: (i) The weighted average score (Soft Vote) of three methods, viz. OR, Correlation Coefficient (CC), and GSS Coefficients (GSS) improves the numerical discrimination of words to identify there positive and negative membership to a class. (ii) A mathematical expression is incorporated in the IGFSS that computes a varying ratio of positive and negative memberships of the terms for each class. The membership is based on the occurrence of the terms in the classes. The proposed SVT is evaluated using four standard classifiers applied on five bench-marked datasets. The experimental results based on Macro_F1 and Micro_F1 measures show that SVT achieves a significant improvement in the performance of classifiers in comparison of standard methods.},
journal = {Applied Intelligence},
month = apr,
pages = {1597–1619},
numpages = {23},
keywords = {Text mining, Text classification, Text analysis, Term frequency, Feature selection}
}

@article{10.1016/j.asoc.2019.105498,
author = {Hafiz, Faizal and Swain, Akshya and Naik, Chirag and Patel, Nitish},
title = {Efficient feature selection of power quality events using two dimensional (2D) particle swarms},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {81},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.105498},
doi = {10.1016/j.asoc.2019.105498},
journal = {Appl. Soft Comput.},
month = aug,
numpages = {14},
keywords = {Power quality, Pattern recognition, Particle swarm optimization, Feature selection, Dimensionality reduction, Classification}
}

@article{10.1007/s00500-016-2257-0,
author = {Feng, Xiang and Yang, Tan and Yu, Huiqun},
title = {A new multi-colony fairness algorithm for feature selection},
year = {2017},
issue_date = {December  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-016-2257-0},
doi = {10.1007/s00500-016-2257-0},
abstract = {As the world gradually transforms from an information world to a data-driven world, areas of pattern recognition and data mining are facing more and more challenges. The process of feature subset selection becomes a necessary part of big data pattern recognition due to the data with explosive growth. Inspired by the behavior of grabbing resources in animals, this paper adds personal grabbing-resource behavior into the model of resource allocation transformed from the model of feature selection. Multi-colony fairness algorithm (MCFA) is proposed to deal with grabbing-resource behaviors in order to obtain a better distribution scheme (i.e., to obtain a better feature subset). The algorithm effectively fuses strategies of the random search and the heuristic search. In addition, it combines methods of filter and wrapper so as to reduce the amount of calculation while improving classification accuracies. The convergence and the effectiveness of the proposed algorithm are verified both from mathematical and experimental aspects. MCFA is compared with other four classic feature selection algorithms such as sequential forward selection, sequential backward selection, sequential floating forward selection, and sequential floating backward selection and three mainstream feature selection algorithms such as relevance---redundancy feature selection, minimal redundancy---maximal relevance, and ReliefF. The comparison results show that the proposed algorithm can obtain better feature subsets both in the aspects of feature subset length which is defined as the number of features in a feature subset and the classification accuracy. The two aspects indicate the efficiency and the effectiveness of the proposed algorithm.},
journal = {Soft Comput.},
month = dec,
pages = {7141–7157},
numpages = {17},
keywords = {Resource allocation, Multi-colony fairness algorithm (MCFA), Grabbing-resource behavior, Feature selection}
}

