@inproceedings{10.1145/3336294.3336304,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Software Product Line Engineering: A Practical Experience},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336304},
doi = {10.1145/3336294.3336304},
abstract = {The lack of mature tool support is one of the main reasons that make the industry to be reluctant to adopt Software Product Line (SPL) approaches. A number of systematic literature reviews exist that identify the main characteristics offered by existing tools and the SPL phases in which they can be applied. However, these reviews do not really help to understand if those tools are offering what is really needed to apply SPLs to complex projects. These studies are mainly based on information extracted from the tool documentation or published papers. In this paper, we follow a different approach, in which we firstly identify those characteristics that are currently essential for the development of an SPL, and secondly analyze whether the tools provide or not support for those characteristics. We focus on those tools that satisfy certain selection criteria (e.g., they can be downloaded and are ready to be used). The paper presents a state of practice with the availability and usability of the existing tools for SPL, and defines different roadmaps that allow carrying out a complete SPL process with the existing tool support.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {164–176},
numpages = {13},
keywords = {tooling roadmap, tool support, state of practice, spl in practice},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1016/j.infsof.2020.106389,
author = {Chac\'{o}n-Luna, Ana Eva and Guti\'{e}rrez, Antonio Manuel and Galindo, Jos\'{e} A. and Benavides, David},
title = {Empirical software product line engineering: A systematic literature review},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106389},
doi = {10.1016/j.infsof.2020.106389},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {22},
keywords = {Systematic literature review, Experiment, Case study, Empirical strategies, Software product lines}
}

@article{10.1007/s11227-021-03627-5,
author = {Kiani, Azaz Ahmed and Hafeez, Yaser and Imran, Muhammad and Ali, Sadia},
title = {A dynamic variability management approach working with agile product line engineering practices for reusing features},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {8},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-021-03627-5},
doi = {10.1007/s11227-021-03627-5},
abstract = {Agile software development (ASD) and software product line (SPL) have shown significant benefits for software engineering processes and practices. Although both methodologies promise similar benefits, they are based on different foundations. SPL encourages systematic reuse that exploits the commonalities of various products belonging to a common domain and manages their variations systematically. In contrast, ASD stresses a flexible and rapid development of products using iterative and incremental approaches. ASD encourages active involvement of customers and their frequent feedback. Both ASD and SPL require alternatives to extend agile methods for several reasons such as (1) to manage reusability and variability across the products of any domain, (2) to avoid the risk of developing core assets that will become obsolete and not used in future projects, and (3) to meet the requirements of changing markets. This motivates the researchers for the integration of ASD and SPL approaches. As a result, an innovative approach called agile product line engineering (APLE) by integrating SPL and ASD has been introduced. The principal aim of APLE is to maximize the benefits of ASD and SPL and address the shortcomings of both. However, combining both is a major challenge. Researchers have proposed a few approaches that try to put APLE into practice, but none of the existing approaches cover all APLE features needed. This paper proposes a new dynamic variability approach for APLE that uses APLE practices for reusing features. The proposed approach (PA) is based on the agile method Scrum and the reactive approach of SPL. In this approach, reusable core assets respond reactively to customer requirements. The PA constructs and develops the SPL architecture iteratively and incrementally. It provides the benefits of reusability and maintainability of SPLs while keeping the delivery-focused approach from agile methods. We conducted a quantitative survey of software companies applying the APLE to assess the performance of the PA and hypotheses of empirical study. Findings of empirical evaluation provide evidence on integrating ASD and SPL and the application of APLE into practices.},
journal = {J. Supercomput.},
month = aug,
pages = {8391–8432},
numpages = {42},
keywords = {Agile product line engineering, Agile software product line, Agile software development, Software product line}
}

@inproceedings{10.1145/2593882.2593888,
author = {Metzger, Andreas and Pohl, Klaus},
title = {Software product line engineering and variability management: achievements and challenges},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593888},
doi = {10.1145/2593882.2593888},
abstract = {Software product line engineering has proven to empower organizations to develop a diversity of similar software-intensive systems (applications) at lower cost, in shorter time, and with higher quality when compared with the development of single systems. Over the last decade the software product line engineering research community has grown significantly. It has produced impressive research results both in terms of quality as well as quantity. We identified over 600 relevant research and experience papers published within the last seven years in established conferences and journals. We briefly summarize the major research achievements of these past seven years. We structure this research summary along a standardized software product line framework. Further, we outline current and future research challenges anticipated from major trends in software engineering and technology.},
booktitle = {Future of Software Engineering Proceedings},
pages = {70–84},
numpages = {15},
keywords = {variability modeling, variability management, requirements engineering, quality assurance, design, Software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3233027.3233045,
author = {Becker, Martin and Zhang, Bo},
title = {How do our neighbours do product line engineering? a comparison of hardware and software product line engineering approaches from an industrial perspective},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233045},
doi = {10.1145/3233027.3233045},
abstract = {Product line engineering (PLE) approaches have been followed in industry for hardware and software solutions for more than three decades now. However, the different engineering disciplines (e.g. mechanics, electrics, software) have developed and evolved their approaches within their own realms, which is fine as long as there is no need for integrated approaches. Driven by the increasing complexity of systems, there is a rising need for interdisciplinary systems engineering these days. Companies engineering cyber-physical systems and their components have to integrate product line engineering approaches across the involved engineering disciplines to enable a global optimization of portfolio, solution structures, and assets along their lifecycle. From a bird's-eye view, there is noticeable commonality but also variety in the approaches followed for PLE in the different engineering disciplines, which renders the integration of approaches a non-trivial endeavour. In order to foster the development of integrated PLE approaches, this paper explores, maps, and compares PLE approaches in the field of hardware and software engineering. Furthermore, the paper identifies integration opportunities and challenges. As the paper targets industrial practitioners, it mainly provides references to respective industrial events and material and does not fully cover related work in the respective research communities.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {190–195},
numpages = {6},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Feature interaction}
}

@inproceedings{10.1145/3106195.3106224,
author = {Tizzei, Leonardo P. and Nery, Marcelo and Segura, Vin\'{\i}cius C. V. B. and Cerqueira, Renato F. G.},
title = {Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106224},
doi = {10.1145/3106195.3106224},
abstract = {In order to achieve economies of scale, a Software as a Service (SaaS) should be configurable, multi-tenant efficient, and scalable. But building SaaS with these characteristics comes at a price of having more complex services. Some works in the literature integrate software product line engineering and service-oriented architecture to tackle the complexity of building multi-tenant SaaS. Most of these works focused on centralized approaches that rely on middleware or platforms, but they do not investigate the use of decentralized architectural style. Microservices architecture is an architectural style that relies on small, decentralized, and autonomous services that work together. Thus, this paper investigates the integrated use of microservices architecture and software produt line techniques to develop multi-tenant SaaS. We conducted an empirical study that analyzes the behavior of software reuse during the evolution of a multi-tenant SaaS. This empirical study showed an average software reuse of 62% of lines of code among tenants. We also provide lessons we learned during the the re-engineering and maintenance of such multi-tenant SaaS.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {205–214},
numpages = {10},
keywords = {Software Reuse, Software Evolution, Service-oriented Architectures, Multi-tenancy, Microservices},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3382025.3414942,
author = {Assun\c{c}\~{a}o, Wesley K. G. and Kr\"{u}ger, Jacob and Mendon\c{c}a, Willian D. F.},
title = {Variability management meets microservices: six challenges of re-engineering microservice-based webshops},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414942},
doi = {10.1145/3382025.3414942},
abstract = {A microservice implements a small unit of functionality that it provides through a network using lightweight protocols. So, microservices can be combined to fulfill tasks and implement features of a larger software system---resembling a variability mechanism in the context of a software product line (SPL). Microservices and SPLs have similar goals, namely facilitating reuse and customizing, but they are usually employed in different contexts. Any developer who has access to the network can provide a microservice for any task, while SPLs are usually intended to implement features of a specific domain. Due to their different concepts, using microservices to implement an SPL or adopting SPL practices (e.g., variability management) for microservices is a challenging cross-area research problem. However, both techniques can complement each other, and thus tackling this problem promises benefits for organizations that employ either technique. In this paper, we reason on the importance of advancing in this direction, and sketch six concrete challenges to initiate research, namely (1) feature identification, (2) variability modeling, (3) variable microservice architectures, (4) interchangeability, (5) deep customization, and (6) re-engineering an SPL. We intend these challenges to serve as a starting point for future research in this cross-area research direction---avoiding that the concepts of one area are reinvented in the other.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {22},
numpages = {6},
keywords = {variability management, software product line, re-engineering, microservices, cloud computing},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3461001.3473060,
author = {Sch\"{a}fer, Andreas and Becker, Martin and Andres, Markus and Kistenfeger, Tim and Rohlf, Florian},
title = {Variability realization in model-based system engineering using software product line techniques: an industrial perspective},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3473060},
doi = {10.1145/3461001.3473060},
abstract = {Efficiently handling system variants is rising of importance in industry and challenges the application of model-based systems engineering.This paper reveals the increasing industrial demand of guidance and decision support on how to handle variants and variability within SysML and UML models. While a substantial amount of variability realization approaches has already been published on source code level, there is little guidance for practitioners on system model level. Hence, there is major uncertainty in dealing with system changes or concurrent system modeling of related system. Due to a poor modularization and variability realization these model variants are ending up in interwoven and complex system models.In this paper, we aim to raise awareness of the need for appropriate guidance and decision support, identify important contextual factors of MBSE that influence variability realization, and derive well known variability mechanisms used in software coding for their applicability in system modeling.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {25–34},
numpages = {10},
keywords = {variant management, variability realization, variability mechanism, system and software product line engineering, model-based systems engineering, decision support, UML, SysML},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2811681.2811703,
author = {Tan, Lei and Lin, Yuqing},
title = {An Aspect-Oriented Feature Modelling Framework for Software Product Line Engineering},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811703},
doi = {10.1145/2811681.2811703},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that focusing on systematic software assets reuse. SPLE treats software products in the same application domains as a product family and developing various of assets could be reused in the product family. Feature modelling is a critical activity of SPLE, which developing the requirement model for product families and providing guidance for individual product implementation. In this paper, we discuss several drawbacks of current feature modelling and propose a solution which adopting aspect-oriented development ideas and approaches. The proposed framework is intended to better manage complex feature relationships, and enhance quality-aware feature modelling. We include a case study of a real-life experience to demonstrate the proposed approach.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {111–115},
numpages = {5},
keywords = {software product line engineering, feature modelling, aspectoriented},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search based software engineering for software product line engineering: a survey and directions for future work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/SPLC.2011.55,
author = {Dao, Tung M. and Lee, Hyesun and Kang, Kyo C.},
title = {Problem Frames-Based Approach to Achieving Quality Attributes in Software Product Line Engineering},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.55},
doi = {10.1109/SPLC.2011.55},
abstract = {In software product line engineering (SPLE), commonality and variability across products of a product line domain are captured typically by a feature model. Reusable components are then developed from features. However, mapping features to components remains a complex task requiring a systematic way of exploring and analyzing various concerns arising from inadequate/insufficient domain assumptions. Essentially, those concerns prevent SPLE from achieving various quality attributes. This paper proposes a problem frames-based approach to addressing this problem. An elevator product line example is used to demonstrate the feasibility of the approach.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {175–180},
numpages = {6},
keywords = {Quality Attributes, Problem Frames, Goal Models, Feature Models},
series = {SPLC '11}
}

@inproceedings{10.1145/3307630.3342418,
author = {Rinc\'{o}n, Luisa and Mazo, Ra\'{u}l and Salinesi, Camille},
title = {Analyzing the Convenience of Adopting a Product Line Engineering Approach: An Industrial Qualitative Evaluation},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342418},
doi = {10.1145/3307630.3342418},
abstract = {Engineering Software Product Lines may be a strategy to reduce costs and efforts for developing software and increasing business productivity. However, it cannot be considered as a "silver bullet" that applies to all types of organizations. Companies must consider pros and cons to determine sound reasons and justify its adoption. In previous work, we proposed the APPLIES evaluation framework to help decision-makers find arguments that may justify (or not) adopting a product line engineering approach. This paper presents our experience using this framework in a mid-sized software development company with more than 25 years of experience but without previous experience in product line engineering. This industrial experience, conducted as a qualitative empirical evaluation, helped us to evaluate to what extent APPLIES is practical to be used in a real environment and to gather ideas from real potential users to improve the framework.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {90–97},
numpages = {8},
keywords = {qualitative evaluation, product line engineering, product line adoption, empirical evaluation},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1007/978-3-319-35122-3_2,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Automated Composition of Service Mashups Through Software Product Line Engineering},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_2},
doi = {10.1007/978-3-319-35122-3_2},
abstract = {The growing number of online resources, including data and services, has motivated both researchers and practitioners to provide methods and tools for non-expert end-users to create desirable applications by putting these resources together leading to the so called mashups. In this paper, we focus on a class of mashups referred to as service mashups. A service mashup is built from existing services such that the developed service mashup offers added-value through new functionalities. We propose an approach which adopts concepts from software product line engineering and automated AI planning to support the automated composition of service mashups. One of the advantages of our work is that it allows non-experts to build and optimize desired mashups with little knowledge of service composition. We report on the results of the experimentation that we have performed which support the practicality and scalability of our proposed work.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {20–38},
numpages = {19},
keywords = {Workflow optimization, Software product lines, Service mashups, Planning, Feature model, Automated composition},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal methods and analysis in software product line engineering: 4th edition of FMSPLE workshop series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {verification, variability, testing, software product lines, semantics, formal methods, evolution},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1109/APSEC.2014.94,
author = {Tan, Lei and Lin, Yuqing and Liu, Li},
title = {Quality Ranking of Features in Software Product Line Engineering},
year = {2014},
isbn = {9781479974269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2014.94},
doi = {10.1109/APSEC.2014.94},
abstract = {Software Product Line Engineering (SPLE) is a systematic software reuse approach that developing a set of similar software products as a family. All the visible characters of the products in a product family are represented as features and their relationships are modelled in a feature model. During application engineering, desired features are selected from the feature model in a configuration process based on the requirements. In this process, the quality of final product should be considered as early as possible which requires identifying and ranking associated features' contributions to related quality attributes before configuring member products. In this paper, we propose a ranking approach to address the issues in current quality based feature ranking approaches, we also include a case study to illustrate our approach at the end.},
booktitle = {Proceedings of the 2014 21st Asia-Pacific Software Engineering Conference - Volume 02},
pages = {57–62},
numpages = {6},
series = {APSEC '14}
}

@inproceedings{10.1145/3106195.3106218,
author = {Krueger, Charles and Clements, Paul},
title = {Enterprise Feature Ontology for Feature-based Product Line Engineering and Operations},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106218},
doi = {10.1145/3106195.3106218},
abstract = {Feature trees have been the standard data structure for representing product diversity in feature-based systems and software product line engineering (PLE). For basic product lines of modest size or complexity, one or several modular feature trees can be sufficient for managing the and resolving the variation present across the engineering assets in the systems engineering 'V' --- from requirements, to design, through implementation, verification, validation, documentation, and more --- in the software, mechanical, and electrical disciplines. However, enterprises seeking to adopt PLE at all levels of their organization, including areas such as product marketing, portfolio planning, manufacturing, supply chain, product sales, product service and maintenance, Internet-of-Things, resource planning, and much more are finding that thousands of nonengineering users need different views and interaction scenarios with a feature diversity representation. This paper describes a feature ontology (a specification of the meaning of terms in the feature modeling realm) that is suitable for managing the feature-based product line engineering and operations in the largest and most complex product line organizations. This ontology is based on layers of abstraction that each incrementally constrain the complexity and combinatorics and targets specific roles in the organization for greater degrees of efficiency, precision, and automation across an entire business enterprise.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {227–236},
numpages = {10},
keywords = {variation points, software product lines, product portfolio, product configurator, feature-based product line engineering, feature profiles, feature modeling, enterprise feature ontology, bill-of-features, Product line engineering, PLE factory},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1016/j.infsof.2013.05.006,
author = {Mohabbati, Bardia and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and M\"{u}ller, Hausi A.},
title = {Combining service-orientation and software product line engineering: A systematic mapping study},
year = {2013},
issue_date = {November, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.006},
doi = {10.1016/j.infsof.2013.05.006},
abstract = {Context: Service-Orientation (SO) is a rapidly emerging paradigm for the design and development of adaptive and dynamic software systems. Software Product Line Engineering (SPLE) has also gained attention as a promising and successful software reuse development paradigm over the last decade and proven to provide effective solutions to deal with managing the growing complexity of software systems. Objective: This study aims at characterizing and identifying the existing research on employing and leveraging SO and SPLE. Method: We conducted a systematic mapping study to identify and analyze related literature. We identified 81 primary studies, dated from 2000-2011 and classified them with respect to research focus, types of research and contribution. Result: The mapping synthesizes the available evidence about combining the synergy points and integration of SO and SPLE. The analysis shows that the majority of studies focus on service variability modeling and adaptive systems by employing SPLE principles and approaches. In particular, SPLE approaches, especially feature-oriented approaches for variability modeling, have been applied to the design and development of service-oriented systems. While SO is employed in software product line contexts for the realization of product lines to reconcile the flexibility, scalability and dynamism in product derivations thereby creating dynamic software product lines. Conclusion: Our study summarizes and characterizes the SO and SPLE topics researchers have investigated over the past decade and identifies promising research directions as due to the synergy generated by integrating methods and techniques from these two areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1845–1859},
numpages = {15},
keywords = {Systematic mapping, Software product lines, Service-oriented architecture}
}

@inproceedings{10.1145/3106195.3106205,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green Configurations of Functional Quality Attributes},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106205},
doi = {10.1145/3106195.3106205},
abstract = {Functional quality attributes (FQAs) are those quality attributes that, to be satisfied, require the incorporation of additional functionality into the application architecture. By adding an FQA (e.g., security) we can improve the quality of the final product, but there is also an increase in energy consumption. This paper proposes a solution to help the software architect to generate configurations of FQAs whilst keeping the energy consumed by the application as low as possible. For this, a usage model is defined for each FQA, taking into account the variables that affect the energy consumption, and that the values of these variables change according to the part of the application where the FQA is required. We extend a Software Product Line that models a family of FQAs to incorporate the variability of the usage model and the existing frameworks that implement FQAs. We generate the most eco-efficient configuration of FQAs by selecting the framework with the most suitable characteristics according to the requirements of the application.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {79–83},
numpages = {5},
keywords = {Variability, SPL, Quality Attributes, FQA, Energy Consumption},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3483899.3483902,
author = {Marchezan, Luciano and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Carbonell, Jo\~{a}o and Rodrigues, Elder and Bernardino, Maicon and Basso, F\'{a}bio},
title = {SPLReePlan - Automated Support for Software Product Line Reengineering Planning},
year = {2021},
isbn = {9781450384193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483899.3483902},
doi = {10.1145/3483899.3483902},
abstract = {The extractive adoption of Software Product Lines (SPL) relies on the reuse of the already developed systems, employing a reengineering process. However, due to the diversity of options found in the daily practice of SPL development, rigorous planning of scenarios is critical to perform SPL reengineering. This diversity is the result of different organizational aspects, such as team experience and product portfolio. Hence, a proper planning process must consider technical and organizational aspects, however, most existing studies in the field do not take into account organizational aspects of the companies. In this work, we present SPLReePlan, an automated framework to aid the SPL reengineering planning taking into account technical and organizational aspects. Our framework is supported by a web-based tool, ready to be used in the industry. To investigate how flexible is SPLReePlan to support the SPL reengineering planning in diverse situations, we extracted eight different scenarios from the SPL literature, which are used as input for the evaluation of SPLReePlan. The results indicate that SPLReePlan can be satisfactorily customized to a variety of scenarios with different artifacts, feature retrieval techniques, and reengineering activities. As a contribution, we discuss the lessons learned within the evaluation, and present challenges that were faced, being a source of information for tool builders or motivating new studies.},
booktitle = {Proceedings of the 15th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {1–10},
numpages = {10},
keywords = {variability management, software product lines, reengineering process, automated support},
location = {Joinville, Brazil},
series = {SBCARS '21}
}

@inproceedings{10.1145/2362536.2362576,
author = {ter Beek, Maurice H. and Becker, Martin and Classen, Andreas and Roos-Frantz, Fabricia and Schaefer, Ina and Wong, Peter Y. H.},
title = {Formal methods and analysis in software product line engineering: 3rd edition of FMSPLE workshop series},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362576},
doi = {10.1145/2362536.2362576},
abstract = {FMSPLE 2012 is the third edition of the FMSPLE workshop series, traditionally affiliated with SPLC, which aims to connect researchers and practitioners interested in raising the efficiency and the effectiveness of SPLE through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {286–287},
numpages = {2},
keywords = {verification, variability, testing, software product lines, semantics, formal methods, evolution},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1016/j.dam.2019.06.008,
author = {Carbonnel, Jessie and Bertet, Karell and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {FCA for software product line representation: Mixing configuration and feature relationships in a unique canonical representation},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {273},
number = {C},
issn = {0166-218X},
url = {https://doi.org/10.1016/j.dam.2019.06.008},
doi = {10.1016/j.dam.2019.06.008},
journal = {Discrete Appl. Math.},
month = feb,
pages = {43–64},
numpages = {22},
keywords = {Concept lattice, Formal concept analysis, Feature model, Software product line}
}

@article{10.1007/s11334-011-0159-y,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {An architecture process maturity model of software product line engineering},
year = {2011},
issue_date = {September 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {7},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-011-0159-y},
doi = {10.1007/s11334-011-0159-y},
abstract = {Software architecture has been a key research area in the software engineering community due to its significant role in creating high-quality software. The trend of developing product lines rather than single products has made the software product line a viable option in the industry. Software product line architecture (SPLA) is regarded as one of the crucial components in the product lines, since all of the resulting products share this common architecture. The increased popularity of software product lines demands a process maturity evaluation methodology. Consequently, this paper presents an architecture process maturity model for software product line engineering to evaluate the current maturity of the product line architecture development process in an organization. Assessment questionnaires and a rating methodology comprise the framework of this model. The objective of the questionnaires is to collect information about the SPLA development process. Thus, in general this work contributes towards the establishment of a comprehensive and unified strategy for the process maturity evaluation of software product line engineering. Furthermore, we conducted two case studies and reported the assessment results, which show the maturity of the architecture development process in two organizations.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {191–207},
numpages = {17},
keywords = {Software product line, Software architecture, Process assessment, Domain engineering, Application engineering}
}

@inproceedings{10.1145/3382026.3431248,
author = {Ferreira, Thiago Nascimento and Vergilio, Silvia Regina and Kessentini, Mauroane},
title = {Many-objective Search-based Selection of Software Product Line Test Products with Nautilus},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431248},
doi = {10.1145/3382026.3431248},
abstract = {The Variability Testing of Software Product Lines (VTSPL) concerns the selection of the most representative products to be tested according to specific goals. Works in the literature use a great variety of objectives and distinct algorithms. However, they neither address all the objectives at the same time nor offer an automatic tool to support this task. To this end, this work introduces Nautilus/VTSPL, a tool to address the VTSPL problem, created by instantiating Nautilus Framework. Nautilus/VTSPL allows the tester to experiment and configure different objectives and categories of many-objective algorithms. The tool also offers support to visualization of the generated solutions, easing the decision-making process.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {1–4},
numpages = {4},
keywords = {sbse, product line testing, many-objective algorithms},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1982185.1982336,
author = {Asadi, Mohsen and Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Mohabbati, Bardia},
title = {Goal-driven software product line engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982336},
doi = {10.1145/1982185.1982336},
abstract = {Feature Models encapsulate functionalities and quality properties of a product family. The employment of feature models for managing variability and commonality of large-scale product families raises an important question: on what basis should the features of a product family be selected for a target software application, which is going to be derived from the product family. Thus, the selection of the most suitable features for a specific application requires the understanding of its stakeholders' intentions and also the relationship between their intentions and the available software features. To address this important issue, we adopt a standard goal-oriented requirements engineering framework, i.e., the i* framework, for identifying stakeholders' intentions and propose an approach for explicitly mapping and bridging between the features of a product family and the goals and objectives of the stakeholders. We propose a novel approach to automatically preconfigure a given feature model based on the objectives of the target product stakeholders. Also, our approach is able to elucidate the rationale behind the selection of the most important features of a family for a target application.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {691–698},
numpages = {8},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1145/3109729.3109744,
author = {Munoz, Daniel-Jesus},
title = {Achieving energy efficiency using a Software Product Line Approach},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109744},
doi = {10.1145/3109729.3109744},
abstract = {Green computing and energy-aware software engineering are trend approaches that try to address the development of applications respectful with the environment. To reduce the energy consumption of an application the developer needs: (i) to identify what are the concerns that will impact more in the energy consumption; (ii) to model the variability of alternative designs and implementations of each concern; (iii) to store and compare the experimentation results related with the energy and time consumption of concerns; (iv) to find out what is the most eco-efficient solution for each concern. HADAS addresses these issues by modelling the variability of energy consuming concerns for different energy contexts. It connects the variability model with a repository that stores energy measurements, providing a Software Product Line (SPL) service, helping developers to reason and find out what are the most eco-friendly configurations. We have an initial implementation of the HADAS toolkit using Clafer. We have tested our implementation with several case studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {131–138},
numpages = {8},
keywords = {Variability, Software Product Line, Repository, Optimisation, Metrics, Energy Efficiency, Clafer},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1885639.1885680,
author = {Lutz, Robyn and Weiss, David and Krishnan, Sandeep and Yang, Jingwei},
title = {Software product line engineering for long-lived, sustainable systems},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The design and operation of long-lived, sustainable systems (LSS) are hampered by limited support for change over time and limited preservation of system knowledge. The solution we propose is to adopt software product-line engineering (SPLE) techniques for use in single, critical systems with requirements for sustainability. We describe how four categories of change in a LSS can be usefully handled as variabilities in a software product line. We illustrate our argument with examples of changes from the Voyager spacecraft.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {430–434},
numpages = {5},
keywords = {variability, sustainable system, software product line, long-lived system, commonality/variability analysis},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.jss.2015.11.005,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {An automatic process for weaving functional quality attributes using a software product line approach},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {112},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.11.005},
doi = {10.1016/j.jss.2015.11.005},
abstract = {We define a family of FQAs ready to be reused in many software architectures.We define an Aspect-Oriented SPL to inject customized FQAs into the applications.Two different implementations of the SPL are provided and compared.Modelling FQAs separately from the applications increases reusability.The final architectures exhibit a high degree of separation of concerns. Some quality attributes can be modelled using software components, and are normally known as Functional Quality Attributes (FQAs). Applications may require different FQAs, and each FQA (e.g., security) can be composed of many concerns (e.g., access control or authentication). They normally have dependencies between them and crosscut the system architecture. The goal of the work presented here is to provide the means for software architects to focus only on application functionality, without having to worry about FQAs. The idea is to model FQAs separately from application functionality following a Software Product Line (SPL) approach. By combining SPL and aspect-oriented mechanisms, we will define a generic process to model and automatically inject FQAs into the application without breaking the base architecture. We will provide and compare two implementations of our generic approach using different variability and architecture description languages: (i) feature models and an aspect-oriented architecture description language; and (ii) the Common Variability Language (CVL) and a MOF-compliant language (e.g., UML). We also discuss the benefits and limitations of our approach. Modelling FQAs separately from the base application has many advantages (e.g., reusability, less coupled components, high cohesive architectures).},
journal = {J. Syst. Softw.},
month = feb,
pages = {78–95},
numpages = {18},
keywords = {Weaving, Software product lines, Quality attributes}
}

@inproceedings{10.1145/2420942.2420948,
author = {Gonz\'{a}lez-Huerta, Javier and Insfran, Emilio and Abrah\~{a}o, Silvia and McGregor, John D.},
title = {Non-functional requirements in model-driven software product line engineering},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420948},
doi = {10.1145/2420942.2420948},
abstract = {Developing variant-rich software systems through the application of the software product line approach requires the management of a wide set of requirements. However, in most cases, the focus of those requirements is limited to the functional requirements. The non-functional requirements are often informally defined and their management does not provide traceability mechanisms for their validation. In this paper, we present a multimodel approach that allows the explicit representation of non-functional requirements for software product lines both at domain engineering, and application engineering levels. The multimodel allows the representation of different viewpoints of a software product line, including the non-functional requirements and the relationships that these non-functional requirements might have with features and functionalities. The feasibility of this approach is illustrated through a specific example from the automotive domain.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {6},
numpages = {6},
keywords = {software product lines, non-functional requirements, model driven engineering},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1145/3442389,
author = {Castro, Thiago and Teixeira, Leopoldo and Alves, Vander and Apel, Sven and Cordy, Maxime and Gheyi, Rohit},
title = {A Formal Framework of Software Product Line Analyses},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3442389},
doi = {10.1145/3442389},
abstract = {A number of product-line analysis approaches lift analyses such as type checking, model checking, and theorem proving from the level of single programs to the level of product lines. These approaches share concepts and mechanisms that suggest an unexplored potential for reuse of key analysis steps and properties, implementation, and verification efforts. Despite the availability of taxonomies synthesizing such approaches, there still remains the underlying problem of not being able to describe product-line analyses and their properties precisely and uniformly. We propose a formal framework that models product-line analyses in a compositional manner, providing an overall understanding of the space of family-based, feature-based, and product-based analysis strategies. It defines precisely how the different types of product-line analyses compose and inter-relate. To ensure soundness, we formalize the framework, providing mechanized specification and proofs of key concepts and properties of the individual analyses. The formalization provides unambiguous definitions of domain terminology and assumptions as well as solid evidence of key properties based on rigorous formal proofs. To qualitatively assess the generality of the framework, we discuss to what extent it describes five representative product-line analyses targeting the following properties: safety, performance, dataflow facts, security, and functional program properties.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {34},
numpages = {37},
keywords = {product-line analysis, Software product lines}
}

@inproceedings{10.1145/2701319.2701326,
author = {Soares, Larissa Rocha and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Non-Functional Properties in Software Product Lines: A Reuse Approach},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701326},
doi = {10.1145/2701319.2701326},
abstract = {Software Product Line Engineering (SPLE) emerges for software organizations interested in customized products at reasonable costs. Based on the selection of features, stakeholders can derive programs satisfying a range of functional properties and non-functional ones. The explicit definition of Non-Functional Properties (NFP) during software configuration has been considered a challenging task. Dealing with them is not well established yet, neither in theory nor in practice. In this sense, we present a framework to specify NFP for SPLE and we also propose a reuse approach that promotes the reuse of NFP values during the product configuration. We discuss the results of a case study aimed to evaluate the applicability of the proposed work.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {67–74},
numpages = {8},
keywords = {Software Product Line, Quality Attributes, Empirical Software Engineering},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@inproceedings{10.1145/3233027.3233028,
author = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
title = {A study and comparison of industrial vs. academic software product line research published at SPLC},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233028},
doi = {10.1145/3233027.3233028},
abstract = {The study presented in this paper aims to provide evidence for the hypothesis that software product line research has been changing and that the works in industry and academia have diverged over time. We analysed a subset (140) of all (593) papers published at the Software Product Line Conference (SPLC) until 2017. The subset was randomly selected to cover all years as well as types of papers. We assessed the research type of the papers (academic or industry), the kind of evaluation (application example, empirical, etc.), and the application domain. Also, we assessed which product line life-cycle phases, development practices, and topics the papers address. We present an analysis of the topics covered by academic vs. industry research and discuss the evolution of these topics and their relation over the years. We also discuss implications for researchers and practitioners. We conclude that even though several topics have received more attention than others, academic and industry research on software product lines are actually rather in line with each other.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {14–24},
numpages = {11},
keywords = {software product lines, industry, academia, SPLC},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {software certification, safety-critical embedded systems, development process},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@inproceedings{10.1145/2491627.2493906,
author = {Bashari, Mahdi and Bagheri, Ebrahim},
title = {Engineering self-adaptive systems and dynamic software product line},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2493906},
doi = {10.1145/2491627.2493906},
abstract = {Self-adaptive systems are a class of software applications, which are able to dynamically transform their internal structure and hence their behavior in response to internal or external stimuli. The transformation may provide the basis for new functionalities or improve or maintain non-functional properties in order to match the application better to its operational requirements and standards. Software Product Line Engineering has rich methods and techniques in variability modeling and management which is one of the main issues in developing self-adaptive systems. Dynamic software product lines (DSPL) have been proposed to exploit the knowledge acquired in SPLE to develop self-adaptive software systems.In this tutorial, we portray the problem of developing self-adaptive systems. Then we investigate how the idea of dynamic software product line could help to deal with the challenges that we face in developing efficient self-adaptive software. We also offer insight into the different approaches that use dynamic software product line engineering for developing self-adaptive systems focusing on practical approaches by showing how the approaches are applied to real case studies and also methods for evaluating these approaches. This tutorial also discuss how DSPL could be used some relevant areas to self-adaptive systems and challenges which still exist in the area.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {285},
numpages = {1},
keywords = {self-adaptive systems, dynamic software product line},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2364412.2364445,
author = {Martinez, Jabier and Thurimella, Anil Kumar},
title = {Collaboration and source code driven bottom-up product line engineering},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364445},
doi = {10.1145/2364412.2364445},
abstract = {Companies that develop similar software systems often transition from single-system development to software product line development. In this transition, reusable assets are identified and incrementally created over a period of time. Bottom-up Software Product Line Engineering approaches aid stakeholders to identify variability from the legacy artifacts. One of these artifacts is the legacy source code. In this paper, we contribute the Collaboration and Source Code Driven Bottom-up approach, with two main enhancements. We apply clone detection and architecture reengineering techniques for identifying variability from the legacy artifacts. These techniques which have been traditionally used for maintaining software are now used for identifying variability and analyze code coupling and cohesion from the legacy code. Our second enhancement is improving stakeholder collaboration by guiding the domain experts in order to decide on variability. In particular, we apply Questions, Options and Criteria technique for capturing rationale and supporting collaboration.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {196–200},
numpages = {5},
keywords = {variability modeling, software product line engineering, rationale, knowledge management, clone detection, architecture reengineering},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/APSEC.2008.45,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Saake, Gunter},
title = {Measuring Non-Functional Properties in Software Product Line for Product Derivation},
year = {2008},
isbn = {9780769534466},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2008.45},
doi = {10.1109/APSEC.2008.45},
abstract = {A software product line (SPL) enables stakeholders to derive different software products for a domain while providing a high degree of reuse of their code units. Software products are derived in a configuration process by composing different code units. The configuration process becomes complex if SPLs contain hundreds of features. In many cases, a stakeholder is not only interested in functional but also in non-functional properties of a desired product. Because SPLs can be used in different application scenarios alternative implementations of already existing functionality are developed to meet special non-functional requirements, like restricted binary size and performance guarantees. To enable these complex configurations we discuss and present techniques to measure non-functional properties of software modules and use these values to compute SPL configurations optimized to the users needs.},
booktitle = {Proceedings of the 2008 15th Asia-Pacific Software Engineering Conference},
pages = {187–194},
numpages = {8},
keywords = {Software Product Lines, Product Derivation, Non-functional Properties},
series = {APSEC '08}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1109/SPLC.2011.44,
author = {Nolan, Andy J. and Abrahao, Silvia and Clements, Paul and McGregor, John D. and Cohen, Sholom},
title = {Towards the Integration of Quality Attributes into a Software Product Line Cost Model},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.44},
doi = {10.1109/SPLC.2011.44},
abstract = {A good estimation tool offers a "model" of a project and is usually used to estimate cost and schedule, but it can also be used to help make trade decisions that affect cost and schedule as well as to estimate risks and opportunities. It was evident that Rolls-Royce needed a cost model to underpin decisions when they launched a Software Product Line initiative. The first generation cost model was based on COCOMO II, which represents the software product as a single size measure (Source Lines of Code) but makes limited use of the architecture or any characteristics of the product being developed. The next generation of the cost model, currently under development, is intended to account for the quality attributes of the core assets and the resulting products in order to estimate their impact on cost and net-benefit to the business. The objective of this paper is to describe our current efforts to integrate key quality attributes into the SPL cost model. We describe the quality attributes selected, the reason for their selection and the benefits we expect to obtain after integrating them into the model.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {203–212},
numpages = {10},
keywords = {Software Product Lines, Safety-Critical Software, Quality Attributes, Industrial Experiences, Cost Estimation},
series = {SPLC '11}
}

@inproceedings{10.1145/3307630.3342385,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {HADAS: Analysing Quality Attributes of Software Configurations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342385},
doi = {10.1145/3307630.3342385},
abstract = {Software Product Lines (SPLs) are highly configurable systems. Automatic analyses of SPLs rely on solvers to navigate complex dependencies among features and find legal solutions. Variability analysis tools are complex due to the diversity of products and domain-specific knowledge. On that, while there are experimental studies that analyse quality attributes, the knowledge is not easily accessible for developers, and its appliance is not trivial. Aiming to allow the industry to quality-explore SPL design spaces, we developed the HADAS assistant that: (1) models systems and collects quality attributes metrics in a cloud repository, and (2) reasons about it helping developers with quality attributes requirements.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {13–16},
numpages = {4},
keywords = {variability, software product line, numerical, model, attribute, NFQA},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s10664-019-09787-6,
author = {Berger, Thorsten and Stegh\"{o}fer, Jan-Philipp and Ziadi, Tewfik and Robin, Jacques and Martinez, Jabier},
title = {The state of adoption and the challenges of systematic variability management in industry},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09787-6},
doi = {10.1007/s10664-019-09787-6},
abstract = {Handling large-scale software variability is still a challenge for many organizations. After decades of research on variability management concepts, many industrial organizations have introduced techniques known from research, but still lament that pure textbook approaches are not applicable or efficient. For instance, software product line engineering—an approach to systematically develop portfolios of products—is difficult to adopt given the high upfront investments; and even when adopted, organizations are challenged by evolving their complex product lines. Consequently, the research community now mainly focuses on re-engineering and evolution techniques for product lines; yet, understanding the current state of adoption and the industrial challenges for organizations is necessary to conceive effective techniques. In this multiple-case study, we analyze the current adoption of variability management techniques in twelve medium- to large-scale industrial cases in domains such as automotive, aerospace or railway systems. We identify the current state of variability management, emphasizing the techniques and concepts they adopted. We elicit the needs and challenges expressed for these cases, triangulated with results from a literature review. We believe our results help to understand the current state of adoption and shed light on gaps to address in industrial practice.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1755–1797},
numpages = {43},
keywords = {Challenges, Multiple-case study, Software product lines, Variability management}
}

@article{10.1504/ijaip.2021.116357,
author = {Rajesh, Sudha and Sekar, A. Chandra},
title = {Evaluation of quality attributes of software design patterns using association rules},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {19},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.116357},
doi = {10.1504/ijaip.2021.116357},
abstract = {For the past decades, there were many analysing methods used which in turn to analyse only the views of a single stakeholder. The analysing progression facilitates us to guarantee the quality of overall design. By doing so there were many limitations that lead to critical situations in the development process. This work proposes to collect all the stakeholders' decisions in a single structural view, which expose the centric-view decision in an architectural design. It also used to deal with the disagreement in vision by examining it, with the premium software quality characteristic. The quality attributes are evaluated by association rules to improve the excellence of the software design. Association rules are implemented using R tool to get the support, confidence, lift and count values of each attribute along with its metric. The best quality attributes are chosen after managing the stakeholders' conflicts, since all the systems are designed due to the stakeholders' concerns. The best design patterns can be evaluated along with the distinguished attributes. These patterns are evaluated by fixing the basic principles of patterns based on their excellence.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {314–327},
numpages = {13},
keywords = {quality attributes, support, design patterns, confidence, association rules}
}

@inproceedings{10.1145/2491627.2491647,
author = {Murashkin, Alexandr and Antkiewicz, Micha\l{} and Rayside, Derek and Czarnecki, Krzysztof},
title = {Visualization and exploration of optimal variants in product line engineering},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491647},
doi = {10.1145/2491627.2491647},
abstract = {The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {111–115},
numpages = {5},
keywords = {visualization, product line engineering, pareto front, optimal variant, feature modeling, exploration, clafer, ClaferMoo visualizer, ClaferMoo},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2364412.2364451,
author = {Machado, Ivan do Carmo},
title = {Towards a reasoning framework for software product line testing},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364451},
doi = {10.1145/2364412.2364451},
abstract = {Testing can still be considered a bottleneck for software product line engineering. The variability implemented in the source artifacts increases its complexity. Due to its key role for product line quality, testing requires cost-effective practices, such as techniques for test selection should be produced to enable companies to experience the substantial production cost savings. In this paper, we present the outline of a Ph.D. research aimed at developing a reasoning framework to improve SPL testing practices. Based on multiple sources of evidence, the framework intends to provide testers with an automated reasoner for determining which techniques may be suitable for a given variability implementation mechanism, and how these should be employed in order to makes testing in a SPL a more effective and efficient practice. We plan to perform empirical evaluations in order to assess the proposal effectiveness.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {229–232},
numpages = {4},
keywords = {variability management, software testing, software product lines, fault models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2499777.2499779,
author = {Antkiewicz, Micha\l{} and B\k{a}k, Kacper and Murashkin, Alexandr and Olaechea, Rafael and Liang, Jia Hui (Jimmy) and Czarnecki, Krzysztof},
title = {Clafer tools for product line engineering},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2499779},
doi = {10.1145/2499777.2499779},
abstract = {Clafer is a lightweight yet expressive language for structural modeling: feature modeling and configuration, class and object modeling, and metamodeling. Clafer Tools is an integrated set of tools based on Clafer. In this paper, we describe some product-line variability modeling scenarios of Clafer Tools from the viewpoints of product-line owner, product-line engineer, and product engineer.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {130–135},
numpages = {6},
keywords = {clafer configurator, ClaferWiki, ClaferMOO visualizer, ClaferMOO, ClaferIG, Clafer},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1109/APSEC.2010.25,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality Attributes Assessment for Feature-Based Product Configuration in Software Product Line},
year = {2010},
isbn = {9780769542669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2010.25},
doi = {10.1109/APSEC.2010.25},
abstract = {Product configuration based on a feature model in software product lines is the process of selecting the desired features based on customers’ requirements. In most cases, application engineers focus on the functionalities of the target product during product configuration process whereas the quality attributes are handled until the final product is produced. However, it is costly to fix the problem if the quality attributes have not been considered in the product configuration stage. The key issue of assessing a quality attribute of a product configuration is to measure the impact on a quality attribute made by the set of functional variable features selected in a configuration. Current existing approaches have several limitations, such as no quantitative measurements provided or requiring existing valid products and heavy human effort for the assessment. To overcome theses limitations, we propose an Analytic Hierarchical Process (AHP) based approach to estimate the relative importance of each functional variable feature on a quality attribute. Based on the relative importance value of each functional variable feature on a quality attribute, the level of quality attributes of a product configuration in software product lines can be assessed. An illustrative example based on the Computer Aided Dispatch (CAD) software product line is presented to demonstrate how the proposed approach works.},
booktitle = {Proceedings of the 2010 Asia Pacific Software Engineering Conference},
pages = {137–146},
numpages = {10},
keywords = {quality attributes assessment, product configuration, Analytic Hierarchical Process (AHP)},
series = {APSEC '10}
}

@inproceedings{10.1145/2648511.2648537,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Gimenes, Itana M. S. and Oizumi, Willian Nalepa},
title = {A search-based approach for software product line design},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648537},
doi = {10.1145/2648511.2648537},
abstract = {The Product Line Architecture (PLA) can be improved by taking into account key factors such as feature modularization, and by continuously evaluating its design according to metrics. Search-Based Software Engineering (SBSE) principles can be used to support an informed-design of PLAs. However, existing search-based design works address only traditional software design not considering intrinsic Software Product Line aspects. This paper presents MOA4PLA, a search-based approach to support the PLA design. It gives a multi-objective treatment to the design problem based on specific PLA metrics. A metamodel to represent the PLA and a novel search operator to improve feature modularization are proposed. Results point out that the application of MOA4PLA leads to PLA designs with well modularized features, contributing to improve features reusability and extensibility. It raises a set of solutions with different design trade-offs that can be used to improve the PLA design.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {237–241},
numpages = {5},
keywords = {software product lines, searchbased PLA design, multi-objective algorithms},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3168365.3168373,
author = {Pereira, Juliana Alves and Schulze, Sandro and Krieter, Sebastian and Ribeiro, M\'{a}rcio and Saake, Gunter},
title = {A Context-Aware Recommender System for Extended Software Product Line Configurations},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168373},
doi = {10.1145/3168365.3168373},
abstract = {Mass customization of standardized products has become a trend to succeed in today's market environment. Software Product Lines (SPLs) address this trend by describing a family of software products that share a common set of features. However, choosing the appropriate set of features that matches a user's individual interests is hampered due to the overwhelming amount of possible SPL configurations. Recommender systems can address this challenge by filtering the number of configurations and suggesting a suitable set of features for the user's requirements. In this paper, we propose a context-aware recommender system for predicting feature selections in an extended SPL configuration scenario, i.e. taking nonfunctional properties of features into consideration. We present an empirical evaluation based on a large real-world dataset of configurations derived from industrial experience in the Enterprise Resource Planning domain. Our results indicate significant improvements in the predictive accuracy of our context-aware recommendation approach over a state-of-the-art binary-based approach.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {Software Product Lines, Recommender Systems, Non-Functional Properties, Feature Model, Configuration},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/2019136.2019172,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Using knowledge-based systems to manage quality attributes in software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019172},
doi = {10.1145/2019136.2019172},
abstract = {Product configuration in a feature model in software product line engineering is a process, in which the desired features are selected based on the customers' functional requirements and non-functional requirements. The functional requirements of the target product can be satisfied by including the proper functional features. However, there is no such a straightforward way to realize the non-functional requirements and quality attributes of the target product. In our early work, we have developed a quantitative based method to assess the quality attributes for a configured product. However, this approach cannot adequately represent the inter-relationships among quality attributes which play an important role in product configuration process. We supplement our previous work by introducing a quality attribute knowledge base (QA_KB) to represent the inter-relationships among different quality attributes in a SPL. Furthermore, we develop algorithms for configuring a product based on customers' quality requirements. We also use a case study to illustrate our approach.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {32},
numpages = {7},
keywords = {software product line, quality attributes, product configuration, non-functional requirements, feature model},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2019136.2019182,
author = {McGregor, John D. and Monteith, J. Yates and Zhang, Jie},
title = {Quantifying value in software product line design},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019182},
doi = {10.1145/2019136.2019182},
abstract = {A software product line is a strategic investment for an organization. Besides the initial decision to use a product line approach other strategic decisions are made, including which variations to accommodate. In this paper we present an adaptation of an equation for computing option values. The equation can be used to understand the economic impact of adding a variation point to the product line architecture. The equation was exercised on multiple sets of hypothetical data and and produced the expected changes from one data set to another. In the future the equation will be validated with data from real projects. We describe some practical sources of values for the parameters of the equation.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {40},
numpages = {7},
keywords = {strategic software design, software engineering},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2892664.2892686,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Towards the dynamic reconfiguration of quality attributes},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892686},
doi = {10.1145/2892664.2892686},
abstract = {There are some Quality Attributes (QAs) whose variability is addressed through functional variability in the software architecture. Separately modelling the variability of these QAs from the variability of the base functionality of the application has many advantages (e.g., a better reusability), and facilitates the reconfiguration of the QA variants at runtime. Many factors may vary the QA functionality: variations in the user preferences and usage needs; variations in the non-functional QAs; variations in resources, hardware, or even in the functionality of the base application, that directly affect the product's QAs. In this paper, we aim to elicit the relationships and dependencies between the functionalities required to satisfy the QAs and all those factors that can provoke a reconfiguration of the software architecture at runtime. We follow an approach in which the variability of the QAs is modelled separately from the base application functionality, and propose a dynamic approach to reconfigure the software architecture based on those reconfiguration criteria.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {131–136},
numpages = {6},
keywords = {variability, software architecture, reconfiguration, SPL, Quality attributes},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@inproceedings{10.1145/3167132.3167353,
author = {Pereira, Juliana Alves and Martinez, Jabier and Gurudu, Hari Kumar and Krieter, Sebastian and Saake, Gunter},
title = {Visual guidance for product line configuration using recommendations and non-functional properties},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167353},
doi = {10.1145/3167132.3167353},
abstract = {Software Product Lines (SPLs) are a mature approach for the derivation of a family of products using systematic reuse. Different combinations of predefined features enable tailoring the product to fit the needs of each customer. These needs are related to functional properties of the system (optional features) as well as non-functional properties (e.g., performance or cost of the final product). In industrial scenarios, the configuration process of a final product is complex and the tool support is usually limited to check functional properties interdependencies. In addition, the importance of nonfunctional properties as relevant drivers during configuration has been overlooked. Thus, there is a lack of holistic paradigms integrating recommendation systems and visualizations that can help the decision makers. In this paper, we propose and evaluate an interrelated set of visualizations for the configuration process filling these gaps. We integrate them as part of the FeatureIDE tool and we evaluate its effectiveness, scalability, and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {2058–2065},
numpages = {8},
keywords = {visualization, software product lines, recommendation systems, feature model, configuration},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1007/s10270-020-00806-5,
author = {Graics, Bence and Moln\'{a}r, Vince and V\"{o}r\"{o}s, Andr\'{a}s and Majzik, Istv\'{a}n and Varr\'{o}, D\'{a}niel},
title = {Mixed-semantics composition of statecharts for the component-based design of reactive systems},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00806-5},
doi = {10.1007/s10270-020-00806-5},
abstract = {The increasing complexity of reactive systems can be mitigated with the use of components and composition languages in model-driven engineering. Designing composition languages is a challenge itself as both practical applicability (support for different composition approaches in various application domains), and precise formal semantics (support for verification and code generation) have to be taken into account. In our Gamma Statechart Composition Framework, we designed and implemented a composition language for the synchronous, cascade synchronous and asynchronous composition of statechart-based reactive components. We formalized the semantics of this composition language that provides the basis for generating composition-related Java source code as well as mapping the composite system to a back-end model checker for formal verification and model-based test case generation. In this paper, we present the composition language with its formal semantics, putting special emphasis on design decisions related to the language and their effects on verifiability and applicability. Furthermore, we demonstrate the design and verification functionality of the composition framework by presenting case studies from the cyber-physical system domain.},
journal = {Softw. Syst. Model.},
month = nov,
pages = {1483–1517},
numpages = {35},
keywords = {Formal verification, Formal semantics, Composition language, Statecharts, Component-based design}
}

@inproceedings{10.1109/SPLC.2011.20,
author = {Siegmund, Norbert and Rosenmuller, Marko and Kastner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable Prediction of Non-functional Properties in Software Product Lines},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.20},
doi = {10.1109/SPLC.2011.20},
abstract = {A software product line is a family of related software products, typically, generated from a set of common assets. Users can select features to derive a product that fulfills their needs. Often, users expect a product to have specific non-functional properties, such as a small footprint or a minimum response time. Because a product line can contain millions of products, it is usually not feasible to generate and measure non-functional properties for each possible product of a product line. Hence, we propose an approach to predict a product's non-functional properties, based on the product's feature selection. To this end, we generate and measure a small set of products, and by comparing the measurements, we approximate each feature's non-functional properties. By aggregating the approximations of selected features, we predict the product's properties. Our technique is independent of the implementation approach and language. We show how already little domain knowledge can improve predictions and discuss trade-offs regarding accuracy and the required number of measurements. Although our approach is in general applicable for quantifiable non-functional properties, we evaluate it for the non-functional property footprint. With nine case studies, we demonstrate that our approach usually predicts the footprint with an accuracy of 98% and an accuracy of over 99% if feature interactions are known.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {160–169},
numpages = {10},
keywords = {software product lines, predicition, non-functional properties, measurement, SPL Conqueror},
series = {SPLC '11}
}

@inproceedings{10.1145/2499777.2500715,
author = {Ishida, Yuzo},
title = {Scalable variability management for enterprise applications with data model driven development},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500715},
doi = {10.1145/2499777.2500715},
abstract = {Unlike embedded systems, some of enterprise systems are evolved over the decades. The predictability of requirements is a key to success in building reusable assets however it is very hard to predict future business context changes, which are driving factors of requirements. Thus, both functional and context variability must be managed in order to satisfy ever-changing requirements. Scalability does matter for enterprise systems in two aspects. One aspect comes from data volume. Once data become big, it is difficult to maintain performance requirements without de-normalizing database schema. Since database de-normalization is driven by non-functional properties, a model driven approach is not feasible if the model cannot express such properties. Another aspect comes from the unpredictability of future functional requirements. A functional decomposition of enterprise systems usually introduces ever-increasing complexity among systems' interactions due to cross-cutting requirements across functional systems. This paper reflects our empirical studies in data intensive large enterprise systems such as retail and telecommunication industries with industry independent application framework to separate functional and non-functional concerns. Our variability management technique is based on database schema modeling, which can be evolved incrementally in scaling an enterprise system with both data and functional aspects.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {90–93},
numpages = {4},
keywords = {type theory, relational algebra, quality attributes, higher-order simple predicate logic, core assets},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@article{10.1016/j.asoc.2016.08.024,
author = {dos Santos Neto, Pedro de Alcntara and Britto, Ricardo and Rablo, Ricardo de Andrade Lira and Cruz, Jonathas Jivago de Almeida and Lira, Werney Ayala Luz},
title = {A hybrid approach to suggest software product line portfolios},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.024},
doi = {10.1016/j.asoc.2016.08.024},
abstract = {Graphical abstractDisplay Omitted HighlightsThe work proposes a hybrid approach to deal with the Product Portfolio Scope Problem.The approach is composed by a solution to deploy the feature relevance indicated by the customers into code assets of a SPL, based on a systematic method (SQFD).The approach includes a method to estimate the cost of an asset based on common and relevant measures related to source code, together with a fuzzy system to deal with the imprecision to set reference values.The work presents an application of an NSGA-II to search for products minimizing the cost and maximizing the relevance of the candidate products.The approach was evaluated using different scenarios, exploring the mains aspects related to method in the practice: size, granularity of features and products search space.The previous version of our hybrid approach was dependent on the employed technologies and algorithms. Herein we reformulate our approach, detaching it from any particular technique/algorithm.The data collection process associated with our approach was improved to facilitate the hybrid approach's usage and mitigate associated construct validity threats.A more comprehensive evaluation, which focused on show the real word usefulness and scalability of our hybrid approach. To validate the usefulness of our approach, it was used the SPL associated with a tool broadly employed in both industrial and academic contexts (ArgoUML-SPL). The scalability of our approach was evaluated using a synthetic SPL.All the experiments were based on the guidelines defined by Arcuri and Briand in order to evaluate the statistical significance of this kind of work. Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1243–1255},
numpages = {13},
keywords = {Software product lines, Search based software engineering, Search based feature model selection, Product portfolio scoping, NSGA-II, Fuzzy inference systems, Feature model selection problem}
}

@inproceedings{10.5220/0007955905140521,
author = {Derras, M. and Deruelle, L. and Douin, J.-M. and Levy, N. and Losavio, F. and Mahamane, R. and Reiner, V.},
title = {Approach for Variability Management of Legal Rights in Human Resources Software Product Lines},
year = {2019},
isbn = {9789897583797},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0007955905140521},
doi = {10.5220/0007955905140521},
abstract = {This work concerns software product lines (SPL); it comes from the experience gained collaborating with Berger-Levrault, a French society leader in Human Resources systems. This enterprise serves many French and European territorial communities. They had a variability problem associated to the differences of applicable legal rights in different countries or territories, and this activity was performed manually at a high cost. On the other hand, functionalities were common and mandatory and did not vary much. The crucial issue in SPL development and practice is to manage the correct selection of variants. However, no standard methods have been developed yet, and industry builds SPL using on-the-market or in-house techniques and methods, aware of the benefits a product line can provide; nevertheless, this development must return the investment, and this is not always the case. In this work an approach to variability management in case of legal rights applicability to different entities is proposed. This architecture-centric and quality-based approach uses a reference architecture that has been built with a bottom-up strategy. Variability is incorporated to the reference architecture at abstract level considering non-functional properties. A production plan to reduce the gap between abstraction and implementation levels is defined.},
booktitle = {Proceedings of the 14th International Conference on Software Technologies},
pages = {514–521},
numpages = {8},
keywords = {Variability Management, Software Product Lines (SPL), Legal Rights., Human Resources},
location = {Prague, Czech Republic},
series = {ICSOFT 2019}
}

@inproceedings{10.1145/3461001.3471147,
author = {Kenner, Andy and May, Richard and Kr\"{u}ger, Jacob and Saake, Gunter and Leich, Thomas},
title = {Safety, security, and configurable software systems: a systematic mapping study},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471147},
doi = {10.1145/3461001.3471147},
abstract = {Safety and security are important properties of any software system, particularly in safety-critical domains, such as embedded, automotive, or cyber-physical systems. Moreover, particularly those domains also employ highly-configurable systems to customize variants, for example, to different customer requirements or regulations. Unfortunately, we are missing an overview understanding of what research has been conducted on the intersection of safety and security with configurable systems. To address this gap, we conducted a systematic mapping study based on an automated search, covering ten years (2011--2020) and 65 relevant (out of 367) publications. We classified each publication based on established security and safety concerns (e.g., CIA triad) as well as the connection to configurable systems (e.g., ensuring security of such a system). In the end, we found that considerably more research has been conducted on safety concerns, but both properties seem under-explored in the context of configurable systems. Moreover, existing research focuses on two directions: Ensuring safety and security properties in product-line engineering; and applying product-line techniques to ensure safety and security properties. Our mapping study provides an overview of the current state-of-the-art as well as open issues, helping practitioners identify existing solutions and researchers define directions for future research.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {148–159},
numpages = {12},
keywords = {software product line engineering, security, safety, mapping study, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@article{10.1016/j.infsof.2012.07.020,
author = {Siegmund, Norbert and Rosenm\"{u}Ller, Marko and K\"{a}Stner, Christian and Giarrusso, Paolo G. and Apel, Sven and Kolesnikov, Sergiy S.},
title = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.020},
doi = {10.1016/j.infsof.2012.07.020},
abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {491–507},
numpages = {17},
keywords = {Software product lines, SPL Conqueror, Prediction, Non-functional properties, Measurement}
}

@inproceedings{10.1145/2499777.2500720,
author = {Nakagawa, Elisa Yumi and Oquendo, Flavio},
title = {Perspectives and challenges of reference architectures in multi software product line},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500720},
doi = {10.1145/2499777.2500720},
abstract = {Multi Software Product Line (MSPL or simply Multi Product Line - MPL) has recently emerged as a new approach to develop software systems, mainly those large, complex ones. This approach can be investigated to the development of Systems-of-Systems (SoS), i.e., a new class of software systems that are resulted by the integration of several operationally independent systems. In another perspective, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the research area of Reference Architecture. In this context, the main motivation of this paper is that, in spite of the positive impact of this type of architecture on reuse and productivity, the use of the knowledge contained in existing reference architectures in order to develop MPL, specially to develop SoS, has not been widely explored yet. In this scenario, the main objective of this paper is to present a set of perspectives and challenges of research to use reference architectures in the context of MPL. For this, we have based on our previous experience as well as the literature of SPL, SoS, and reference architecture. As result, we have observed that reference architectures together with MPL seems to be a quite promising research topic. To conclude, we also intend this paper can identify new required research in this context, contributing to improve reuse and productivity in MPL.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {100–103},
numpages = {4},
keywords = {system-of-systems, reference architecture, multi product line},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/2851613.2851977,
author = {Kim, Jin Hyun and Legay, Axel and Traonouez, Louis-Marie and Acher, Mathieu and Kang, Sungwon},
title = {A formal modeling and analysis framework for software product line of preemptive real-time systems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851977},
doi = {10.1145/2851613.2851977},
abstract = {This paper presents a formal analysis framework to analyze a family of platform products w.r.t. real-time properties. First, we propose an extension of the widely-used feature model, called Property Feature Model (PFM), that distinguishes features and properties explicitly Second, we present formal behavioral models of components of a real-time scheduling unit such that all real-time scheduling units implied by a PFM are automatically composed to be analyzed against the properties given by the PFM. We apply our approach to the verification of the schedulability of a family of scheduling units using the symbolic and statistical model checkers of Uppaal.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1562–1565},
numpages = {4},
keywords = {model checking, platform-constrained, scheduling systems, software product line engineering},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/3307630.3342384,
author = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
title = {MetricHaven: More than 23,000 Metrics for Measuring Quality Attributes of Software Product Lines},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342384},
doi = {10.1145/3307630.3342384},
abstract = {Variability-aware metrics are designed to measure qualitative aspects of software product lines. As we identified in a prior SLR [6], there exist already many metrics that address code or variability separately, while the combination of both has been less researched. MetricHaven fills this gap, as it extensively supports combining information from code files and variability models. Further, we also enable the combination of well established single system metrics with novel variability-aware metrics, going beyond existing variability-aware metrics. Our tool supports most prominent single system and variability-aware code metrics. We provide configuration support for already implemented metrics, resulting in 23,342 metric variations. Further, we present an abstract syntax tree developed for MetricHaven, that allows the realization of additional code metrics.Tool: https://github.com/KernelHaven/MetricHavenVideo: https://youtu.be/vPEmD5Sr6gM},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {25–28},
numpages = {4},
keywords = {variability models, software product lines, metrics, implementation, feature models, SPL},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2364412.2364459,
author = {Lee, Hyesun and Yang, Jin-seok and Kang, Kyo C.},
title = {VULCAN: architecture-model-based workbench for product line engineering},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364459},
doi = {10.1145/2364412.2364459},
abstract = {Adaptability and reusability are important quality attributes for software targeted for global market due to diverse market needs, ever increasing number of features, rapidly changing technologies, and various laws/standards of different countries. In response to these requirements, software development organizations are interested in product line engineering and searching for support tools. However, most of the existing tools for supporting product line engineering focus only on providing mechanisms for instantiating products without adequately supporting development of software assets that are adaptable and reusable.To address this problem, we provide a CASE tool, called VULCAN, that provides architecture models/patterns that are adaptable/reusable and also supports mechanisms for instantiating products from assets. We have applied VULCAN to various product lines including glucose management systems and elevator control systems, and we could experience that maintainability of the assets has improved substantially because a large portion of the assets are specifications rather than low-level code and product-specific code is generated from the specifications.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {260–264},
numpages = {5},
keywords = {software product line, feature-oriented, architecture-model-based},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1109/SEAA.2014.48,
author = {Soares, Larissa Rocha and Potena, Pasqualina and Machado, Ivan do Carmo and Crnkovic, Ivica and Almeida, Eduardo Santana de},
title = {Analysis of Non-functional Properties in Software Product Lines: A Systematic Review},
year = {2014},
isbn = {9781479957958},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2014.48},
doi = {10.1109/SEAA.2014.48},
abstract = {Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.},
booktitle = {Proceedings of the 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {328–335},
numpages = {8},
keywords = {Systematic Literature Review, Software Product Lines, Product Derivation, Non-functional Properties},
series = {SEAA '14}
}

@article{10.4018/ijkss.2014100102,
author = {Tian, Kun},
title = {Adding More Agility to Software Product Line Methods: A Feasibility Study on Its Customization Using Agile Practices},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100102},
doi = {10.4018/ijkss.2014100102},
abstract = {Software Product Line Methods SPLMs have been continuously gaining attention, especially in practice, for on one hand, they address diverse market needs while controlling costs by planned systematic reuse in core assets development domain engineering, and on another hand, they reduce products' time-to-market, achieving a certain level of agility in product development application engineering. More cost-effective and agile as they are than traditional development methods for producing families of similar products, SPLMs still seem to be heavy weight in nature. In SPLMs, significant up-front commitments are involved in development of a flexible product platform, which will be modified into a range of products sharing common features. Agile Methods AMs share similar goals with SPLMs, e.g., on rapidly delivering high quality software that meets the changing needs of stakeholders. However, they appear to differ significantly practices. The purpose of this work is to compare Agile and Software Product line approaches from fundamental goals/principles, engineering, software quality assurance, sand project management perspectives, etc. The results of the study can be used to determine the feasibility of tailoring a software product line approach with Agile practices, resulting in a lighter-weight approach that provides mass customization, reduced time-to-market, and improved customer satisfaction.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {17–34},
numpages = {18},
keywords = {Time-To-Market, Software Product Line Methods SPLMs, Customer Satisfaction, Agile Methods AMs}
}

@inproceedings{10.1145/2695664.2695797,
author = {Tizzei, Leonardo P. and Azevedo, Leonardo G. and de Bayser, Maximilien and Cerqueira, Renato F. G.},
title = {Architecting cloud tools using software product line techniques: an exploratory study},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695797},
doi = {10.1145/2695664.2695797},
abstract = {Multitenant cloud computing tools are usually complex and have to manage variabilities to support customization. Software Product Line (SPL) techniques have been successfully applied in the industry to manage variability in complex systems. However, few works in the literature discuss the application of SPL techniques to architect industry cloud computing tools, resulting in a lack of support to cloud architects on how to apply such techniques. This work presents how software product line techniques can be applied for architecting cloud tools, and discusses the benefits, drawbacks, and some challenges of applying such techniques to develop a real industry cloud tool, named as Installation Service.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1441–1448},
numpages = {8},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2797433.2797456,
author = {Galster, Matthias},
title = {Architecting for Variability in Quality Attributes of Software Systems},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797456},
doi = {10.1145/2797433.2797456},
abstract = {Variability in software systems is usually concerned with variability in features and functionality. However, variability also occurs in quality attributes (e.g., performance, security) and quality attribute requirements (for example, a performance requirement may state that a system must respond to a user request within 0.1 seconds). We discuss what variability in quality attributes is, including several scenarios in which variability in quality attributes can occur. We then discuss the state of research and what we know about variability in quality attributes, including some existing research to address the challenge of identifying, implementing and managing variability in quality attributes. Finally, we discuss potential directions for future research.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {software architecture, quality attributes, Variability},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@article{10.1016/j.infsof.2012.09.007,
author = {Guana, Victor and Correal, Dario},
title = {Improving software product line configuration: A quality attribute-driven approach},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.007},
doi = {10.1016/j.infsof.2012.09.007},
abstract = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {541–562},
numpages = {22},
keywords = {Variability management, Software architecture, Sensitivity points, Quality evaluation, Model driven - software product lines, Domain specific modeling}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Software quality, Measurement, Mapping study, Design-time quality attributes}
}

@inproceedings{10.1145/2851613.2851964,
author = {Cool, Benjamin and Knieke, Christoph and Rausch, Andreas and Schindler, Mirco and Strasser, Arthur and Vogel, Martin and Brox, Oliver and Jauns-Seyfried, Stefanie},
title = {From product architectures to a managed automotive software product line architecture},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851964},
doi = {10.1145/2851613.2851964},
abstract = {To keep the software development for vehicles cost efficient, software components are reused for different variants as well as for succeeding generations. Furthermore, cost reductions are achieved by software sharing between the Original Equipment Manufacturer (OEM) and the suppliers. However, as a consequence of the blackboxed view caused by software sharing, no common detailed software product line architecture specification for the Electronic Control Unit (ECU) software exists, as it would be required for analyzing the quality of the product line architecture, planning changes on the product line architecture, checking the compliance between the product architecture and the product line architecture, and therefore, avoiding architecture erosion. Thus, after several product generations, software erosion is growing steadily, resulting in an increasing effort of reusing software components, and planning of further development. Here, we propose an approach for repairing an eroded software consisting of a set of product architectures by applying strategies for recovery and discovery of the product line architecture. Furthermore, we give a methodology for a long-term manageable, plannable, and reuseable software product line architecture for automotive software systems.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1350–1353},
numpages = {4},
keywords = {architecture evolution, architecture quality measures, automotive, software erosion, software product lines},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1007/s00766-013-0165-8,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Dynamic decision models for staged software product line configuration},
year = {2014},
issue_date = {June      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0165-8},
doi = {10.1007/s00766-013-0165-8},
abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process.},
journal = {Requir. Eng.},
month = jun,
pages = {187–212},
numpages = {26},
keywords = {Utility elicitation, Stakeholder preferences, Software product lines, Feature models}
}

@inproceedings{10.1109/SEAA.2013.20,
author = {Horcas, Jos\'{e} Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Variability and Dependency Modeling of Quality Attributes},
year = {2013},
isbn = {9780769550916},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2013.20},
doi = {10.1109/SEAA.2013.20},
abstract = {Functional Quality Attributes (FQAs) are quality attributes that have strong functional implications and so can be easily modeled by software components. Thus, we use an aspect-oriented software product line approach, to model the commonalities and variabilities of FQAs from the early stages of the software development. However, FQAs cannot be modeled in isolation since they usually have dependencies and interactions between them. In this paper we focus on identifying and modeling the dependencies among different FQAs. These dependencies are automatically incorporated into the final software architecture of the system under development, even when the software architect may be unaware of them.},
booktitle = {Proceedings of the 2013 39th Euromicro Conference on Software Engineering and Advanced Applications},
pages = {185–188},
numpages = {4},
keywords = {quality attributes, feature models, dependencies, AO-ADL},
series = {SEAA '13}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {software variability, computer-aided software engineering, Software engineering}
}

@inproceedings{10.1145/2993412.3004847,
author = {Anvaari, Mohsen and S\o{}rensen, Carl-Fredrik and Zimmermann, Olaf},
title = {Associating architectural issues with quality attributes: a survey on expert agreement},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3004847},
doi = {10.1145/2993412.3004847},
abstract = {The architectural decision-making process is a complex and crucial endeavor in companies that develop large and distributed software systems. In this process, choosing and evaluating a solution for each architectural issue depends on decision drivers. The drivers are mainly the business factors (e.g., cost, time-to-market, etc.) and software quality attributes (e.g., security, adaptability, etc.). This paper examines whether there is agreement among experts in associating (i.e., relating) architectural issues with relevant quality attributes. We conducted a survey with 37 experts from several industrial domains who, at least once a month, make one or more architectural decisions. The results show there is poor agreement among these experts in identifying and scoring relevant quality attributes for each architectural issue. Poor agreement implies that the associating task is subjective, and that experts inconsistently define and interpret the relevance of various quality attributes for a given architectural issue that may hurt the sustainability of their architectural decisions. This paper suggests that practitioners in their decision-making should employ approaches that are more systematic. The approaches should be supported by methods and tools designed to diminish the biases of intuitive, experience-based approaches of associating architectural issues with quality attributes.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {11},
numpages = {7},
keywords = {survey, software quality attribute, inter-rater agreement, architectural synthesis, architectural decision},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Structural complexity, Software product line, Software prediction model, Quality attributes, Maintainability, Feature model, Controlled experimentation}
}

@article{10.1016/j.infsof.2012.07.017,
author = {Ghezzi, Carlo and Molzam Sharifloo, Amir},
title = {Model-based verification of quantitative non-functional properties for software product lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.017},
doi = {10.1016/j.infsof.2012.07.017},
abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {508–524},
numpages = {17},
keywords = {Software product lines, Quality analysis, Probabilistic model checking, Parametric verification, Non-functional requirements}
}

@inproceedings{10.1145/2896982.2896988,
author = {Cu, Cuong and Zheng, Yongjie},
title = {Architecture-centric derivation of products in a software product line},
year = {2016},
isbn = {9781450341646},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896982.2896988},
doi = {10.1145/2896982.2896988},
abstract = {It is essential to architecture-centric product line development that product line architecture can be used to drive activities specific to product line development, such as product derivation. This requires a mechanism that can automatically derive the architecture and code of a product instance from the customization of product line architecture. In this paper, we analyze the insufficiency of two existing solutions in this area and present an architecture-centric approach that meets the requirement. The approach can support product line differences in platforms and functions, and generate both product line code and product code. It is based on a product line implementation mechanism that combines a code generation and separation pattern with an architecture-based code annotation technique. We have implemented the approach, and finished a preliminary evaluation with a chat application.},
booktitle = {Proceedings of the 8th International Workshop on Modeling in Software Engineering},
pages = {27–33},
numpages = {7},
keywords = {software architecture, product line architecture, architecture-centric development},
location = {Austin, Texas},
series = {MiSE '16}
}

@inproceedings{10.1109/SPLC.2011.33,
author = {Ghezzi, Carlo and Sharifloo, Amir Molzam},
title = {Verifying Non-functional Properties of Software Product Lines: Towards an Efficient Approach Using Parametric Model Checking},
year = {2011},
isbn = {9780769544878},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2011.33},
doi = {10.1109/SPLC.2011.33},
abstract = {In this paper, we describe how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions in the early stages of development. Furthermore, we discuss how verification time can surprisingly be reduced by applying parametric model checking instead of classic model checking, and show that the approach can be effective in practice.},
booktitle = {Proceedings of the 2011 15th International Software Product Line Conference},
pages = {170–174},
numpages = {5},
keywords = {Software Product Lines, Probabilistic Model Checking, Non-Functional Requirements},
series = {SPLC '11}
}

@inproceedings{10.1109/APSEC.2010.26,
author = {Sincero, Julio and Schroder-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {Approaching Non-functional Properties of Software Product Lines: Learning from Products},
year = {2010},
isbn = {9780769542669},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2010.26},
doi = {10.1109/APSEC.2010.26},
abstract = {Approaching the configuration of non-functional properties (NFPs) in traditional software systems is not an easy task, addressing the configuration of these properties in software product lines (SPLs) imposes even further challenges. Therefore, we have devised the Feedback Approach, which extends the traditional SPL development techniques in order to improve the configuration of NFPs. In this work we present the general guidelines of our approach and also we show the feasibility of the idea by presenting a case study using the Linux Kernel.},
booktitle = {Proceedings of the 2010 Asia Pacific Software Engineering Conference},
pages = {147–155},
numpages = {9},
keywords = {Software Product Lines, Non-Functional Properties, Linux},
series = {APSEC '10}
}

@article{10.1016/j.infsof.2015.09.004,
author = {Souza Neto, Pl\'{a}cido A. and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A.},
title = {Designing service-based applications in the presence of non-functional properties},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.004},
doi = {10.1016/j.infsof.2015.09.004},
abstract = {ContextThe development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users. ObjectiveThis paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view. MethodOur analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. ResultsThis paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. ConclusionThis systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {84–105},
numpages = {22},
keywords = {Systematic mapping, Service-based software process, Non-functional requirements}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Software product lines, Longitudinal study, Fault proneness prediction, Cross-release reuse, Cross-product reuse, Cross-product prediction, Assessment}
}

@inproceedings{10.1007/978-3-662-43652-3_34,
author = {Zulkoski, Ed and Kleynhans, Chris and Yee, Ming-Ho and Rayside, Derek and Czarnecki, Krzysztof},
title = {Optimizing Alloy for Multi-objective Software Product Line Configuration},
year = {2014},
isbn = {9783662436516},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-43652-3_34},
doi = {10.1007/978-3-662-43652-3_34},
abstract = {Software product line SPL engineering involves the modeling, analysis, and configuration of variability-rich systems. We improve the performance of the multi-objective optimization of SPLs in Alloy by several orders of magnitude with two techniques.First, we rewrite the model to remove binary relations that map to integers, which enables removing most of the integer atoms from the universe. SPL models often require using large bitwidths, hence the number of integer atoms in the universe can be orders of magnitude more than the other atoms. In our approach, the tuples for these integer-valued relations are computed outside the sat solver before returning the solution to the user. Second, we add a checkpointing facility to Kodkod, which allows the multi-objective optimization algorithm to reuse previously computed internal sat solver state, after backtracking.Together these result in orders of magnitude improvement in using Alloy as a multi-objective optimization tool for software product lines.},
booktitle = {Proceedings of the 4th International Conference on Abstract State Machines, Alloy, B, TLA, VDM, and Z - Volume 8477},
pages = {328–333},
numpages = {6},
keywords = {Product Lines, Multi-objective Optimization, Kodkod, Alloy},
location = {Toulouse, France},
series = {ABZ 2014}
}

@article{10.1016/j.scico.2012.05.003,
author = {Laguna, Miguel A. and Crespo, Yania},
title = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.05.003},
doi = {10.1016/j.scico.2012.05.003},
abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {1010–1034},
numpages = {25},
keywords = {Software product line, Refactoring, Reengineering, Legacy system, Evolution}
}

@inproceedings{10.5555/1753235.1753258,
author = {Ganesan, Dharmalingam and Lindvall, Mikael and Ackermann, Chris and McComas, David and Bartholomew, Maureen},
title = {Verifying architectural design rules of the flight software product line},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {This paper presents experiences of verifying architectural design rules of the NASA Core Flight Software (CFS) product line implementation. The goal is to check whether the implementation is consistent with the CFS' architectural rules derived from the developer's guide. The results indicate that consistency checking helps a) identifying architecturally significant deviations that were eluded during code reviews, b) clarifying the design rules to the team, and c) assessing the overall implementation quality. Furthermore, it helps connecting business goals to architectural principles, and to the implementation. This paper is the first step in the definition of a method for analyzing and evaluating product line implementations from an architecture-centric perspective.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {161–170},
numpages = {10},
keywords = {architectural rules, business goals, flight software, implemented architecture},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.advengsoft.2014.01.011,
author = {Rossel, Pedro O. and Bastarrica, Mar\'{\i}a Cecilia and Hitschfeld-Kahler, Nancy and D\'{\i}az, Violeta and Medina, Mario},
title = {Domain modeling as a basis for building a meshing tool software product line},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {70},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2014.01.011},
doi = {10.1016/j.advengsoft.2014.01.011},
abstract = {Meshing tools are highly complex software for generating and managing geometrical discretizations. Due to their complexity, they have generally been developed by end users - physicists, forest engineers, mechanical engineers - with ad hoc methodologies and not by applying well established software engineering practices. Different meshing tools have been developed over the years, making them a good application domain for Software Product Lines (SPLs). This paper proposes building a domain model that captures the different domain characteristics such as features, goals, scenarios and a lexicon, and the relationships among them. The model is partly specified using a formal language. The domain model captures product commonalities and variabilities as well as the particular characteristics of different SPL products. The paper presents a rigorous process for building the domain model, where specific roles, activities and artifacts are identified. This process also clearly establishes consistency and completeness conditions. The usefulness of the model and the process are validated by using them to generate a software product line of Tree Stem Deformation (TSD) meshing tools. We also present Meshing Tool Generator, a software that follows the SPL approach for generating meshing tools belonging to the TSD SPL. We show how an end user can easily generate three different TSD meshing tools using Meshing Tool Generator.},
journal = {Adv. Eng. Softw.},
month = apr,
pages = {77–89},
numpages = {13},
keywords = {Tree stem deformation, Software product line, Meshing tools, Domain model, Domain analysis, Code generator}
}

@article{10.1016/j.infsof.2007.10.013,
author = {Ahmed, Faheem and Capretz, Luiz Fernando},
title = {The software product line architecture: An empirical investigation of key process activities},
year = {2008},
issue_date = {October, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.10.013},
doi = {10.1016/j.infsof.2007.10.013},
abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1098–1113},
numpages = {16},
keywords = {Software product line, Software engineering, Software architecture, Empirical study, Domain engineering}
}

@inproceedings{10.1145/2996890.3007893,
author = {Ruiz, Carlos and Duran-Limon, Hector A. and Parlavantzas, Nikos},
title = {Towards a software product line-based approach to adapt IaaS cloud configurations},
year = {2016},
isbn = {9781450346160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2996890.3007893},
doi = {10.1145/2996890.3007893},
abstract = {Cloud computing is nowadays one of the most promising IT technologies, since it provides seemingly unlimited resources on demand at low costs. Hence, different types of applications have been migrated to IaaS environments, e.g. multi-tier (distributed) applications. However, in order to benefit from such characteristics, cloud configurations (i.e. virtual resource configurations) should be designed accordingly to the necessities of the applications. Furthermore, such configurations have to provide the required resources not only at the application deployment-time, but also during the whole application execution time. Hence, adaptive paradigms are required when designing solutions to cloud applications with dynamic resource requirements. Software Product Lines (SPLs) provide great flexibility and a high level of abstraction to describe complete system configurations. Even though SPLs are not commonly used to describe changes after an initial product (configuration) has been created, their inherent characteristics can enable producing the required virtual resource configuration to adapt applications after their initial deployment, i.e., at runtime. In this paper, we present an approach to create and adapt cloud configurations at the IaaS level by using SPLs. We focus on the architectural design of our solution as well as on the possible implementation challenges we could face.},
booktitle = {Proceedings of the 9th International Conference on Utility and Cloud Computing},
pages = {398–403},
numpages = {6},
keywords = {software product lines, self-adaptation, cloud computing},
location = {Shanghai, China},
series = {UCC '16}
}

@article{10.1016/j.infsof.2011.01.001,
author = {Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
year = {2011},
issue_date = {July, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.01.001},
doi = {10.1016/j.infsof.2011.01.001},
abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {707–721},
numpages = {15},
keywords = {Variability, Software product line, Requirements, Feature, Evolution, Context}
}

@article{10.1007/s10270-015-0471-3,
author = {Bonif\'{a}cio, Rodrigo and Borba, Paulo and Ferraz, Cristiano and Accioly, Paola},
title = {Empirical assessment of two approaches for specifying software product line use case scenarios},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0471-3},
doi = {10.1007/s10270-015-0471-3},
abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {97–123},
numpages = {27},
keywords = {Usage scenarios, Software product lines, Software modularity, Requirements engineering, Experimentation in software engineering}
}

@inproceedings{10.1145/3336294.3336309,
author = {Temple, Paul and Acher, Mathieu and Perrouin, Gilles and Biggio, Battista and Jezequel, Jean-Marc and Roli, Fabio},
title = {Towards Quality Assurance of Software Product Lines with Adversarial Configurations},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336309},
doi = {10.1145/3336294.3336309},
abstract = {Software product line (SPL) engineers put a lot of effort to ensure that, through the setting of a large number of possible configuration options, products are acceptable and well-tailored to customers' needs. Unfortunately, options and their mutual interactions create a huge configuration space which is intractable to exhaustively explore. Instead of testing all products, machine learning is increasingly employed to approximate the set of acceptable products out of a small training sample of configurations. Machine learning (ML) techniques can refine a software product line through learned constraints and a priori prevent non-acceptable products to be derived. In this paper, we use adversarial ML techniques to generate adversarial configurations fooling ML classifiers and pinpoint incorrect classifications of products (videos) derived from an industrial video generator. Our attacks yield (up to) a 100% misclassification rate and a drop in accuracy of 5%. We discuss the implications these results have on SPL quality assurance.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {277–288},
numpages = {12},
keywords = {software variability, software testing, software product line, quality assurance, machine learning},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1109/ICIS.2012.43,
author = {Ryu, Duksan and Lee, Dan and Baik, Jongmoon},
title = {Designing an Architecture of SNS Platform by Applying a Product Line Engineering Approach},
year = {2012},
isbn = {9780769546940},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICIS.2012.43},
doi = {10.1109/ICIS.2012.43},
abstract = {The demand of new Social Networking Service (SNS) is high because the SNSs have been popular these days. In order to deliver various SNSs as early as possible, software product line (SPL) approach can be useful. By using the state of the practices of SPL, this paper shows how to manage commonalities and variabilities of SNS. Specifically, to make an architecture design, presented practices include: understanding relevant domains, requirements engineering, architecture definition. The strengths and weaknesses of Face book architecture are evaluated with the Architecture Tradeoff Analysis Method (ATAM). As a result of applying a framework for SPL practice, layered view and component-based view are illustrated along with variabilities represented by Product Line UML-based Software Engineering (PLUS) and Orthogonal Variability Model (OVM). Based on the analysis of requirements of SNS, additional services such as file sharing and instant messaging are represented as optional components. In case of Face book, three key quality attributes, i.e., availability, scalability, and privacy are analyzed by using quality attribute utility tree. We identified that Face book employs client-server architecture. Through ATAM, Peer-to-Peer (P2P) approach promoting privacy is explained.},
booktitle = {Proceedings of the 2012 IEEE/ACIS 11th International Conference on Computer and Information Science},
pages = {559–564},
numpages = {6},
keywords = {software product line, social networking service, architecture evaluation, architecture design},
series = {ICIS '12}
}

@inproceedings{10.1145/2602458.2602460,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Injecting quality attributes into software architectures with the common variability language},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602460},
doi = {10.1145/2602458.2602460},
abstract = {Quality attributes that add new behavior to the functional software architecture are known as functional quality attributes (FQAs). These FQAs are applied to pieces of software from small components to entire systems, usually crosscutting some of them. Due to this crosscutting nature, modeling them separately from the base application has many advantages (e.g. reusability, less coupled architectures). However, different applications may require different configurations of an FQA (e.g. different levels of security), so we need a language that: (i) easily expresses the variability of the FQAs at the architectural level; and that (ii) also facilitates the automatic generation of architectural configurations with custom-made FQAs. In this sense, the Common Variability Language (CVL) is extremely suited for use at the architectural level, not requiring the use of a particular architectural language to model base functional requirements. In this paper we propose a method based on CVL to: (i) model separately and generate FQAs customized to the application requirements; (ii) automatically inject customized FQA components into the architecture of the applications. We quantitatively evaluate our approach and discuss its benefits with a case study.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {35–44},
numpages = {10},
keywords = {weaving, variability, spl, quality attributes, cvl},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@article{10.4018/ijismd.2014070103,
author = {Lotz, Alex and Ingl\'{e}s-Romero, Juan F. and Stampfer, Dennis and Lutz, Matthias and Vicente-Chicote, Cristina and Schlegel, Christian},
title = {Towards a Stepwise Variability Management Process for Complex Systems: A Robotics Perspective},
year = {2014},
issue_date = {July 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {3},
issn = {1947-8186},
url = {https://doi.org/10.4018/ijismd.2014070103},
doi = {10.4018/ijismd.2014070103},
abstract = {Complex systems are executed in environments with a huge number of potential situations and contingencies, therefore a mechanism is required to express dynamic variability at design-time that can be efficiently resolved in the application at run-time based on the then available information. We present an approach for dynamic variability modeling and its exploitation at run-time. It supports different developer roles and allows the separation of two different kinds of dynamic variability at design-time: (i) variability related to the system operation, and (ii) variability associated with QoS. The former provides robustness to contingencies, maintaining a high success rate in task fulfillment. The latter focuses on the quality of the application execution (defined in terms of non-functional properties like safety or task efficiency) under changing situations and limited resources. The authors also discuss different alternatives for the run-time integration of the two variability management mechanisms, and show real-world robotic examples to illustrate them.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = jul,
pages = {55–74},
numpages = {20},
keywords = {Variability Management, VML, SmartTCL, Service Robotics, Modeling Run-Time Variability}
}

@article{10.1016/j.jss.2013.12.038,
author = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort\'{e}s, Antonio and Hinchey, Mike},
title = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.12.038},
doi = {10.1016/j.jss.2013.12.038},
abstract = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
journal = {J. Syst. Softw.},
month = may,
pages = {3–23},
numpages = {21},
keywords = {Software architecture, Feature models, Dynamic variability, Dynamic Software Product Lines}
}

@inproceedings{10.1145/2304676.2304679,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Respecting component architecture to migrate product copies to a software product line},
year = {2012},
isbn = {9781450313483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304676.2304679},
doi = {10.1145/2304676.2304679},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating existing, customised product copies to a product line is still an open issue due to the required comprehension of differences among products and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present how component architecture information can be used to enhance a variabilty reverse engineering process to target this challenge and show the relevance of component architecture in the individual requirements on the resulting SPL. We further provide an illustrating example to show how the concept is applied.},
booktitle = {Proceedings of the 17th International Doctoral Symposium on Components and Architecture},
pages = {7–12},
numpages = {6},
keywords = {software product line, reverse engineering, component architecture},
location = {Bertinoro, Italy},
series = {WCOP '12}
}

@article{10.1007/s11219-011-9156-5,
author = {Roos-Frantz, Fabricia and Benavides, David and Ruiz-Cort\'{e}s, Antonio and Heuer, Andr\'{e} and Lauenroth, Kim},
title = {Quality-aware analysis in product line engineering with the orthogonal variability model},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9156-5},
doi = {10.1007/s11219-011-9156-5},
abstract = {Software product line engineering is about producing a set of similar products in a certain domain. A variability model documents the variability amongst products in a product line. The specification of variability can be extended with quality information, such as measurable quality attributes (e.g., CPU and memory consumption) and constraints on these attributes (e.g., memory consumption should be in a range of values). However, the wrong use of constraints may cause anomalies in the specification which must be detected (e.g., the model could represent no products). Furthermore, based on such quality information, it is possible to carry out quality-aware analyses, i.e., the product line engineer may want to verify whether it is possible to build a product that satisfies a desired quality. The challenge for quality-aware specification and analysis is threefold. First, there should be a way to specify quality information in variability models. Second, it should be possible to detect anomalies in the variability specification associated with quality information. Third, there should be mechanisms to verify the variability model to extract useful information, such as the possibility to build a product that fulfils certain quality conditions (e.g., is there any product that requires less than 512 MB of memory?). In this article, we present an approach for quality-aware analysis in software product lines using the orthogonal variability model (OVM) to represent variability. We propose to map variability represented in the OVM associated with quality information to a constraint satisfaction problem and to use an off-the-shelf constraint programming solver to automatically perform the verification task. To illustrate our approach, we use a product line in the automotive domain which is an example that was created in a national project by a leading car company. We have developed a prototype tool named FaMa-OVM, which works as a proof of concepts. We were able to identify void models, dead and false optional elements, and check whether the product line example satisfies quality conditions.},
journal = {Software Quality Journal},
month = sep,
pages = {519–565},
numpages = {47},
keywords = {Software product lines, Quality-aware analysis, Quality modelling, Orthogonal variability model, Automated analysis}
}

@inproceedings{10.1109/ICWS.2015.20,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {SPL-TQSSS: A Software Product Line Approach for Stateful Service Selection},
year = {2015},
isbn = {9781467372725},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICWS.2015.20},
doi = {10.1109/ICWS.2015.20},
abstract = {An important problem in Web services composition process is optimal selection of services meeting the user functional requirements (tasks of a workflow) and ensuring a reliable execution of the composition. Therefore, non-functional properties of services such as their transactional behavior as well as their Quality of Service (QoS) must be considered. In this context, a challenging objective is to assist users in integrating on the fly the operations of services to realize their required tasks by further meeting their transactional and QoS preferences. Towards this purpose, we present SPLTQSSS, a Software Product Line based approach for Stateful (conversation-based) Service Selection problem with Transactional and QoS support. SPL-TQSSS considers the set of functionally-equivalent services as part of a service family by modeling their internal operations using Feature Models. Then, SPL-TQSSS chooses the best services, from the service families matching with every task of the workflow, which fit with the user transactional preference and satisfy QoS constraints.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Web Services},
pages = {73–80},
numpages = {8},
keywords = {Variability, Transactional, Software Product Line, Service Selection, QoS, Feature Model},
series = {ICWS '15}
}

@inproceedings{10.1145/3461001.3461660,
author = {Michelon, Gabriela Karoline and Obermann, David and Assun\c{c}\~{a}o, Wesley K. G. and Linsbauer, Lukas and Gr\"{u}nbacher, Paul and Egyed, Alexander},
title = {Managing systems evolving in space and time: four challenges for maintenance, evolution and composition of variants},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3461660},
doi = {10.1145/3461001.3461660},
abstract = {Software companies need to provide a large set of features satisfying functional and non-functional requirements of diverse customers, thereby leading to variability in space. Feature location techniques have been proposed to support software maintenance and evolution in space. However, so far only one feature location technique also analyses the evolution in time of system variants, which is required for feature enhancements and bug fixing. Specifically, existing tools for managing a set of systems over time do not offer proper support for keeping track of feature revisions, updating existing variants, and creating new product configurations based on feature revisions. This paper presents four challenges concerning such capabilities for feature (revision) location and composition of new product configurations based on feature/s (revisions). We also provide a benchmark containing a ground truth and support for computing metrics. We hope that this will motivate researchers to provide and evaluate tool-supported approaches aiming at managing systems evolving in space and time. Further, we do not limit the evaluation of techniques to only this benchmark: we introduce and provide instructions on how to use a benchmark extractor for generating ground truth data for other systems. We expect that the feature (revision) location techniques maximize information retrieval in terms of precision, recall, and F-score, while keeping execution time and memory consumption low.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {75–80},
numpages = {6},
keywords = {software product line, repository mining, feature revision, feature location, benchmark extractor},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2430502.2430511,
author = {Kolesnikov, Sergiy S. and Apel, Sven and Siegmund, Norbert and Sobernig, Stefan and K\"{a}stner, Christian and Senkaya, Semah},
title = {Predicting quality attributes of software product lines using software and network measures and sampling},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430511},
doi = {10.1145/2430502.2430511},
abstract = {Software product-line engineering aims at developing families of related products that share common assets to provide customers with tailor-made products. Customers are often interested not only in particular functionalities (i.e., features), but also in non-functional quality attributes, such as performance, reliability, and footprint. Measuring quality attributes of all products of a product line usually does not scale. In this research-in-progress report, we propose a systematic approach aiming at efficient and scalable prediction of quality attributes of products. To this end, we establish predictors for certain categories of quality attributes (e.g., a predictor for high memory consumption) based on software and network measures, and receiver operating characteristic analysis. We use these predictors to guide a sampling process that takes the assets of a product line as input and determines the products that fall into the category denoted by the given predictor (e.g., products with high memory consumption). We propose to use predictors to make the process of finding "acceptable" products more efficient. We discuss and compare several strategies to incorporate predictors in the sampling process.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {5},
keywords = {software product lines, sampling, quality attributes, prediction, metrics},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.5555/2337223.2337302,
author = {Cordy, Maxime and Classen, Andreas and Perrouin, Gilles and Schobbens, Pierre-Yves and Heymans, Patrick and Legay, Axel},
title = {Simulation-based abstractions for software product-line model checking},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) engineering is a software engineering paradigm that exploits the commonality between similar software products to reduce life cycle costs and time-to-market. Many SPLs are critical and would benefit from efficient verification through model checking. Model checking SPLs is more difficult than for single systems, since the number of different products is potentially huge. In previous work, we introduced Featured Transition Systems (FTS), a formal, compact representation of SPL behaviour, and provided efficient algorithms to verify FTS. Yet, we still face the state explosion problem, like any model checking-based verification. Model abstraction is the most relevant answer to state explosion. In this paper, we define a novel simulation relation for FTS and provide an algorithm to compute it. We extend well-known simulation preservation properties to FTS and thus lay the theoretical foundations for abstraction-based model checking of SPLs. We evaluate our approach by comparing the cost of FTS-based simulation and abstraction with respect to product-by-product methods. Our results show that FTS are a solid foundation for simulation-based model checking of SPL.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {672–682},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1007/978-3-642-12107-4_8,
author = {Alf\'{e}rez, Mauricio and Santos, Jo\~{a}o and Moreira, Ana and Garcia, Alessandro and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Amaral, Vasco},
title = {Multi-view composition language for software product line requirements},
year = {2009},
isbn = {3642121063},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12107-4_8},
doi = {10.1007/978-3-642-12107-4_8},
abstract = {Composition of requirements models in Software Product Line (SPL) development enables stakeholders to derive the requirements of target software products and, very important, to reason about them. Given the growing complexity of SPL development and the various stakeholders involved, their requirements are often specified from heterogeneous, partial views. However, existing requirements composition languages are very limited to generate specific requirements views for SPL products. They do not provide specialized composition rules for referencing and composing elements in recurring requirements models, such as use cases and activity models. This paper presents a multi-view composition language for SPL requirements, the Variability Modeling Language for Requirements (VML4RE). This language describes how requirements elements expressed in different models should be composed to generate a specific SPL product. The use of VML4RE is illustrated with UML-based requirements models defined for a home automation SPL case study. The language is evaluated with additional case studies from different application domains, such as mobile phones and sales management.},
booktitle = {Proceedings of the Second International Conference on Software Language Engineering},
pages = {103–122},
numpages = {20},
keywords = {variability management, software product lines, requirements reuse, requirements engineering, cmposition languages},
location = {Denver, CO},
series = {SLE'09}
}

@article{10.1016/j.infsof.2012.08.010,
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.08.010},
doi = {10.1016/j.infsof.2012.08.010},
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {320–343},
numpages = {24},
keywords = {Variability, Systematic literature review, Service-based systems, Quality attributes}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Software product lines, Reuse, Prediction, Post-release defects, Failure-prone files, Change metrics}
}

@inproceedings{10.1109/SBCARS.2010.13,
author = {Oliveira Junior, Edson A. and Maldonado, Jose C. and Gimenes, Itana M. S.},
title = {Empirical Validation of Complexity and Extensibility Metrics for Software Product Line Architectures},
year = {2010},
isbn = {9780769542591},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SBCARS.2010.13},
doi = {10.1109/SBCARS.2010.13},
abstract = {The software product line (PL) architecture (PLA) is one of the most important PL core assets as it is the abstraction of the products that can be generated, and it represents similarities and variabilities of a PL. Its quality attributes analysis and evaluation can serve as a basis for analyzing the managerial and economical values of a PL. We proposed metrics for PLA complexity and extensibility quality attributes. This paper is concerned with the empirical validation of such metrics. As a result of the experimental work we can conclude that the metrics are relevant indicators of complexity and extensibility of PLA by presenting their correlation analysis.},
booktitle = {Proceedings of the 2010 Fourth Brazilian Symposium on Software Components, Architectures and Reuse},
pages = {31–40},
numpages = {10},
keywords = {software product line, product line architecture, metrics, extensibility, empirical validation, complexity},
series = {SBCARS '10}
}

@inproceedings{10.1145/2031759.2031768,
author = {Lence, Ram\'{o}n and Fuentes, Lidia and Pinto, M\'{o}nica},
title = {Quality attributes and variability in AO-ADL software architectures},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031768},
doi = {10.1145/2031759.2031768},
abstract = {The quality attributes of a system are determined, to a large extend, by the decisions taken early on in the development process, noticeably affecting the specification of its software architecture. This is especially true for attributes such as security, usability, context awareness, etc., that have strong functional implications -- i.e. they require the incorporation of Specific functionality to the application architecture in order to satisfy them. Our approach models functional quality attributes considering that: (1) they are complex enough so as to be modeled by a large set of related concerns and the compositions among them. For instance, security includes authentication, access control, privacy, encryption, auditing, etc; (2) the same quality attributes are required by several applications, and thus should be modeled as separate, ready-to-use (re)usable architectural solutions that final applications can incorporate without "being previously prepared" for it; and (3) not all the concerns that are part of a quality attribute need to be instantiated for a particular application (e.g. only the authentication and access control concerns of security are required). In order to consider all the above requirements, in this paper we present a software product line approach that permits modeling the variability of quality attributes using feature models, and generating different configurations of their software architecture depending on the particular concerns required by each application.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {7},
numpages = {10},
keywords = {variability, quality attributes, architectural templates, VML, Hydra, AO-ADL},
location = {Essen, Germany},
series = {ECSA '11}
}

@article{10.1007/s10270-020-00839-w,
author = {Pol’la, Matias and Buccella, Agustina and Cechich, Alejandra},
title = {Analysis of variability models: a systematic literature review},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00839-w},
doi = {10.1007/s10270-020-00839-w},
abstract = {Dealing with variability, during Software Product Line Engineering (SPLE), means trying to allow software engineers to develop a set of similar applications based on a manageable range of variable functionalities according to expert users’ needs. Particularly, variability management (VM) is an activity that allows flexibility and a high level of reuse during software development. In the last years, we have witnessed a proliferation of methods, techniques and supporting tools for VM in general, and for its analysis in particular. More precisely, a specific field has emerged, named (automated) variability analysis, focusing on verifying variability models across the SPLE’s phases. In this paper, we introduce a systematic literature review of existing proposals (as primary studies) focused on analyzing variability models. We define a classification framework, which is composed of 20 sub-characteristics addressing general aspects, such as scope and validation, as well as model-specific aspects, such as variability primitives, reasoner type. The framework allows to look at the analysis of variability models during its whole life cycle—from design to derivation—according to the activities involved during an SPL development. Also, the framework helps us answer three research questions defined for showing the state of the art and drawing challenges for the near future. Among the more interesting challenges, we can highlight the needs of more applications in industry, the existence of more mature tools, and the needs of providing more semantics in the way of variability primitives for identifying inconsistencies in the models.},
journal = {Softw. Syst. Model.},
month = aug,
pages = {1043–1077},
numpages = {35},
keywords = {Supporting tools, Variability management, Software Product Line, Variability analysis}
}

@article{10.1016/j.infsof.2010.12.006,
author = {Chen, Lianping and Ali Babar, Muhammad},
title = {A systematic review of evaluation of variability management approaches in software product lines},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.006},
doi = {10.1016/j.infsof.2010.12.006},
abstract = {ContextVariability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. ObjectiveThe objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. MethodWe carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. ResultsWe selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. ConclusionThe findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {344–362},
numpages = {19},
keywords = {Variability management, Systematic literature reviews, Software product line, Empirical studies}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Software product lines, SPL Conqueror, Non-functional properties, Measurement and optimization, Feature-oriented software development}
}

@inproceedings{10.1145/2245276.2231956,
author = {Horikoshi, Hisayuki and Nakagawa, Hiroyuki and Tahara, Yasuyuki and Ohsuga, Akihiko},
title = {Dynamic reconfiguration in self-adaptive systems considering non-functional properties},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231956},
doi = {10.1145/2245276.2231956},
abstract = {Self-adaptive systems have recently been receiving much attention because of their ability to cope with the changes of environment, failures, and unanticipated events. These systems need an adaptation mechanism, which automatically computes the possible configurations, and decides the most appropriate configuration to fit the environment. In particular, the satisfaction of non-functional requirements must be considered when selecting the best reconfiguration. However, there are trade-off problems among non-functional requirements. Moreover, the adaptation mechanisms are typically developed separately from the components to be implemented, and it complicates the construction of such systems. We propose (1) a feature-oriented analysis technique, which can identify adaptation points, and calculate the contribution to non-functional goals of the configuration; (2) a component specification model, which extends an architectural description language for self-adaptation; (3) a reconfiguration framework aimed to reduce the complexity of the reconfiguration and generate the best configuration at run-time. We evaluate the feasibility of our framework by four different scenarios, and show that our framework reduces the complexity of the reconfiguration, and solves the trade-off problem among non-functional requirements.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1144–1150},
numpages = {7},
keywords = {software architecture, self-adaptive systems, feature-oriented analysis, dynamic reconfiguration, architecture description language},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2430502.2430516,
author = {Lanceloti, Leandro A. and Maldonado, Jos\'{e} C. and Gimenes, Itana M. S. and Oliveira, Edson A.},
title = {SMartyParser: a XMI parser for UML-based software product line variability models},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430516},
doi = {10.1145/2430502.2430516},
abstract = {Variability management is an important issue for the software-intensive systems domain. Such an issue is essential for the success of software product line (SPL) adoption strategies. Although it is a well-discussed subject in the SPL community, there is a lack of tool support for environments that handle UML-based SPL variabilities, as several variability management approaches take UML as a basis, specially its profiling mechanism. Such environments might handle variabilities for several reasons, such as, evaluating SPLs, defining and applying metrics based on a SPL modeling, and automating the product generation. Therefore, this paper presents the SMartyParser, a parser for processing UML-based SPL models. Such models can be obtained, in the XMI format, from every UML specification-compliant tool. Such a parser provides several services to make it easier the handling of variability data in a particular SPL environment/tool. SMartyParser was built by taking the Open Core framework as a basis for processing XMI files. A parser use example is presented by taking into account the SPL Arcade Game Maker UML models.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {5},
keywords = {variability management, stereotype, software product line, parser, XMI, UML},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@inproceedings{10.1109/SERA.2010.17,
author = {Tanhaei, Mohammad and Moaven, Shahrouz and Habibi, Jafar and Ahmadi, Hamed},
title = {Toward a Business Model for Software Product Line Architecture},
year = {2010},
isbn = {9780769540757},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SERA.2010.17},
doi = {10.1109/SERA.2010.17},
abstract = {Nowadays, software product line is an approach to reduce costs of software development, decrease time to market, and increase capabilities of reuse in designing and exploiting software development processes. Moreover, other quality attributes of the project domain should be considered to enhance quality of the product. Meanwhile, taking advantage of software product line makes developers capable of estimating development costs and time to market in a more realistic way. However, old approaches to estimate cost of development and foresee time to market are not suitable enough for software product line. In this paper, some important business parameters and a way to calculate cost and time to market in a product line are presented. Changing components among time, portion of the change in a specific product and organization issues are observed in the estimation function.},
booktitle = {Proceedings of the 2010 Eighth ACIS International Conference on Software Engineering Research, Management and Applications},
pages = {50–56},
numpages = {7},
series = {SERA '10}
}

@inproceedings{10.1145/2362536.2362548,
author = {Soltani, Samaneh and Asadi, Mohsen and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
title = {Automated planning for feature model configuration based on functional and non-functional requirements},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362548},
doi = {10.1145/2362536.2362548},
abstract = {Feature modeling is one of the main techniques used in Software Product Line Engineering to manage the variability within the products of a family. Concrete products of the family can be generated through a configuration process. The configuration process selects and/or removes features from the feature model according to the stakeholders' requirements. Selecting the right set of features for one product from amongst all of the available features in the feature model is a complex task because: 1) the multiplicity of stakeholders' functional requirements; 2) the positive or negative impact of features on non-functional properties; and 3) the stakeholders' preferences w.r.t. the desirable non-functional properties of the final product. Many configurations techniques have already been proposed to facilitate automated product derivation. However, most of the current proposals are not designed to consider stakeholders' preferences and constraints especially with regard to non-functional properties. We address the software product line configuration problem and propose a framework, which employs an artificial intelligence planning technique to automatically select suitable features that satisfy both the stakeholders' functional and non-functional preferences and constraints. We also provide tooling support to facilitate the use of our framework. Our experiments show that despite the complexity involved with the simultaneous consideration of both functional and non-functional properties our configuration technique is scalable.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {56–65},
numpages = {10},
keywords = {software product line engineering, planning techniques, feature model, configuration, artificial intelligence},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/1887899.1887951,
author = {Lopez-Herrejon, Roberto E.},
title = {On the need of safe software product line architectures},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A Software Product Line (SPL) is a family of related software systems distinguished by the different sets of features each system provides. Over the last decade, the substantial benefits of SPL practices have been extensively documented and corroborated both in academia and industry. Several architecture methods have been proposed that employ different artifacts for expressing the components of a SPL, their properties and relationships. Of crucial importance for any SPL architecture method is to guarantee that the variability, for instance as expressed in feature models, is not only preserved but also kept consistent across all artifacts used. In this research challenge paper we argue that Safe Composition - the guarantee that all programs of a product line are type safe - can be leveraged to address this guarantee for structural properties of SPL architectures and the challenges that that entails.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {493–496},
numpages = {4},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@inproceedings{10.1145/3307630.3342705,
author = {Krieter, Sebastian},
title = {Enabling Efficient Automated Configuration Generation and Management},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342705},
doi = {10.1145/3307630.3342705},
abstract = {Creating and managing valid configurations is one of the main tasks in software product line engineering. Due to the often complex constraints from a feature model, some kind of automated configuration generation is required to facilitate the configuration process for users and developers. For instance, decision propagation can be applied to support users in configuring a product from a software product line (SPL) with less manual effort and error potential, leading to a semi-automatic configuration process. Furthermore, fully-automatic configuration processes, such as random sampling or t-wise interaction sampling can be employed to test or to optimize an SPL. However, current techniques for automated configuration generation still do not scale well to SPLs with large and complex feature models. Within our thesis, we identify current challenges regarding the efficiency and effectiveness of the semi- and fully-automatic configuration process and aim to address these challenges by introducing novel techniques and improving current ones. Our preliminary results show already show promising progress for both, the semi- and fully-automatic configuration process.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {215–221},
numpages = {7},
keywords = {uniform random sampling, t-wise sampling, software product lines, decision propagation, configurable system},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1007/s11219-011-9146-7,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {A systematic review of quality attributes and measures for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9146-7},
doi = {10.1007/s11219-011-9146-7},
abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e.g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92% of the measures evaluate attributes that are related to maintainability. In addition, 67% of the measures are used during the design phase of Domain Engineering, and 56% are applied to evaluate the product line architecture. However, only 25% of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines.},
journal = {Software Quality Journal},
month = sep,
pages = {425–486},
numpages = {62},
keywords = {Systematic literature review, Software product lines, Quality attributes, Quality, Measures}
}

@article{10.1016/j.infsof.2006.08.008,
author = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
title = {A framework for evaluating reusability of core asset in product line engineering},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.008},
doi = {10.1016/j.infsof.2006.08.008},
abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {740–760},
numpages = {21},
keywords = {Reusability, Quality model, Product line engineering, Metric, Core asset}
}

@article{10.1007/s10664-015-9414-4,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and O\'{z} Cinne\'{z}Ide, Mel and Deb, Kalyanmoy},
title = {On the use of many quality attributes for software refactoring: a many-objective search-based software engineering approach},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9414-4},
doi = {10.1007/s10664-015-9414-4},
abstract = {Search-based software engineering (SBSE) solutions are still not scalable enough to handle high-dimensional objectives space. The majority of existing work treats software engineering problems from a single or bi-objective point of view, where the main goal is to maximize or minimize one or two objectives. However, most software engineering problems are naturally complex in which many conflicting objectives need to be optimized. Software refactoring is one of these problems involving finding a compromise between several quality attributes to improve the quality of the system while preserving the behavior. To this end, we propose a novel representation of the refactoring problem as a many-objective one where every quality attribute to improve is considered as an independent objective to be optimized. In our approach based on the recent NSGA-III algorithm, the refactoring solutions are evaluated using a set of 8 distinct objectives. We evaluated this approach on one industrial project and seven open source systems. We compared our findings to: several other many-objective techniques (IBEA, MOEA/D, GrEA, and DBEA-Eps), an existing multi-objective approach a mono-objective technique and an existing refactoring technique not based on heuristic search. Statistical analysis of our experiments over 31 runs shows the efficiency of our approach.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2503–2545},
numpages = {43},
keywords = {Software quality, Search-based software engineering, Refactoring, Many-objective optimization}
}

@inproceedings{10.1145/3106195.3106212,
author = {Marimuthu, C. and Chandrasekaran, K.},
title = {Systematic Studies in Software Product Lines: A Tertiary Study},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106212},
doi = {10.1145/3106195.3106212},
abstract = {Software product lines are widely used in the software industries to increase the re-usability and to decrease maintenance cost. On the other hand, systematic reviews are widely used in the software engineering research community to provide the overview of the research field and practitioners guidelines. Researchers have conducted many systematic studies on the different aspects of SPLs. To the best of our knowledge, till now there is no tertiary study conducted on systematic studies of SPL related research topics. In this paper, we aim at conducting a systematic mapping study of existing systematic studies to report the overview of the findings for researchers and practitioners. We performed snowballing and automated search to find out the relevant systematic studies. As a result, we analyzed 60 relevant studies to answer 5 research questions. The main focus of this tertiary study is to highlight the research topics, type of published reviews, active researchers and publication forums. Additionally, we highlight some of the limitations of the systematic studies. The important finding of this study is that the research field is well matured as the systematic studies covered a wide range of research topics. Another important finding is that many studies provided information for practitioners as well as researchers which is a notable improvement in the systematic reviews. However, many studies failed to assess the quality of the primary studies which is the major limitation of the existing systematic studies.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {143–152},
numpages = {10},
keywords = {tertiary study, systematic review, software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3236405.3236426,
author = {Belarbi, Maouaheb},
title = {A methodological framework to enable the generation of code from DSML in SPL},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236426},
doi = {10.1145/3236405.3236426},
abstract = {Software Product Line has acquired a significant momentum at the end of the 1990ies since it allows the production of variable software systems corresponding to the same domain portfolio. The effectiveness of the derivation process depends on how well variability is defined and implemented which is a crucial topic area that was addressed among two essential trends: On the one hand, starting from Domain Specific Modelling Language to express domain requirements and automate the code generation with Model-Driven Engineering techniques and on the second hand, exploiting the soar of variability mechanisms.In this context, the current research presents a method that unifies the two aforementioned approaches to cover the overall strategies by defining a framework that allows a better code generation in terms of documentation, maintainability, rapidity,etc. The starting point is the usage of the Domain Specific Modelling Language to represent the stakeholders requirements. Then, the resulting meta-model will be converted into one our several Feature Diagrams on which variability mechanisms can be applied to generate all the family products.A preliminary experiment has been undertaken to design the methodology of the proposed software factory in a meta-model. The validation task was evaluated with an academic use case called HandiWeb developed to facilitate handicap persons access to the internet. The first results allow us to put the hand on the key challenges that must be resolved by the proposed methodology.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {64–71},
numpages = {8},
keywords = {variability, software factory, methodology, SPL, DSML},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@article{10.1016/j.infsof.2012.04.009,
author = {Engstr\"{o}M, Emelie and Runeson, Per},
title = {Test overlay in an emerging software product line - An industrial case study},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.04.009},
doi = {10.1016/j.infsof.2012.04.009},
abstract = {Context: In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. Aims: This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy. Method: An in-depth case study was launched including both qualitative and quantitative observations. Results: A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied. Conclusions: Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {581–594},
numpages = {14},
keywords = {Software testing, Redundancy, Product-line, Overlay, Efficiency, Case study}
}

@inproceedings{10.1145/3307630.3342416,
author = {Rodriguez, Germania and P\'{e}rez, Jennifer and Benavides, David},
title = {Accessibility Variability Model: The UTPL MOOC Case Study},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342416},
doi = {10.1145/3307630.3342416},
abstract = {Several approaches to define Variability Models (VM) of non-functional requirements or quality attributes have been proposed. However, these approaches have focused on specific quality attributes rather than more general non-functional aspects established by standards such as ISO/IEC 25010 for software evaluation and quality. Thus, developing specific software products by selecting features and at the same time measuring the level of compliance with a standard/guideline is a challenge. In this work, we present the definition of an accessibility VM based on the web content accessibility guides (WCAG) 2.1 W3C recommendation, to obtain a quantitative measure to improve or construct specific SPL products that require to be accessibility-aware. This paper is specially focused on illustrating the experience of measuring the accessibility in a software product line (SPL) in order to check if it is viable measuring products and recommending improvements in terms of features before addressing the construction of accessibility-aware products. The adoption of the VM accessibility has been putted into practice through a pilot case study, the MOOC (Massive Open Online Course) initiative of the Universidad T\'{e}cnica Particular de Loja. The conduction of this pilot case study has allowed us to illustrate how it is possible to model and measure the accessibility in SPL using accessibility VM, as well as to recommend accessibility configuration improvements for the construction of new or updated MOOC platforms.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {114–121},
numpages = {8},
keywords = {software product lines, software development techniques, software creation and management, software and its engineering, reusability},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3307630.3342403,
author = {Berger, Thorsten and Collet, Philippe},
title = {Usage Scenarios for a Common Feature Modeling Language},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342403},
doi = {10.1145/3307630.3342403},
abstract = {Feature models are recognized as a de facto standard for variability modeling. Presented almost three decades ago, dozens of different variations and extensions to the original feature-modeling notation have been proposed, together with hundreds of variability management techniques building upon feature models. Unfortunately, despite several attempts to establish a unified language, there is still no emerging consensus on a feature-modeling language that is both intuitive and simple, but also expressive enough to cover a range of important usage scenarios. There is not even a documented and commonly agreed set of such scenarios.Following an initiative among product-line engineering researchers in September 2018, we present 14 usage scenarios together with examples and requirements detailing each scenario. The scenario descriptions are the result of a systematic process, where members of the initiative authored original descriptions, which received feedback via a survey, and which we then refined and extended based on the survey results, reviewers' comments, and our own expertise. We also report the relevance of supporting each usage scenario for the language, as perceived by the initiative's members, prioritizing each scenario. We present a roadmap to build and implement a first version of the envisaged common language.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {174–181},
numpages = {8},
keywords = {unified language, software product lines, feature models},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382026.3431247,
author = {Meixner, Kristof},
title = {Integrating Variability Modeling of Products, Processes, and Resources in Cyber-Physical Production Systems Engineering},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431247},
doi = {10.1145/3382026.3431247},
abstract = {The Industry 4.0 initiative envisions the flexible and optimized production of customized products on Cyber-Physical Production Systems (CPPSs) that consist of subsystems coordinated to conduct complex production processes. Hence, accurate CPPS modeling requires integrating the modeling of variability for Product-Process-Resource (PPR) aspects. Yet, current variability modeling approaches treat structural and behavioral variability separately, leading to inaccurate CPPS production models that impede CPPS engineering and optimization. This paper proposes a PhD project for integrated variability modeling of PPR aspects to improve the accuracy of production models with variability for CPPS engineers and production optimizers. The research project follows the Design Science approach aiming for the iterative design and evaluation of (a) a framework to categorize currently incomplete and scattered models and methods for PPR variability modeling as a foundation for an integrated model; and (b) a modeling approach for more accurate integrated PPR variability modeling. The planned research will provide the Software Product Line (SPL) and CPPS engineering research communities with (a) novel models, methods, and insights on integrated PPR variability modeling, (b) open data from CPPS engineering use cases for common modeling, and (c) empirical data from field studies for shared analysis and evaluation.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {96–103},
numpages = {8},
keywords = {Variability Modelling, Product-Process-Resource, Cyber-Physical Production System},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2737166.2737171,
author = {Mouelhi, Sebti and Agrou, Khalid and Chouali, Samir and Mountassir, Hassan},
title = {Object-Oriented Component-Based Design using Behavioral Contracts: Application to Railway Systems},
year = {2015},
isbn = {9781450334716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737166.2737171},
doi = {10.1145/2737166.2737171},
abstract = {In this paper, we propose a formal approach for the design of object-oriented component-based systems using behavioral contracts. This formalism merges interface automata describing communication protocols of components with the semantics of their operations. On grounds of consistency with the object-oriented paradigms, we revisit the notions of incremental design and independent implementability of interface automata by novel definitions of components compatibility, composition, and refinement. Our work is illustrated by a design case study of CBTC railway systems.},
booktitle = {Proceedings of the 18th International ACM SIGSOFT Symposium on Component-Based Software Engineering},
pages = {49–58},
numpages = {10},
keywords = {refinement, railway systems, object-oriented components, method semantics, interface automata, behavioral contracts},
location = {Montr\'{e}al, QC, Canada},
series = {CBSE '15}
}

@article{10.1016/S0164-1212(02)00081-X,
author = {Lutz, Robyn R. and Gannod, Gerald C.},
title = {Analysis of a software product line architecture: an experience report},
year = {2003},
issue_date = {15 June 2003},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {66},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/S0164-1212(02)00081-X},
doi = {10.1016/S0164-1212(02)00081-X},
abstract = {This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a "good" product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned.},
journal = {J. Syst. Softw.},
month = jun,
pages = {253–267},
numpages = {15}
}

@inproceedings{10.1145/2934466.2962728,
author = {Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and de Almeida, Eduardo Santana},
title = {RiPLE-HC: visual support for features scattering and interactions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2962728},
doi = {10.1145/2934466.2962728},
abstract = {With the ever increasing popularity of JavaScript in different domains to build bigger and more complex software systems, variability management may be deemed as an affordable strategy. In this sense, Software Product Lines (SPL) engineering is one of the most successful paradigms to accomplish the necessary modularity and systematic reuse of code artifacts for that purpose. In previous work, we present tool support to hybrid composition of JavaScript-based product lines, called RiPLE-HC, which we now extend to incorporate a means to deal with feature interactions and feature annotation scattering in a more smooth way. The proposed tool support may provide practitioners with an easy-to-use approach to implement crosscutting features by increasing the awareness of the developers about the features implementation.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {320–323},
numpages = {4},
keywords = {software product line engineering, javascript, featureide, feature scattering visualization, eclipse plugin},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3382025.3414943,
author = {Th\"{u}m, Thomas},
title = {A BDD for Linux? the knowledge compilation challenge for variability},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414943},
doi = {10.1145/3382025.3414943},
abstract = {What is the number of valid configurations for Linux? How to generate uniform random samples for Linux? Can we create a binary decision diagram for Linux? It seems that the product-line community tries hard to answer such questions for Linux and other configurable systems. However, attempts are often not published due to the publication bias (i.e., unsuccessful attempts are not published). As a consequence, researchers keep trying by potentially spending redundant effort. The goal of this challenge is to guide research on these computationally complex problems and to foster the exchange between researchers and practitioners.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {16},
numpages = {6},
keywords = {software product line, software configuration, satisfiability solving, product configuration, knownledge compilation, feature models, decision models, configurable system, binary decision diagrams, artificial intelligence},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3233027.3236395,
author = {Pereira, Juliana Alves and Maciel, Lucas and Noronha, Thiago F. and Figueiredo, Eduardo},
title = {Heuristic and exact algorithms for product configuration in software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3236395},
doi = {10.1145/3233027.3236395},
abstract = {The Software Product Line (SPL) configuration field is an active area of research and has attracted both practitioners and researchers attention in the last years. A key part of an SPL configuration is a feature model that represents features and their dependencies (i.e., SPL configuration rules). This model can be extended by adding Non-Functional Properties (NFPs) as feature attributes resulting in Extended Feature Models (EFMs). Configuring products from an EFM requires considering the configuration rules of the model and satisfying the product functional and non-functional requirements. Although the configuration of a product arising from EFMs may reduce the space of valid configurations, selecting the most appropriate set of features is still an overwhelming task due to many factors including technical limitations and diversity of contexts. Consequently, configuring large and complex SPLs by using configurators is often beyond the users' capabilities of identifying valid combinations of features that match their (non-functional) requirements. To overcome this limitation, several approaches have modeled the product configuration task as a combinatorial optimization problem and proposed constraint programming algorithms to automatically derive a configuration. Although these approaches do not require any user intervention to guarantee the optimality of the generated configuration, due to the NP-hard computational complexity of finding an optimal variant, exact approaches have inefficient exponential time. Thus, to improve scalability and performance issues, we introduced the adoption of a greedy heuristic algorithm and a biased random-key genetic algorithm (BRKGA). Our experiment results show that our proposed heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, the greedy heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Although the BRKGA heuristic slightly outperformed the greedy heuristic, it has shown larger running times (especially on the largest instances). Therefore, to ensure a good user experience and enable a very fast configuration task, we extended a state-of-the-art configurator with the proposed greedy heuristic approach.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {247},
numpages = {1},
keywords = {software product lines, software product line configuration, search-based software engineering, configuration optimization},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2491627.2491631,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: a case study in the telecommunication domain},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491631},
doi = {10.1145/2491627.2491631},
abstract = {In the research on software product lines, product variants typically differ by their functionality, and quality attributes are more or less similar across products. To accumulate empirical evidence, this paper presents a descriptive case study of performance variability in a software product line of mobile network base stations. The goal is to study the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The results highlight that the evolution of customer needs motivates performance variability; performance variability can be realized either with software or hardware variability strategy, with the latter often being prevailing; and the software strategy can be kept focused by downgrading performance.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {32–41},
numpages = {10},
keywords = {variability, software product line, case study, architecture},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/1383559.1383571,
author = {Tawhid, Rasha and Petriu, Dorina C.},
title = {Towards automatic derivation of a product performance model from a UML software product line model},
year = {2008},
isbn = {9781595938732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1383559.1383571},
doi = {10.1145/1383559.1383571},
abstract = {Software Product Line (SPL) engineering is a software development approach that takes advantage of the commonality and variability between products from a family, and supports the generation of specific products by reusing a set of core family assets. This paper proposes a UML model transformation approach for software product lines to derive a performance model for a specific product. The input to the proposed technique, the "source model", is a UML model of a SPL with performance annotations, which uses two separate profiles: a "product line" profile from literature for specifying the commonality and variability between products, and the MARTE profile recently standardized by OMG for performance annotations. The source model is generic and therefore its performance annotations must be parameterized. The proposed derivation of a performance model for a concrete product requires two steps: a) the transformation of a SPL model to a UML model with performance annotations for a given product, and b) the transformation of the outcome of the first step into a performance model. This paper focuses on the first step, whereas the second step will use the PUMA transformation approach of annotated UML models to performance models, developed in previous work. The output of the first step, named "target model", is a UML model with MARTE annotations, where the variability expressed in the SPL model has been analyzed and bound to a specific product, and the generic performance annotations have been bound to concrete values for the product. The proposed technique is illustrated with an e-commerce case study.},
booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
pages = {91–102},
numpages = {12},
keywords = {uml, software product line, software performance engineering, model transformation, marte},
location = {Princeton, NJ, USA},
series = {WOSP '08}
}

@inproceedings{10.1145/3109729.3109749,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Extending the Common Variability Language (CVL) Engine: A practical tool},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109749},
doi = {10.1145/3109729.3109749},
abstract = {The Common Variability Language (CVL) has become a reference in the specification and resolution of variability in the last few years. Despite the multiple advantages of CVL (orthogonal variability, architecture variability resolution, MOF-compliant, standard proposed,...), several approaches require extending and/or modifying the CVL approach in different ways in order to fulfill the industrial needs for variability modeling in Software Product Lines. However, the community lacks a tool that would enable proposed extensions and the integration of novel approaches to be put into practice. Existing tools that provide support for CVL are incomplete or are mainly focused on the variability model's editor, instead of executing the resolution of the variability over the base models. Moreover, there is no API that allows direct interaction with the CVL engine to extend or use it in an independent application. In this paper, we identify the extension points of the CVL approach with the goal of making the CVL engine more flexible, and to help software architects in the task of resolving the variability of their products. The practical tool presented here is a working implementation of the CVL engine, that can be extended through a proposed API.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {32–37},
numpages = {6},
keywords = {Variability, Software Product Line, CVL},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/HICSS.2009.472,
title = {Towards Tool Support for the Configuration of Non-Functional Properties in SPLs},
year = {2009},
isbn = {9780769534503},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2009.472},
doi = {10.1109/HICSS.2009.472},
abstract = {The configuration of NFPs (non-functional properties) is a crucial problem in the development of software-intensive systems. Most of the approaches currently available tackle this problem during software design. However, at this stage, NFPs cannot be properly predicted. As a solution for this problem we present the new extensions of the Feedback approach which aims at improving the configuration of NFPs in SPLs. We introduce our set of tools that are used to support the approach and show how to use them by applying it to the well-known SPL (The Graph Product Line) that was suggested as a platform for evaluating SPL technologies.},
booktitle = {Proceedings of the 42nd Hawaii International Conference on System Sciences},
pages = {1–7},
numpages = {7},
series = {HICSS '09}
}

@inproceedings{10.1145/2420942.2420944,
author = {Olaechea, Rafael and Stewart, Steven and Czarnecki, Krzysztof and Rayside, Derek},
title = {Modelling and multi-objective optimization of quality attributes in variability-rich software},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420944},
doi = {10.1145/2420942.2420944},
abstract = {Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {2},
numpages = {6},
keywords = {software product lines, multi-objective optimization},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@inproceedings{10.1145/3461002.3473944,
author = {Ballesteros, Joaqu\'{\i}n and Fuentes, Lidia},
title = {Transfer learning for multiobjective optimization algorithms supporting dynamic software product lines},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473944},
doi = {10.1145/3461002.3473944},
abstract = {Dynamic Software Product Lines (DSPLs) are a well-accepted approach for self-adapting Cyber-Physical Systems (CPSs) at run-time. The DSPL approaches make decisions supported by performance models, which capture system features' contribution to one or more optimization goals. Combining performance models with Multi-Objectives Evolutionary Algorithms (MOEAs) as decision-making mechanisms is common in DSPLs. However, MOEAs algorithms start solving the optimization problem from a randomly selected population, not finding good configurations fast enough after a context change, requiring too many resources so scarce in CPSs. Also, the DSPL engineer must deal with the hardware and software particularities of the target platform in each CPS deployment. And although each system instantiation has to solve a similar optimization problem of the DSPL, it does not take advantage of experiences gained in similar CPS. Transfer learning aims at improving the efficiency of systems by sharing the previously acquired knowledge and applying it to similar systems. In this work, we analyze the benefits of transfer learning in the context of DSPL and MOEAs testing on 8 feature models with synthetic performance models. Results are good enough, showing that transfer learning solutions dominate up to 71% of the non-transfer learning ones for similar DSPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {51–59},
numpages = {9},
keywords = {transfer learning, self-adaptation, multiobjective optimization algorithms, dynamic software product lines, cyber-physical systems},
location = {Leicester, United Kindom},
series = {SPLC '21}
}

@inproceedings{10.1145/3109729.3109751,
author = {Krieter, Sebastian and Pinnecke, Marcus and Kr\"{u}ger, Jacob and Sprey, Joshua and Sontag, Christopher and Th\"{u}m, Thomas and Leich, Thomas and Saake, Gunter},
title = {FeatureIDE: Empowering Third-Party Developers},
year = {2017},
isbn = {9781450351195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109729.3109751},
doi = {10.1145/3109729.3109751},
abstract = {FeatureIDE is a popular open-source tool for modeling, implementing, configuring, and analyzing software product lines. However, FeatureIDE's initial design was lacking mechanisms that facilitate extension and reuse of core implementations. In current releases, we improve these traits by providing a modular concept for core data structures and functionalities. As a result, we are facilitating the usage of external implementations for feature models and file formats within FeatureIDE. Additionally, we provide a Java library containing FeatureIDE's core functionalities, including feature modeling and configuration. This allows developers to use these functionalities in their own tools without relying on external dependencies, such as the Eclipse framework.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
pages = {42–45},
numpages = {4},
keywords = {feature-oriented software development, feature modeling, configuration, Software product line},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1145/3106195.3106215,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-healing in Service Mashups Through Feature Adaptation},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106215},
doi = {10.1145/3106195.3106215},
abstract = {The composition of the functionality of multiple services into a single unique service mashup has received wide interest in the recent years. Given the distributed nature of these mashups where the constituent services can be located on different servers, it is possible that a change in the functionality or availability of a constituent service result in the failure of the service mashup. In this paper, we propose a novel method based on the Software Product Line Engineering (SPLE) paradigm which is able to find an alternate valid service mashup which has maximum possible number of original service mashup features in order to mitigate a service failure when complete recovery is not possible. This method also has an advantage that it can recover or mitigate the failure automatically without requiring the user to specify any adaptation rule or strategy. We show the practicality of our proposed approach through extensive experiments.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {94–103},
numpages = {10},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.5555/1885639.1885642,
author = {Bagheri, Ebrahim and Di Noia, Tommaso and Ragone, Azzurra and Gasevic, Dragan},
title = {Configuring software product line feature models based on Stakeholders' soft and hard requirements},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Feature modeling is a technique for capturing commonality and variability. Feature models symbolize a representation of the possible application configuration space, and can be customized based on specific domain requirements and stakeholder goals. Most feature model configuration processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semi-automated feature model configuration is performed. The model configuration derivation process that we propose respects the soundness and completeness properties.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {16–31},
numpages = {16},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2499777.2500725,
author = {Varshosaz, Mahsa and Khosravi, Ramtin},
title = {Discrete time Markov chain families: modeling and verification of probabilistic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500725},
doi = {10.1145/2499777.2500725},
abstract = {Software product line engineering (SPLE) enables systematic reuse in development of a family of related software systems by explicitly defining commonalities and variabilities among the individual products in the family. Nowadays, SPLE is used in a variety of complex domains such as avionics and automotive. As such domains include safety critical systems which exhibit probabilistic behavior, there is a major need for modeling and verification approaches dealing with probabilistic aspects of systems in the presence of variabilities. In this paper, we introduce a mathematical model, Discrete Time Markov Chain Family (DTMCF), which compactly represents the probabilistic behavior of all the products in the product line. We also provide a probabilistic model checking method to verify DTMCFs against Probabilistic Computation Tree Logic (PCTL) properties. This way, instead of verifying each product individually, the whole family is model checked at once, resulting in the set of products satisfying the desired property. This reduces the required cost for model checking by eliminating redundant processing caused by the commonalities among the products.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {34–41},
numpages = {8},
keywords = {variable discrete time Markov chains, software product line, probabilistic model checking},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1007/11767718_28,
author = {Chang, Soo Ho and Kim, Soo Dong and Rhew, Sung Yul},
title = {A variability-centric approach to instantiating core assets in product line engineering},
year = {2006},
isbn = {3540346821},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11767718_28},
doi = {10.1007/11767718_28},
abstract = {As a key activity in product line engineering (PLE), instantiation is a task to generate target applications by resolving variability embedded in core assets. However, instantiation is often conducted in manual and ad-hoc fashion, largely replying on domain knowledge and experience. Hence, it can easily lead to technical problems in precisely specifying decision model consisting of product-specific variation points and variants, and in handling inter-variant conflicts/dependency. To overcome this difficulty, it is desirable to develop a systematic process which includes a set of systematic activities, detailed instructions, and concrete specification of artifacts. In this paper, we first propose a meta-model of a core asset to specify its key elements. Then, we represent a comprehensive process that defines key instantiation activities, representations of artifacts, and work instructions. With the proposed process, one can instantiate core assets more effectively and systematically.},
booktitle = {Proceedings of the 7th International Conference on Product-Focused Software Process Improvement},
pages = {334–347},
numpages = {14},
location = {Amsterdam, The Netherlands},
series = {PROFES'06}
}

@inproceedings{10.1145/2648511.2648550,
author = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
title = {A methodical approach to product line adoption},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648550},
doi = {10.1145/2648511.2648550},
abstract = {The evolution of the U.S. Army's Live Training Transformation (LT2) product line of combat training systems, including the move by the Army to consolidate management of the product line under a single contracting team, has provided a natural experiment that validates the hypothesis that product line engineering practices are more effective than traditional software engineering practices, and has demonstrated which product line adoption approaches are more successful than others. By analyzing this natural experiment, the product line team has been able to apply a methodical approach to product line adoption across the development organization and successfully adopt second generation product line processes. This paper explores that methodical approach. It will enumerate the steps that led to successes and explore the contributing factors and unintended consequences of failures along the way. Additionally this paper will explore how this approach is being employed to extend the LT2 product line beyond software.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {340–349},
numpages = {10},
keywords = {variation points, software product lines, second generation product line engineering, product portfolio, product line governance, product line engineering, product line adoption, product configurator, product baselines, feature profiles, feature modeling, feature constraints hierarchical product lines, bill-of-features},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {variability extraction, software product lines, reverse engineering, requirement documents, feature identification},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3307630.3342411,
author = {Meixner, Kristof and Rabiser, Rick and Biffl, Stefan},
title = {Towards Modeling Variability of Products, Processes and Resources in Cyber-Physical Production Systems Engineering},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342411},
doi = {10.1145/3307630.3342411},
abstract = {Planning and developing Cyber-Physical Production Systems (CPPS) are multi-disciplinary engineering activities that rely on effective and efficient knowledge exchange for better collaboration between engineers of different disciplines. The Product-Process-Resource (PPR) approach allows modeling products produced by industrial processes using specific production resources. In practice, a CPPS manufactures a portfolio of product type variants, i.e., a product line. Therefore, engineers need to create and maintain several PPR models to cover PPR variants and their evolving versions. In this paper, we detail a representative use case, identify challenges for using Variability Modeling (VM) methods to describe and manage PPR variants, and present a first solution approach based on cooperation with domain experts at an industry partner, a system integrator of automation for high-performance CPPS. We conclude that integrating basic variability concepts into PPR models is a promising first step and describe our further research plans to support PPR VM in CPPS.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {49–56},
numpages = {8},
keywords = {variability modelling, product-process-resource, cyber-physical production system},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3461001.3471155,
author = {Martin, Hugo and Acher, Mathieu and Pereira, Juliana Alves and J\'{e}z\'{e}quel, Jean-Marc},
title = {A comparison of performance specialization learning for configurable systems},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471155},
doi = {10.1145/3461001.3471155},
abstract = {The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {46–57},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/3382025.3414962,
author = {Chrszon, Philipp and Baier, Christel and Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha},
title = {From features to roles},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414962},
doi = {10.1145/3382025.3414962},
abstract = {The detection of interactions is a challenging task present in almost all stages of software development. In feature-oriented system design, this task is mainly investigated for interactions of features within a single system, detected by their emergent behaviors. We propose a formalism to describe interactions in hierarchies of feature-oriented systems (hierarchical interactions) and the actual situations where features interact (active interplays). Based on the observation that such interactions are also crucial in role-based systems, we introduce a compositional modeling framework based on concepts and notions of roles, comprising role-based automata (RBAs). To describe RBAs, we present a modeling language that is close to the input language of the probabilistic model checker Prism. To exemplify the use of RBAs, we implemented a tool that translates RBA models into Prism and thus enables the formal analysis of functional and non-functional properties including system dynamics, contextual changes, and interactions. We carry out two case studies as a proof of concept of such analyses: First, a peer-to-peer protocol case study illustrates how undesired hierarchical interactions can be discovered automatically. Second, a case study on a self-adaptive production cell demonstrates how undesired interactions influence quality-of-service measures such as reliability and throughput.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {19},
numpages = {11},
keywords = {verification, roles, formal methods, feature-oriented systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/1958746.1958805,
author = {Kapova, Lucia},
title = {Reusable QoS specifications for systematic component-based design},
year = {2011},
isbn = {9781450305198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1958746.1958805},
doi = {10.1145/1958746.1958805},
abstract = {For successful and effective software development the ability to predict impact of design decisions in early development stages is crucial. Typically, to provide accurate predictions the models have to include low-level details such as used design patterns (e.g., concurrency design patterns) and underlying middleware platform. These details influence Quality of Service (QoS) metrics, thus are essential for accurate prediction of extra-functional properties such as performance and reliability. Existing approaches do not consider the relation of actual implementations and performance models used for prediction. Furthermore, they neglect the broad variety of implementations and middleware platforms, possible configurations, and varying usage scenarios. To allow more accurate performance predictions, we extend classical performance engineering by automated model refinements based on a library of reusable performance completions.},
booktitle = {Proceedings of the 2nd ACM/SPEC International Conference on Performance Engineering},
pages = {415–416},
numpages = {2},
keywords = {palladio component model, model refinement, completion},
location = {Karlsruhe, Germany},
series = {ICPE '11}
}

@inproceedings{10.1145/3461001.3471149,
author = {Lesoil, Luc and Acher, Mathieu and T\'{e}rnava, Xhevahire and Blouin, Arnaud and J\'{e}z\'{e}quel, Jean-Marc},
title = {The interplay of compile-time and run-time options for performance prediction},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471149},
doi = {10.1145/3461001.3471149},
abstract = {Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {100–111},
numpages = {12},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1109/APSEC.2004.12,
author = {Kim, Soo Dong and Chang, Soo Ho and Chang, Chee Won},
title = {A Systematic Method to Instantiate Core Assets in Product Line Engineering},
year = {2004},
isbn = {0769522459},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2004.12},
doi = {10.1109/APSEC.2004.12},
abstract = {Product line engineering (PLE) is one of the recent and effective reuse approaches, and it consists of two processes; framework engineering and application engineering. Framework engineering is to model and realize a core asset which represents common functionality and quality attributes in a target domain, and application engineering is to generate a target product by instantiating the core asset. Hence, a core asset is a key element of PLE, and the quality of core assets largely determines the overall quality of products. Although numerous PLE methodologies have been introduced, it is still unclear what should be the elements of a core asset and how the core asset should be instantiated for each application. That is, the overall process and instructions of instantiating core assets have not been studied enough. In this paper, we first define a meta-model of core assets, propose a process of instantiation, and define its instructions on how to carry out each activity in practice. We believe that the proposed meta-model and the process framework can effectively be used in developing core asset and applications in practice.},
booktitle = {Proceedings of the 11th Asia-Pacific Software Engineering Conference},
pages = {92–98},
numpages = {7},
series = {APSEC '04}
}

@inproceedings{10.1145/3307630.3342414,
author = {Th\"{u}m, Thomas and Teixeira, Leopoldo and Schmid, Klaus and Walkingshaw, Eric and Mukelabai, Mukelabai and Varshosaz, Mahsa and Botterweck, Goetz and Schaefer, Ina and Kehrer, Timo},
title = {Towards Efficient Analysis of Variation in Time and Space},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342414},
doi = {10.1145/3307630.3342414},
abstract = {Variation is central to today's software development. There are two fundamental dimensions to variation: Variation in time refers to the fact that software exists in numerous revisions that typically replace each other (i.e., a newer version supersedes an older one). Variation in space refers to differences among variants that are designed to coexist in parallel. There are numerous analyses to cope with variation in space (i.e., product-line analyses) and others that cope with variation in time (i.e., regression analyses). The goal of this work is to discuss to which extent product-line analyses can be applied to revisions and, conversely, where regression analyses can be applied to variants. In addition, we discuss challenges related to the combination of product-line and regression analyses. The overall goal is to increase the efficiency of analyses by exploiting the inherent commonality between variants and revisions.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {57–64},
numpages = {8},
keywords = {variability-aware analysis, variability management, software variation, software product lines, software evolution, software configuration management, regression analysis, product-line analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3336294.3336302,
author = {Str\"{u}ber, Daniel and Mukelabai, Mukelabai and Kr\"{u}ger, Jacob and Fischer, Stefan and Linsbauer, Lukas and Martinez, Jabier and Berger, Thorsten},
title = {Facing the Truth: Benchmarking the Techniques for the Evolution of Variant-Rich Systems},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336302},
doi = {10.1145/3336294.3336302},
abstract = {The evolution of variant-rich systems is a challenging task. To support developers, the research community has proposed a range of different techniques over the last decades. However, many techniques have not been adopted in practice so far. To advance such techniques and to support their adoption, it is crucial to evaluate them against realistic baselines, ideally in the form of generally accessible benchmarks. To this end, we need to improve our empirical understanding of typical evolution scenarios for variant-rich systems and their relevance for benchmarking. In this paper, we establish eleven evolution scenarios in which benchmarks would be beneficial. Our scenarios cover typical lifecycles of variant-rich system, ranging from clone &amp; own to adopting and evolving a configurable product-line platform. For each scenario, we formulate benchmarking requirements and assess its clarity and relevance via a survey with experts in variant-rich systems and software evolution. We also surveyed the existing benchmarking landscape, identifying synergies and gaps. We observed that most scenarios, despite being perceived as important by experts, are only partially or not at all supported by existing benchmarks-a call to arms for building community benchmarks upon our requirements. We hope that our work raises awareness for benchmarking as a means to advance techniques for evolving variant-rich systems, and that it will lead to a benchmarking initiative in our community.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {177–188},
numpages = {12},
keywords = {software variability, software evolution, product lines, benchmark},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414963,
author = {Creff, Stephen and Noir, J\'{e}r\^{o}me Le and Lenormand, Eric and Madel\'{e}nat, S\'{e}bastien},
title = {Towards facilities for modeling and synthesis of architectures for resource allocation problem in systems engineering},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414963},
doi = {10.1145/3382025.3414963},
abstract = {Exploring architectural design space is often beyond human capacity and makes architectural design a difficult task. Model-based systems engineering must include assistance to the system designer in identifying candidate architectures to subsequently analyze tradeoffs. Unfortunately, existing languages and approaches do not incorporate this concern, generally favoring solution analysis over exploring a set of candidate architectures.In this paper, we explore the advantages of designing and configuring the variability problem to solve one of the problems of exploring (synthesizing) candidate architectures in systems engineering: the resource allocation problem. More specifically, this work reports on the use of the Clafer modeling language and its gateway to the CSP Choco Solver, on an industrial case study of heterogeneous hardware resource allocation (GPP-GPGPU-FPGA).Based on experiments on the modeling in Clafer, and the impact of its translation into the constraint programming paradigm (performance studies), discussions highlight some issues concerning facilities for modeling and synthesis of architectures and recommendations are proposed towards the use of this variability approach.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {32},
numpages = {11},
keywords = {variability modeling, empirical study, constraint solving, architecture synthesis, allocation problem},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1007/978-3-540-87698-4_8,
author = {Domis, Dominik and Trapp, Mario},
title = {Integrating Safety Analyses and Component-Based Design},
year = {2008},
isbn = {9783540876977},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-87698-4_8},
doi = {10.1007/978-3-540-87698-4_8},
abstract = {In recent years, awareness of how software impacts safety has increased rapidly. Instead of regarding software as a black box, more and more standards demand safety analyses of software architectures and software design. Due to the complexity of software-intensive embedded systems, safety analyses easily become very complex, time consuming, and error prone. To overcome these problems, safety analyses have to be integrated into the complete development process as tightly as possible. This paper introduces an approach to integrating safety analyses into a component-oriented, model-based software engineering approach. The reasons for this are twofold: First, component- and model-based development have already been proven in practical use to handle complexity and reduce effort. Second, they easily support the integration of functional and non-functional properties into design, which can be used to integrate safety analyses.},
booktitle = {Proceedings of the 27th International Conference on Computer Safety, Reliability, and Security},
pages = {58–71},
numpages = {14},
location = {Newcastle upon Tyne, UK},
series = {SAFECOMP '08}
}

@inproceedings{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo tree search for feature model analyses: a general framework for decision-making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
doi = {10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12},
keywords = {variability modeling, software product lines, monte carlo tree search, feature models, configurable systems},
location = {Leicester, United Kingdom},
series = {SPLC '21}
}

@inproceedings{10.1145/2934466.2934473,
author = {Olaechea, Rafael and Fahrenberg, Uli and Atlee, Joanne M. and Legay, Axel},
title = {Long-term average cost in featured transition systems},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934473},
doi = {10.1145/2934466.2934473},
abstract = {A software product line is a family of software products that share a common set of mandatory features and whose individual products are differentiated by their variable (optional or alternative) features. Family-based analysis of software product lines takes as input a single model of a complete product line and analyzes all its products at the same time. As the number of products in a software product line may be large, this is generally preferable to analyzing each product on its own. Family-based analysis, however, requires that standard algorithms be adapted to accomodate variability.In this paper we adapt the standard algorithm for computing limit average cost of a weighted transition system to software product lines. Limit average is a useful and popular measure for the long-term average behavior of a quality attribute such as performance or energy consumption, but has hitherto not been available for family-based analysis of software product lines. Our algorithm operates on weighted featured transition systems, at a symbolic level, and computes limit average cost for all products in a software product line at the same time. We have implemented the algorithm and evaluated it on several examples.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {109–118},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Vulnerability Analysis and Management, Vulnerability, Variability Model, Feature Model, Exploit},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2377816.2377821,
author = {Garcia-Alonso, Jose and Olmeda, Javier Berrocal and Murillo, Juan Manuel},
title = {Architectural variability management in multi-layer web applications through feature models},
year = {2012},
isbn = {9781450313094},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377816.2377821},
doi = {10.1145/2377816.2377821},
abstract = {The development of large web applications has focused on the use of increasingly complex architectures based on the layer architectural pattern and different development frame-works. Many techniques have been proposed to deal with this increasing complexity, mostly in the field of model-based development which abstracts the architects and designers from the architectural and technological complexities. However, these techniques do not take into account the great variability of these architectures, and therefore limit the architectural options available for their users. We here describe a feature model that captures the architectural and technological variability of multilayer applications. Using this feature model as the core of a model-driven development process, we are able to incorporate architectural and technological variability into the model-based development of multilayer applications. This approach keeps complexity under control whilst flexibility on choosing technologies is not penalized},
booktitle = {Proceedings of the 4th International Workshop on Feature-Oriented Software Development},
pages = {29–36},
numpages = {8},
keywords = {multilayer architectures, model-driven development, feature model, development frameworks, design patterns},
location = {Dresden, Germany},
series = {FOSD '12}
}

@inproceedings{10.1145/2499777.2500719,
author = {Schr\"{o}ter, Reimar and Siegmund, Norbert and Th\"{u}m, Thomas},
title = {Towards modular analysis of multi product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500719},
doi = {10.1145/2499777.2500719},
abstract = {Software product-line engineering enables efficient development of tailor-made software by means of reusable artifacts. As practitioners increasingly develop software systems as product lines, there is a growing potential to reuse product lines in other product lines, which we refer to as multi product line. We identify challenges when developing multi product lines and propose interfaces for different levels of abstraction ranging from variability modeling to functional and non-functional properties. We argue that these interfaces ease the reuse of product lines and identify research questions that need to be solved toward modular analysis of multi product lines.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {96–99},
numpages = {4},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3336294.3336306,
author = {Ghamizi, Salah and Cordy, Maxime and Papadakis, Mike and Traon, Yves Le},
title = {Automated Search for Configurations of Convolutional Neural Network Architectures},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336306},
doi = {10.1145/3336294.3336306},
abstract = {Convolutional Neural Networks (CNNs) are intensively used to solve a wide variety of complex problems. Although powerful, such systems require manual configuration and tuning. To this end, we view CNNs as configurable systems and propose an end-to-end framework that allows the configuration, evaluation and automated search for CNN architectures. Therefore, our contribution is threefold. First, we model the variability of CNN architectures with a Feature Model (FM) that generalizes over existing architectures. Each valid configuration of the FM corresponds to a valid CNN model that can be built and trained. Second, we implement, on top of Tensorflow, an automated procedure to deploy, train and evaluate the performance of a configured model. Third, we propose a method to search for configurations and demonstrate that it leads to good CNN models. We evaluate our method by applying it on image classification tasks (MNIST, CIFAR-10) and show that, with limited amount of computation and training, our method can identify high-performing architectures (with high accuracy). We also demonstrate that we outperform existing state-of-the-art architectures handcrafted by ML researchers. Our FM and framework have been released to support replication and future research.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {119–130},
numpages = {12},
keywords = {neural architecture search, feature model, configuration search, NAS, AutoML},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/3382025.3414965,
author = {Young, Jeffrey M. and Walkingshaw, Eric and Th\"{u}m, Thomas},
title = {Variational satisfiability solving},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414965},
doi = {10.1145/3382025.3414965},
abstract = {Incremental satisfiability (SAT) solving is an extension of classic SAT solving that allows users to efficiently solve a set of related SAT problems by identifying and exploiting shared terms. However, using incremental solvers effectively is hard since performance is sensitive to a problem's structure and the order sub-terms are fed to the solver, and the burden to track results is placed on the end user. For analyses that generate sets of related SAT problems, such as those in software product lines, incremental SAT solvers are either not used at all, used but not explicitly stated so in the literature, or used but suffer from the aforementioned usability problems. This paper translates the ordering problem to an encoding problem and automates the use of incremental SAT solving. We introduce variational SAT solving, which differs from incremental SAT solving by accepting all related problems as a single variational input and returning all results as a single variational output. Our central idea is to make explicit the operations of incremental SAT solving, thereby encoding differences between related SAT problems as local points of variation. Our approach automates the interaction with the incremental solver and enables methods to automatically optimize sharing of the input. To evaluate our methods we construct a prototype variational SAT solver and perform an empirical analysis on two real-world datasets that applied incremental solvers to software evolution scenarios. We show, assuming a variational input, that the prototype solver scales better for these problems than naive incremental solving while also removing the need to track individual results.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {18},
numpages = {12},
keywords = {variation, software product lines, satisfiability solving, choice calculus},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.5555/1105634.1105651,
author = {de Oliveira, Edson Alves and Gimenes, Itana M. S. and Huzita, Elisa Hatsue Moriya and Maldonado, Jos\'{e} Carlos},
title = {A variability management process for software product lines},
year = {2005},
publisher = {IBM Press},
abstract = {The software product line approach (PL) promotes the generation of specific products from a set of core assets for a given domain. This approach is applicable to domains in which products have well-defined commonalities and variation points. Variability management is concerned with the management of the differences between products throughout the PL lifecycle. This paper presents a UML-based process for variability management that allows identification, representation and delimitation of variabilities as well as identification of mechanisms for variability implementation. The process is illustrated with excerpts of a case study carried out within the context of an existing PL for the Workflow Management System (WfMS) domain. The case study was carried out based on the experimental software engineering concepts. The results have shown that the proposed process has made explicit a higher number of variabilities than does the existing PL process, and it offers better support for variability tracing.},
booktitle = {Proceedings of the 2005 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {225–241},
numpages = {17},
location = {Toranto, Ontario, Canada},
series = {CASCON '05}
}

@inproceedings{10.1145/2934466.2934474,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M\"{a}nnist\"{o}, Tomi},
title = {Purposeful performance variability in software product lines: a comparison of two case studies},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934474},
doi = {10.1145/2934466.2934474},
abstract = {Within software product lines, customers may have different quality needs. To produce products with purposefully different quality attributes, several challenges must be addressed. First, one must be able to distinguish product quality attributes to the customers in a meaningful way. Second, one must create the desired quality attribute differences during product-line architecture design and derivation. To study how performance is varied purposefully in software product lines, we conducted a comparison and re-analysis of two industrial case studies in the telecommunication and mobile game domains. The results show that performance variants must be communicated to the customer in a way that links to customer value and her role. When performance or its adaptation are crucial for the customer, performance differences must be explicitly "designed in" with software or hardware means. Due to the emergent nature of performance, it is important to test performance and manage how other variability affects performance.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {144–153},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3307630.3342404,
author = {Th\"{u}m, Thomas and Seidl, Christoph and Schaefer, Ina},
title = {On Language Levels for Feature Modeling Notations},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342404},
doi = {10.1145/3307630.3342404},
abstract = {Configuration is a key enabling technology for the engineering of systems and software as wells as physical goods. A selection of configuration options (aka. features) is often enough to automatically generate a product tailored to the needs of a customer. It is common that not all combinations of features are possible in a given domain. Feature modeling is the de-facto standard for specifying features and their valid combinations. However, a pivotal hurdle for practitioners, researchers, and teachers in applying feature modeling is that there are hundreds of tools and languages available. While there have been first attempts to define a standard feature modeling language, they still struggle with finding an appropriate level of expressiveness. If the expressiveness is too high, the language will not be adopted, as it is too much effort to support all language constructs. If the expressiveness is too low, the language will not be adopted, as many interesting domains cannot be modeled in such a language. Towards a standard feature modeling notation, we propose the use of language levels with different expressiveness each and discuss criteria to be used to define such language levels. We aim to raise the awareness on the expressiveness and eventually contribute to a standard feature modeling notation.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {158–161},
numpages = {4},
keywords = {variability modeling, product lines, language design, feature model, expressiveness, automated analysis},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2019136.2019168,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Exploring the use of reference architectures in the development of product line artifacts},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019168},
doi = {10.1145/2019136.2019168},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. In another context, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the Reference Architecture research area. In spite of the positive impact of this type of architecture on reuse and productivity, the use of existing domain-specific reference architectures as basis of SPL has not been widely explored. The main contribution of this paper is to present how and when elements contained in existing reference architectures could contribute to the building of SPL artifacts during development of an SPL. We have observed that, in fact, reference architectures could make an important contribution to improving reuse and productivity, which are also important concerns in SPL.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {28},
numpages = {8},
keywords = {software product line, reference architecture, SPL design method},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/1217935.1217955,
author = {Krishna, Arvind S. and Gokhale, Aniruddha S. and Schmidt, Douglas C.},
title = {Context-specific middleware specialization techniques for optimizing software product-line architectures},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217955},
doi = {10.1145/1217935.1217955},
abstract = {Product-line architectures (PLAs) are an emerging paradigm for developing software families for distributed real-time and embedded (DRE) systems by customizing reusable artifacts, rather than hand-crafting software from scratch. To reduce the effort of developing software PLAs and product variants for DRE systems, developers are applying general-purpose -- ideally standard -- middleware platforms whose reusable services and mechanisms support a range of application quality of service (QoS) requirements, such as low latency and jitter. The generality and flexibility of standard middleware, however, often results in excessive time/space overhead for DRE systems, due to lack of optimizations tailored to meet the specific QoS requirements of different product variants in a PLA.This paper provides the following contributions to the study of middleware specialization techniques for PLA-based DRE systems. First, we identify key dimensions of generality in standard middleware stemming from framework implementations, deployment platforms, and middleware standards. Second, we illustrate how context-specific specialization techniques can be automated and used to tailor standard middleware to better meet the QoS needs of different PLA product variants. Third, we quantify the benefits of applying automated tools to specialize a standard Realtime CORBA middleware implementation. When applied together, these middleware specializations improved our application product variant throughput by ~65%, average- and worst-case end-to-end latency measures by ~43% and ~45%, respectively, and predictability by a factor of two over an already optimized middleware implementation, with little or no effect on portability, standard middleware APIs, or application software implementations, and interoperability.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {205–218},
numpages = {14},
keywords = {specializations, product lines, middleware},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@inproceedings{10.5555/998675.999419,
author = {Matinlassi, Mari},
title = {Comparison of Software Product Line Architecture Design Methods: COPA, FAST, FORM, KobrA and QADA},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Product line architectures (PLAs) have been undercontinuous attention in the software research communityduring the past few years. Although several methods havebeen established to create PLAs there are not availablestudies comparing PLA methods. Five methods are knownto answer the needs of software product lines: COPA,FAST, FORM, KobrA and QADA. In this paper, anevaluation framework is introduced for comparing PLAdesign methods. The framework considers the methodsfrom the points of view of method context, user, structureand validation. Comparison revealed distinguishableideologies between the methods. Therefore, methods donot overlap even though they all are PLA design methods.All the methods have been validated on various domains.The most common domains are telecommunicationinfrastructure and information domains. Some of themethods apply software standards; at least OMG\'{y}s MDAfor method structures, UML for language and IEEE Std-1471-2000 for viewpoint definitions.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {127–136},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.1145/3382025.3414945,
author = {G\"{o}ttmann, Hendrik and Luthmann, Lars and Lochau, Malte and Sch\"{u}rr, Andy},
title = {Real-time-aware reconfiguration decisions for dynamic software product lines},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414945},
doi = {10.1145/3382025.3414945},
abstract = {Dynamic Software Product Lines (DSPL) have recently shown promising potentials as integrated engineering methodology for (self-)adaptive software systems. Based on the software-configuration principles of software product lines, DSPL additionally foster reconfiguration capabilities to continuously adapt software products to ever-changing environmental contexts. However, in most recent works concerned with finding near-optimal reconfiguration decisions, real-time aspects of reconfiguration processes are usually out of scope. In this paper, we present a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Those real-time aware DSPL specifications are internally translated into timed automata, a well-founded formalism for real-time behaviors. This representation allows for formally reasoning about consistency and worst-case/best-case execution-time behaviors of sequences of reconfiguration decisions. The technique is implemented in a prototype tool and experimentally evaluated with respect to a set of case studies1.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {13},
numpages = {11},
keywords = {timed automata, reconfiguration decisions, dynamic software product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2499777.2500710,
author = {Gabillon, Yoann and Biri, Nicolas and Otjacques, Beno\^{\i}t},
title = {Methodology to integrate multi-context UI variations into a feature model},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500710},
doi = {10.1145/2499777.2500710},
abstract = {Software product line (SPL) paradigm aims to explore commonalities and variabilities in a set of applications for developing an efficient derivation of products. One of the most common ways to model variability in this paradigm is to use a Feature Model. However, variability in SPL is often limited to functional features. The User Interface (UI) variations are modeled as entire UIs and thus these variations are not reusable and inspectable. Research in the Human Computer Interaction (HCI) field has proven the importance of variability for non functional, purely UI centric features. The HCI community has proposed several levels of abstraction for multi-context UI design. Indeed, new variations can be introduced at each abstraction level. UI designers are used to them and they usually introduce variability at each step of the UI definition without using SPL. To build usable softwares that take into account UI, we propose to merge functional concerns and UI concerns, providing a methodology to integrate variability of both aspects into a single Feature Model.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {74–81},
numpages = {8},
keywords = {variability, user interface, usability, software product line, multi-context, feature model, abstraction levels},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/645882.672260,
author = {Voget, Stefan and Becker, Martin},
title = {Establishing a Software Product Line in an Immature Domain},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Often product lines are applied to "stable domains" (i.e., a set of common features is identifiable in advance and the evolution of the domain is manageable during the lifetime of the product line). These prerequisites are not always given. But there may be market pressure that requires developing products with systematic and preplanned reuse in a domain that is difficult to grasp. In such a case the product line approach also offers a set of methods that helps to overcome the risks of an immature domain.In this paper we discuss some risks in context of immature domains. For some challenges we present approaches to manage them. The considerations are substantiated by experiences in the domain of entertainment and infotainment systems in an automotive context. The development is deeply influenced by technological changes (e.g., Internet, MP3-player, UMTS) that challenge the successful deployment of product line technology.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {60–67},
numpages = {8},
series = {SPLC 2}
}

@inproceedings{10.1145/2791060.2791106,
author = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
title = {Evolving an industrial analytics product line architecture},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791106},
doi = {10.1145/2791060.2791106},
abstract = {This paper focuses on an industrial experience with software product lines of analytics-enabled solutions, specifically the evolution of the software product line architecture for a Subject Matter Expert Workbench toolset which supports analytic plugins for multiple software product lines. As context, the toolset product line was intended for integration of expert knowledge into a family of industrial asset health applications at runtime. The toolset architecture is now being evolved to build and manage plugins for multiple Industrial Analytics solutions (software systems and services) beyond asset health. This evolution is driving changes in the desired architecture qualities of the toolset; widening the stakeholder pool and influencing priorities; affecting the architecture tradeoffs and decisions; and triggering updates to the product line architecture, the guidance for applying it, and the current prototype of the toolset. We describe our experiences in handling this evolution, assess lessons learned, and discuss potential relevance to other product line scenarios.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {263–272},
numpages = {10},
keywords = {software product line, reusability, performance, knowledge, interoperability, industrial analytics, extensibility, asset health},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791099,
author = {Filho, Jo\~{a}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
title = {Assessing product line derivation operators applied to Java source code: an empirical study},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791099},
doi = {10.1145/2791060.2791099},
abstract = {Product Derivation is a key activity in Software Product Line Engineering. During this process, derivation operators modify or create core assets (e.g., model elements, source code instructions, components) by adding, removing or substituting them according to a given configuration. The result is a derived product that generally needs to conform to a programming or modeling language. Some operators lead to invalid products when applied to certain assets, some others do not; knowing this in advance can help to better use them, however this is challenging, specially if we consider assets expressed in extensive and complex languages such as Java. In this paper, we empirically answer the following question: which product line operators, applied to which program elements, can synthesize variants of programs that are incorrect, correct or perhaps even conforming to test suites? We implement source code transformations, based on the derivation operators of the Common Variability Language. We automatically synthesize more than 370,000 program variants from a set of 8 real large Java projects (up to 85,000 lines of code), obtaining an extensive panorama of the sanity of the operations.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {36–45},
numpages = {10},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233039,
author = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
title = {N-dimensional tensor factorization for self-configuration of software product lines at runtime},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233039},
doi = {10.1145/3233027.3233039},
abstract = {Dynamic software product lines demand self-adaptation of their behavior to deal with runtime contextual changes in their environment and offer a personalized product to the user. However, taking user preferences and context into account impedes the manual configuration process, and thus, an efficient and automated procedure is required. To automate the configuration process, context-aware recommendation techniques have been acknowledged as an effective mean to provide suggestions to a user based on their recognized context. In this work, we propose a collaborative filtering method based on tensor factorization that allows an integration of contextual data by modeling an N-dimensional tensor User-Feature-Context instead of the traditional two-dimensional User-Feature matrix. In the proposed approach, different types of non-functional properties are considered as additional contextual dimensions. Moreover, we show how to self-configure software product lines by applying our N-dimensional tensor factorization recommendation approach. We evaluate our approach by means of an empirical study using two datasets of configurations derived for medium-sized product lines. Our results reveal significant improvements in the predictive accuracy of the configuration over a state-of-the-art non-contextual matrix factorization approach. Moreover, it can scale up to a 7-dimensional tensor containing hundred of configurations in a couple of milliseconds.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {87–97},
numpages = {11},
keywords = {software product lines, self-configuration, runtime decision-making, recommender systems},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2499777.2500711,
author = {Ciolfi Felice, Marianela and Filho, Joao Bosco Ferreira and Acher, Mathieu and Blouin, Arnaud and Barais, Olivier},
title = {Interactive visualisation of products in online configurators: a case study for variability modelling technologies},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500711},
doi = {10.1145/2499777.2500711},
abstract = {Numerous companies develop interactive environments to assist users in customising sales products through the selection of configuration options. A visual representation of these products is an important factor in terms of user experience. However, an analysis of 100+ existing configurators highlights that not all provide visual representations of configured products. One of the current challenges is the trade-off developers face between either the memory consuming use of pregenerated images of all the combinations of options, or rendering products on the fly, which is non trivial to implement efficiently. We believe that a new approach to associate product configurations to visual representations is needed to compose and render them dynamically. In this paper we present a formal statement of the problem and a model-driven perspective for addressing it as well as our ongoing work and further challenges.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {82–85},
numpages = {4},
keywords = {variability modelling, user interface, software product line, configurator},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.1145/3233027.3233035,
author = {Varshosaz, Mahsa and Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Runge, Tobias and Mousavi, Mohammad Reza and Schaefer, Ina},
title = {A classification of product sampling for software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233035},
doi = {10.1145/3233027.3233035},
abstract = {The analysis of software product lines is challenging due to the potentially large number of products, which grow exponentially in terms of the number of features. Product sampling is a technique used to avoid exhaustive testing, which is often infeasible. In this paper, we propose a classification for product sampling techniques and classify the existing literature accordingly. We distinguish the important characteristics of such approaches based on the information used for sampling, the kind of algorithm, and the achieved coverage criteria. Furthermore, we give an overview on existing tools and evaluations of product sampling techniques. We share our insights on the state-of-the-art of product sampling and discuss potential future work.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {1–13},
numpages = {13},
keywords = {testing, software product lines, sampling algorithms, feature interaction, domain models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing platforms for multiple software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, multiple product lines, industrial experience},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2362536.2362546,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {A systematically conducted literature review: quality attribute variability in software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362546},
doi = {10.1145/2362536.2362546},
abstract = {Typically, products in a software product line differ by their functionality, and quality attributes are not intentionally varied. Why, how, and which quality attributes to vary has remained an open issue. A systematically conducted literature review on quality attribute variability is presented, where primary studies are selected by reading all content of full studies in Software Product Line Conference. The results indicate that the success of feature modeling influences the proposed approaches, different approaches suit specific quality attributes differently, and empirical evidence on industrial quality variability is lacking.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {41–45},
numpages = {5},
keywords = {variability, systematic literature review, quality attribute},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2791060.2791066,
author = {Dhungana, Deepak and Falkner, Andreas and Haselb\"{o}ck, Alois and Schreiner, Herwig},
title = {Smart factory product lines: a configuration perspective on smart production ecosystems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791066},
doi = {10.1145/2791060.2791066},
abstract = {Smart production aims to increase the flexibility of the production processes and be more efficient in the use of resources. Two important pillars of this initiative are "smart products" and "smart factories". From the perspective of product line engineering, these can be seen as two product lines (product line of factories and product line of goods) that need to be integrated for a common systems engineering approach. In this paper, we look at this problem from the perspective of configuration technologies, outline the research challenges in this area and illustrate our vision using an industrial example. The factory product line goes hand-in-hand with the product line of the products to be manufactured. Future research in product line engineering needs to consider an ecosystem of a multitude of stakeholders - e.g., factory component vendors, product designers, factory owners/operators and end-consumers.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {201–210},
numpages = {10},
keywords = {smart production, smart product, smart factory, product line of factories, product and production configuration},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3233027.3233036,
author = {Hamza, Mostafa and Walker, Robert J. and Elaasar, Maged},
title = {CIAhelper: towards change impact analysis in delta-oriented software product lines},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233036},
doi = {10.1145/3233027.3233036},
abstract = {Change is inevitable for software systems to deal with the evolving environment surrounding them, and applying changes requires careful design and implementation not to break existing functionalities. Evolution in software product lines (SPLs) is more complex compared to evolution for individual products: a change applied to a single feature might affect all the products in the whole product family. In this paper we present an approach for change impact analysis in delta-oriented programming (DOP), an existing language aimed at supporting SPLs. We propose the CIAHelper tool to identify dependencies within a DOP program, by analyzing the semantics of both the code artifacts and variability models to construct a directed dependency graph. We also consider how the source code history could be used to enhance the recall of detecting the affected artifacts given a change proposal. We evaluate our approach by means of five case studies on two different DOP SPLs.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {31–42},
numpages = {12},
keywords = {variability model, feature model, delta-oriented programming, code assets, change impact analysis},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/2364412.2364426,
author = {Villela, Karina and Arif, Taslim and Zanardini, Damiano},
title = {Towards product configuration taking into account quality concerns},
year = {2012},
isbn = {9781450310956},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2364412.2364426},
doi = {10.1145/2364412.2364426},
abstract = {The configuration of concrete products from a product line infrastructure is the process of resolving the variability captured in the product line according to a company's market strategy or specific customer's requirements. Several aspects influence the selection of features for a concrete product, such as dependencies and constraints between features, the different stakeholders involved in the process, the desired degree of quality, and cost constraints. This paper presents the vision of a configurator that will focus on providing indicators of security and performance for features and empowering its users to interactively observe the effect of the selected set of features on these two quality characteristics. We propose the use of reusable expert knowledge and static analysis for obtaining the indicators of security and performance, respectively. The two main issues to be investigated are: (1) to which degree the configuration process should be automated; and (2) how exactly to obtain indicators of security and performance for features that can be used to predict the security and performance of whole configurations.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 2},
pages = {82–90},
numpages = {9},
keywords = {static analysis, quality concerns, product line engineering, product configuration, feature models},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s10664-004-6190-y,
author = {Svahnberg, Mikael and Wohlin, Claes},
title = {An Investigation of a Method for Identifying a Software Architecture Candidate with Respect to Quality Attributes},
year = {2005},
issue_date = {April     2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-004-6190-y},
doi = {10.1007/s10664-004-6190-y},
abstract = {To sustain the qualities of a software system during evolution, and to adapt the quality attributes as the requirements evolve, it is necessary to have a clear software architecture that is understood by all developers and to which all changes to the system adheres. This software architecture can be created beforehand, but must also be updated to reflect changes in the domain, and hence the requirements of the software. The choice of which software architecture to use is typically based on informal decisions. There exist, to the best of our knowledge, little factual knowledge of which quality attributes are supported or obstructed by different architecture approaches. In this paper we present an empirical study of a method that enables quantification of the perceived support different software architectures give for different quality attributes. This in turn enables an informed decision of which architecture candidate best fit the mixture of quality attributes required by a system being designed.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {149–181},
numpages = {33},
keywords = {quality attributes, analytic hierarchy process, Software architectures}
}

@inproceedings{10.1145/2934466.2934492,
author = {Groher, Iris and Weinreich, Rainer and Buchgeher, Georg and Schossleitner, Robert},
title = {Reusable architecture variants for customer-specific automation solutions},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934492},
doi = {10.1145/2934466.2934492},
abstract = {Manufacturing execution systems (MES) are key elements of industrial automation systems. MES can be deployed at different levels of scale from a single site or plant to a company with globally distributed production sites all over the world. Establishing or extending an MES is a complex process, which requires taking the already existing software and system architecture into account in addition to the desired MES features. We developed an approach and an associated tool to support the process of creating offers for customer-specific MES solutions based on a vendor-specific automation platform. We define architecture variants for selecting a specific MES feature set and for supporting different MES expansion stages. Additionally, we provide an architecture modeling approach to explore the integration with existing software and system infrastructures. The approach has been applied at the STIWA Group, a vendor of MES for industrial production lines.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {242–251},
numpages = {10},
keywords = {manufacturing execution system (MES), feature set, customer-specific offer, automation platform, architecture variants},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2934466.2946045,
author = {Noir, J\'{e}rome Le and Madel\'{e}nat, S\'{e}bastien and Gailliard, Gr\'{e}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
title = {A decision-making process for exploring architectural variants in systems engineering},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946045},
doi = {10.1145/2934466.2946045},
abstract = {In systems engineering, practitioners shall explore numerous architectural alternatives until choosing the most adequate variant. The decision-making process is most of the time a manual, time-consuming, and error-prone activity. The exploration and justification of architectural solutions is ad-hoc and mainly consists in a series of tries and errors on the modeling assets. In this paper, we report on an industrial case study in which we apply variability modeling techniques to automate the assessment and comparison of several candidate architectures (variants). We first describe how we can use a model-based approach such as the Common Variability Language (CVL) to specify the architectural variability. We show that the selection of an architectural variant is a multi-criteria decision problem in which there are numerous interactions (veto, favor, complementary) between criteria.We present a tooled process for exploring architectural variants integrating both CVL and the MYRIAD method for assessing and comparing variants based on an explicit preference model coming from the elicitation of stakeholders' concerns. This solution allows understanding differences among variants and their satisfactions with respect to criteria. Beyond variant selection automation improvement, this experiment results highlight that the approach improves rationality in the assessment and provides decision arguments when selecting the preferred variants.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {277–286},
numpages = {10},
keywords = {systems engineering, multi-criteria decision analysis, model-driven engineering, design exploration, decision-making, architecture},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An overview on analysis tools for software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {type checking, tool support, theorem proving, testing, static analysis, software product lines, sampling, non-functional properties, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/ICESS.2009.30,
author = {Anne, Matthieu and He, Ruan and Jarboui, Tahar and Lacoste, Marc and Lobry, Olivier and Lorant, Guirec and Louvel, Maxime and Navas, Juan and Olive, Vincent and Polakovic, Juraj and Poulhi\`{e}s, Marc and Pulou, Jacques and Seyvoz, St\'{e}phane and Tous, Julien and Watteyne, Thomas},
title = {Think: View-Based Support of Non-functional Properties in Embedded Systems},
year = {2009},
isbn = {9780769536781},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICESS.2009.30},
doi = {10.1109/ICESS.2009.30},
abstract = {Component-Based Software Engineering (CBSE) does not yet fully address non-functional requirements of embedded systems. To reach this goal, we show how to extend a component model like FRACTAL with relevant abstractions such as threads, protection rings, or security domains. The FRACTAL Architecture Description Language (ADL) is extended by means of properties that tag components, bindings, and interfaces of the system architectural definition with execution schemes, dynamic reconfiguration strategies, protection and isolation patterns, or QoS features. Each extension captures a property-specific "system view" offering a sound basis to address some non-functional requirement. These extensions were experimented in the THINK framework, a C-based implementation of FRACTAL. Results show that THINK provides a generic and efficient approach to fully support these extensions thanks to a customizable toolchain.},
booktitle = {Proceedings of the 2009 International Conference on Embedded Software and Systems},
pages = {147–156},
numpages = {10},
keywords = {Separation of Concerns, Embedded Systems, Component-Based Operating Systems, Component-Based Design},
series = {ICESS '09}
}

@inproceedings{10.1145/3474624.3474626,
author = {Bettin, Giovanna and OliveiraJr, Edson},
title = {SMartyPerspective: a perspective-based inspection technique for software product lines},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474626},
doi = {10.1145/3474624.3474626},
abstract = {Software Product Line (SPL) is an approach for reusing software artifacts for a specific domain. To improve products quality, verification and validation activities for SPL artifacts are necessary, thus defects are not propagated to derived products. The lack of techniques to exploit the inherited SPL reuse characteristics, mainly in early phases, provides an opportunity to investigate how to improve SPL quality. Perspective-Based Reading (PBR) has proven to be a feasible inspection technique, as it considers different scenarios and perspectives of reviewers of software artifacts. Therefore, in this paper, we specify and evaluate SMartyPerspective, a PBR technique to inspect UML-based SPL diagrams (use case, class, sequence, and component) and feature diagrams. SMartyPerspective comprises Domain Engineering perspectives: Product Manager, Domain Requirements Engineer, Domain Architect, Domain Developer, and Domain Asset Manager. We evaluated it by carrying out a TAM-based qualitative study with 19 participants with experience in SPL and software inspections. We also used coding to analyze the open questions. Obtained results provide initial evidence SMartyPerspective is feasible for inspecting its supported diagrams.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {90–94},
numpages = {5},
keywords = {UML, TAM, Software Product Line, SPL Inspections, SMarty, Perspective-Based Reading, Defects},
location = {Joinville, Brazil},
series = {SBES '21}
}

@inproceedings{10.1145/2019136.2019158,
author = {Guana, Victor and Correal, Dario},
title = {Variability quality evaluation on component-based software product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019158},
doi = {10.1145/2019136.2019158},
abstract = {Quality assurance and evaluation in Model Driven Software Product Lines (MD-SPLs) are pivotal points for the growing and solidification of the generative software factories. They are framed as one of the future fact methodologies for the construction of software systems. Although several approximations address the problem of generative environments, software product line scope expression, and core asset definition, not many of them try to solve, as a fundamental step, the automation of the quality attribute evaluation in the MD-SPL development cycle. This paper presents a model-driven engineering method and a tool for the quality evaluation of product line configurations through a cross architectural view analysis.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {19},
numpages = {8},
keywords = {sensitivity point, quality attribute, model-driven software product line, model composition, domain specific modeling},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3493244.3493250,
author = {Wolfart, Daniele and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Martinez, Jabier},
title = {Variability Debt: Characterization, Causes and Consequences},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493250},
doi = {10.1145/3493244.3493250},
abstract = {Variability is an inherent property of software systems to create families of products dealing with needs of different customers and environments. However, some practices to manage variability may incur technical debt. For example, the use of opportunistic reuse strategies, e.g., clone-and-own, harms maintenance and evolution activities; or deciding to abandon variability management and deriving a single product with all the features might threaten system usability. These examples are common problems found in practice but, to the best of or knowledge, not properly investigated from the perspective of technical debt. To expand the knowledge on the research and practice of technical debt in the perspective of variability management, we report results of this phenomenon, which we defined as variability debt. Our work is based on 52 industrial case studies that report problems observed in the use of opportunistic reuse. The results show that variability debt is caused by business, operational and technical aspects; leads to complex maintenance, creates difficulties to customize and create new products, misuse of human resources, usability problems; and impacts artifacts along the whole life-cycle. Although some of these issues are investigated in the field of systematic variability management, e.g., software product lines, our contribution is to present them from a technical debt perspective to enrich and create synergies between the two fields. As additional contribution, we present a catalog of variability debts in the light of technical debts found in the literature.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {17},
numpages = {10},
keywords = {Variability management, Variability Debt, Technical Debt, Software Product Lines},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-based test case selection of cyber-physical system product lines for simulation-based validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, search-based software engineering, cyber-physical system product lines},
location = {Beijing, China},
series = {SPLC '16}
}

@inproceedings{10.1145/3336294.3336297,
author = {Munoz, Daniel-Jesus and Oh, Jeho and Pinto, M\'{o}nica and Fuentes, Lidia and Batory, Don},
title = {Uniform Random Sampling Product Configurations of Feature Models That Have Numerical Features},
year = {2019},
isbn = {9781450371384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336294.3336297},
doi = {10.1145/3336294.3336297},
abstract = {Analyses of Software Product Lines (SPLs) rely on automated solvers to navigate complex dependencies among features and find legal configurations. Often these analyses do not support numerical features with constraints because propositional formulas use only Boolean variables. Some automated solvers can represent numerical features natively, but are limited in their ability to count and Uniform Random Sample (URS) configurations, which are key operations to derive unbiased statistics on configuration spaces.Bit-blasting is a technique to encode numerical constraints as propositional formulas. We use bit-blasting to encode Boolean and numerical constraints so that we can exploit existing #SAT solvers to count and URS configurations. Compared to state-of-art Satisfiability Modulo Theory and Constraint Programming solvers, our approach has two advantages: 1) faster and more scalable configuration counting and 2) reliable URS of SPL configurations. We also show that our work can be used to extend prior SAT-based SPL analyses to support numerical features and constraints.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume A},
pages = {289–301},
numpages = {13},
keywords = {software product lines, propositional formula, numerical features, model counting, feature model, bit-blasting},
location = {Paris, France},
series = {SPLC '19}
}

@inproceedings{10.1145/2791060.2791075,
author = {Fang, Miao and Leyh, Georg and Doerr, Joerg and Elsner, Christoph and Zhao, Jingjing},
title = {Towards model-based derivation of systems in the industrial automation domain},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791075},
doi = {10.1145/2791060.2791075},
abstract = {Many systems in the industrial automation domain include information systems. They manage manufacturing processes and control numerous distributed hardware and software components. In current practice, the development and reuse of such systems is costly and time-consuming, due to the variability of systems' topology and processes. Up to now, product line approaches for systematic modeling and management of variability have not been well established for such complex domains.In this paper, we present a model-based approach to support the derivation of systems in the target domain. The proposed architecture of the derivation infrastructure enables feature-, topology- and process configuration to be integrated into the multi-staged derivation process. We have developed a prototype to prove feasibility and improvement of derivation efficiency. We report the evaluation results that we collected through semi-structured interviews from domain stakeholders. The results show high potential to improve derivation efficiency by adopting the approach in practice. Finally, we report the lessons learned that raise the opportunities and challenges for future research.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {283–292},
numpages = {10},
keywords = {variability modeling, product line, model-based engineering, derivation},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1109/GCCW.2006.2,
author = {Shi, Ke},
title = {A Component Based Design Tool for Networked Embedded Software Supporting Non-Functional Analysis},
year = {2006},
isbn = {0769526950},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/GCCW.2006.2},
doi = {10.1109/GCCW.2006.2},
abstract = {In this paper a new approach for building networked embedded software is presented. The approach is based on the composition of reusable components with the addition of a perspective contract principle for modeling non-functional properties. Nonfunctional properties are an important aspect of networked embedded software, and this is why they are modeled separately. As such, the component view presented here differs from traditional component based views, where focus is laid on the functional part. The ideas discussed in the paper have been implemented in a tool. This tool enables the construction of networked embedded software by means of components and perspective contracts. Currently, a queuing network based algorithm that considers all non-functional properties together performs a static analysis on the perspective contracts before execution of the application.},
booktitle = {Proceedings of the Fifth International Conference on Grid and Cooperative Computing Workshops},
pages = {327–334},
numpages = {8},
series = {GCCW '06}
}

@inproceedings{10.1145/2019136.2019152,
author = {Kozuka, Nobuaki and Ishida, Yuzo},
title = {Building a product line architecture for variant-rich enterprise applications using a data-oriented approach},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019152},
doi = {10.1145/2019136.2019152},
abstract = {IT industry in Japan has grown by providing specific made-to-order enterprise applications for various industries. Most of enterprise applications are built upon relational database management system (RDBMS), which takes the responsibility of keeping data integrity and data manipulation. However, data explosion in recent years especially in retail and telecommunication industries makes IT industry difficult to satisfy quality attributes such as scalability, availability and data consistency with traditional development techniques. From the beginning of this century, NRI has built and refined product line architecture as a primary core asset for such data intensive industries, which have very rich variations in functional and nonfunctional requirements of their enterprise applications. This paper summarizes key criteria to build such an architecture based on our ten years experience in developing dozens of mission critical IT systems as product families for those industries.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {14},
numpages = {6},
keywords = {relational database management system, quality attributes, product line architecture, enterprise applications, data oriented approach, data intensiveness, core asset development},
location = {Munich, Germany},
series = {SPLC '11}
}

@article{10.1007/s10664-014-9353-5,
author = {Asadi, Mohsen and Soltani, Samaneh and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {The effects of visualization and interaction techniques on feature model configuration},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9353-5},
doi = {10.1007/s10664-014-9353-5},
abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1706–1743},
numpages = {38},
keywords = {Tools, Software product line engineering, Controlled experiment}
}

@inproceedings{10.1145/2362536.2362570,
author = {Braga, Rosana T. V. and Trindade, Onofre and Branco, Kalinka R. L. J. Castelo and Lee, Jaejoon},
title = {Incorporating certification in feature modelling of an unmanned aerial vehicle product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362570},
doi = {10.1145/2362536.2362570},
abstract = {Safety critical systems (e.g., an avionics control system for safe flight) are often required to achieve certification under pre-established standards (e.g., DO-178B for software considerations in airborne systems and equipment certification). We have been working with our industrial partner for the last three years to develop product line assets for their avionics software product line (SPL) and, recently, we encountered two major challenges regarding certification. Firstly, an individual product must be certified, but each may require a different certification level: there might be variations in the certification requirements according to specific system usage contexts. Secondly, certification involves not only product but also process, as standards such as DO-178B also assess the quality of the development process. In this paper, we propose to include a certification view during feature modelling to provide a better understanding of the relationships between features and a certification level required for each product. The experience of introducing certification into the design model of an Unmanned Aerial Vehicle (UAV) SPL is presented to illustrate some key ideas. We also describe the lessons we have learned from this experience.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {249–258},
numpages = {10},
keywords = {software product lines, software certification, feature modelling, critical software development},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3302333.3302336,
author = {Galster, Matthias},
title = {Variability-intensive Software Systems: Product Lines and Beyond},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302336},
doi = {10.1145/3302333.3302336},
abstract = {Product line engineering emerged from the software reuse and generative programming movements of the 70s. However, in today's competitive and fast-paced markets where users expect software to adapt to their specific needs, most modern software-intensive products and services are variability-intensive, regardless of whether they are part of a product line or not. In this talk, we explore how building variability-intensive software systems influences the various software product lifecycle stages. Furthermore, we look beyond variability in functional features: We explore the role of quality attributes in the design of variability-intensive software systems and discuss how to address the challenge of identifying and managing variability in quality attributes. Finally, based on the example of software product line research, we explore how research on variability has been changing over time and if research trends in industry and academia have diverged.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {3},
numpages = {1},
keywords = {software quality attributes, research and practice, product lines and families, Variability-intensive software systems},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@inproceedings{10.1145/2648511.2648521,
author = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
title = {Comparison of exact and approximate multi-objective optimization for software product lines},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648521},
doi = {10.1145/2648511.2648521},
abstract = {Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {92–101},
numpages = {10},
keywords = {software product lines, multi-objective optimization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2491627.2491630,
author = {Linsbauer, Lukas and Lopez-Herrejon, E. Roberto and Egyed, Alexander},
title = {Recovering traceability between features and code in product variants},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491630},
doi = {10.1145/2491627.2491630},
abstract = {Many companies offer a palette of similar software products though they do not necessarily have a Software Product Line (SPL). Rather, they start building and selling individual products which they then adapt, customize and extend for different customers. As the number of product variants increases, these companies then face the severe problem of having to maintain them all. Software Product Lines can be helpful here - not so much as a platform for creating new products but as a means of maintaining the existing ones with their shared features. Here, an important first step is to determine where features are implemented in the source code and in what product variants. To this end, this paper presents a novel technique for deriving the traceability between features and code in product variants by matching code overlaps and feature overlaps. This is a difficult problem because a feature's implementation not only covers its basic functionality (which does not change across product variants) but may include code that deals with feature interaction issues and thus changes depending on the combination of features present in a product variant. We empirically evaluated the approach on three non-trivial case studies of different sizes and domains and found that our approach correctly identifies feature to code traces except for code that traces to multiple disjunctive features, a rare case involving less than 1% of the code.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {131–140},
numpages = {10},
keywords = {traceability, product variants, features},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2648511.2648516,
author = {Reinhartz-Berger, Iris and Figl, Kathrin},
title = {Comprehensibility of orthogonal variability modeling languages: the cases of CVL and OVM},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648516},
doi = {10.1145/2648511.2648516},
abstract = {As the complexity and variety of systems and software products have increased, the ability to manage their variability effectively and efficiently became crucial. To this end, variability can be specified either as an integral part of the development artifacts or in a separate orthogonal variability model. Lately, orthogonal variability models attract a lot of attention due to the fact that they do not require changing the complexity of the development artifacts and can be used in conjunction with different development artifacts. Despite this attention and to the best of our knowledge, no empirical study examined the comprehensibility of orthogonal variability models.In this work, we conducted an exploratory experiment to examine potential comprehension problems in two common orthogonal variability modeling languages, namely, Common Variability Language (CVL) and Orthogonal Variability Model (OVM). We examined the comprehensibility of the variability models and their relations to the development artifacts for novice users. To measure comprehensibility we used comprehension score (i.e., percentage of correct solution), time spent to complete tasks, and participants' perception of difficulty of different model constructs. The results showed high comprehensibility of the variability models, but low comprehensibility of the relations between the variability models and the development artifacts. Although the comprehensibility of CVL and OVM was similar in terms of comprehension score and time spent to complete tasks, novice users perceived OVM as more difficult to comprehend.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {42–51},
numpages = {10},
keywords = {variability analysis, model comprehension, empirical study, OVM, CVL},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2647908.2655977,
author = {El Yamany, Ahmed Eid and Shaheen, Mohamed and Sayyad, Abdel Salam},
title = {OPTI-SELECT: an interactive tool for user-in-the-loop feature selection in software product lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655977},
doi = {10.1145/2647908.2655977},
abstract = {Opti-Select is an Interactive Multi-objective feature analysis and optimization tool for software product lines configuration and feature models optimization based on an innovative UIL (User-In-the-loop) idea. In this tool, the experience of system analysts and stakeholders are merged with optimization techniques and algorithms.Opti-Select interactive tool is an integrated set of techniques providing step by step feature model and attribute configuration, selecting and excluding features, solution set optimization, and user interaction utilities that can all together reach satisfactory set of solutions that fits stakeholder preferences.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {126–129},
numpages = {4},
keywords = {user-in-the-loop (UIL), software product lines, search-based software engineering, product line engineering, optimal variant, optimal feature selection, multi-objective optimization, modeling, features, feature models, feature modeling, exploration, Pareto front visualization},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2648511.2648533,
author = {Simidchieva, Borislava I. and Osterweil, Leon J.},
title = {Generation, composition, and verification of families of human-intensive systems},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648533},
doi = {10.1145/2648511.2648533},
abstract = {Software products are rarely developed without providing different sets of features to better meet varying user needs, whether through tiered products as part of a product line or different subscription levels for software as a service (SaaS). Software product line approaches for generating and maintaining a family of different variants of software products address such needs for variation quite well. Real-world human-intensive systems (HISs) display similar needs for families of variants. A key contribution of this paper is to show how many of these needs can be rigorously and systematically addressed by adapting established techniques from system and software product line engineering (SPLE).In this paper, we present an approach for creating such families by explicitly modeling variation in HISs. We focus on two kinds of variation we have previously described in other work---functional detail variation and service variation. We describe a prototype system that is able to meet the need for these kinds of variation within an existing modeling framework and present a case study of the application of our prototype system to generate a family in an HIS from the domain of elections. Our approach also demonstrates how to perform model-checking of this family to discover whether any variants in the family may violate specified system requirements.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {207–216},
numpages = {10},
keywords = {system variation, software product lines, process families},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/2362536.2362560,
author = {Lettner, Daniela and Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Supporting end users with business calculations in product configuration},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362560},
doi = {10.1145/2362536.2362560},
abstract = {Business calculations like break-even, return on investment, or cost are essential in many domains to support decision making while configuring products. For instance, customers and sales people need to estimate and compare the business value of different product variants. Some product line approaches provide initial support, e.g., by defining quality attributes in relation to features. However, an approach that allows domain engineers to easily define business calculations together with variability models is still lacking. In product configuration, calculation results need to be instantly presented to end users after making configuration choices. Further, due to the often high number of calculations, the presentation of calculation results to end users can be challenging. These challenges cannot be addressed by integrating off-the-shelf applications performing the calculations with product line tools. We thus present an approach based on dedicated calculation models that are related to variability models. Our approach seamlessly integrates business calculations with product configuration and provides support for formatting calculations and calculation results. We use the DOPLER tool suite to deploy calculations together with variability models to end users in product configuration. We evaluate the expressiveness and practical relevance of the approach by investigating the development of business calculations for 15 product lines from the domain of industrial automation.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {171–180},
numpages = {10},
keywords = {variability models, product configuration, business calculations},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Software product line, Quality attributes assessment, Product configuration, Non-functional requirement (NFR) framework, Feature model, Analytic hierarchical process (AHP)}
}

@article{10.1145/2853073.2853082,
author = {Soujanya, K. L.S. and AnandaRao, A.},
title = {A Generic Framework for Configuration Management of SPL and Controlling Evolution of Complex Software Products},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853082},
doi = {10.1145/2853073.2853082},
abstract = {Efficient configuration management system is crucial for the success of any software product line (SPL). Due to ever changing needs of customers, SPL undergoes constant changes that are to be tracked in real time. In the context of customer-driven development, anticipation and change management are to be given paramount importance. It demands implementation of software variability that drives home changed, extended and customized configurations besides economy at scale. Moreover, the emergence of distributed technologies, the unprecedented growth of component based, serviceoriented systems throw ever increasing challenges to software product line configuration management. Derivation of a new product is a dynamic process in software product line that should consider functionality and quality attributes. Very few approaches are found on configuration management (CM) of SPL though CM is enough matured for traditional products. They are tailor made and inadequate to provide a general solution. Stated differently, a comprehensive approach for SPL configuration management and product derivation is still to be desired. In this paper, we proposed a framework that guides in doing so besides helping in SPL definitions in generic way. Our framework facilitates SPL configuration management and product derivation based on critical path analysis, weight computation and feedback. We proposed two algorithms namely Quality Driven Product Derivation (QDPD) and Composition Analysis algorithm for generating satisfied compositions and to find best possible composition respectively. The usage of weights and critical path analysis improves quality of product derivation. The framework is extensible and flexible thus it can be leveraged with variability-aware design patterns and ontology. We built a prototype that demonstrates the proof of concept. We tested our approach with Dr. School product line. The results reveal that the framework supports configuration management of SPL and derivation of high quality product in the product line. We evaluated results with ground truth to establish significance of our implementation},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {1–10},
numpages = {10},
keywords = {weighted approach, product derivation, critical path analysis, configuration management, Software product line}
}

@inproceedings{10.5555/1753235.1753255,
author = {Sun, Hongyu and Lutz, Robyn R. and Basu, Samik},
title = {Product-line-based requirements customization for web service compositions},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Customizing web services according to users' individual functional and non-functional requirements has become increasingly difficult as the number of users increases. This paper introduces a new way to customize and verify composite web services by incorporating a software product-line engineering approach into web-service composition. The approach uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web-service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user selects values for the parameters of variation, we provide a more efficient way to customize web services. A decision model, illustrated with examples from an emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space may also help improve the quality and reliability of the composite services and reduce the cost of re-verifying the same compositions.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {141–150},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1007/978-3-642-31095-9_12,
author = {Ghaddar, Ali and Tamzalit, Dalila and Assaf, Ali and Bitar, Abdalla},
title = {Variability as a service: outsourcing variability management in multi-tenant saas applications},
year = {2012},
isbn = {9783642310942},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31095-9_12},
doi = {10.1007/978-3-642-31095-9_12},
abstract = {In order to reduce the overall application expenses and time to market, SaaS (Software as a Service) providers tend to outsource several parts of their IT resources to other services providers. Such outsourcing helps SaaS providers in reducing costs and concentrating on their core competences: software domain expertises, business-processes modeling, implementation technologies and frameworks etc. However, when a SaaS provider offers a single application instance for multiple customers following the multi-tenant model, these customers (or tenants) requirements may differ, generating an important variability management concern. We believe that variability management should also be outsourced and considered as a service. The novelty of our work is to introduce the new concept of Variability as a Service (VaaS) model. It induces the appearance of VaaS providers. The objective is to relieve the SaaS providers looking forward to adopt such attractive multi-tenant solution, from developing a completely new and expensive variability solution beforehand. We present in this paper the first stage of our work: the VaaS meta-model and the VariaS component.},
booktitle = {Proceedings of the 24th International Conference on Advanced Information Systems Engineering},
pages = {175–189},
numpages = {15},
keywords = {variability, multi-tenant, SaaS},
location = {Gda\'{n}sk, Poland},
series = {CAiSE'12}
}

@inproceedings{10.1145/3218585.3218670,
author = {Martins, Luana Almeida and Parreira, Paulo Afonso and Freire, Andr\'{e} Pimenta and Costa, Heitor},
title = {Exploratory Study on the Use of Software Product Lines in the Development of Quality Assistive Technology Software},
year = {2018},
isbn = {9781450364676},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3218585.3218670},
doi = {10.1145/3218585.3218670},
abstract = {The use of Software Product Line for the development of Assistive Technologies has not been widely explored yet. However, some studies point to the viability of using this approach to develop Assistive Technology software. Through this approach, important limiting factors to use Assistive Technologies can be overcome. These factors are related to the acquisition costs and difficulty to find products corresponding to specific and varying user needs. Considering that Software Product Line approach provides mass customization of software products, the specific needs of each user can be more easily satisfied by software developers. Furthermore, the reuse of code artifacts to development provides a fall in the acquisition cost of these software products. We present in this paper a literature review that aims to investigate how this approach has been applied to the development of Assistive Technology software. Also, we present some quality factors that should be considered to develop Assistive Technologies using Software Product Lines. Thus, the main findings of the review are grouped in order to find the main gaps to be explored in future work.},
booktitle = {Proceedings of the 8th International Conference on Software Development and Technologies for Enhancing Accessibility and Fighting Info-Exclusion},
pages = {262–269},
numpages = {8},
keywords = {Software Quality, Software Product Line, Assistive Technology},
location = {Thessaloniki, Greece},
series = {DSAI '18}
}

@inproceedings{10.1145/2499777.2500718,
author = {Abbas, Nadeem and Andersson, Jesper},
title = {Architectural reasoning for dynamic software product lines},
year = {2013},
isbn = {9781450323253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499777.2500718},
doi = {10.1145/2499777.2500718},
abstract = {Software quality is critical in today's software systems. A challenge is the trade-off situation architects face in the design process. Designers often have two or more alternatives, which must be compared and put into context before a decision is made. The challenge becomes even more complex for dynamic software product lines, where domain designers have to take runtime variations into consideration as well. To address the problem we propose extensions to an architectural reasoning framework with constructs/artifacts to define and model a domain's scope and dynamic variability. The extended reasoning framework encapsulates knowledge to understand and reason about domain quality behavior and self-adaptation as a primary variability mechanism. The framework is demonstrated for a self-configuration property, self-upgradability on an educational product-line.},
booktitle = {Proceedings of the 17th International Software Product Line Conference Co-Located Workshops},
pages = {117–124},
numpages = {8},
keywords = {variability, software product lines, architectural reasoning, adaptation},
location = {Tokyo, Japan},
series = {SPLC '13 Workshops}
}

@inproceedings{10.5555/564092.564116,
author = {Urting, David and Berbers, Yolande and Van Baelen, Stefan and Holvoet, Tom and Vandewoude, Yves and Rigole, Peter},
title = {A tool for component based design of embedded software},
year = {2002},
isbn = {0909925887},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {In this paper a new approach for building embedded applications is presented. The approach is based on the composition of reusable components with the addition of a contract principle for modelling non-functional constraints. Non-functional constraints are an important aspect of embedded systems, and this is why they are modelled separately. As such, the component view presented here differs from traditional component based views, where focus is laid on the functional part. The ideas discussed in the paper have been implemented in a tool. This tool enables the construction of embedded software by means of components and contracts. Currently, runtime mechanisms - that enable runtime monitoring of the contracts - are being included.},
booktitle = {Proceedings of the Fortieth International Conference on Tools Pacific: Objects for Internet, Mobile and Embedded Applications},
pages = {159–168},
numpages = {10},
keywords = {component, contract, embedded, real-time, tool},
location = {Sydney, Australia},
series = {CRPIT '02}
}

@article{10.1016/j.asoc.2016.07.040,
author = {Xue, Yinxing and Zhong, Jinghui and Tan, Tian Huat and Liu, Yang and Cai, Wentong and Chen, Manman and Sun, Jun},
title = {IBED},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.07.040},
doi = {10.1016/j.asoc.2016.07.040},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose to combine IBEA and DE for the optimal feature selection in SPLE.We propose a feedback-directed method into EAs to improve the correctness of results.Our IBED with the seeding method has significantly shortened the search time.In most cases, IBED finds more unique and non-dominated solutions than IBEA. Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1215–1231},
numpages = {17},
keywords = {Software product line engineering, Optimal feature selection, Indicator-based evolutionary algorithm (IBEA), Differential evolutionary algorithm (DE)}
}

@inproceedings{10.1145/2491627.2491635,
author = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
title = {Multi-objective test generation for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491635},
doi = {10.1145/2491627.2491635},
abstract = {Software Products Lines (SPLs) are families of products sharing common assets representing code or functionalities of a software product. These assets are represented as features, usually organized into Feature Models (FMs) from which the user can configure software products. Generally, few features are sufficient to allow configuring millions of software products. As a result, selecting the products matching given testing objectives is a difficult problem.The testing process usually involves multiple and potentially conflicting testing objectives to fulfill, e.g. maximizing the number of optional features to test while at the same time both minimizing the number of products and minimizing the cost of testing them. However, most approaches for generating products usually target a single objective, like testing the maximum amount of feature interactions. While focusing on one objective may be sufficient in certain cases, this practice does not reflect real-life testing situations.The present paper proposes a genetic algorithm to handle multiple conflicting objectives in test generation for SPLs. Experiments conducted on FMs of different sizes demonstrate the effectiveness, feasibility and practicality of the introduced approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {62–71},
numpages = {10},
keywords = {test generation, software product lines, multi-objective optimization, genetic algorithms, feature models},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1007/s42979-021-00541-8,
author = {Saber, Takfarinas and Brevet, David and Botterweck, Goetz and Ventresque, Anthony},
title = {Reparation in Evolutionary Algorithms for Multi-objective Feature Selection in Large Software Product Lines},
year = {2021},
issue_date = {May 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {3},
url = {https://doi.org/10.1007/s42979-021-00541-8},
doi = {10.1007/s42979-021-00541-8},
abstract = {Software Product Lines Engineering is the area of software engineering that aims to systematise the modelling, creation and improvement of groups of interconnected software systems by formally expressing possible alternative products in the form of Feature Models. Deriving a software product/system from a feature model is called Feature Configuration. Engineers select the subset of features (software components) from a feature model that suits their needs, while respecting the underlying relationships/constraints of the system–which is challenging on its own. Since there exist several (and often antagonistic) perspectives on which the quality of software could be assessed, the problem is even more challenging as it becomes a multi-objective optimisation problem. Current multi-objective feature selection in software product line approaches (e.g., SATIBEA) combine the scalability of a genetic algorithm (IBEA) with a solution reparation approach based on a SAT solver or one of its derivatives. In this paper, we propose MILPIBEA, a novel hybrid algorithm which combines IBEA with the accuracy of a mixed-integer linear programming (MILP) reparation. We show that the MILP reparation modifies fewer features from the original infeasible solutions than the SAT reparation and in a shorter time. We also demonstrate that MILPIBEA outperforms SATIBEA on average on various multi-objective performance metrics, especially on the largest feature models. The other major challenge in software engineering in general and in software product lines, in particular, is evolution. While the change in software components is common in the software engineering industry, the particular case of multi-objective optimisation of evolving software product lines is not well-tackled yet. We show that MILPIBEA is not only able to better take advantage of the evolution than SATIBEA, but it is also the one that continues to improve the quality of the solutions when SATIBEA stagnates. Overall, IBEA performs better when combined with MILP instead of SAT reparation when optimising the multi-objective feature selection in large and evolving software product lines.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {14},
keywords = {Mixed-integer linear programming, Reparation, Evolutionary algorithm, Multi-objective optimisation, Feature selection, Software product line}
}

@inproceedings{10.5555/776816.776947,
author = {Knauber, Peter and Bosch, Jan},
title = {ICSE workshop on: Software Variability Management},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {During recent years, the amount of variability that has to be supported by a software artifact is growing considerably and its management is developing as a main challenge during development, usage, and evolution of software artifacts. Successful management of variability in software artifacts leads to better customizable software products that are in turn likely to result in higher market success.The aim of this workshop is to study software variability management both from a 'problems' and from a 'solutions' perspective by bringing together people from industrial practice and from applied research in academia to present and discuss their respective experience.Issues to be addressed include, but are not limited to, technological, process, and organizational aspects as well as notation, assessment, design, and evolution aspects.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {779–780},
numpages = {2},
keywords = {variability management, software variability, software customization, software configuration, software adaptation},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/2019136.2019150,
author = {Serajzadeh, Hadi and Shams, Fereidoon},
title = {The application of swarm intelligence in service-oriented product lines},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019150},
doi = {10.1145/2019136.2019150},
abstract = {Changing markets and environments has made the ability to rapidly adapt to these changes a necessity in software systems. However the costs of changing and adapting systems to new requirements still remains an unsolved issue. In this context service-oriented software product lines were introduced with the aim to combine the reusability of software product line with the flexibility of service-oriented architecture. Although this approach helps build flexible software systems with high levels of reuse, certain issues are raised. The main issue is the complexity that a service-oriented product line will face. Developing systems from internal and external assets, taking into consideration the variety and number of these assets, can cause problems in deciding which asset is best suited for the system. To help solve these issues we propose the use of approaches based on artificial intelligence. In this paper we show how swarm intelligence can be used in service-oriented product lines to reduce complexity and find optimal solutions for the development of software systems. We also present an example of the application of swarm intelligence in finding the optimal product for a service-oriented product line.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {12},
numpages = {7},
keywords = {swarm intelligence, service-oriented product line, optimization},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/2491627.2491651,
author = {Nakagawa, Elisa Yumi and Becker, Martin and Maldonado, Jos\'{e} Carlos},
title = {Towards a process to design product line architectures based on reference architectures},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491651},
doi = {10.1145/2491627.2491651},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. From another perspective, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the research area of Reference Architecture. In spite of the positive impact of this type of architecture on reuse and productivity, the use of the knowledge contained in existing reference architectures in order to develop SPL has not been widely explored yet. The main contribution of this paper is to present a process, named ProSA-RA2PLA, that systematizes the use of reference architectures for building product line architectures. To illustrate the application of this process, we have built a product line architecture for an SPL of software testing tools using a reference architecture of that domain. Based on initial results, we have observed that benefits can be achieved, mainly regarding improvement in reuse and productivity to develop SPL.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {157–161},
numpages = {5},
keywords = {reference architecture, knowledge sharing},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.1145/2791060.2791096,
author = {F\'{e}derle, \'{E}dipo Luis and do Nascimento Ferreira, Thiago and Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {OPLA-tool: a support tool for search-based product line architecture design},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791096},
doi = {10.1145/2791060.2791096},
abstract = {The Product Line Architecture (PLA) design is a complex task, influenced by many factors such as feature modularization and PLA extensibility, which are usually evaluated according to different metrics. Hence, the PLA design is an optimization problem and problems like that have been successfully solved in the Search-Based Software Engineering (SBSE) area, by using metaheuristics such as Genetic Algorithm. Considering this fact, this paper introduces a tool named OPLA-Tool, conceived to provide computer support to a search-based approach for PLA design. OPLA-Tool implements all the steps necessary to use multi-objective optimization algorithms, including PLA transformations and visualization through a graphical interface. OPLA-Tool receives as input a PLA at the class diagram level, and produces a set of good alternative diagrams in terms of cohesion, feature modularization and reduction of crosscutting concerns.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {370–373},
numpages = {4},
keywords = {search-based software engineering, product line architecture design, multi-objective evolutionary algorithms},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/2791060.2791082,
author = {Hotz, Lothar and Wang, Yibo and Riebisch, Matthias and G\"{o}tz, Olaf and Lackhove, Josef},
title = {Evaluation across multiple views for variable automation systems},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791082},
doi = {10.1145/2791060.2791082},
abstract = {Automation systems in industry are often software-intensive systems consisting of software and hardware components. During their development several engineers of different disciplines are involved, such as mechanical, electrical and software engineering. Each engineer focuses on specific system aspects to be developed. To enable an efficient development, product lines especially with feature models for variability modeling are promising technologies. In order to reduce the complexity of both feature models and development process, views on feature models can be applied. The use of views for filtering purposes constitutes an established method. However, views also enable further options missing in current approaches, such as evaluations regarding requirements, including non-functional ones. This paper presents an approach for evaluation across multiple views to enable collaborative development for developers who focus on different system aspects. We validate our approach by applying it in an industrial project for the planning of flying saws.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {311–315},
numpages = {5},
keywords = {product lines, multi-criteria evaluation, feature model, consistency check, configuration, automation systems},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@article{10.1007/s00766-014-0203-1,
author = {D\'{\i}az, Jessica and P\'{e}rez, Jennifer and Garbajosa, Juan},
title = {A model for tracing variability from features to product-line architectures: a case study in smart grids},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0203-1},
doi = {10.1007/s00766-014-0203-1},
abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
journal = {Requir. Eng.},
month = sep,
pages = {323–343},
numpages = {21},
keywords = {Variability, Traceability modeling, Software product line engineering, Product-line architecture}
}

@inproceedings{10.1145/2362536.2362573,
author = {Bartholdt, J\"{o}rg and Becker, Detlef},
title = {Scope extension of an existing product line},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362573},
doi = {10.1145/2362536.2362573},
abstract = {At the beginning, creating a product line needs a well defined and narrow scope to meet short time to market demands. When established, there is a tendency to broaden the scope and to cover more domains and products.We have undergone a scope extension of our medical diagnostic platform that was implemented while the platform and (existing) products were evolving. In this paper, we list best practices for the migration process and how to come to a sustainable solution without cannibalizing the existing platform and products.In particular, we describe our way of identification beneficial sub-domains using C/V analysis and give an example scenario with alignments in order to increase commonality. We explain the maturity considerations for deciding on reuse of existing implementations and a carve-out strategy to split existing assets into common modules and product-line specific extensions. Furthermore, we describe our best practices for making the scope extension sustainable in a long term, using various types of governance means. We briefly complement these experiences with further insights gained during execution of this endeavor.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {275–282},
numpages = {8},
keywords = {scope extension, hierarchical product-line, governance, C/V analysis},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/2648511.2648525,
author = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
title = {Preference-based feature model configuration with multiple stakeholders},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648525},
doi = {10.1145/2648511.2648525},
abstract = {Feature model configuration is known to be a hard, error-prone and time-consuming activity. This activity gets even more complicated when it involves multiple stakeholders in the configuration process. Research work has proposed approaches to aid multi-stakeholder feature model configuration, but they rely on systematic processes that constraint decisions of some of the stakeholders. In this paper, we propose a novel approach to improve the multi-stakeholder configuration process, considering stakeholders' preferences expressed through both hard and soft constraints. Based on such preferences, we recommend different product configurations using different strategies from the social choice theory. We conducted an empirical study to evaluate the effectiveness of our strategies with respect to individual stakeholder satisfaction and fairness among all stakeholders. Results indicate that particular strategies perform best with respect to these aspects.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {132–141},
numpages = {10},
keywords = {social choice, preferences, feature model configuration},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1002/smr.1568,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire and Azanza, Maider},
title = {Embedded software product lines: domain and application engineering model‐based analysis processes},
year = {2014},
issue_date = {April 2014},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {26},
number = {4},
issn = {2047-7473},
url = {https://doi.org/10.1002/smr.1568},
doi = {10.1002/smr.1568},
abstract = {Nowadays, embedded systems are gaining importance. At the same time, the development of their software is increasing its complexity, having to deal with quality, cost, and time‐to‐market issues among others. With stringent quality requirements such as performance, early verification and validation become critical in these systems. In this regard, advanced development paradigms such as model‐driven engineering and software product line engineering bring considerable benefits to the development and validation of embedded system software. However, these benefits come at the cost of increasing process complexity. This work presents a process based on UML and MARTE for the analysis of embedded model‐driven product lines. It specifies the tasks, the involved roles, and the workproducts that form the process and how it is integrated in the more general development process. Existing tools that support the tasks to be performed in the process are also described. A classification of such tools and a study of traceability among them are provided, allowing engineering teams to choose the most adequate chain of tools to support the process. Copyright © 2012 John Wiley &amp; Sons, Ltd.Embedded systems are becoming ubiquitous, and software running on them is fundamental for them to function. At the same time, the development of their software is increasing its complexity, dealing with cost, time to market, and quality, among others. With stringent quality requirements such as performance, early verification and validation of their software is essential for assuring software quality. In this setting, this work presents a process that supports model‐based analysis of an embedded software product line to assure temporal requirements.  


image
image},
journal = {J. Softw. Evol. Process},
month = apr,
pages = {419–433},
numpages = {14},
keywords = {performance, quality attributes, model‐driven development, model‐based analysis process, software product line}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jan,
articleno = {2},
numpages = {46},
keywords = {test selection, test prioritisation, multi-objective optimisation, Software product line}
}

@inproceedings{10.1145/2362536.2362556,
author = {Nunes, Camila and Garcia, Alessandro and Lucena, Carlos and Lee, Jaejoon},
title = {History-sensitive heuristics for recovery of features in code of evolving program families},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362556},
doi = {10.1145/2362536.2362556},
abstract = {A program family might degenerate due to unplanned changes in its implementation, thus hindering the maintenance of family members. This degeneration is often induced by feature code that is changed individually in each member without considering other family members. Hence, as a program family evolves over time, it might no longer be possible to distinguish between common and variable features. One of the imminent activities to address this problem is the history-sensitive recovery of program family's features in the code. This recovery process encompasses the analysis of the evolution history of each family member in order to classify the implementation elements according to their variability nature. In this context, this paper proposes history-sensitive heuristics for the recovery of features in code of degenerate program families. Once the analysis of the family history is carried out, the feature elements are structured as Java project packages; they are intended to separate those elements in terms of their variability degree. The proposed heuristics are supported by a prototype tool called RecFeat. We evaluated the accuracy of the heuristics in the context of 33 versions of 2 industry program families. They presented encouraging results regarding recall measures that ranged from 85% to 100%; whereas the precision measures ranged from 71% to 99%.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {136–145},
numpages = {10},
keywords = {software evolution, program families, heuristics, feature recovery},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3377930.3390215,
author = {Silva, Diego Fernandes da and Okada, Luiz Fernando and Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Enhancing search-based product line design with crossover operators},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390215},
doi = {10.1145/3377930.3390215},
abstract = {The Product Line Architecture (PLA) is one of the most important artifacts of a Software Product Line. PLA designing has been formulated as a multi-objective optimization problem and successfully solved by a state-of-the-art search-based approach. However, the majority of empirical studies optimize PLA designs without applying one of the fundamental genetic operators: the crossover. An operator for PLA design, named Feature-driven Crossover, was proposed in a previous study. In spite of the promising results, this operator occasionally generated incomplete solutions. To overcome these limitations, this paper aims to enhance the search-based PLA design optimization by improving the Feature-driven Crossover and introducing a novel crossover operator specific for PLA design. The proposed operators were evaluated in two well-studied PLA designs, using three experimental configurations of NSGA-II in comparison with a baseline that uses only mutation operators. Empirical results show the usefulness and efficiency of the presented operators on reaching consistent solutions. We also observed that the two operators complement each other, leading to PLA design solutions with better feature modularization than the baseline experiment.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1250–1258},
numpages = {9},
keywords = {software product line, software architecture, recombination operators, multi-objective evolutionary algorithm},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2019136.2019178,
author = {Brataas, Gunnar and Jiang, Shanshan and Reichle, Roland and Geihs, Kurt},
title = {Performance property prediction supporting variability for adaptive mobile systems},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019178},
doi = {10.1145/2019136.2019178},
abstract = {A performance property prediction (PPP) method for component-based self-adaptive applications is presented. Such performance properties are required by an adaptation middleware for reasoning about adaptation activities. Our PPP method is based on the Structure and Performance (SP) framework, a conceptually simple, yet powerful performance modelling framework based on matrices. The main contribution of this paper are the integration of SP-based PPP into a comprehensive model- and variability-based adaptation framework for context-aware mobile applications. A meta model for the SP method is described. The framework is demonstrated using a practical example.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {37},
numpages = {8},
keywords = {mobile systems, autonomic computing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1007/978-3-030-64694-3_17,
author = {Benmerzoug, Amine and Yessad, Lamia and Ziadi, Tewfik},
title = {Analyzing the Impact of Refactoring Variants on Feature Location},
year = {2020},
isbn = {978-3-030-64693-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-64694-3_17},
doi = {10.1007/978-3-030-64694-3_17},
abstract = {Due to the increasing importance of feature location process, several studies evaluate the performance of different techniques based on IR strategies and a set of software variants as input artifacts. The proposed techniques attempt to improve the results obtained but it is often a difficult task. None of the existing feature location techniques considers the changing nature of the input artifacts, which may undergo series of refactoring changes. In this paper, we investigate the impact of refactoring variants on the feature location techniques. We first evaluate the performance of two techniques through the ArgoUML SPL benchmark when the variants are refactored. We then discuss the degraded results and the possibility of restoring them. Finally, we outline a process of variant alignment that aims to preserve the performance of the feature location.},
booktitle = {Reuse in Emerging Software Engineering Practices: 19th International Conference on Software and Systems Reuse, ICSR 2020, Hammamet, Tunisia, December 2–4, 2020, Proceedings},
pages = {279–291},
numpages = {13},
keywords = {Refactoring, Feature location, Software Product Line},
location = {Hammamet, Tunisia}
}

@inproceedings{10.1145/2362536.2362568,
author = {Hofman, Peter and Stenzel, Tobias and Pohley, Thomas and Kircher, Michael and Bermann, Andreas},
title = {Domain specific feature modeling for software product lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362568},
doi = {10.1145/2362536.2362568},
abstract = {This paper summarizes our experience with introducing feature modeling into a product line for imaging and therapy systems in the Siemens Healthcare Sector. Determining and negotiating the scope in a product line that spans several business units with their own economic goals is challenging. Feature modeling offers a good way to do variability/commonality analysis for complex product lines. A precondition for feature modeling is the identification of all features supporting the product line. To identify these features, we developed a method for systematically deriving a feature model top down based on domain know-how. We call this method domain specific feature modeling. As the primary artifact to describe the problem space, a domain specific feature model additionally improves the requirement understanding for all stakeholders by considerably improving the scoping, traceability, testing, efficiency and transparency of planning activities and making the development efforts easier to estimate. In this paper, we share our experience with domain specific feature modeling in a large platform project and describe the lessons learned. We describe our general approach that can also be used for other domains.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {229–238},
numpages = {10},
keywords = {variability analysis, product line, feature modeling, feature dependency diagram, domain specific feature model, commonality analysis, agile, Scrum},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.1145/3350768.3351299,
author = {de Oliveira, Davi Cedraz S. and Bezerra, Carla I. M.},
title = {Development of the Maintainability Index for SPLs Feature Models Using Fuzzy Logic},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351299},
doi = {10.1145/3350768.3351299},
abstract = {The variability of the common features in an Software Product Line (SPL) can be managed by an feature model, an artifact that consist of a tree-shaped diagram, that describe the features identified in the products and the possible relationships between them. Guarantee the quality of the feature model may be essential to ensure that errors do not propagate across all products. The process of evaluating the quality of a product or artifact can be done using measures, which may reflect the characteristics, sub-characteristics or attributes of quality. However, the isolated values of each measure do not allow access to a whole quality of the feature model, since most of the measures cover several specific aspects that are not correlated. In this context, this paper proposes the aggregation of measures in order to evaluate the maintainability of the feature model in SPL. We aim to investigate how to aggregate these measures and access the respective sub-characteristics by means of a single aggregate value that has the same available information as a set of measures. For this, we have used the theory of Fuzzy Logic as a technique for aggregation of these measures. The new aggregate measure represents the maintainability index of a feature models (MIFM) was obtained. Moreover, to evaluate the MIFM, we applied it to a set of models. It was verified that the aggregate measure obtained allows to measure if a feature models has a high or low maintainability index, supporting the domain engineer in the evaluation of the maintenance of the feature model in a faster and more precise way.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {357–366},
numpages = {10},
keywords = {Software Product Line, Quality Evaluation, Measures, Fuzzy Logic, Feature Models},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1007/s10664-014-9359-z,
author = {Myll\"{a}rniemi, Varvana and Savolainen, Juha and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Performance variability in software product lines: proposing theories from a case study},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9359-z},
doi = {10.1007/s10664-014-9359-z},
abstract = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1623–1669},
numpages = {47},
keywords = {Variability, Software product line, Software architecture, Case study}
}

@article{10.1007/s00766-014-0201-3,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasingham and Gopalasundaram, Umatharan},
title = {On building architecture-centric product line architecture},
year = {2015},
issue_date = {September 2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0201-3},
doi = {10.1007/s00766-014-0201-3},
abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives.},
journal = {Requir. Eng.},
month = sep,
pages = {301–321},
numpages = {21},
keywords = {Software product line, Software performance, Requirements validation, Patterns, Architecture evaluation, Architectural prototyping}
}

@inproceedings{10.1145/3129790.3129818,
author = {Munoz, Daniel-Jesus and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Green software development and research with the HADAS toolkit},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129818},
doi = {10.1145/3129790.3129818},
abstract = {Energy is a critical resource, and designing a sustainable software architecture is a non-trivial task. Developers require energy metrics that support sustainable software architectures reflecting quality attributes such as security, reliability, performance, etc., identifying what are the concerns that impact more in the energy consumption. A variability model of different designs and implementations of an energy model should exist for this task, as well as a service that stores and compares the experimentation results of energy and time consumption of each concern, finding out what is the most eco-efficient solution. The experimental measurements are performed by energy experts and researchers that share the energy model and metrics in a collaborative repository. HADAS confronts these tasks modelling and reasoning with the variability of energy consuming concerns for different energy contexts, connecting HADAS variability model with its energy efficiency collaborative repository, establishing a Software Product Line (SPL) service. Our main goal is to help developers to perform sustainability analyses finding out the eco-friendliest architecture configurations. A HADAS toolkit prototype is implemented based on a Clafer model and Choco solver, and it has been tested with several case studies.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {205–211},
numpages = {7},
keywords = {variability, software product line, repository, optimisation, metrics, energy efficiency, clafer, CVL},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

@inproceedings{10.1007/978-3-642-25535-9_29,
author = {Mohabbati, Bardia and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek and Asadi, Mohsen and Bagheri, Ebrahim and Bo\v{s}kovi\'{c}, Marko},
title = {A quality aggregation model for service-oriented software product lines based on variability and composition patterns},
year = {2011},
isbn = {9783642255342},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-25535-9_29},
doi = {10.1007/978-3-642-25535-9_29},
abstract = {Quality evaluation is a challenging task in monolithic software systems. It is even more complex when it comes to Service-Oriented Software Product Lines (SOSPL), as it needs to analyze the attributes of a family of SOA systems. In SOSPL, variability can be planned and managed at the architectural level to develop a software product with the same set of functionalities but different degrees of non-functional quality attribute satisfaction. Therefore, architectural quality evaluation becomes crucial due to the fact that it allows for the examination of whether or not the final product satisfies and guarantees all the ranges of quality requirements within the envisioned scope. This paper addresses the open research problem of aggregating QoS attribute ranges with respect to architectural variability. Previous solutions for quality aggregation do not consider architectural variability for composite services. Our approach introduces variability patterns that can possibly occur at the architectural level of an SOSPL. We propose an aggregation model for QoS computation which takes both variability and composition patterns into account.},
booktitle = {Proceedings of the 9th International Conference on Service-Oriented Computing},
pages = {436–451},
numpages = {16},
keywords = {variability management, software product line (SPL), service-oriented architecture (SOA), service variability, process family, non-functional properties, feature modeling, QoS aggregation},
location = {Paphos, Cyprus},
series = {ICSOC'11}
}

@article{10.1007/s11219-017-9400-8,
author = {Alf\'{e}rez, Mauricio and Acher, Mathieu and Galindo, Jos\'{e} A. and Baudry, Benoit and Benavides, David},
title = {Modeling variability in the video domain: language and experience report},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9400-8},
doi = {10.1007/s11219-017-9400-8},
abstract = {In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)--something impossible at the beginning of the project.},
journal = {Software Quality Journal},
month = mar,
pages = {307–347},
numpages = {41},
keywords = {Video testing, Variability modeling, Software product line engineering, Feature modeling, Domain-specific languages, Configuration, Automated reasoning}
}

@inproceedings{10.5220/0004745201110118,
author = {Oliveira Jr., Edson and M. S. Gimenes, Itana},
title = {Empirical Validation of Product-line Architecture Extensibility Metrics},
year = {2014},
isbn = {9789897580284},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0004745201110118},
doi = {10.5220/0004745201110118},
abstract = {The software product line (PL) approach has been applied as a successful software reuse technique for specificdomains. The SPL architecture (PLA) is one of the most important SPL core assets as it is the abstraction ofthe products that can be generated, and it represents similarities and variabilities of a PL. Its quality attributesanalysis and evaluation can serve as a basis for analyzing the managerial and economical values of a PL. Thisanalysis can be quantitatively supported by metrics. Thus, we proposed metrics for the PLA extensibilityquality attribute. This paper is concerned with the empirical validation of such metrics. As a result of the experimentalwork we can provide evidence that the proposed metrics serve as relevant indicators of extensibilityof PLA by presenting a correlation analysis.},
booktitle = {Proceedings of the 16th International Conference on Enterprise Information Systems - Volume 2},
pages = {111–118},
numpages = {8},
keywords = {Variability Management., Software Product Line Architecture, Metrics, Extensibility, Empirical Validation},
location = {Lisbon, Portugal},
series = {ICEIS 2014}
}

@inproceedings{10.5555/1753235.1753249,
author = {Montagud, Sonia and Abrah\~{a}o, Silvia},
title = {Gathering current knowledge about quality evaluation in software product lines},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Recently, a number of methods and techniques for assessing the quality of software product lines have been proposed. However, to the best of our knowledge, there is no study which summarizes all the existing evidence about them. This paper presents a systematic review that investigates what methods and techniques have been employed (in the last 10 years) to evaluate the quality of software product lines and how they were employed. A total of 39 research papers have been reviewed from an initial set of 1388 papers. The results show that 25% of the papers reported evaluations at the Design phase of the Domain Engineering phase. The most widely used mechanism for modeling quality attributes was extended feature models and the most evaluated artifact was the base architecture. In addition, the results of the review have identified several research gaps. Specifically, 77% of the papers employed case studies as a "proof of concept" whereas 23% of the papers did not perform any type of validation. Our results are particularly relevant in positioning new research activities and in the selection of quality evaluation methods or techniques that best fit a given purpose.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {91–100},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Utility Elicitation, Software Product Line, Economic Value, Decision Process, Configuration Process}
}

@article{10.1016/j.jss.2018.05.069,
author = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Self-adaptation of service compositions through product line reconfiguration},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.069},
doi = {10.1016/j.jss.2018.05.069},
journal = {J. Syst. Softw.},
month = oct,
pages = {84–105},
numpages = {22},
keywords = {Self adaptation, Software product lines, Feature model, Service composition}
}

@article{10.1145/3088440,
author = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
title = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
url = {https://doi.org/10.1145/3088440},
doi = {10.1145/3088440},
abstract = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community.},
journal = {ACM Trans. Comput. Educ.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {variability modeling, software product line teaching, software engineering teaching, Software product lines}
}

@inproceedings{10.1145/1987875.1987886,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {Model based analysis process for embedded software product lines},
year = {2011},
isbn = {9781450307307},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1987875.1987886},
doi = {10.1145/1987875.1987886},
abstract = {Nowadays, embedded system development is increasing its complexity dealing with quality, cost and time-to-market among others. Quality attributes are an important issue to consider in embedded software development where time issues may be critical. Development paradigms such as Model Driven Development and Software Product Lines can be an adequate alternative to traditional software development and validation methods due to the characteristics of embedded systems. But for a proper validation and verification based on MARTE model analysis, all variability issues and critical quality attributes that take part in analysis must be properly modelled and managed. Therefore, a model analysis process for Model Driven Embedded Software Product Lines has been defined as some process lacks have been found.},
booktitle = {Proceedings of the 2011 International Conference on Software and Systems Process},
pages = {53–62},
numpages = {10},
keywords = {software product line, schedulability, quality attributes, performance, model driven development, model based analysis process},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSSP '11}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Models, Classification, Framework, Software product lines, Feature model}
}

@inproceedings{10.5555/1753235.1753263,
author = {Than Tun, Thein and Boucher, Quentin and Classen, Andreas and Hubaux, Arnaud and Heymans, Patrick},
title = {Relating requirements and feature configurations: a systematic approach},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {A feature model captures various possible configurations of products within a product family. When configuring a product, several features are selected and composed. Selecting features at the program level has a general limitation of not being able to relate the resulting configuration to its requirements. As a result, it is difficult to decide whether a given configuration of features is optimal. An optimal configuration satisfies all stakeholder requirements and quantitative constraints, while ensuring that there is no extraneous feature in it. In relating requirements and feature configurations, we use the description of the problem world context in which the software is designed to operate as the intermediate description between them. The advantage of our approach is that feature selection can be done at the requirements level, and an optimal program level configuration can be generated from the requirements selected. Our approach is illustrated with a real-life problem of configuring a satellite communication software. The use of an existing tool to support our approach is also discussed.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {201–210},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1145/2737182.2737183,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Representing and Configuring Security Variability in Software Product Lines},
year = {2015},
isbn = {9781450334709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737182.2737183},
doi = {10.1145/2737182.2737183},
abstract = {In a software product line, security may need to be varied. Consequently, security variability must be managed both from the customer and product line architecture point of view. We utilize design science to build an artifact and a generalized design theory for representing and configuring security and functional variability from the requirements to the architecture in a configurable software product line. An open source web shop product line, Magento, is used as a case example to instantiate and evaluate the contribution. The results indicate that security variability can be represented and distinguished as countermeasures; and that a configurator tool is able to find consistent products as stable models of answer set programs.},
booktitle = {Proceedings of the 11th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {1–10},
numpages = {10},
keywords = {variability, software product line, software architecture, security},
location = {Montr\'{e}al, QC, Canada},
series = {QoSA '15}
}

@inproceedings{10.5555/1753235.1753270,
author = {Slegers, Walter J.},
title = {Building automotive product lines around managed interfaces},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {TomTom is extending its current business of portable navigation devices into the embedded automotive navigation domain. Portable navigation devices have a high pace of innovation and moderate diversity. Automotive devices traditionally have a slower pace of innovation and high diversity. Their integration in the vehicle needs to comply with formal and intrusive automotive requirements.How can both worlds be combined, offering an increased innovation and reduced lead time in the automotive domain? We introduced an architectural decoupling with explicit management of interfaces to support a product line approach with systematic reuse across business units enabling an increase of diversity in these different market segments.This paper describes business, architecture, organization, and process aspects of this approach with special attention to the architecture and the management of interfaces.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {257–264},
numpages = {8},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@article{10.1016/j.jss.2008.07.046,
author = {Eriksson, Magnus and B\"{o}rstler, J\"{u}rgen and Borg, Kjell},
title = {Managing requirements specifications for product lines - An approach and industry case study},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.07.046},
doi = {10.1016/j.jss.2008.07.046},
abstract = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context.},
journal = {J. Syst. Softw.},
month = mar,
pages = {435–447},
numpages = {13},
keywords = {Variability management, Software product line, Natural-language requirements specification, Feature model}
}

@article{10.1016/j.cl.2016.09.004,
author = {M\'{e}ndez-Acu\~{n}a, David and Galindo, Jos\'{e} A. and Degueule, Thomas and Combemale, Beno\^{\i}t and Baudry, Beno\^{\i}t},
title = {Leveraging Software Product Lines Engineering in the development of external DSLs},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.09.004},
doi = {10.1016/j.cl.2016.09.004},
abstract = {The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns - such as state machines or petri nets - used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. HighlightsSurvey on the applicability of software product lines in the construction of DSLs.General life-cycle for language product lines.Mapping current approaches on language product lines and technological spaces.Research map in language product lines engineering.},
journal = {Comput. Lang. Syst. Struct.},
month = nov,
pages = {206–235},
numpages = {30},
keywords = {Variability management, Software language engineering, Software Product Lines Engineering, Domain-specific languages}
}

@article{10.1145/2853073.2853095,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {Variability for Qualities in Software Architecture},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2853073.2853095},
doi = {10.1145/2853073.2853095},
abstract = {Variability is a key factor of most systems. While there are many works covering variability in functionality, there is a research gap regarding variability in software qualities. There is an obvious imbalance between the importance of variability in the context of quality attributes, and the intensity of research in this area. To improve this situation, the First International Workshop on VAri- ability for QUalIties in SofTware Architecture (VAQUITA) was held jointly with ECSA 2015 in Cavtat/Dubrovnik, Croatia as a one-day workshop. The goal of VAQUITA was to investigate and stimulate the discourse about the matter of variability, qualities, and software architectures. The workshop featured three research paper presentations, one keynote talk, and two working group discussions. In this workshop report, we summarize the keynote talk and the presented papers. Additionally, we present the results of the working group discussions},
journal = {SIGSOFT Softw. Eng. Notes},
month = feb,
pages = {32–35},
numpages = {4},
keywords = {variability, quality attributes, Software architecture}
}

@inproceedings{10.5220/0006791403830391,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Handling Tenant-Specific Non-Functional Requirements through a Generic SLA},
year = {2018},
isbn = {9789897583001},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006791403830391},
doi = {10.5220/0006791403830391},
abstract = {In a multi-tenant architecture of a Software as a Service (SaaS) application, one single instance is shared amongdifferent tenants. However, this architectural style supports only the commonalities among tenants and doesnot cope with the variations and the specific context of each tenant. These variations concern either functionalor non-functional properties. In this paper, we deal with non-functional variability in SaaS services in orderto support the different quality levels that a service may have. For that purpose, we propose an approachthat considers Service Level Agreements (SLAs) as Families in terms of Software Product Line Engineering.We define two metamodels: NFVariability metamodel and VariableSLA metamodel. The first one modelsand captures variability in quality attributes of services. The second one models a dynamic and variableSLA. Model-to-model transformations are performed to transform Feature Model (NFVariability metamodelinstance) to Generic SLA (VariableSLA instance) in order to dynamically deal with the tenant-specific nonfunctionalrequirements.},
booktitle = {Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {383–391},
numpages = {9},
keywords = {SaaS, SPLE, SLA., QoS Characteristics, Non-Functional Variability, MDE},
location = {Funchal, Madeira, Portugal},
series = {ENASE 2018}
}

@inproceedings{10.1145/2647908.2655981,
author = {Acher, Mathieu and Alf\'{e}rez, Mauricio and Galindo, Jos\'{e} A. and Romenteau, Pierre and Baudry, Benoit},
title = {ViViD: a variability-based tool for synthesizing video sequences},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655981},
doi = {10.1145/2647908.2655981},
abstract = {We present ViViD, a variability-based tool to synthesize variants of video sequences. ViViD is developed and used in the context of an industrial project involving consumers and providers of video processing algorithms. The goal is to synthesize synthetic video variants with a wide range of characteristics to then test the algorithms. We describe the key components of ViViD (1) a variability language and an environment to model what can vary within a video sequence; (2) a reasoning back-end to generate relevant testing configurations; (3) a video synthesizer in charge of producing variants of video sequences corresponding to configurations. We show how ViViD can synthesize realistic videos with different characteristics such as luminances, vehicles and persons that cover a diversity of testing scenarios.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {143–147},
numpages = {5},
keywords = {video generation, variability modeling, prioritization, multimedia, combinatorial interaction testing, T-wise},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/1621607.1621633,
author = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
title = {Mapping problem-space to solution-space features: a feature interaction approach},
year = {2009},
isbn = {9781605584942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1621607.1621633},
doi = {10.1145/1621607.1621633},
abstract = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation.},
booktitle = {Proceedings of the Eighth International Conference on Generative Programming and Component Engineering},
pages = {167–176},
numpages = {10},
keywords = {software product line engineering, problem-solution feature interactions, distributed runtime adaptation, default logic, configuration knowledge, DLV},
location = {Denver, Colorado, USA},
series = {GPCE '09}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing selection of competing features via feedback-directed evolutionary algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = {Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds.},
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {evolutionary algorithms, Software product line, SAT solvers},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.5220/0004973404600465,
author = {Florencio da Silva, Jackson Raniel and Soares de Melo Filho, Aloisio and Cardoso Garcia, Vinicius},
title = {Toward a QoS Based Run-time Reconfiguration in Service-oriented Dynamic Software Product Lines},
year = {2014},
isbn = {9789897580284},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0004973404600465},
doi = {10.5220/0004973404600465},
abstract = {Ford invented the product line that makes possible to mass produce by reducing the delivery time and production costs. Regarding the software industry, this, roughly presents both a manufacturing and mass production that generates products that are denoted as individual software and standard software (Pohl et al., 2005): a clear influence of Fordism in the development paradigm of Software Product Lines (SPL). However, this development paradigm was not designed to support user requirements changes at run-time. Faced with this problem, the academy has developed and proposed the Dynamic Software Product Lines (DSPL) (Hallsteinsen et al., 2008) paradigm. Considering this scenario, we objective contribute to DSPL field presenting a new way of thinking which DSPL features should be connected at run-time to a product based on an analysis of quality attributes in service levels specified by the user. In order to validate the proposed approach we tested it on a context-aware DSPL. At the end of the exploratory validation we can observe the effectiveness of the proposed approach in the DSPL which it was applied. However, it is necessary to perform another studies in order to achieve statistical evidences of this effectiveness.},
booktitle = {Proceedings of the 16th International Conference on Enterprise Information Systems - Volume 2},
pages = {460–465},
numpages = {6},
keywords = {Software Product Line, SPL, SOA, Dynamic Software Product Line., DSPL},
location = {Lisbon, Portugal},
series = {ICEIS 2014}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Systematic literature review, Product configuration, Extended product line}
}

@inproceedings{10.1109/SPLC.2008.37,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Variability Driven Quality Evaluation in Software Product Lines},
year = {2008},
isbn = {9780769533032},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SPLC.2008.37},
doi = {10.1109/SPLC.2008.37},
abstract = {Variability is a key aspect in software product lines. Functional variability has been largely studied as a way to obtain all the desired products for a line. Quality variability, less understood and more complex, has not received so much attention by researchers. However, different members of the line may require different levels of a quality attribute. The design phase is a good point to assure that quality attributes requirements are met within the product line so this means paying attention to software architecture evaluation during Domain Engineering. The quality evaluation in software product lines is much more complicated than in single-systems as products can require different quality levels and the product line can have variability on design that in turn affects quality. The evaluation of all the products of a line is very expensive. Thus, ways of reducing the evaluation efforts are necessary. Herein is presented a method for facilitating cost-effective quality evaluation of a product line taking into consideration variability on quality attributes.},
booktitle = {Proceedings of the 2008 12th International Software Product Line Conference},
pages = {243–252},
numpages = {10},
keywords = {variability, evaluation, Quality attributes},
series = {SPLC '08}
}

@inproceedings{10.1145/2361999.2362028,
author = {Abbas, Nadeem and Andersson, Jesper and Weyns, Danny},
title = {Modeling variability in product lines using domain quality attribute scenarios},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362028},
doi = {10.1145/2361999.2362028},
abstract = {The concept of variability is fundamental in software product lines and a successful implementation of a product line largely depends on how well domain requirements and their variability are specified, managed, and realized. While developing an educational software product line, we identified a lack of support to specify variability in quality concerns. To address this problem we propose an approach to model variability in quality concerns, which is an extension of quality attribute scenarios. In particular, we propose domain quality attribute scenarios, which extend standard quality attribute scenarios with additional information to support specification of variability and deriving product specific scenarios. We demonstrate the approach with scenarios for robustness and upgradability requirements in the educational software product line.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {135–142},
numpages = {8},
keywords = {variability, software product lines, scenarios, quality attributes},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@article{10.4018/jismd.2012100101,
author = {Asadi, Mohsen and Mohabbati, Bardia and Ga\v{s}evic, Dragan and Bagheri, Ebrahim and Hatala, Marek},
title = {Developing Semantically-Enabled Families of Method-Oriented Architectures},
year = {2012},
issue_date = {October 2012},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {4},
issn = {1947-8186},
url = {https://doi.org/10.4018/jismd.2012100101},
doi = {10.4018/jismd.2012100101},
abstract = {Method Engineering ME aims to improve software development methods by creating and proposing adaptation frameworks whereby methods are created to provide suitable matches with the requirements of the organization and address project concerns and fit specific situations. Therefore, methods are defined and modularized into components stored in method repositories. The assembly of appropriate methods depends on the particularities of each project, and rapid method construction is inevitable in the reuse and management of existing methods. The ME discipline aims at providing engineering capability for optimizing, reusing, and ensuring flexibility and adaptability of methods; there are three key research challenges which can be observed in the literature: 1 the lack of standards and tooling support for defining, publishing, discovering, and retrieving methods which are only locally used by their providers without been largely adapted by other organizations; 2 dynamic adaptation and assembly of methods with respect to imposed continuous changes or evolutions of the project lifecycle; and 3 variability management in software methods in order to enable rapid and effective construction, assembly and adaptation of existing methods with respect to particular situations. The authors propose semantically-enabled families of method-oriented architecture by applying service-oriented product line engineering principles and employing Semantic Web technologies.},
journal = {Int. J. Inf. Syst. Model. Des.},
month = oct,
pages = {1–26},
numpages = {26},
keywords = {Software Product Line, Software Development, Semantic Web, Method Oriented Architecture MOA, Method Engineering}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Software product lines, Search-based software engineering, Multi-objective evolutionary algorithms, Feature models, Constraint solving}
}

@article{10.1016/j.jss.2010.01.048,
author = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
title = {A feature-oriented approach for developing reusable product line assets of service-based systems},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.01.048},
doi = {10.1016/j.jss.2010.01.048},
abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1123–1136},
numpages = {14},
keywords = {Software product line engineering, Software architecture styles, Software architecture, Service-based systems, Feature-oriented}
}

@inproceedings{10.1145/2814251.2814263,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Th\"{u}m, Thomas},
title = {Using decision rules for solving conflicts in extended feature models},
year = {2015},
isbn = {9781450336864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814251.2814263},
doi = {10.1145/2814251.2814263},
abstract = {Software Product Line Engineering has introduced feature modeling as a domain analysis technique used to represent the variability of software products and decision-making scenarios. We present a model-based transformation approach to solve conflicts among configurations performed by different stakeholders on feature models. We propose the usage of a domain-specific language named CoCo to specify attributes as non-functional properties of features, and to describe business-related decision rules in terms of costs, time, and human resources. These specifications along with the stakeholders' configurations and the feature model are transformed into a constraint programming problem, on which decision rules are executed to find a non-conflicting set of solution configurations that are aligned to business objectives. We evaluate CoCo's compositionality and model complexity simplification while using a set of motivating decision scenarios.},
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
pages = {149–160},
numpages = {12},
keywords = {model transformation chain, extended feature model, domain-specific language, constraint satisfaction problem, conflicting configurations, Domain engineering},
location = {Pittsburgh, PA, USA},
series = {SLE 2015}
}

@article{10.1016/j.eswa.2015.02.020,
author = {Dermeval, Diego and Ten\'{o}rio, Thyago and Bittencourt, Ig Ibert and Silva, Alan and Isotani, Seiji and Ribeiro, M\'{a}rcio},
title = {Ontology-based feature modeling},
year = {2015},
issue_date = {July 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {11},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.02.020},
doi = {10.1016/j.eswa.2015.02.020},
abstract = {We compare two ontology-based feature modeling styles by conducting an experiment.The results show that ontology factor has statistical significance in all metrics.The results show that the ontology based on instances is more flexible.The results show that the ontology based on instances demands less time to change. A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
journal = {Expert Syst. Appl.},
month = jul,
pages = {4950–4964},
numpages = {15},
keywords = {Software product line, Ontology, Feature modeling, Empirical software engineering}
}

@inproceedings{10.1007/978-3-030-79382-1_24,
author = {Munoz, Daniel-Jesus and Gurov, Dilian and Pinto, Monica and Fuentes, Lidia},
title = {Category Theory Framework for Variability Models with Non-functional Requirements},
year = {2021},
isbn = {978-3-030-79381-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-79382-1_24},
doi = {10.1007/978-3-030-79382-1_24},
abstract = {In Software Product Line (SPL) engineering one uses Variability Models (VMs) as input to automated reasoners to generate optimal products according to certain Quality Attributes (QAs). Variability models, however, and more specifically those including numerical features (i.e., NVMs), do not natively support QAs, and consequently, neither do automated reasoners commonly used for variability resolution. However, those satisfiability and optimisation problems have been covered and refined in other relational models such as databases.Category Theory (CT) is an abstract mathematical theory typically used to capture the common aspects of seemingly dissimilar algebraic structures. We propose a unified relational modelling framework subsuming the structured objects of VMs and QAs and their relationships into algebraic categories. This abstraction allows a combination of automated reasoners over different domains to analyse SPLs. The solutions’ optimisation can now be natively performed by a combination of automated theorem proving, hashing, balanced-trees and chasing algorithms. We validate this approach by means of the edge computing SPL tool HADAS.},
booktitle = {Advanced Information Systems Engineering: 33rd International Conference, CAiSE 2021, Melbourne, VIC, Australia, June 28 – July 2, 2021, Proceedings},
pages = {397–413},
numpages = {17},
keywords = {Category theory, Quality attribute, Non-functional requirement, Feature, Numerical variability model},
location = {Melbourne, VIC, Australia}
}

@article{10.1016/j.infsof.2013.02.007,
author = {Santos Rocha, Roberto dos and Fantinato, Marcelo},
title = {The use of software product lines for business process management},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.02.007},
doi = {10.1016/j.infsof.2013.02.007},
abstract = {ContextBusiness Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT. ObjectivePresenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM. MethodA SLR was conducted with four research questions formulated to evaluate PL approaches for BPM. Results63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches. ConclusionsThe found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1355–1373},
numpages = {19},
keywords = {Software product line, PL, Business process management, BPM}
}

@article{10.1016/j.jss.2019.01.057,
author = {Kr\"{u}ger, Jacob and Mukelabai, Mukelabai and Gu, Wanzi and Shen, Hui and Hebig, Regina and Berger, Thorsten},
title = {Where is my feature and what is it about? A case study on recovering feature facets},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.057},
doi = {10.1016/j.jss.2019.01.057},
journal = {J. Syst. Softw.},
month = jun,
pages = {239–253},
numpages = {15},
keywords = {Software product line, Feature facets, Case study, Bitcoin-wallet, Marlin, Feature location}
}

@article{10.1016/j.infsof.2017.01.012,
author = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, ystein},
title = {Investigating styles in variability modeling},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.01.012},
doi = {10.1016/j.infsof.2017.01.012},
abstract = {ContextA common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. ObjectiveThe goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models hierarchical and constrained where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. MethodWe conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 920 nodes and 819 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. ResultsThe results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. ConclusionsPrior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {81–102},
numpages = {22},
keywords = {Variability modeling, Textual constraints, Product line engineering, Hierarchical modeling, Feature modeling, Empirical research, Comprehensibility, Cognitive aspects}
}

@inproceedings{10.1145/1878450.1878475,
author = {Lobato, Luanna Lopes and O'Leary, P\'{a}draig and de Almeida, Eduardo Santana and de Lemos Meira, S\'{\i}lvio Romero},
title = {The importance of documentation, design and reuse in risk management for SPL},
year = {2010},
isbn = {9781450304030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1878450.1878475},
doi = {10.1145/1878450.1878475},
abstract = {Software Product Lines (SPL) is a methodology focusing on systematic software reuse, multiple benefits have been reported as a result of this type of software development. However, establishing a SPL is not a simple task. It is a challenging activity raising many challenges for engineering and management. This research aims to manage the risks during SPL development to provide traceability among them. For this, it is important that the risks are documented and there is a common design related to them. As solution, we identified the strengths and weakness in SPL development and the importance in designing of communication for risk documentation.},
booktitle = {Proceedings of the 28th ACM International Conference on Design of Communication},
pages = {143–150},
numpages = {8},
keywords = {web products, software product line, documentation},
location = {S\~{a}o Carlos, S\~{a}o Paulo, Brazil},
series = {SIGDOC '10}
}

@inproceedings{10.1145/3180155.3180159,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Schr\"{o}ter, Reimar and Saake, Gunter},
title = {Propagating configuration decisions with modal implication graphs},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180159},
doi = {10.1145/3180155.3180159},
abstract = {Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {898–909},
numpages = {12},
keywords = {configuration, decision propagation, software product line},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3168365.3168375,
author = {Bezerra, Carla I. M. and Andrade, Rossana M. C. and Monteiro, Jos\'{e} M. S. and Cedraz, Davi},
title = {Aggregating Measures using Fuzzy Logic for Evaluating Feature Models},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168375},
doi = {10.1145/3168365.3168375},
abstract = {In the context of Software Product Lines (SPLs), evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the SPL. One way to evaluate a feature model is to use measures. However, measures alone are not enough to characterize the feature model quality, because most of them cover specific aspects, such as the number of features. So, there is a need for methods to aggregate measures at the level of quality sub-characteristic or characteristic. In this paper, we aim to investigate how to aggregate measures that have been proposed to evaluate the quality of feature models in SPL. We have used the fuzzy logic theory in order to aggregate these measures. The new aggregated measures can be applied to evaluate different and complex aspects of a feature model, such as: size, stability, flexibility and dynamicity. Moreover, to evaluate the use of the new aggregate measures, we applied them in different feature models. Our findings suggest that aggregate measures can assist the domain engineer in evaluating the maintainability of feature models.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {35–42},
numpages = {8},
keywords = {Software Product Line, Measures, Fuzzy Logic, Feature Models},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.neucom.2019.06.075,
author = {Xue, Yani and Li, Miqing and Shepperd, Martin and Lauria, Stasha and Liu, Xiaohui},
title = {A novel aggregation-based dominance for Pareto-based evolutionary algorithms to configure software product lines},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {364},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.06.075},
doi = {10.1016/j.neucom.2019.06.075},
journal = {Neurocomput.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {Multi-objective optimization, Evolutionary algorithm, Software product line, Optimal feature selection}
}

@inproceedings{10.5555/2666064.2666077,
author = {Leitner, Andrea and Wei\ss{}, Reinhold and Kreiner, Christian},
title = {Optimizing problem space representations through domain multi-modeling},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This work states that there is a need for an optimized problem space representation for heterogeneous domains. We identify two modeling paradigms widely used in practice: Domain-Specific Modeling (DSM) and Feature-Oriented Domain Modeling (FODM). Each modeling paradigm favors different domain characteristics. Especially the fact that software often is embedded either in a system or in a process and, therefore, is strongly influenced by its environment enforces the demand for a combined representation.We propose a concept for a multi-modeling approach based on existing technology. Multi-modeling means the combination of the two main modeling paradigms to represent a heterogeneous domain. The major benefit of the approach is the reduction of representation complexity by optimizing the representation of single subdomains. This will be shown on one representative case study from the automotive domain. Another advantage is the improved stakeholder communication because of familiar notations. A discussion of limitations shows potential for future work.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {49–52},
numpages = {4},
keywords = {variant-rich component model, software product line engineering, model-based development, domain modeling, binding times, automotive software product line},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/1842752.1842813,
author = {Galv\~{a}o, Ism\^{e}nia and van den Broek, Pim and Ak\c{s}it, Mehmet},
title = {A model for variability design rationale in SPL},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842813},
doi = {10.1145/1842752.1842813},
abstract = {The management of variability in software product lines goes beyond the definition of variations, traceability and configurations. It involves a lot of assumptions about the variability and related models, which are made by the stakeholders all over the product line but almost never handled explicitly. In order to better manage the design with variability, we must consider the rationale behind its specification. In this paper we present a model for the specification of variability design rationale and its application to the modelling of architectural variability in software product lines.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {332–335},
numpages = {4},
keywords = {variability, software product line, software architecture, design rationale},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Framework, Systematic mapping study, Collaborative configuration, Product lines}
}

@article{10.1016/j.jss.2013.10.010,
author = {White, Jules and Galindo, Jos\'{e} A. and Saxena, Tripti and Dougherty, Brian and Benavides, David and Schmidt, Douglas C.},
title = {Evolving feature model configurations in software product lines},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {87},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.10.010},
doi = {10.1016/j.jss.2013.10.010},
abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
journal = {J. Syst. Softw.},
month = jan,
pages = {119–136},
numpages = {18},
keywords = {Software product line, Multi-step configuration, Feature model}
}

@inproceedings{10.1145/2892664.2892700,
author = {Horcas, Jose-Miguel and Pinto, M\'{o}nica and Fuentes, Lidia and Zschaler, Steffen},
title = {Towards contractual interfaces for reusable functional quality attribute operationalisations},
year = {2016},
isbn = {9781450340335},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2892664.2892700},
doi = {10.1145/2892664.2892700},
abstract = {The quality of a software system can be measured by the extent to which it possesses a desired combination of quality attributes (QAs). While some QAs are achieved implicitly through the interaction of various functional components of the system, others (e.g., security) can be encapsulated in dedicated software components. These QAs are known as functional quality attributes (FQAs). As applications may require different FQAs, and each FQA can be composed of many concerns (e.g., access control and authentication), integrating FQAs is very complex and requires dedicated expertise. Software architects are required to manually define FQA components, identify appropriate points in their architecture where to weave them, and verify that the composition of these FQA components with the other components is correct. This is a complex and error prone process. In our previous work we defined reusable FQAs by encapsulating them as aspectual architecture models that can be woven into a base architecture. So far, the joinpoints for weaving had to be identified manually. This made it difficult for software architects to verify that they have woven all the necessary FQAs into all the right places. In this paper, we address this problem by introducing a notion of contract for FQAs so that the correct application of an FQA (or one of its concerns) can be checked or, alternatively, appropriate binding points can be identified and proposed to the software architect automatically.},
booktitle = {Companion Proceedings of the 15th International Conference on Modularity},
pages = {201–205},
numpages = {5},
keywords = {Weaving Patterns, Quality Attributes, Model-Driven Development, Aspect-Orientation},
location = {M\'{a}laga, Spain},
series = {MODULARITY Companion 2016}
}

@article{10.1016/j.infsof.2015.12.004,
author = {Lu, Hong and Yue, Tao and Ali, Shaukat and Zhang, Li},
title = {Model-based incremental conformance checking to enable interactive product configuration},
year = {2016},
issue_date = {April 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {72},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.12.004},
doi = {10.1016/j.infsof.2015.12.004},
abstract = {ContextModel-based product line engineering (PLE) is a paradigm that can enable automated product configuration of large-scale software systems, in which models are used as an abstract specification of commonalities and variabilities of products of a product line. ObjectiveIn the context of PLE, providing immediate feedback on the correctness of a manual configuration step to users has a practical impact on whether a configuration process with tool support can be successfully adopted in practice. MethodIn an existing work, a UML-based variability modeling methodology named as SimPL and an interactive configuration process was proposed. Based on the existing work, we propose an automated, incremental and efficient conformance checking approach to ensure that the manual configuration of a variation point conforms to a set of pre-defined conformance rules specified in the Object Constraint Language (OCL). The proposed approach, named as Zen-CC, has been implemented as an integrated part of our product configuration and derivation tool: Zen-Configurator. ResultsThe performance and scalability of Zen-CC have been evaluated with a real-world case study. Results show that Zen-CC significantly outperformed two baseline engines in terms of performance. Besides, the performance of Zen-CC remains stable during the configuration of all the 10 products of the product line and its efficiency also remains un-impacted even with the growing product complexity, which is not the case for both of the baseline engines. ConclusionThe results suggest that Zen-CC performs practically well and is much more scalable than the two baseline engines and is scalable for configuring products with a larger number of variation points.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {68–89},
numpages = {22},
keywords = {Variation point, Product line engineering, Model based engineering, Interactive product configuration, Incremental conformance checking}
}

@inproceedings{10.1145/1985484.1985493,
author = {Chastek, Gary and Donohoe, Patrick and McGregor, John D.},
title = {Commonality and variability analysis for resource constrained organizations},
year = {2011},
isbn = {9781450305846},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985484.1985493},
doi = {10.1145/1985484.1985493},
abstract = {This position paper describes our current work in adapting a software product line technique to the constraints of a development organization. We report on applying a commonality and variability analysis with an organization adopting a software product line approach while facing sever resource constraints because of current product development commitments. The immediate focus of the paper is on blending commonality and variability analysis into the organization's existing requirements development process. The longer-term goal of this work is to facilitate the transition to product lines in a minimally intrusive way. The paper describes how the approach was introduced and implemented, and summarizes the benefits achieved and the issues arising from the work to date.},
booktitle = {Proceedings of the 2nd International Workshop on Product Line Approaches in Software Engineering},
pages = {31–34},
numpages = {4},
keywords = {software product line, commonality and variability analysis},
location = {Waikiki, Honolulu, HI, USA},
series = {PLEASE '11}
}

@inproceedings{10.1145/3194078.3194082,
author = {Pukhkaiev, Dmytro and G\"{o}tz, Sebastian},
title = {BRISE: energy-efficient benchmark reduction},
year = {2018},
isbn = {9781450357326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194078.3194082},
doi = {10.1145/3194078.3194082},
abstract = {A considerable portion of research activities in computer science heavily relies on the process of benchmarking, e.g., to evaluate a hypothesis in an empirical study. The goal is to reveal how a set of independent variables (factors) influences one or more dependent variables. With a vast number of factors or a high amount of factors' values (levels), this process becomes time- and energy-consuming. Current approaches to lower the benchmarking effort suffer from two deficiencies: (1) they focus on reducing the number of factors and, hence, are inapplicable to experiments with only two factors, but a vast number of levels and (2) being adopted from, e.g., combinatorial optimization they are designed for a different search space structure and, thus, can be very wasteful. This paper provides an approach for benchmark reduction, based on adaptive instance selection and multiple linear regression. We evaluate our approach using four empirical studies, which investigate the effect made by dynamic voltage and frequency scaling in combination with dynamic concurrency throttling on the energy consumption of a computing system (parallel compression, sorting, and encryption algorithms as well as database query processing). Our findings show the effectiveness of the approach. We can save 78% of benchmarking effort, while the result's quality decreases only by 3 pp, due to using only a near-optimal configuration.},
booktitle = {Proceedings of the 6th International Workshop on Green and Sustainable Software},
pages = {23–30},
numpages = {8},
keywords = {non-functional properties, fractional factorial design, benchmarking, adaptive instance selection, active learning},
location = {Gothenburg, Sweden},
series = {GREENS '18}
}

@inproceedings{10.1145/2556624.2556634,
author = {Lytra, Ioanna and Eichelberger, Holger and Tran, Huy and Leyh, Georg and Schmid, Klaus and Zdun, Uwe},
title = {On the interdependence and integration of variability and architectural decisions},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556634},
doi = {10.1145/2556624.2556634},
abstract = {In software product line engineering, the design of assets for reuse and the derivation of software products entails low-level and high-level decision making. In this process, two major types of decisions must be addressed: variability decisions, i.e., decisions made as part of variability management, and architectural decisions, i.e., fundamental decisions to be made during the design of the architecture of the product line or the products. In practice, variability decisions often overlap with or influence architectural decisions. For instance, resolving a variability may enable or prevent some architectural options. This inherent interdependence has not been explicitly and systematically targeted in the literature, and therefore, is mainly resolved in an ad hoc and informal manner today. In this paper, we discuss possible ways how variability and architectural decisions interact, as well as their management and integration in a systematic manner. We demonstrate the integration between the two types of decisions in a motivating case and leverage existing tools for implementing our proposal.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {8},
keywords = {variability decisions, software product lines, product derivation, architectural decisions},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1007/s10009-013-0298-6,
author = {Ferrari, Alessio and Spagnolo, Giorgio O. and Martelli, Giacomo and Menabeni, Simone},
title = {From commercial documents to system requirements: an approach for the engineering of novel CBTC solutions},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-013-0298-6},
doi = {10.1007/s10009-013-0298-6},
abstract = {Communications-based train control (CBTC) systems are the new frontier of automated train control and operation. Currently developed CBTC platforms are actually very complex systems including several functionalities, and every installed system, developed by a different company, varies in extent, scope, number, and even names of the implemented functionalities. International standards have emerged, but they remain at a quite abstract level, mostly setting terminology. This paper presents the results of an experience in defining a global model of CBTC, by mixing semi-formal modelling and product line engineering. The effort has been based on an in-depth market analysis, not limiting to particular aspects but considering as far as possible the whole picture. The paper also describes a methodology to derive novel CBTC products from the global model, and to define system requirements for the individual CBTC components. To this end, the proposed methodology employs scenario-based requirements elicitation aided with rapid prototyping. To enhance the quality of the requirements, these are written in a constrained natural language (CNL), and evaluated with natural language processing (NLP) techniques. The final goal is to go toward a formal representation of the requirements for CBTC systems. The overall approach is discussed, and the current experience with the implementation of the method is presented. In particular, we show how the presented methodology has been used in practice to derive a novel CBTC architecture.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {647–667},
numpages = {21},
keywords = {Product line engineering, Formal methods, Experience report, Constrained natural language, CENELEC, CBTC}
}

@article{10.1016/j.infsof.2012.02.002,
author = {Holl, Gerald and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
year = {2012},
issue_date = {August, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.02.002},
doi = {10.1016/j.infsof.2012.02.002},
abstract = {Context: Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented. Objective: The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method: Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results: The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions: We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {828–852},
numpages = {25},
keywords = {Systematic literature review, Product line engineering, Multi product lines, Large-scale systems}
}

@article{10.1016/j.micpro.2021.103964,
author = {Gokilavani, N. and Bharathi, B.},
title = {Multi-Objective based test case selection and prioritization for distributed cloud environment},
year = {2021},
issue_date = {Apr 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {82},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2021.103964},
doi = {10.1016/j.micpro.2021.103964},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {6},
keywords = {Cloud environment, Software testing, Similarity-based clustering, Test case prioritization, Test case selection, Particle swarm optimization, Software product line}
}

@article{10.1016/j.procs.2017.08.206,
author = {Mani, Neel and Helfert, Markus and Pahl, Claus},
title = {A Domain-specific Rule Generation Using Model-Driven Architecture in Controlled Variability Model},
year = {2017},
issue_date = {September 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {112},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2017.08.206},
doi = {10.1016/j.procs.2017.08.206},
abstract = {The business environment changes rapidly and needs to adapt to the enterprise business systems must be considered for new types of requirements to accept changes in the business strategies and processes. This raises new challenges that the traditional development approaches cannot always provide a complete solution in an efficient way. However, most of the current proposals for automatic generation are not devised to cope with rapid integration of the changes in the business requirement of end user (stakeholders and customers) resource. Domain-specific Rules constitute a key element for domain specific enterprise application, allowing configuration of changes, and management of the domain constraint within a domain. In this paper, we propose an approach to the development of an automatic generation of the domain-specific rules by using variability feature model and ontology definition of domain model concepts coming from Software product line engineering and Model Driven Architecture. We provide a process approach to generate a domain-specific rule based on the end user requirement.},
journal = {Procedia Comput. Sci.},
month = sep,
pages = {2354–2362},
numpages = {9},
keywords = {Variability Model, Rule Generation, Model Driven Architecture, Domain-specific rules, Business Process Model}
}

@inproceedings{10.1145/3168365.3168377,
author = {Ananieva, Sofia and Klare, Heiko and Burger, Erik and Reussner, Ralf},
title = {Variants and Versions Management for Models with Integrated Consistency Preservation},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168377},
doi = {10.1145/3168365.3168377},
abstract = {Modern software systems are often developed and maintained by describing them in several modeling and programming languages. To reduce complexity and improve understandability of such systems, models represent specific views on the system. These views have semantic interrelations (e.g., by sharing common or dependent information) that need to be kept consistent during evolution of the system. Apart from that, modern systems need to run in many different contexts and be highly configurable to satisfy the demand for fully customizable products. Such variable systems often comprise various dependencies from which inconsistencies may arise. Combining solutions for consistency management with variants and versions management, however, comes with many challenges.In this research-in-progress paper, we introduce the VaVe approach which makes variants and versions management aware of automated consistency preservation in the context of multi-view modeling. We explain core features of the approach and reason about its benefits and limitations.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {3–10},
numpages = {8},
keywords = {Variability Management, Software Product Lines, Delta-Based Consistency Preservation},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.5555/1753235.1753246,
author = {Hendrickson, Scott A. and Wang, Yang and van der Hoek, Andr\'{e} and Taylor, Richard N. and Kobsa, Alfred},
title = {Modeling PLA variation of privacy-enhancing personalized systems},
year = {2009},
publisher = {Carnegie Mellon University},
address = {USA},
abstract = {Privacy-enhancing personalized (PEP) systems address individual users' privacy preferences as well as privacy laws and regulations. Building such systems entails modeling two different domains: (a) privacy constraints as mandated by law, voluntary self-regulation, or users' individual privacy preferences, and modeled by legal professionals, and (b) software architectures as dictated by available software components and modeled by software architects. Both can evolve independently, e.g., as new laws go into effect or new components become available. In prior work, we proposed modeling PEP systems using a product line architecture (PLA). However, with an extensional PLA, these domain models became strongly entangled making it difficult to modify one without inadvertently affecting the other. This paper evaluates an approach towards modeling both domains within an intensional PLA. We find evidence that this results in a clearer separation between the two domain models, making each easier to evolve and maintain.},
booktitle = {Proceedings of the 13th International Software Product Line Conference},
pages = {71–80},
numpages = {10},
location = {San Francisco, California, USA},
series = {SPLC '09}
}

@inproceedings{10.1109/ECBS.2008.14,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Evaluation of Quality Attribute Variability in Software Product Families},
year = {2008},
isbn = {9780769531410},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2008.14},
doi = {10.1109/ECBS.2008.14},
abstract = {Software product family or line is a software engineering paradigm that systematizes reuse. In Software Product Line Engineering, two phases are distinguished: Domain Engineering which is in charge of developing a common infrastructure and assets and Application Engineering which makes use of those assets to generate the products. One of the key aspects of product lines is variability and its management. However, the main focus has been on functional variability and quality variability in software product lines has not received so much attention by researchers. In a product line different members of the line may require different levels of a quality requirement, for instance they could differ in terms of their availability, security, reliability, etc. Due to this variability, quality evaluation in software product lines is much more complicated that in single-systems. One alternative is to evaluate all the products of a line but it is very expensive and ways of reducing evaluation efforts are necessary. In this direction, the paper presents a method for facilitating cost-effective quality evaluation of a product line taking into consideration variability on quality attributes.},
booktitle = {Proceedings of the 15th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems},
pages = {255–264},
numpages = {10},
keywords = {variability, quality attributes, evaluation, Software product lines},
series = {ECBS '08}
}

@article{10.1016/j.entcs.2016.02.007,
author = {Zela Ruiz, Jael and Rubira, Cec\'{\i}lia M.},
title = {Quality of Service Conflict During Web Service Monitoring},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {321},
number = {C},
issn = {1571-0661},
url = {https://doi.org/10.1016/j.entcs.2016.02.007},
doi = {10.1016/j.entcs.2016.02.007},
abstract = {Web services have become one of the most used technologies in service-oriented systems. Its popularity is due to its property to adapt to any context. As a consequence of the increasing number of Web services on the Internet and its important role in many applications today, Web service quality has become a crucial requirement and demanded by service consumers. Terms of quality levels are written between service providers and service consumers to ensure a degree of quality. The use of monitoring tools to control service quality levels is very important. Quality attributes suffer variations in their values during runtime, this is produced by many factors such as a memory leak, deadlock, race data, inconsistent data, etc. However, sometimes monitoring tools can impact negatively affecting the quality of service when they are not properly used and configured, producing possible conflicts between quality attributes. This paper aims to show the impact of monitoring tools over service quality, two of the most important quality attributes - performance and accuracy - were chosen to be monitored. A case study is conducted to present and evaluate the relationship between performance and accuracy over a Web service. As a result, conflict is found between performance and accuracy, where performance was the most affected, because it presented a degradation in its quality level during monitoring.},
journal = {Electron. Notes Theor. Comput. Sci.},
month = mar,
pages = {113–127},
numpages = {15},
keywords = {Web Services, SOA, Quality of Service, Quality Attributes, Performance, Monitoring Tools, Conflict, Accuracy}
}

@inproceedings{10.1145/2797433.2797455,
author = {Alebrahim, Azadeh and Fa\ss{}bender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta and Zdun, Uwe},
title = {1st Workshop on VAriability for QUalIties in SofTware Architecture (VAQUITA): Workshop Introduction},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797455},
doi = {10.1145/2797433.2797455},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {22},
numpages = {2},
keywords = {variability, quality attributes, Software architecture},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@inproceedings{10.1007/978-3-642-41533-3_24,
author = {Gonz\'{a}lez-Huerta, Javier and Insfr\'{a}n, Emilio and Abrah\~{a}o, Silvia},
title = {Defining and Validating a Multimodel Approach for Product Architecture Derivation and Improvement},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_24},
doi = {10.1007/978-3-642-41533-3_24},
abstract = {Software architectures are the key to achieving the non-functional requirements NFRs in any software project. In software product line SPL development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement QuaDAI. A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method ATAM. The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {388–404},
numpages = {17},
keywords = {Software Product Lines, Quality Attributes, Model Transformations, Controlled Experiment, Architectural Patterns}
}

@inproceedings{10.5555/2666064.2666070,
author = {Sozen, Neset and Merlo, Ettore},
title = {Adapting software product lines for complex certifiable avionics software},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {In avionics, the size and complexity of software-intensive systems increased considerably during recent years. Besides the size and the complexity, certification constraints also had negative impact on the cost and schedule of avionics software projects. Model-Driven Development (MDD) and Software Product Lines Engineering (SPLE) offer an opportunity to improve the avionics software development process, reduce the cost and improve the time to market.Complexity of avionics software and certification constraints pose several challenges to SPLE adoption. Software Product Lines (SPL) framework must provide bi-directional traceability between requirements and low level software assets (e.g. code and test), facilitate production of certification deliverables, allow validation on the target platform and provide code coverage. Also, SPL offer a scheme to manage the complexity of avionics software systems through variability management tools.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {21–24},
numpages = {4},
keywords = {software product line, model-driven development, certifiable software, avionics software, DO-178C},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@inproceedings{10.1145/2000259.2000286,
author = {Cavalcanti, Ricardo de Oliveira and de Almeida, Eduardo Santana and Meira, Silvio R.L.},
title = {Extending the RiPLE-DE process with quality attribute variability realization},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000286},
doi = {10.1145/2000259.2000286},
abstract = {Software product lines engineering is a viable way to achieve the productivity gains desired by companies. Product line architecture must benefit from commonalities among products in the family and enable the variability among them. The aspect of variability in quality attributes has been neglected or ignored by most of the researchers as attention has been mainly put in functional variability. This paper describes an architecture and design process for software product lines that can properly deal with quality attribute variability. The proposed approach enhances the RiPLE-DE process for software product line engineering with activities and guidelines for quality attribute variability. An initial experimental study is presented to characterize and evaluate the proposed process enhancements.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {159–164},
numpages = {6},
keywords = {software reuse, software product lines (spl), software architecture, quality attribute variability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1145/2851613.2851959,
author = {Noorian, Mahdi and Bagheri, Ebrahim and Du, Weichang},
title = {Quality-centric feature model configuration using goal models},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851959},
doi = {10.1145/2851613.2851959},
abstract = {In software product line engineering, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of feature models in addition to the diversity of the stake-holders' expectations, the configuration process is viewed as a complex optimization problem. In this paper, we propose a holistic approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. Here, we model stakeholders' functional and non-functional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stake-holders' objectives, and their preferences in the form of an integer linear program to automatically perform feature selection.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1296–1299},
numpages = {4},
keywords = {configuration process, feature model, goal model},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.5555/2820656.2820661,
author = {Segura, Vin\'{\i}cius C. V. B. and Tizzei, Leonardo P. and de F. Ramirez, Jo\~{a}o Paulo and dos Santos, Marcelo N. and Azevedo, Leonardo G. and de G. Cerqueira, Renato F.},
title = {WISE-SPL: bringing multi-tenancy to the weather InSights environment system},
year = {2015},
publisher = {IEEE Press},
abstract = {Weather conditions affect many cities and companies. The WISE (Weather InSights Environment) system serves as a central place to gather and present weather related information for decision makers. It was initially developed to fit a single tenant. Due to a multi-tenant opportunity, WISE is evolving to be deployed on a Cloud environment to support on-demand computing resources and multiple clients. Software product line techniques were applied to model common and variable features of tenants. WISE-SPL enables the derivation of products for each client and also the deployment on Cloud infrastructure. The contribution of this work is a demonstration and discussion of benefits and limitations in applying SPL techniques, following a extractive approach, to build a multi-tenant Cloud application.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {7–10},
numpages = {4},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/2465478.2465495,
author = {Klatt, Benjamin and K\"{u}ster, Martin},
title = {Improving product copy consolidation by architecture-aware difference analysis},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465495},
doi = {10.1145/2465478.2465495},
abstract = {Software product lines (SPL) are a well-known concept to efficiently develop product variants. However, migrating customised product copies to a product line is still a labour-intensive challenge due to the required comprehension of differences among the implementations and SPL design decisions. Most existing SPL approaches are focused on forward engineering. Only few aim to handle SPL evolution, but even those lack support of variability reverse engineering, which is necessary for migrating product copies to a product line. In this paper, we present our continued concept on using component architecture information to enhance a variability reverse engineering process. Including this information particularly improves the difference identification as well as the variation point analysis and -aggregation steps. We show how the concept can be applied by providing an illustrating example.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {117–122},
numpages = {6},
keywords = {software product line, reverse engineering, component architecture},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@article{10.1016/j.jss.2019.02.028,
author = {Jakubovski Filho, Helson Luiz and Ferreira, Thiago Nascimento and Vergilio, Silvia Regina},
title = {Preference based multi-objective algorithms applied to the variability testing of software product lines},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.028},
doi = {10.1016/j.jss.2019.02.028},
journal = {J. Syst. Softw.},
month = may,
pages = {194–209},
numpages = {16},
keywords = {Preference-Based algorithms, Search-Based software engineering, Software product line testing}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making testing product lines more efficient by improving the testability of product line architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {testing, testability, software product line, evaluation, design, architecture},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1007/978-3-642-33666-9_46,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel and Walawege, Suneth},
title = {A product line modeling and configuration methodology to support model-based testing: an industrial case study},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_46},
doi = {10.1007/978-3-642-33666-9_46},
abstract = {Product Line Engineering (PLE) is expected to enhance quality and productivity, speed up time-to-market and decrease development effort, through reuse—the key mechanism of PLE. In addition, one can also apply PLE to support systematic testing and more specifically model-based testing (MBT) of product lines—the original motivation behind this work. MBT has shown to be cost-effective in many industry sectors but at the expense of building models of the system under test (SUT). However, the modeling effort to support MBT can significantly be reduced if an adequate product line modeling and configuration methodology is followed, which is the main motivation of this paper. The initial motivation for this work emerged while working with MBT for a Video Conferencing product line at Cisco Systems, Norway. In this paper, we report on our experience in modeling product family models and various types of behavioral variability in the Saturn product line. We focus on behavioral variability in UML state machines since the Video Conferencing Systems (VCSs) exhibit strong state-based behavior and these models are the main drivers for MBT; however, the approach can be also tailored to other UML diagrams. We also provide a mechanism to specify and configure various types of variability using stereotypes and Aspect-Oriented Modeling (AOM). Results of applying our methodology to the Saturn product line modeling and configuration process show that the effort required for modeling and configuring products of the product line family can be significantly reduced.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {726–742},
numpages = {17},
keywords = {product line engineering, model-based testing, behavioral variability, aspect-oriented modeling, UML state machine},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/1529282.1529388,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hn\v{e}tynka, Petr and Malohlava, Michal},
title = {Using a product line for creating component systems},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529388},
doi = {10.1145/1529282.1529388},
abstract = {Component systems have become a wide-spread technology and found their place in several application domains. Each component system has its specifics and particularities that reflect its focus and the application domain it is intended for. Although important, the diversity of component systems leads to a number of problems including having different tools for each systems, unnecessary duplication of functionality and problems with integration when several domains are to be targeted. Based on categorization of component application domains, we propose a "meta-component system", which provides a software product line for creating custom component systems. We focus especially on the deployment and execution environment, which is where most diversities are found. We demonstrate the usage of the "meta-component system" and propose how it is to be realized by two core concepts of SOFA 2, namely connector generator and microcomponents.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {501–508},
numpages = {8},
keywords = {runtime environment, product line engineering, generative programming, component systems},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1109/ECBS.2008.53,
author = {Thao, Cheng and Munson, Ethan V. and Nguyen, Tien N.},
title = {Software Configuration Management for Product Derivation in Software Product Families},
year = {2008},
isbn = {9780769531410},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2008.53},
doi = {10.1109/ECBS.2008.53},
abstract = {A key process in software product line (SPL) engineering is product derivation, which is the process of building software products from a base set of core assets. During product derivation, the components in both core assets and derived software products are modified to meet needs for different functionality, platforms, quality attributes, etc. However, existing software configuration management (SCM) systems do not sufficiently support the derivation process in SPL. In this paper, we introduce a novel SCM system that is well-suited for product derivation in SPL. Our tool, MoSPL handles version management at the component level via its product versioning and data models. It explicitly manages logical constraints and derivation relations among components in both core assets and derived products, thus enabling the automatic propagation of changes in the core assets to their copies in derived products and vice versa. The system can also detect conflicting changes to different copies of components in software product lines.},
booktitle = {Proceedings of the 15th Annual IEEE International Conference and Workshop on the Engineering of Computer Based Systems},
pages = {265–274},
numpages = {10},
keywords = {Software Product Line, Software Configuration Management, Product Derivation},
series = {ECBS '08}
}

@inproceedings{10.1145/1944892.1944913,
author = {Nguyen, Tuan and Colman, Alan and Talib, Muhammad Adeel and Han, Jun},
title = {Managing service variability: state of the art and open issues},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944913},
doi = {10.1145/1944892.1944913},
abstract = {In addition to inherited characteristics from software variability, service variability exposes two distinct characteristics that impose certain challenges in variability management. These characteristics are: i) Different types of variability and their inter-relationships; and ii) Dynamic and recursive variability communication among different stakeholders. This paper elaborates these distinct characteristics in detail with a case study. The challenges brought about by these distinct characteristics in managing variability also are highlighted. We present a review of related work in service variability management and briefly propose our ongoing approach to addressing these challenges.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {165–173},
numpages = {9},
keywords = {web services, variability management, variability communication, service variability, service oriented computing},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1016/j.cageo.2014.09.004,
author = {Buccella, Agustina and Cechich, Alejandra and Pol'la, Matias and Arias, Maximiliano and del Socorro Doldan, Maria and Morsan, Enrique},
title = {Marine ecology service reuse through taxonomy-oriented SPL development},
year = {2014},
issue_date = {December 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {73},
number = {C},
issn = {0098-3004},
url = {https://doi.org/10.1016/j.cageo.2014.09.004},
doi = {10.1016/j.cageo.2014.09.004},
abstract = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain. HighlightsSolutions for software reuse for GIS domains by using standard information.Domain-specific taxonomy for supporting the generation of software artifacts.Guides for using geographic services in order to fulfill different GIS functionalities of systems in the domain.Evaluation of the effectiveness of the taxonomy and guides when building an SPL and two derived products.Improvements on time and costs of new GIS products being developed.},
journal = {Comput. Geosci.},
month = dec,
pages = {108–121},
numpages = {14},
keywords = {Software reuse, ISO 19100 standards, Geographic information systems, Domain-specific taxonomies, Domain engineering}
}

@article{10.1007/s10270-015-0459-z,
author = {S\'{a}nchez, Ana B. and Segura, Sergio and Parejo, Jos\'{e} A. and Ruiz-Cort\'{e}s, Antonio},
title = {Variability testing in the wild: the Drupal case study},
year = {2017},
issue_date = {February  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0459-z},
doi = {10.1007/s10270-015-0459-z},
abstract = {Variability testing techniques search for effective and manageable test suites that lead to the rapid detection of faults in systems with high variability. Evaluating the effectiveness of these techniques in realistic settings is a must, but challenging due to the lack of variability-intensive systems with available code, automated tests and fault reports. In this article, we propose using the Drupal framework as a case study to evaluate variability testing techniques. First, we represent the framework variability using a feature model. Then, we report on extensive non-functional data extracted from the Drupal Git repository and the Drupal issue tracking system. Among other results, we identified 3392 faults in single features and 160 faults triggered by the interaction of up to four features in Drupal v7.23. We also found positive correlations relating the number of bugs in Drupal features to their size, cyclomatic complexity, number of changes and fault history. To show the feasibility of our work, we evaluated the effectiveness of non-functional data for test case prioritization in Drupal. Results show that non-functional attributes are effective at accelerating the detection of faults, outperforming related prioritization criteria as test case similarity.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {173–194},
numpages = {22},
keywords = {Variability-intensive systems, Variability testing, Test case selection, Test case prioritization, Non-functional properties, Automated testing}
}

@inproceedings{10.1145/1509239.1509259,
author = {Niu, Nan and Easterbrook, Steve},
title = {Concept analysis for product line requirements},
year = {2009},
isbn = {9781605584423},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1509239.1509259},
doi = {10.1145/1509239.1509259},
abstract = {Traditional methods characterize a software product line's requirements using either functional or quality criteria. This appears to be inadequate to assess modularity, detect interferences, and analyze trade-offs. We take advantage of both symmetric and asymmetric views of aspects, and perform formal concept analysis to examine the functional and quality requirements of an evolving product line. The resulting concept lattice provides a rich notion which allows remarkable insights into the modularity and interactions of requirements. We formulate a number of problems that aspect-oriented product line requirements engineering should address, and present our solutions according to the concept lattice. We describe a case study applying our approach to analyze a mobile game product line's requirements, and review lessons learned.},
booktitle = {Proceedings of the 8th ACM International Conference on Aspect-Oriented Software Development},
pages = {137–148},
numpages = {12},
keywords = {quality attribute scenarios, product line engineering, functional requirements profiles, formal concept analysis},
location = {Charlottesville, Virginia, USA},
series = {AOSD '09}
}

@inproceedings{10.1145/2602458.2602462,
author = {Pressler, Michael and Viehl, Alexander and Bringmann, Oliver and Rosenstiel, Wolfgang},
title = {Execution cost estimation for software deployment in component-based embedded systems},
year = {2014},
isbn = {9781450325776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602458.2602462},
doi = {10.1145/2602458.2602462},
abstract = {We present an approach for the determination of initial mapping configurations for embedded software components on modern embedded heterogeneous processor architectures. The presented work combines the advantages of component-based design and properties obtained from source-code analysis. The goal is a very fast estimation of execution costs for multiple hardware/software component pairs even before the hardware is physically available.},
booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {123–128},
numpages = {6},
keywords = {system architecture, performance prediction, initial mapping determination, execution cost estimation, component-based design},
location = {Marcq-en-Bareul, France},
series = {CBSE '14}
}

@inproceedings{10.1145/3023956.3023968,
author = {Mjeda, Anila and Wasala, Asanka and Botterweck, Goetz},
title = {Decision spaces in product lines, decision analysis, and design exploration: an interdisciplinary exploratory study},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023968},
doi = {10.1145/3023956.3023968},
abstract = {Context. From recent works on product properties resulting from configurations and the optimisation of these properties, one comes quickly to more complex challenges such as multi-objective optimisation, conflicting objectives, multiple stakeholders, and conflict resolution. The intuition is that Software Product Line Engineering (SPLE) can draw from other disciplines that deal with decision spaces and complex decision scenarios.Objectives. We aim to (1) explore links to such disciplines, (2) systematise and compare concepts, and (3) identify opportunities, where SPLE approaches can be enriched.Method. We undertake an exploratory study: Starting from common SPLE activities and artefacts, we identify aspects where we expect to find corresponding counterparts in other disciplines. We focus on Multiple Criteria Decision Analysis (MCDA), Multi-Objective Optimisation (MOO), and Design Space Exploration (DSE), and perform a comparison of the key concepts.Results. The resulting comparison relates SPLE activities and artefacts to concepts from MCDA, MOO, and DSE and identifies areas where SPLE approaches can be enriched. We also provide examples of existing work at the intersections of SPLE with the other fields. These findings are aimed to foster the conversation on research opportunities where SPLE can draw techniques from other disciplines dealing with complex decision scenarios.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {68–75},
numpages = {8},
keywords = {multi-objective optimisation, multi-criteria decision analysis, design-space exploration, decision modelling},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1007/978-3-662-45234-9_20,
author = {Collet, Philippe},
title = {Domain Specific Languages for Managing Feature Models: Advances and Challenges},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_20},
doi = {10.1007/978-3-662-45234-9_20},
abstract = {Managing multiple and complex feature models is a tedious and error-prone activity in software product line engineering. Despite many advances in formal methods and analysis techniques, the supporting tools and APIs are not easily usable together, nor unified. In this paper, we report on the development and evolution of the Familiar Domain-Specific Language DSL. Its toolset is dedicated to the large scale management of feature models through a good support for separating concerns, composing feature models and scripting manipulations. We overview various applications of Familiar and discuss both advantages and identified drawbacks. We then devise salient challenges to improve such DSL support in the near future.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {273–288},
numpages = {16}
}

@article{10.1145/1183236.1183260,
author = {Sugumaran, Vijayan and Park, Sooyong and Kang, Kyo C.},
title = {Introduction},
year = {2006},
issue_date = {December 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1183236.1183260},
doi = {10.1145/1183236.1183260},
journal = {Commun. ACM},
month = dec,
pages = {28–32},
numpages = {5}
}

@article{10.1016/j.csi.2019.04.011,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Matalonga, Santiago},
title = {Trends in software reuse research: A tertiary study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.04.011},
doi = {10.1016/j.csi.2019.04.011},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {18},
keywords = {Tertiary study, Systematic literature review, Trends in software reuse, Software reuse}
}

@inproceedings{10.1145/2377576.2377592,
author = {Shakhimardanov, Azamat and Hochgeschwender, Nico and Kraetzschmar, Gerhard K.},
title = {Component models in robotics software},
year = {2010},
isbn = {9781450302906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377576.2377592},
doi = {10.1145/2377576.2377592},
abstract = {In recent years increased research activity in robotics has led to advancements in both hardware and software technologies. More complex hardware required increasingly sophisticated software infrastructures to operate it, and led to the development of several different robotics software frameworks. The driving forces behind the development of such frameworks is to cope with the heterogeneous and distributed nature of robotics software applications and to exploit more advanced software technologies in the robotics domain. So far, though, there has been not much effort to foster cooperation among these frameworks, neither on conceptual nor on implementation levels. Our research aims to analyse existing robotics software frameworks in order to identify possible levels of interoperability among them. The problem is tackled by determining a set of software concepts, in our case centering around component-based software development, which are used to determine a set of common architectural elements in an analysis of existing robotics software frameworks. The result is that these common elements can be used as interoperability points among software frameworks. Exploiting such interoperability gives developers new architectural design choices and fosters reuse of functionality already developed, albeit in another framework. It is also highly relevant for the development of new robotics software frameworks, as it opens smoother migration paths for developers to switch from one framework to another.},
booktitle = {Proceedings of the 10th Performance Metrics for Intelligent Systems Workshop},
pages = {82–87},
numpages = {6},
location = {Baltimore, Maryland},
series = {PerMIS '10}
}

@inproceedings{10.5555/1885639.1885675,
author = {Clements, Paul and McGregor, John D. and Bass, Len},
title = {Eliciting and capturing business goals to inform a product line's business case and architecture},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Business goals constitute an important kind of knowledge for a software product line. They inform the product line's business case and they inform its architecture and quality attribute requirements. This paper establishes the connection between business goals and a product line's business case and architecture. It then presents a set of common business goal categories, gleaned from a systematic search of the business literature that can be used to elicit an organization's business goals from key stakeholders. Finally, it presents a well-defined method, which we have tried out in practice, for eliciting and capturing business goals and tying them to quality attribute requirements.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {393–405},
numpages = {13},
keywords = {software product line, quality attribute requirements, product line architecture, business goals, business goal scenario, business case, architecture},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/2556624.2556628,
author = {Lengauer, Philipp and Bitto, Verena and Angerer, Florian and Gr\"{u}nbacher, Paul and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Where has all my memory gone? determining memory characteristics of product variants using virtual-machine-level monitoring},
year = {2014},
isbn = {9781450325561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556624.2556628},
doi = {10.1145/2556624.2556628},
abstract = {Non-functional properties such as memory footprint have recently gained importance in software product line research. However, determining the memory characteristics of individual features and product variants is extremely challenging. We present an approach that supports the monitoring of memory characteristics of individual features at the level of Java virtual machines. Our approach provides extensions to Java virtual machines to track memory allocations and deal-locations of individual features based on a feature-to-code mapping. The approach enables continuous monitoring at the level of features to detect anomalies such as memory leaks, excessive memory consumption, or abnormal garbage collection times in product variants. We provide an evaluation of our approach based on different product variants of the DesktopSearcher product line. Our experiment with different program inputs demonstrates the feasibility of our technique.},
booktitle = {Proceedings of the 8th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {13},
numpages = {8},
keywords = {monitoring, memory footprint, feature-oriented software development, Java},
location = {Sophia Antipolis, France},
series = {VaMoS '14}
}

@article{10.1016/j.scico.2017.10.013,
author = {Castro, Thiago and Lanna, Andr and Alves, Vander and Teixeira, Leopoldo and Apel, Sven and Schobbens, Pierre-Yves},
title = {All roads lead to Rome},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2017.10.013},
doi = {10.1016/j.scico.2017.10.013},
abstract = {The formalization of seven strategies for product-line reliability analysis.The first feature-family-product-based strategy for product-line model checking.A general principle for lifting analyses to product lines using ADDs.Proofs that the formalized strategies commute.All strategies proven sound with respect to single-product reliability analysis. Software product line engineering is a means to systematically manage variability and commonality in software systems, enabling the automated synthesis of related programs (products) from a set of reusable assets. However, the number of products in a software product line may grow exponentially with the number of features, so it is practically infeasible to quality-check each of these products in isolation. There is a number of variability-aware approaches to product-line analysis that adapt single-product analysis techniques to cope with variability in an efficient way. Such approaches can be classified along three analysis dimensions (product-based, family-based, and feature-based), but, particularly in the context of reliability analysis, there is no theory comprising both (a) a formal specification of the three dimensions and resulting analysis strategies and (b) proof that such analyses are equivalent to one another. The lack of such a theory hinders formal reasoning on the relationship between the analysis dimensions and derived analysis techniques. We formalize seven approaches to reliability analysis of product lines, including the first instance of a feature-family-product-based analysis in the literature. We prove the formalized analysis strategies to be sound with respect to the probabilistic approach to reliability analysis of a single product. Furthermore, we present a commuting diagram of intermediate analysis steps, which relates different strategies and enables the reuse of soundness proofs between them.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {116–160},
numpages = {45},
keywords = {Verification, Software product lines, Reliability analysis, Product-line analysis, Model checking}
}

@article{10.1007/s10515-010-0076-6,
author = {Dhungana, Deepak and Gr\"{u}nbacher, Paul and Rabiser, Rick},
title = {The DOPLER meta-tool for decision-oriented variability modeling: a multiple case study},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-010-0076-6},
doi = {10.1007/s10515-010-0076-6},
abstract = {The variability of a product line is typically defined in models. However, many existing variability modeling approaches are rigid and don't allow sufficient domain-specific adaptations. We have thus been developing a flexible and extensible approach for defining product line variability models. Its main purposes are to guide stakeholders through product derivation and to automatically generate product configurations. Our approach is supported by the DOPLER (  D ecision-  O riented  P roduct  L ine  E ngineering for effective  R euse) meta-tool that allows modelers to specify the types of reusable assets, their attributes, and dependencies for their specific system and context. The aim of this paper is to investigate the suitability of our approach for different domains. More specifically, we explored two research questions regarding the implementation of variability and the utility of DOPLER for variability modeling in different domains. We conducted a multiple case study consisting of four cases in the domains of industrial automation systems and business software. In each of these case studies we analyzed variability implementation techniques. Experts from our industry partners then developed domain-specific meta-models, tool extensions, and variability models for their product lines using DOPLER. The four cases demonstrate the flexibility of the DOPLER approach and the extensibility and adaptability of the supporting meta tool.},
journal = {Automated Software Engg.},
month = mar,
pages = {77–114},
numpages = {38},
keywords = {Product line engineering, Meta-tools, Decision models}
}

@inproceedings{10.5555/2093889.2093921,
author = {Bagheri, Ebrahim and Asadi, Mohsen and Ensan, Faezeh and Ga\v{s}evi\'{c}, Dragan and Mohabbati, Bardia},
title = {Bringing semantics to feature models with SAFMDL},
year = {2011},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software product line engineering is a paradigm that advocates the reusability of software engineering assets and the rapid development of new applications for a target domain. These objectives are achieved by capturing the commonalities and variabilities between the applications of a target domain and through the development of comprehensive and variability-covering domain models. The domain models developed within the software product line development process need to cover all of the possible features and aspects of the target domain. In other words, the domain models often described using feature models should be elaborate representations of the feature space of that domain. In order to operationalize feature-based representations of a software application, appropriate implementation mechanisms need to be employed. In this paper, we propose a Semantic Web-oriented language, called Semantic Annotations for Feature Modeling Description Language (SAFMDL) that provides the means to semantically describe feature models. We will show that using SAFMDL along with Semantic Web Query techniques, we are able to bridge the gap between software product lines and SOA technology. Our proposed work allows software practitioners to use Semantic Web technology to quickly and rapidly develop new software products based on SOA technology from software product lines.},
booktitle = {Proceedings of the 2011 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {287–300},
numpages = {14},
location = {Toronto, Ontario, Canada},
series = {CASCON '11}
}

@inproceedings{10.5555/1885639.1885685,
author = {Medeiros, Fl\'{a}vio M. and de Almeida, Eduardo S. and Meira, Silvio R. L.},
title = {SOPLE-DE: an approach to design service-oriented product line architectures},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software reuse is crucial for enterprises interested in software quality and productivity gains. In this context, Software Product Line (SPL) and Service-Oriented Architecture (SOA) are two reuse strategies that share common goals and can be used together to increase reuse and produce service-oriented systems faster, cheaper and customizable to specific customers. In this sense, this work investigates the problem of designing software product lines using service-oriented architectures, and presents a systematic approach to design software product lines based on services. The proposed approach provides guidance to identify, design and document architectural components, services, service compositions and their associated flows. In addition, an initial experimental study performed with the intention of validating and refining the approach is also depicted demonstrating that the proposed solution can be viable.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {456–460},
numpages = {5},
keywords = {software product line (SPL), software architecture and software development processes, service-oriented architecture (SOA)},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/1629716.1629735,
author = {Asadi, Mohsen and Mohabbati, Bardia and Kaviani, Nima and Ga\v{s}evi\'{c}, Dragan and Bo\v{s}kovi\'{c}, Marko and Hatala, Marek},
title = {Model-driven development of families of Service-Oriented Architectures},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629735},
doi = {10.1145/1629716.1629735},
abstract = {The paradigms of Service Oriented Architecture (SOA) and Software Product Line Engineering (SPLE) facilitate the development of families of software-intensive products. Software Product Line practices can be leveraged to support the development of service-oriented applications to promote the reusability of assets throughout the iterative and incremental development of software product families. Such an approach enables various service oriented business processes and software products of the same family to be systematically created and integrated. In this paper, we advocate integration of software product line engineering with model driven engineering to enable a model driven specification of software services, capable of creating software products from a family of software services. Using the proposed method, we aim to provide a consistent view of a composed software system from a higher business administration perspective to lower levels of service implementation and deployment. We demonstrate how Model Driven Engineering (MDE) can help with injecting the set of required commonalities and variabilities of a software product from a high level business process design to the lower levels of service use.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {95–102},
numpages = {8},
keywords = {business process management, semantic web, service-oriented architectures, software product lines},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.comnet.2014.08.018,
author = {Ansari, Junaid and Meshkova, Elena and Masood, Wasif and Muslim, Arham and Riihij\"{a}rvi, Janne and M\"{a}h\"{o}nen, Petri},
title = {CONFab},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {74},
number = {PA},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2014.08.018},
doi = {10.1016/j.comnet.2014.08.018},
abstract = {Extreme diversity of application requirements and a large number of different available protocols are key characteristics of Wireless Sensor Networks (WSNs). There is a need for a systematic approach to rapidly compose and optimize application specific protocol stacks in an automated fashion. In this article we present the design, implementation and performance evaluation of CONFab, a framework for automatic protocol stack composition founded on the component based optimization approach. We treat a protocol stack as a collection of interdependent configurable components and have a goal to find the most suitable composition of components, as well as optimal parameters selection of individual components in an optimal fashion. CONFab captures a deployment scenario description, relates it to the desired performance metrics, and suggests suitable protocol stacks and parameter settings. It utilizes an ontology centric knowledge base to select components from a pool of alternatives and reason on their compatibility, thus creating appropriate protocol stacks. The framework is equipped with a number of additional plugins that allow, for instance, incorporating feedback from deployed systems and user inputs to anticipate network performance. The plugin mechanism also enables incorporating further advanced optimization routines, such as genetic algorithms, which can be used for optimization of component parameters and efficient exploration of the corresponding state space. We use a set of well-known medium access control and routing protocols to validate the framework on the Indriya testbed in different user specified application and deployment conditions. Our experimental results show that CONFab framework with its component based design is a powerful enabler in obtaining protocol stacks that suit application requirements and thereby achieving high performance characteristics for the network.},
journal = {Comput. Netw.},
month = dec,
pages = {89–108},
numpages = {20},
keywords = {Protocol stack composition, Optimization, Ontology, Knowledge base, Deployment feedback, Component based design}
}

@article{10.4018/IJCAC.2019100105,
author = {Aouzal, Khadija and Hafiddi, Hatim and Dahchour, Mohamed},
title = {Policy-Driven Middleware for Multi-Tenant SaaS Services Configuration},
year = {2019},
issue_date = {Oct 2019},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {4},
issn = {2156-1834},
url = {https://doi.org/10.4018/IJCAC.2019100105},
doi = {10.4018/IJCAC.2019100105},
abstract = {The multi-tenancy architecture allows software-as-a-service applications to serve multiple tenants with a single instance. This is beneficial as it leverages economies of scale. However, it does not cope with the specificities of each tenant and their variability; notably, the variability induced in the required quality levels that differ from a tenant to another. Hence, sharing one single instance hampers the fulfillment of these quality levels for all the tenants and leads to service level agreement violations. In this context, this article proposes a policy-driven middleware that configures the service according to the non-functional requirements of the tenants. The adopted approach combines software product lines engineering and model driven engineering principles. It spans the quality attributes lifecycle, from documenting them to annotating the service components with them as policies, and it enables dynamic configuration according to service level agreements terms of the tenants.},
journal = {Int. J. Cloud Appl. Comput.},
month = oct,
pages = {86–106},
numpages = {21},
keywords = {SPLE, SLA, SaaS, Policy, Non-Functional Variability, Multi-Tenancy, MDE}
}

@inproceedings{10.1007/978-3-540-68073-4_16,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Quality Assessment in Software Product Lines},
year = {2008},
isbn = {9783540680628},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-68073-4_16},
doi = {10.1007/978-3-540-68073-4_16},
abstract = {In a software product line, quality assessment is especially important because an error or an inadequate design decision can be spread into a lot of products. Moreover, in a product line, different members of the line may require different quality attributes. In this paper, a method for quality aware software product line engineering that takes into account the variability of quality aspects and facilitates quality assessment is presented.},
booktitle = {Proceedings of the 10th International Conference on Software Reuse: High Confidence Software Reuse in Large Systems},
pages = {178–181},
numpages = {4},
location = {Beijing, China},
series = {ICSR '08}
}

@article{10.1007/s00766-013-0184-5,
author = {Alf\'{e}rez, Mauricio and Bonif\'{a}cio, Rodrigo and Teixeira, Leopoldo and Accioly, Paola and Kulesza, Uir\'{a} and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Borba, Paulo},
title = {Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0184-5},
doi = {10.1007/s00766-013-0184-5},
abstract = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity,
 stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirements
approaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications.},
journal = {Requir. Eng.},
month = nov,
pages = {355–376},
numpages = {22},
keywords = {Requirements specification, Software product lines, Use scenarios, Variability modeling}
}

@article{10.1007/s00766-013-0183-6,
author = {Lee, Jaejoon and Kang, Kyo C. and Sawyer, Pete and Lee, Hyesun},
title = {A holistic approach to feature modeling for product line requirements engineering},
year = {2014},
issue_date = {November  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0183-6},
doi = {10.1007/s00766-013-0183-6},
abstract = {Requirements engineering (RE) offers the means to discover, model, and manage the requirements of the products that comprise a product line, while software product line engineering (SPLE) offers the means of realizing the products' requirements from a common base of software assets. In practice, however, RE and SPLE have proven to be less complementary than they should. While some RE techniques, particularly goal modeling, support the exploration of alternative solutions, the appropriate solution is typically conditional on context and a large product line may have many product-defining contexts. Thus, scalability and traceability through into product line features are key challenges for RE. Feature modeling, by contrast, has been widely accepted as a way of modeling commonality and variability of products of a product line that may be very complex. In this paper, we propose a goal-driven feature modeling approach that separates a feature space in terms of problem space and solution space features, and establish explicit mappings between them. This approach contributes to reducing the inherent complexity of a mixed-view feature model, deriving key engineering drivers for developing core assets of a product line, and facilitating the quality-based product configuration.},
journal = {Requir. Eng.},
month = nov,
pages = {377–395},
numpages = {19},
keywords = {Product line requirements engineering, Goal modeling, Feature space, Feature modeling viewpoints, Feature modeling}
}

@article{10.1016/j.scico.2013.10.010,
author = {Sousa Ferreira, Gabriel Coutinho and Gaia, Felipe Nunes and Figueiredo, Eduardo and De Almeida Maia, Marcelo},
title = {On the use of feature-oriented programming for evolving software product lines - A comparative study},
year = {2014},
issue_date = {November, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {93},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2013.10.010},
doi = {10.1016/j.scico.2013.10.010},
abstract = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features.},
journal = {Sci. Comput. Program.},
month = nov,
pages = {65–85},
numpages = {21},
keywords = {Variability management, Software product lines, Feature-oriented programming, Design patterns, Conditional compilation}
}

@inproceedings{10.5555/1887899.1887907,
author = {Lung, Chung-Horng and Balasubramaniam, Balasangar and Selvarajah, Kamalachelva and Elankeswaran, Poopalasinkam and Gopalasundaram, Umatharan},
title = {Towards architecture-centric software generation},
year = {2010},
isbn = {3642151132},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Architecture-centric software generation has the potential to support flexible design and large-scale reuse. This paper describes the development of an architecture-centric framework that consists of multiple architecture alternatives, from which the architect can select and generate a working prototype in a top-down manner through a user interface rather than building it from scratch. The framework is primarily built with well-understood design patterns in distributed and concurrent computing. The development process involves extensive domain analysis, variability management, and bottom-up component engineering effort. The framework enables the architect or designer to effectively conduct upfront software architecture analysis and/or rapid architectural prototyping.},
booktitle = {Proceedings of the 4th European Conference on Software Architecture},
pages = {38–52},
numpages = {15},
keywords = {variability management, patterns, generative technique, domain analysis, concurrency, architecture-centric development},
location = {Copenhagen, Denmark},
series = {ECSA'10}
}

@inproceedings{10.1145/1982185.1982522,
author = {Mohabbati, Bardia and Hatala, Marek and Ga\v{s}evi\'{c}, Dragan and Asadi, Mohsen and Bo\v{s}kovi\'{c}, Marko},
title = {Development and configuration of service-oriented systems families},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982522},
doi = {10.1145/1982185.1982522},
abstract = {Software Product Lines (SPLs) are families of software systems which share a common sets of feature and are developed through common set of core assets in order to promotes software reusability, mass customization, reducing cost, time-to-market and improving the quality of the product. SPLs are sets (i.e., families) of software applications developed as a whole for a specific business domain. Particular applications are derived from software families by selecting the desired features through configuration process. Traditionally, SPLs are implemented with systematically developed components, shared by members of the SPLs and reused every time a new application is derived. In this paper, we propose an approach to the development and configuration of Service-Oriented SPLs in which services are used as reusable assets and building blocks of implementation. Our proposed approach also suggests prioritization of family features according to stakeholder's non-functional requirements (NFRs) and preferences. Priorities of NFRs are used to filter the most important features of the family, which is performed by Stratified Analytic Hierarchical Process (S-AHP). The priorities also are used further for the selection of appropriate services implementation for business processes realizing features. We apply Mixed Integer Linear Programming to find the optimal service selection within the constraints boundaries specified by stakeholders.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {1606–1613},
numpages = {8},
keywords = {software product line, service-oriented architecture, service selection, optimization, feature-oriented development},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.5555/2555523.2555556,
author = {Bagheri, Ebrahim and Ensan, Faezeh},
title = {Light-weight software product lines for small and medium-sized enterprises (SMEs)},
year = {2013},
publisher = {IBM Corp.},
address = {USA},
abstract = {Product line engineering practices promote the idea of systematic reuse of core assets and have been reported to decrease time-to-market and development costs for new products. However, our recent efforts to transfer our product line engineering knowledge to several of our small and medium-size enterprise industrial partner showed that there are challenges that need to be addressed before core product line engineering ideas can be deployed in SME context. These challenges include upfront investment costs, business traceability, levels of abstraction of functional features and semantic distinction between functional and non-functional software aspects. In order to address these challenges within the context of SMEs, we adopt and extend the behavior-driven development methodology in a way to not only offer agility in practice but also to equip software developers with the means to capture and manage software variability within the behavior-driven development process. We introduce the details of the extended methodology and discuss its advantages and disadvantages in detail.},
booktitle = {Proceedings of the 2013 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {311–324},
numpages = {14},
location = {Ontario, Canada},
series = {CASCON '13}
}

@article{10.1007/s00766-003-0166-0,
author = {Thompson, Jeffrey M. and Heimdahl, Mats P.},
title = {Structuring product family requirements for n-dimensional and hierarchical product lines},
year = {2003},
issue_date = {February  2003},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-003-0166-0},
doi = {10.1007/s00766-003-0166-0},
abstract = {The software product-line approach (for software product families) is one of the success stories of software reuse. When applied, it can result in cost savings and increases in productivity. In addition, in safety-critical systems the approach has the potential for reuse of analysis and testing results, which can lead to a safer system. Nevertheless, there are times when it seems like a product family approach should work when, in fact, there are difficulties in properly defining the boundaries of the product family. In this paper, we draw on our experiences in applying the software product-line approach to a family of mobile robots, a family of flight guidance systems, and a family of cardiac pacemakers, as well as case studies done by others to (1) illustrate how domain structure can currently limit applicability of product-line approaches to certain domains and (2) demonstrate our progress towards a solution using a set-theoretic approach to reason about domains of what we call n-dimensional and hierarchical product families.},
journal = {Requir. Eng.},
month = feb,
pages = {42–54},
numpages = {13},
keywords = {Requirements structuring, Requirements reuse, Product line modelling, Product line engineering, Domain Engineering}
}

@inproceedings{10.5555/1885639.1885687,
author = {Belategi, Lorea and Sagardui, Goiuria and Etxeberria, Leire},
title = {MARTE mechanisms to model variability when analyzing embedded software product lines},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Nowadays, embedded systems development is increasing its complexity dealing with quality among others. Model Driven Development (MDD) and Software Product Line (SPL) can be adequate paradigms to traditional development and validation methods. MARTE (UML Profile for Modeling and Analysis of Real-Time and Embedded systems) profile facilitates model analysis thus ensuring quality achievement from models. SPL requires taking into account variability like functional, quality attributes, platform and allocation. Therefore, variability mechanisms of MARTE profile have been studied in order to perform embedded SPL model analysis.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {466–470},
numpages = {5},
keywords = {variability, software product lines, model analysis, embedded software, MARTE},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.jss.2014.10.037,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Galindo, Jos\'{e} A. and Parejo, Jos\'{e} A. and Benavides, David and Segura, Sergio and Egyed, Alexander},
title = {An assessment of search-based techniques for reverse engineering feature models},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.10.037},
doi = {10.1016/j.jss.2014.10.037},
abstract = {HighlightsSearch based techniques perform well for reverse engineering feature models.Different algorithms and objectives favour precision and recall differently.The F1 objective function provides a trade-off between precision and recall. Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on Fβ, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
journal = {J. Syst. Softw.},
month = may,
pages = {353–369},
numpages = {17},
keywords = {Search Based Software Engineering, Reverse engineering, Feature model}
}

@article{10.1016/j.jss.2009.10.011,
author = {Sun, Chang-ai and Rossing, Rowan and Sinnema, Marco and Bulanov, Pavel and Aiello, Marco},
title = {Modeling and managing the variability of Web service-based systems},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.10.011},
doi = {10.1016/j.jss.2009.10.011},
abstract = {Web service-based systems are built orchestrating loosely coupled, standardized, and internetworked programs. If on the one hand, Web services address the interoperability issues of modern information systems, on the other hand, they enable the development of software systems on the basis of reuse, greatly limiting the necessity for reimplementation. Techniques and methodologies to gain the maximum from this emerging computing paradigm are in great need. In particular, a way to explicitly model and manage variability would greatly facilitate the creation and customization of Web service-based systems. By variability we mean the ability of a software system to be extended, changed, customized or configured for use in a specific context. We present a framework and related tool suite for modeling and managing the variability of Web service-based systems for design and run-time, respectively. It is an extension of the COVAMOF framework for the variability management of software product families, which was developed at the University of Groningen. Among the novelties and advantages of the approach are the full modeling of variability via UML diagrams, the run-time support, and the low involvement of the user. All of which leads to a great deal of automation in the management of all kinds of variability.},
journal = {J. Syst. Softw.},
month = mar,
pages = {502–516},
numpages = {15},
keywords = {Web services, Variability modeling, Variability management, Service engineering}
}

@article{10.1145/1163514.1178645,
author = {Sinnema, Marco and van der Ven, Jan Salvador and Deelstra, Sybren},
title = {Using variability modeling principles to capture architectural knowledge},
year = {2006},
issue_date = {September 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1163514.1178645},
doi = {10.1145/1163514.1178645},
abstract = {In the field of software architectures, there is an emerging awareness of the importance of architectural decisions. In this view, the architecting process is explained as a decision process, while the design and eventually the software system are seen as the result of this decision process. However, the effects of different alternatives on the quality of the system often remain implicit. In the field of software product families, the same issues arise when configuring products. We propose to use the proven expertise from COVAMOF, a framework for managing variability, to solve the issues that arise when relating quality attributes to architectural decisions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {5–es},
numpages = {6},
keywords = {quality attributes, architectural knowledge, architectural decisions}
}

@article{10.1007/s10009-012-0250-1,
author = {Wong, Peter Y. and Albert, Elvira and Muschevici, Radu and Proen\c{c}a, Jos\'{e} and Sch\"{a}fer, Jan and Schlatte, Rudolf},
title = {The ABS tool suite: modelling, executing and analysing distributed adaptable object-oriented systems},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0250-1},
doi = {10.1007/s10009-012-0250-1},
abstract = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {567–588},
numpages = {22},
keywords = {Variability, Tool support, Software product line, Formal modelling and analysis, Feature modelling, Concurrency}
}

@inproceedings{10.1007/978-3-642-39031-9_10,
author = {Silva, Eduardo and Medeiros, Ana Luisa and Cavalcante, Everton and Batista, Thais},
title = {A lightweight language for software product lines architecture description},
year = {2013},
isbn = {9783642390302},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-39031-9_10},
doi = {10.1007/978-3-642-39031-9_10},
abstract = {The architecture description of a software product line (SPL) is essential to make it clear how the architecture realizes the feature model and to represent both the domain and application engineering architectural artefacts. However, most architecture description languages (ADLs) for SPL have limited support regarding variability management and they do not express the relationship between features and the architecture, besides the lack of tools for graphical and textual modelling and a non-clear separation between the domain and application engineering activities. In order to overcome these deficiencies, this paper presents LightPL-ACME, an ADL whose main goal is to be a simple, lightweight language for the SPL architecture description, and enable the association between the architectural specification and the artefacts involved in the SPL development process, including the relationship with the feature model and the representation of both domain and application engineering elements.},
booktitle = {Proceedings of the 7th European Conference on Software Architecture},
pages = {114–121},
numpages = {8},
keywords = {software product lines architectures, architecture description languages, LightPL-ACME, ACME},
location = {Montpellier, France},
series = {ECSA'13}
}

@inproceedings{10.5555/1885639.1885643,
author = {Lee, Kwanwoo and Kang, Kyo C.},
title = {Usage context as key driver for feature selection},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product derivation in software product line engineering starts with selection of variable features manifested in a feature model. Selection of variable features for a particular product, however, is not made arbitrarily. There are various factors affecting feature selection. We experienced that the usage context of a product is often the primary driver for feature selection. In this paper, we propose a model showing how product usage contexts are related to product features, and present a method for developing such a model during the domain engineering process and utilizing it to derive an optimal product configuration during the application engineering process. An elevator control software example is used to illustrate and validate the concept and the method.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {32–46},
numpages = {15},
keywords = {product usage contexts, product derivation, feature modeling, commonality and variability},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/1216262.1216265,
author = {Kabanda, Salah and Adigun, Mathew},
title = {Extending model driven architecture benefits to requirements engineering},
year = {2006},
isbn = {1595935673},
publisher = {South African Institute for Computer Scientists and Information Technologists},
address = {ZAF},
url = {https://doi.org/10.1145/1216262.1216265},
doi = {10.1145/1216262.1216265},
abstract = {This work focuses on developing a requirement engineering model (RSPL) based on a Model Driven Architecture (MDA) and Web-tier Application Framework (WAF), to support automatic and interactive requirements generation when creating families of systems. In realizing the model, two goals were targeted namely (i) to construct a RE model that support automatic transformation of domain features into actor-specific requirements; and (ii) to design and implement an interactive web based tool for requirements engineering. The result obtained is twofold: (i) adopting MDA during RE for a product line reduced costs and development time; (ii) tool implementation based on WAF ensured that support for different client types was possible. In conclusion, the study is a contribution to a recently advocated idea that requirements generation could be model-driven. The result shows that the idea is promising with respect to requirement reuse and improving communication barriers among members of a system development team.},
booktitle = {Proceedings of the 2006 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries},
pages = {22–30},
numpages = {9},
keywords = {model driven architecture, requirement specification model for product lines, software product line engineering, web-tier application framework},
location = {Somerset West, South Africa},
series = {SAICSIT '06}
}

@article{10.1007/s11219-011-9153-8,
author = {Mussbacher, Gunter and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Amyot, Daniel},
title = {AoURN-based modeling and analysis of software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9153-8},
doi = {10.1007/s11219-011-9153-8},
abstract = {Software Product Line Engineering concerns itself with domain engineering and application engineering. During domain engineering, the whole product family is modeled with a preferred flavor of feature models and additional models as required (e.g., domain models or scenario-based models). During application engineering, the focus shifts toward a single family member and the configuration of the member's features. Recently, aspectual concepts have been employed to better encapsulate individual features of a Software Product Line (SPL), but the existing body of SPL work does not include a unified reasoning framework that integrates aspect-oriented feature description artifacts with the capability to reason about stakeholders' goals while taking feature interactions into consideration. Goal-oriented SPL approaches have been proposed, but do not provide analysis capabilities that help modelers meet the needs of the numerous stakeholders involved in an SPL while at the same time considering feature interactions. We present an aspect-oriented SPL approach for the requirements phase that allows modelers (a) to capture features, goals, and scenarios in a unified framework and (b) to reason about stakeholders' needs and perform trade-off analyses while considering undesirable interactions that are not obvious from the feature model. The approach is based on the Aspect-oriented User Requirements Notation (AoURN) and helps identify, prioritize, and choose products based on analysis results provided by AoURN editor and analysis tools. We apply the AoURN-based SPL framework to the Via Verde SPL to demonstrate the feasibility of this approach through the selection of a Via Verde product configuration that satisfies stakeholders' needs and results in a high-level, scenario-based specification that is free from undesirable feature interactions.},
journal = {Software Quality Journal},
month = sep,
pages = {645–687},
numpages = {43},
keywords = {User Requirements Notation, Software product lines, Scenario-based requirements engineering, Goal-based requirements engineering, Feature interactions, Aspect-oriented modeling}
}

@inproceedings{10.1145/2430502.2430510,
author = {Lee, Hyesun and Kang, Kyo Chul},
title = {A design feature-based approach to deriving program code from features: a step towards feature-oriented software development},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430510},
doi = {10.1145/2430502.2430510},
abstract = {Feature-oriented software development is a software development paradigm that uses "features" as the first class objects in designing program instead of objects as with the object-orientation. Most of researches on feature-oriented software development attempt to derive program code directly from a feature model, which presents several problems: there is no explicit attempt to embed required quality attributes into code; and features tend to cut across program units and it is difficult to derive program units from the features. To address these problems, a design-feature-based approach is proposed in this paper. Design feature model captures implementation-level design decisions explicitly. It bridges the abstraction gap between features (i.e., functionalities in abstraction) and program units (i.e., concrete implementation). Design features of a design feature model are identified based on required quality attributes. We demonstrated the feasibility of the proposed approach by providing a case study of an arcade game software product line. Initial lessons learned and research agenda are also introduced in the paper.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {6},
keywords = {quality attribute, feature-oriented software development, design feature model},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Configurable systems, Machine learning, Software product lines, Systematic literature review}
}

@article{10.1016/j.jss.2013.06.034,
author = {Alf\'{e}rez, G. H. and Pelechano, V. and Mazo, R. and Salinesi, C. and Diaz, D.},
title = {Dynamic adaptation of service compositions with variability models},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.034},
doi = {10.1016/j.jss.2013.06.034},
abstract = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
journal = {J. Syst. Softw.},
month = may,
pages = {24–47},
numpages = {24},
keywords = {Web service composition, Verification, Variability, Models at runtime, Dynamic software product line, Dynamic adaptation, Constraint programming, Autonomic computing}
}

@inproceedings{10.1109/PESOS.2009.5068815,
author = {Mietzner, Ralph and Metzger, Andreas and Leymann, Frank and Pohl, Klaus},
title = {Variability modeling to support customization and deployment of multi-tenant-aware Software as a Service applications},
year = {2009},
isbn = {9781424437160},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/PESOS.2009.5068815},
doi = {10.1109/PESOS.2009.5068815},
abstract = {More and more companies are offering their software by following the Software as a Service (SaaS) model. The promise of the SaaS model is to exploit economies of scale on the provider side by hosting multiple customers (or tenants) on the same hardware and software infrastructure. However, to attract a significant number of tenants, SaaS applications have to be customizable to fulfill the varying functional and quality requirements of individual tenants. In this paper, we describe how variability modeling techniques from software product line engineering can support SaaS providers in managing the variability of SaaS applications and their requirements. Specifically, we propose using explicit variability models to systematically derive customization and deployment information for individual SaaS tenants. We also demonstrate how variability models could be used to systematically consider information about already deployed SaaS applications for efficiently deploying SaaS applications for new tenants. We illustrate our approach by a running example for a meeting planning application.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Principles of Engineering Service Oriented Systems},
pages = {18–25},
numpages = {8},
series = {PESOS '09}
}

@article{10.1016/j.jss.2009.06.048,
author = {Khurum, Mahvish and Gorschek, Tony},
title = {A systematic review of domain analysis solutions for product lines},
year = {2009},
issue_date = {December, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.048},
doi = {10.1016/j.jss.2009.06.048},
abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.},
journal = {J. Syst. Softw.},
month = dec,
pages = {1982–2003},
numpages = {22},
keywords = {Usefulness, Usability, Systematic review, Empirical evidence, Domain scoping, Domain modeling, Domain analysis}
}

@inproceedings{10.1145/1868433.1868445,
author = {Trujillo, Salvador and Perez, Antonio and Gonzalez, David and Hamid, Brahim},
title = {Towards the integration of advanced engineering paradigms into RCES: raising the issues for the safety-critical model-driven product-line case},
year = {2010},
isbn = {9781450303682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868433.1868445},
doi = {10.1145/1868433.1868445},
abstract = {The conception and design of Resource Constrained Embedded Systems is an inherently complex endeavor. In particular, non-functional requirements from security, dependability and variability are exacerbating this complexity. Recent times have seen a paradigm shift in terms of design through the combination of multiple software engineering paradigms together, namely, Model Driven Engineering and Software Product Line Engineering. Such paradigm shift is changing the way systems are developed nowadays, reducing development time significantly. Embedded systems are a case in point where a range of products for assorted domains such as energy, transportation, automotive, and so on are conceived as a family. However, most of the work so far has been focused on functional parts. The purpose of this talk is to foster some discussion during the workshop on the issues that need to be faced for these techniques to be applicable for Resource Constrained Embedded Systems for which security and dependability are primary requirements.},
booktitle = {Proceedings of the International Workshop on Security and Dependability for Resource Constrained Embedded Systems},
articleno = {9},
numpages = {4},
keywords = {software product lines, resource constrained embedded systems, model-driven development, dependability},
location = {Vienna, Austria},
series = {S&amp;D4RCES '10}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/2420942.2420949,
author = {Henia, Rafik and Rioux, Laurent and Vergnaud, Thomas},
title = {Industrial adaptation of MARTE for early scheduling analysis of component-based applications},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420949},
doi = {10.1145/2420942.2420949},
abstract = {Scheduling analysis techniques are well adapted for performance estimation at early design stages of component-based applications, since they rely on an abstraction of the timing properties. These properties can be annotated to the design model using well-known standards such as the UML Profile for MARTE. However, due to its high syntax complexity, MARTE suffers from a limited acceptance in the industry. In this work, we propose an adaptation of the MARTE syntax in the context of the development of Software Design Radio systems, according to designer needs.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {7},
numpages = {2},
keywords = {real-time embedded systems, early scheduling analysis, component-based design, MyCCM, MARTE},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1016/j.jss.2007.06.002,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Industrial validation of COVAMOF},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.06.002},
doi = {10.1016/j.jss.2007.06.002},
abstract = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
journal = {J. Syst. Softw.},
month = apr,
pages = {584–600},
numpages = {17},
keywords = {Software Variability Management, Product family engineering, Industrial validation}
}

@inproceedings{10.1145/1629716.1629738,
author = {Alf\'{e}rez, Mauricio and Moreira, Ana and Kulesza, Uir\'{a} and Ara\'{u}jo, Jo\~{a}o and Mateus, Ricardo and Amaral, Vasco},
title = {Detecting feature interactions in SPL requirements analysis models},
year = {2009},
isbn = {9781605585673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629716.1629738},
doi = {10.1145/1629716.1629738},
abstract = {The consequences of unwanted feature interactions in a Software Product Line (SPL) can range from minor problems to critical software failures. However, detecting feature interactions in reasonably complex model-based SPLs is a non-trivial task. This is due to the often large number of interdependent models that describe the SPL features and the lack of support for analyzing the relationships inside those models. We believe that the early detection of the points, where two or more features interact --- based on the models that describe the behavior of the features ---, is a starting point for the detection of conflicts and inconsistencies between features, and therefore, take an early corrective action.This vision paper foresees a process to find an initial set of points where it is likely to find potential feature interactions in model-based SPL requirements, by detecting: (i) dependency patterns between features using use case models; and (ii) overlapping between use case scenarios modeled using activity models.We focus on requirements models, which are special, since they do not contain many details about the structural components and the interactions between the higher-level abstraction modules of the system. Therefore, use cases and activity models are the means that help us to analyze the functionality of a complex system looking at it from a high level end-user view to anticipate the places where there are potential feature interactions. We illustrate the approach with a home automation SPL and then discuss about its applicability.},
booktitle = {Proceedings of the First International Workshop on Feature-Oriented Software Development},
pages = {117–123},
numpages = {7},
keywords = {feature interactions, software product lines requirements},
location = {Denver, Colorado, USA},
series = {FOSD '09}
}

@article{10.1016/j.infsof.2010.03.014,
author = {Alves, Vander and Niu, Nan and Alves, Carina and Valen\c{c}a, George},
title = {Requirements engineering for software product lines: A systematic literature review},
year = {2010},
issue_date = {August, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.03.014},
doi = {10.1016/j.infsof.2010.03.014},
abstract = {Context: Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed. Objective: This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement. Method: A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009. Results: The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy. Conclusions: Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {806–820},
numpages = {15},
keywords = {Systematic literature review, Software product lines, Requirements engineering}
}

@inproceedings{10.1145/3194133.3194143,
author = {Olaechea, Rafael and Atlee, Joanne and Legay, Axel and Fahrenberg, Uli},
title = {Trace checking for dynamic software product lines},
year = {2018},
isbn = {9781450357159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194133.3194143},
doi = {10.1145/3194133.3194143},
abstract = {A key objective of self-adaptive systems is to continue to provide optimal quality of service when the environment changes. A dynamic software product line (DSPL) can benefit from knowing how its various product variants would have performed (in terms of quality of service) with respect to the recent history of inputs. We propose a family-based analysis that simulates all the product variants of a DSPL simultaneously, at runtime, on recent environmental inputs to obtain an estimate of the quality of service that each one of the product variants would have had, provided it had been executing. We assessed the efficiency of our DSPL analysis compared to the efficiency of analyzing each product individually on three case studies. We obtained mixed results due to the explosion of quality-of-service values for the product variants of a DSPL. After introducing a simple data abstraction on the values of quality-of- service variables, our DSPL analysis is between 1.4 and 7.7 times faster than analyzing the products one at a time.},
booktitle = {Proceedings of the 13th International Conference on Software Engineering for Adaptive and Self-Managing Systems},
pages = {69–75},
numpages = {7},
location = {Gothenburg, Sweden},
series = {SEAMS '18}
}

@article{10.1016/j.jss.2017.11.004,
author = {Carvalho, Michelle Larissa Luciano and da Silva, Matheus Lessa Gonalves and Gomes, Gecynalda Soares da Silva and Santos, Alcemir Rodrigues and Machado, Ivan do Carmo and Souza, Magno Lu de Jesus and de Almeida, Eduardo Santana},
title = {On the implementation of dynamic software product lines},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.004},
doi = {10.1016/j.jss.2017.11.004},
abstract = {A set of criteria to characterize mechanisms suitable to implement dynamic variability.A characterization of thirteen DSPL-ready variability mechanisms.Empirical evaluation of OOP and AOP from the perspective of DSPL evolution.Evidence showing that AOP is a feasible strategy to implement DSPL projects. Dynamic Software Product Line (DSPL) engineering is a paradigm aimed at handling adaptations at runtime. An inherent challenge in DSPL engineering is to reduce the design complexity of adaptable software, particularly in terms of evolution. Existing research only recently started to investigate evolution in this field, but does not assess the impact of different implementations under software quality in evolutionary scenarios. This work presents a characterization of thirteen dynamic variability mechanisms. Based on such characterization, we implemented a DSPL using Object-oriented Programming (OOP) mechanisms. From this implementation, we evidenced that DSPL requires changes and extensions to design, in terms of functionality and adaptation capabilities. Since Aspect-oriented Programming (AOP) was well ranked according to characterization and some studies have demonstrated the likely synergies between AOP and DSPL, we decided to compare it with OOP. We empirically evaluated how OOP and AOP could affect source code quality from the viewpoint of an evolving DSPL. As a result, AOP yields better results in terms of size, SoC, cohesion, and coupling measures. Conversely, AOP provides lower change propagation impact. Although the packages in AOP were more susceptible to changes than in OOP, we could indicate that AOP may be a feasible strategy for DSPL implementation.},
journal = {J. Syst. Softw.},
month = feb,
pages = {74–100},
numpages = {27},
keywords = {Variability mechanisms, Software evolution, Evidence-based software engineering, Dynamic software product lines}
}

@inproceedings{10.1145/2973839.2973842,
author = {Lima, Crescencio and Chavez, Christina},
title = {A Systematic Review on Metamodels to Support Product Line Architecture Design},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973842},
doi = {10.1145/2973839.2973842},
abstract = {Product Line Architecture (PLA) design is a key activity for developing successful Software Product Line (SPL) projects. PLA design is a difficult task, mostly due to the complexity of the software systems that SPLs deal with, and their variabilities. Metamodels have been used to support the representation of assets that compose a PLA, SPL variability and the relationships among them. The goal of this study is to characterize the use of metamodeling on PLA design, aiming to identify the main characteristics of metamodels, the elements used for PLA and variability representation and trace the evolution of metamodels. We conducted a systematic literature review to identify the primary studies on the use of metamodels in PLA Design. Thirty-five studies that proposed metamodels to support PLA design were selected. The review main findings are: (i) it is difficult to identify the existence of research trends because the number of publication varies and metamodels lack standardization; (ii) several metamodels support feature representation; (iii) the majority of studies addressed variability representation with variation points in UML diagrams; and, (iv) five evolution lines that describe how metamodels evolved over the years were identified.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {13–22},
numpages = {10},
keywords = {Variability, Systematic Literature Review, Software Product Lines, Product Line Architecture, Metamodels},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inproceedings{10.1145/2897045.2897047,
author = {da Mota Silveira Neto, Paulo Anselmo and de Santana, Taijara Loiola and de Almeida, Eduardo Santana and Cavalcanti, Yguarata Cerqueira},
title = {RiSE events: a testbed for software product lines experimentation},
year = {2016},
isbn = {9781450341769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897045.2897047},
doi = {10.1145/2897045.2897047},
abstract = {Software Product Lines (SPL) demand mature software engineering, planning and reuse, adequate practices of management and development, and also the ability to deal with organizational issues and architectural complexity. Thus, it is important the development of new techniques, tools and methods to deal with SPL complexity required by the variability management. To address this issue, an SPL has been proposed, where the existing variability was implemented by applying conditional compilation. Moreover, no framework was used to develop it, allowing any researcher to use the SPL without losing time learning some framework. In this work, we implemented an SPL test bed containing 34 functional features has 26.457 lines of code, 1493 methods and 496 classes.},
booktitle = {Proceedings of the 1st International Workshop on Variability and Complexity in Software Design},
pages = {12–13},
numpages = {2},
keywords = {variability, test bed, software product lines, security and availability tacticts},
location = {Austin, Texas},
series = {VACE '16}
}

@inproceedings{10.1145/2897695.2897701,
author = {Abilio, Ramon and Vale, Gustavo and Figueiredo, Eduardo and Costa, Heitor},
title = {Metrics for feature-oriented programming},
year = {2016},
isbn = {9781450341776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897695.2897701},
doi = {10.1145/2897695.2897701},
abstract = {Feature-oriented programming (FOP) is a programming technique to implement software product lines based on composition mechanisms called refinements. A software product line is a set of software systems that share a common, managed set of features satisfying the specific needs of a particular market segment. The literature reports various software metrics for software product lines developed using object-oriented and aspect-oriented programming. However, after a literature review, we observed that we lack the definition of FOP-specific metrics. Based on this observation, this paper proposes a set of eight novel metrics for feature-oriented programming. These metrics were derived both from our experience in FOP and from existing software metrics. We demonstrate the applicability of the proposed metrics by applying them to a software product line.},
booktitle = {Proceedings of the 7th International Workshop on Emerging Trends in Software Metrics},
pages = {36–42},
numpages = {7},
keywords = {software quality, software product lines, software metrics, feature-oriented programming},
location = {Austin, Texas},
series = {WETSoM '16}
}

@article{10.1561/1000000053,
author = {Benveniste, Albert and Caillaud, Beno\^{\i}t and Nickovic, Dejan and Passerone, Roberto and Raclet, Jean-Baptiste and Reinkemeier, Philipp and Sangiovanni-Vincentelli, Alberto and Damm, Werner and Henzinger, Thomas A. and Larsen, Kim G.},
title = {Contracts for System Design},
year = {2018},
issue_date = {Mar 2018},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {12},
number = {2–3},
issn = {1551-3939},
url = {https://doi.org/10.1561/1000000053},
doi = {10.1561/1000000053},
abstract = {Recently, contract-based design has been proposed as an “orthogonal” approach
that complements system design methodologies proposed so far to
cope with the complexity of system design. Contract-based design provides
a rigorous scaffolding for verification, analysis, abstraction/refinement, and
even synthesis. A number of results have been obtained in this domain but
a unified treatment of the topic that can help put contract-based design in
perspective was missing. This monograph intends to provide such a treatment
where contracts are precisely defined and characterized so that they can
be used in design methodologies with no ambiguity. In particular, this monograph
identifies the essence of complex system design using contracts through
a mathematical “meta-theory”, where all the properties of the methodology
are derived from a very abstract and generic notion of contract. We show that
the meta-theory provides deep and illuminating links with existing contract
and interface theories, as well as guidelines for designing new theories. Our
study encompasses contracts for both software and systems, with emphasis
on the latter. We illustrate the use of contracts with two examples: requirement
engineering for a parking garage management, and the development of
contracts for timing and scheduling in the context of the AUTOSAR methodology
in use in the automotive sector.},
journal = {Found. Trends Electron. Des. Autom.},
month = mar,
pages = {124–400},
numpages = {281},
keywords = {System design, Contract based design, Component based design, Formal semantics, Interfaces, Embedded systems}
}

@article{10.1016/j.jss.2019.06.003,
author = {Capilla, Rafael and Fuentes, Lidia and Lochau, Malte},
title = {Software variability in dynamic environments},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {156},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.06.003},
doi = {10.1016/j.jss.2019.06.003},
journal = {J. Syst. Softw.},
month = oct,
pages = {62–64},
numpages = {3}
}

@article{10.1016/j.infsof.2006.08.001,
author = {Sinnema, Marco and Deelstra, Sybren},
title = {Classifying variability modeling techniques},
year = {2007},
issue_date = {July, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {7},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.08.001},
doi = {10.1016/j.infsof.2006.08.001},
abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {717–739},
numpages = {23},
keywords = {Variability modeling, Variability management, Software product family, Classification}
}

@inproceedings{10.1145/3297280.3297511,
author = {Allian, Ana Paula and Sena, Bruno and Nakagawa, Elisa Yumi},
title = {Evaluating variability at the software architecture level: an overview},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297511},
doi = {10.1145/3297280.3297511},
abstract = {Software architecture are designed for developing software systems needed for a diverse of business goals. Consequently, architecture has to deal with a significant amount of variability in functionality and quality attributes to create different products. Due to this variability, the evaluation in software architectures is much more complex, as different alternatives of systems might be developed leading to an expensive and time consuming task. Several methods and techniques have been proposed to evaluate product line architectures (PLAs) aiming to asses whether or not the architecture will lead to the desired quality attributes. However, there is little consensus on the existing evaluations methods is most suitable for evaluating variability in software architectures, instead of only considering PLAs. Understanding and explicitly evaluating variations in architectures is a cost-effective way of mitigating substantial risk to organizations and their software systems. Therefore, the main contribution of this research work is to present the state of the art about means for evaluating software architectures (including, PLAs, software architectures, reference and enterprise architectures) that contain variability information. We conducted a Systematic Mapping Study (SMS) to provide an overview and insight to practitioners about the most relevant techniques and methods developed for this evaluation. Results indicate that most evaluation techniques assess variability as a quality attribute in PLAs through scenario-based; however, little is known about their real effectiveness as most studies present gaps and lack of evaluation, which difficult the usage of such techniques in an industrial environment.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2354–2361},
numpages = {8},
keywords = {evaluation, software architecture, software variability, systematic mapping study},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1016/j.infsof.2011.11.009,
author = {Angelov, Samuil and Grefen, Paul and Greefhorst, Danny},
title = {A framework for analysis and design of software reference architectures},
year = {2012},
issue_date = {April, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.11.009},
doi = {10.1016/j.infsof.2011.11.009},
abstract = {Context: A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty. Objective: The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures. Method: We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication. Results: Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures. Conclusion: This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {417–431},
numpages = {15},
keywords = {Software reference architecture, Software product line architecture, Software domain architecture, Software architecture design}
}

@inproceedings{10.1145/3106237.3106252,
author = {Kn\"{u}ppel, Alexander and Th\"{u}m, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
title = {Is there a mismatch between real-world feature models and product-line research?},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106252},
doi = {10.1145/3106237.3106252},
abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {291–302},
numpages = {12},
keywords = {require constraints, model transformation, feature modeling, expressiveness, exclude constraints, cross-tree constraints, Software product lines},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.5555/2821357.2821367,
author = {Baresi, Luciano and Quinton, Cl\'{e}ment},
title = {Dynamically evolving the structural variability of dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {A Dynamic Software Product Line (dspl) is a widely used approach to handle variability at runtime, e.g., by activating or deactivating features to adapt the running configuration. With the emergence of highly configurable and evolvable systems, dspls have to cope with the evolution of their structural variability, i.e., the Feature Model (fm) used to derive the configuration. So far, little is known about the evolution of the fm while a configuration derived from this fm is running. In particular, such a dynamic evolution changes the dspl configuration space, which is thus unsynchronized with the running configuration and its adaptation capabilities. In this position paper, we propose and describe an initial architecture to manage the dynamic evolution of dspls and their synchronization. In particular, we explain how this architecture supports the evolution of dspls based on fms extended with cardinality and attributes, which, to the best of our knowledge, has never been addressed yet.},
booktitle = {Proceedings of the 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7},
location = {Florence, Italy},
series = {SEAMS '15}
}

@inproceedings{10.1145/3180155.3180257,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180257},
doi = {10.1145/3180155.3180257},
abstract = {The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1231–1242},
numpages = {12},
keywords = {multi-objective integer programming (MOIP), multi-objective optimization (MOO), optimal feature selection problem},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1561/1000000034,
author = {Sifakis, Joseph},
title = {Rigorous System Design},
year = {2013},
issue_date = {Apr 2013},
publisher = {Now Publishers Inc.},
address = {Hanover, MA, USA},
volume = {6},
number = {4},
issn = {1551-3939},
url = {https://doi.org/10.1561/1000000034},
doi = {10.1561/1000000034},
abstract = {The monograph advocates rigorous system design as a coherent and accountable model-based process leading from requirements to correct implementations. It presents the current state of the art in system design, discusses its limitations, and identifies possible avenues for overcoming them.A rigorous system design flow is defined as a formal accountable and iterative process composed of steps, and based on four principles: (1) separation of concerns; (2) component-based construction; (3) semantic coherency; and (4) correctness-by-construction. The combined application of these principles allows the definition of a methodology clearly identifying where human intervention and ingenuity are needed to resolve design choices, as well as activities that can be supported by tools to automate tedious and error-prone tasks. An implementable system model is progressively derived by source-to-source automated transformations in a single host component-based language rooted in well-defined semantics. Using a single modeling language throughout the design flow enforces semantic coherency. Correct-by-construction techniques allow well-known limitations of a posteriori verification to be overcome and ensure accountability. It is possible to explain, at each design step, which among the requirements are satisfied and which may not be satisfied.The presented view for rigorous system design has been amply implemented in the BIP (Behavior, Interaction, Priority) component framework and substantiated by numerous experimental results showing both its relevance and feasibility.The monograph concludes with a discussion advocating a systemcentric vision for computing, identifying possible links with other disciplines, and emphasizing centrality of system design.},
journal = {Found. Trends Electron. Des. Autom.},
month = apr,
pages = {293–362},
numpages = {70},
keywords = {Er Computer Engineering, Pf Semiconductors, Ee Electrical Engineering, Rigorous system design, Correctness by construction, Component-based design, Design science}
}

@inproceedings{10.5555/2818754.2818819,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Le Traon, Yves},
title = {Combining multi-objective search and constraint solving for configuring large software product lines},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p &lt; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (\^{A}12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {517–528},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3424771.3424809,
author = {de Oliveira Rosa, Thatiane and Daniel, Jo\~{a}o Francisco Lino and Guerra, Eduardo Martins and Goldman, Alfredo},
title = {A Method for Architectural Trade-off Analysis Based on Patterns: Evaluating Microservices Structural Attributes},
year = {2020},
isbn = {9781450377690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424771.3424809},
doi = {10.1145/3424771.3424809},
abstract = {Architectural patterns are powerful tools that assist software architects in the decision-making process, as they enable to identify the software domain, to satisfy quality attributes, and to create large-scale reuse design techniques. However, choosing the most appropriate patterns for a given project is a difficult task, because while there is a wide range of patterns, there is a lack of knowledge about them, among software architects. In order to mitigate this problem, we developed a systematic method for architectural trade-off analysis based on patterns. In order to demonstrate the application of this method, we conducted a study to identify architectural patterns of microservices that influence structural design decisions related to the size of services, database sharing, and level of service coupling. All in all, this method helps software architects to identify and understand the patterns that best suit the project needs, and that can guide the architecture in the desired direction.},
booktitle = {Proceedings of the European Conference on Pattern Languages of Programs 2020},
articleno = {35},
numpages = {8},
keywords = {trade-off analysis, systematic method, microservice, architectural patterns},
location = {Virtual Event, Germany},
series = {EuroPLoP '20}
}

@inproceedings{10.1109/ICSE.2019.00092,
author = {Lazreg, Sami and Cordy, Maxime and Collet, Philippe and Heymans, Patrick and Mosser, S\'{e}bastien},
title = {Multifaceted automated analyses for variability-intensive embedded systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00092},
doi = {10.1109/ICSE.2019.00092},
abstract = {Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {854–865},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1007/978-3-642-04211-9_19,
author = {Rossel, Pedro O. and Perovich, Daniel and Bastarrica, Mar\'{\i}a Cecilia},
title = {Reuse of Architectural Knowledge in SPL Development},
year = {2009},
isbn = {9783642042102},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-04211-9_19},
doi = {10.1007/978-3-642-04211-9_19},
abstract = {Software Product Lines (SPL) promote reuse within an application domain in an organized fashion. Preimplemented software components are arranged according to a product line architecture (PLA). Balancing possibly conflicting quality attributes of all potential products makes PLA design a challenging task. Moreover, if quality attributes are part of the variabilities of the SPL, then a unique PLA may result highly inconvenient for particular configurations. We consider the PLA as a set of architectural decisions organized by the features in the Feature Model. A particular product architecture (PA) is defined as the subset of decisions associated to the chosen features for the product. Architectural knowledge is then reused among products and when new features are required in the SPL. Variability at the quality attribute level will impact the style of the resulting architecture, thus choosing different quality features will produce PAs following different styles, even within the same SPL. We use MDE techniques to operationalize this procedure and we illustrate the technique using the case of a Meshing Tool SPL.},
booktitle = {Proceedings of the 11th International Conference on Software Reuse: Formal Foundations of Reuse and Domain Engineering},
pages = {191–200},
numpages = {10},
location = {Falls Church, Virginia},
series = {ICSR '09}
}

@inproceedings{10.1145/1385486.1385488,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Schirmeier, Horst and Sincero, Julio and Apel, Sven and Leich, Thomas and Spinczyk, Olaf and Saake, Gunter},
title = {FAME-DBMS: tailor-made data management solutions for embedded systems},
year = {2008},
isbn = {9781595939647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385486.1385488},
doi = {10.1145/1385486.1385488},
abstract = {Data management functionality is not only needed in large-scale server systems, but also in embedded systems. Resource restrictions and heterogeneity of hardware, however, complicate the development of data management solutions for those systems. In current practice, this typically leads to the redevelopment of data management because existing solutions cannot be reused and adapted appropriately. In this paper, we present our ongoing work on FAME-DBMS, a research project that explores techniques to implement highly customizable data management solutions, and illustrate how such systems can be created with a software product line approach. With this approach a concrete instance of a DBMS is derived by composing features of the DBMS product line that are needed for a specific application scenario. This product derivation process is getting complex if a large number of features is available. Furthermore, in embedded systems also non-functional properties, e.g., memory consumption, have to be considered when creating a DBMS instance. To simplify the derivation process we present approaches for its automation.},
booktitle = {Proceedings of the 2008 EDBT Workshop on Software Engineering for Tailor-Made Data Management},
pages = {1–6},
numpages = {6},
location = {Nantes, France},
series = {SETMDM '08}
}

@inproceedings{10.1145/2188286.2188304,
author = {Tawhid, Rasha and Petriu, Dorina},
title = {User-friendly approach for handling performance parameters during predictive software performance engineering},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188304},
doi = {10.1145/2188286.2188304},
abstract = {A Software Product Line (SPL) is a set of similar software systems that share a common set of features. Instead of building each product from scratch, SPL development takes advantage of the reusability of the core assets shared among the SPL members. In this work, we integrate performance analysis in the early phases of SPL development process, applying the same reusability concept to the performance annotations. Instead of annotating from scratch the UML model of every derived product, we propose to annotate the SPL model once with generic performance annotations. After deriving the model of a product from the family model by an automatic transformation, the generic performance annotations need to be bound to concrete product-specific values provided by the developer. Dealing manually with a large number of performance annotations, by asking the developer to inspect every diagram in the generated model and to extract these annotations is an error-prone process. In this paper we propose to automate the collection of all generic parameters from the product model and to present them to the developer in a user-friendly format (e.g., a spreadsheet per diagram, indicating each generic parameter together with guiding information that helps the user in providing concrete binding values). There are two kinds of generic parametric annotations handled by our approach: product-specific (corresponding to the set of features selected for the product) and platform-specific (such as device choices, network connections, middleware, and runtime environment). The following model transformations for (a) generating a product model with generic annotations from the SPL model, (b) building the spreadsheet with generic parameters and guiding information, and (c) performing the actual binding are all realized in the Atlas Transformation Language (ATL).},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {109–120},
numpages = {12},
keywords = {uml, spl, performance model, performance completion, model-driven development, marte, atl},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@article{10.1007/s00766-015-0243-1,
author = {Pacheco, C. and Garcia, I. and Calvo-Manzano, J. A. and Arcilla, M.},
title = {Reusing functional software requirements in small-sized software enterprises: a model oriented to the catalog of requirements},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {22},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-015-0243-1},
doi = {10.1007/s00766-015-0243-1},
abstract = {Software reuse can be defined as the process of creating software products from the existing ones rather than developing software from scratch. Thus, software reuse is normally proposed to increase software productivity and quality and leads to economic benefits. In this sense, the reuse of software requirements has received important attention because it provides a solid support to develop quality software through obtaining and reusing quality software requirements [i.e., software product line (SPL) approach used in large-sized software enterprises]. However, the small-sized enterprises--which represent up to 85 % of all software organizations in many countries around the world--cannot implement a SPL approach because it does not fit with the context, properties, and complexity of their software projects. Moreover, the software engineering community has not adequately explored a more proper approach in the context of small-sized software enterprises. The use of a software requirements catalog could be this proper approach. In this context, the aim of this paper was to introduce the requirements reuse model for software requirements catalog (RRMSRC). Also, a set of guidelines to perform the main activities defined for reusing functional requirements within small-sized software enterprises is provided. As evidence of its feasibility, RRMSRC has been used in an industrial context, and the obtained results and learned lessons are summarized.},
journal = {Requir. Eng.},
month = jun,
pages = {275–287},
numpages = {13},
keywords = {Small-sized software enterprises, Requirements reuse process, Requirements engineering, Functional requirements catalog}
}

@inproceedings{10.1145/1017753.1017782,
author = {Stankovic, John A. and Nagaraddi, Prashant and Yu, Zhendong and He, Zhimin and Ellis, Brian},
title = {Exploiting prescriptive aspects: a design time capability},
year = {2004},
isbn = {1581138601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1017753.1017782},
doi = {10.1145/1017753.1017782},
abstract = {Aspect oriented programming (AOP), when used well, has many advantages. Aspects are however, programming-time constructs, i.e., they relate to source code. Previously, we developed a tool called VEST that extended aspects to design time for embedded systems. Two types of design time aspects were identified which we labeled aspect checks and prescriptive aspects. In the original VEST tool several keys aspect checks and a simple form of prescriptive aspects were implemented. Prescriptive aspects are extremely powerful and result in many design time advantages and uses. This paper enhances and exploits the concept of prescriptive aspects well beyond its original purpose and results. A new prescriptive language is developed and implemented in the VEST tool. We also use prescriptive aspects in a case study for an avionics application and evaluate its benefits. The result is a tool with significant and new features for building distributed real-time embedded systems. It is shown in the case study that design time is shortened by 69%.},
booktitle = {Proceedings of the 4th ACM International Conference on Embedded Software},
pages = {165–174},
numpages = {10},
keywords = {prescriptive aspects, component-based design, aspects},
location = {Pisa, Italy},
series = {EMSOFT '04}
}

@article{10.1007/s11219-005-4250-1,
author = {Kazman, Rick and Bass, Len and Klein, Mark and Lattanze, Tony and Northrop, Linda},
title = {A Basis for Analyzing Software Architecture Analysis Methods},
year = {2005},
issue_date = {December  2005},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {13},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-005-4250-1},
doi = {10.1007/s11219-005-4250-1},
abstract = {A software architecture is a key asset for any organization that builds complex software-intensive systems. Because of an architecture's central role as a project blueprint, organizations should analyze the architecture before committing resources to it. An analysis helps to ensure that sound architectural decisions are made. Over the past decade a large number of architecture analysis methods have been created, and at least two surveys of these methods have been published. This paper examines the criteria for analyzing architecture analysis methods, and suggests a new set of criteria that focus on the essence of what it means to be an architecture analysis method. These criteria could be used to compare methods, to help understand the suitability of a method, or to improve a method. We then examine two methods--the Architecture Tradeoff Analysis Method and Architecture-level Modifiability Analysis--in light of these criteria, and provide some insight into how these methods can be improved.},
journal = {Software Quality Journal},
month = dec,
pages = {329–355},
numpages = {27},
keywords = {software architecture, quality attributes, architecture analysis, analysis methods}
}

@article{10.1016/j.infsof.2021.106620,
author = {Tran, Huynh Khanh Vi and Unterkalmsteiner, Michael and B\"{o}rstler, J\"{u}rgen and Ali, Nauman bin},
title = {Assessing test artifact quality—A tertiary study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106620},
doi = {10.1016/j.infsof.2021.106620},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {22},
keywords = {Quality assurance, Test artifact quality, Test suite quality, Test case quality, Software testing}
}

@article{10.1016/j.cl.2018.05.004,
author = {Combemale, Benoit and Kienzle, J\"{o}rg and Mussbacher, Gunter and Barais, Olivier and Bousse, Erwan and Cazzola, Walter and Collet, Philippe and Degueule, Thomas and Heinrich, Robert and J\'{e}z\'{e}quel, Jean-Marc and Leduc, Manuel and Mayerhofer, Tanja and Mosser, S\'{e}bastien and Sch\"{o}ttle, Matthias and Strittmatter, Misha and Wortmann, Andreas},
title = {Concern-oriented language development (COLD): Fostering reuse in language engineering},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.05.004},
doi = {10.1016/j.cl.2018.05.004},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {139–155},
numpages = {17},
keywords = {Language reuse, Language concern, Domain-specific languages}
}

@article{10.1007/s10270-016-0569-2,
author = {Al-Hajjaji, Mustafa and Th\"{u}m, Thomas and Lochau, Malte and Meinicke, Jens and Saake, Gunter},
title = {Effective product-line testing using similarity-based product prioritization},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0569-2},
doi = {10.1007/s10270-016-0569-2},
abstract = {A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {499–521},
numpages = {23},
keywords = {Test-case prioritization, Software product lines, Product-line testing, Model-based testing, Combinatorial interaction testing}
}

@article{10.1007/s11219-014-9258-y,
author = {Galindo, Jos\'{e} A. and Turner, Hamilton and Benavides, David and White, Jules},
title = {Testing variability-intensive systems using automated analysis: an application to Android},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9258-y},
doi = {10.1007/s11219-014-9258-y},
abstract = {Software product lines are used to develop a set of software products that, while being different, share a common set of features. Feature models are used as a compact representation of all the products (e.g., possible configurations) of the product line. The number of products that a feature model encodes may grow exponentially with the number of features. This increases the cost of testing the products within a product line. Some proposals deal with this problem by reducing the testing space using different techniques. However, a daunting challenge is to explore how the cost and value of test cases can be modeled and optimized in order to have lower-cost testing processes. In this paper, we present TESting vAriAbiLity Intensive Systems (TESALIA), an approach that uses automated analysis of feature models to optimize the testing of variability-intensive systems. We model test value and cost as feature attributes, and then we use a constraint satisfaction solver to prune, prioritize and package product line tests complementing prior work in the software product line testing literature. A prototype implementation of TESALIA is used for validation in an Android example showing the benefits of maximizing the mobile market share (the value function) while meeting a budgetary constraint.},
journal = {Software Quality Journal},
month = jun,
pages = {365–405},
numpages = {41},
keywords = {Testing, Software product lines, Feature models, Automated analysis, Android}
}

@article{10.1016/j.infsof.2006.05.003,
author = {Olumofin, Femi G. and Mi\v{s}i\'{c}, Vojislav B.},
title = {A holistic architecture assessment method for software product lines},
year = {2007},
issue_date = {April, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.05.003},
doi = {10.1016/j.infsof.2006.05.003},
abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {309–323},
numpages = {15},
keywords = {Software product line architectures, Software architecture assessment, Architecture Tradeoff Analysis Method (ATAM)}
}

@article{10.1007/s10664-014-9336-6,
author = {Sobernig, Stefan and Apel, Sven and Kolesnikov, Sergiy and Siegmund, Norbert},
title = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9336-6},
doi = {10.1007/s10664-014-9336-6},
abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition--it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features  in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1670–1705},
numpages = {36},
keywords = {Structural coupling, Structural cohesion, Software product lines, Software measurement, Fuji, Feature-oriented programming}
}

@article{10.1007/s11219-018-9424-8,
author = {Alkharabsheh, Khalid and Crespo, Yania and Manso, Esperanza and Taboada, Jos\'{e} A.},
title = {Software Design Smell Detection: a systematic mapping study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9424-8},
doi = {10.1007/s11219-018-9424-8},
abstract = {Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18&nbsp;years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.},
journal = {Software Quality Journal},
month = sep,
pages = {1069–1148},
numpages = {80},
keywords = {Systematic mapping study, Quality models, Detection tools, Antipatterns, DesignSmell}
}

@inproceedings{10.1145/2915970.2915985,
author = {Wnuk, Krzysztof and Kollu, Ravichandra Kumar},
title = {A systematic mapping study on requirements scoping},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2915985},
doi = {10.1145/2915970.2915985},
abstract = {Context: Requirements scoping is one of the key activities in requirements management but also a major risk for project management. Continuously changing scope may create a congestion state in handling the requirements inflow which causes negative consequences, e.g. delays or scope creep. Objectives: In this paper, we look at requirements scoping literature outside Software Product Line (SPL) by exploring the current literature on the phenomenon, summarizing publication trends, performing thematic analysis and analyzing the strength of the evidence in the light of rigor and relevance assessment. Method: We run a Systematic Mapping Study (SMS) using snowballing procedure, supported by a database search for the start set identification, and identified 21 primary studies and 2 secondary studies. Results: The research interest in this area steadily increases and includes mainly case studies, validation or evaluation studies. The results were categorized into four themes: definitions, negative effects associated with scoping, challenges and identified methods/tools. The identified scope management techniques are also matched against the identified requirements scoping challenges.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {32},
numpages = {11},
keywords = {systematic mapping study, snowballing, requirements scoping},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.1145/2304736.2304757,
author = {Pascual, Gustavo Garc\'{\i}a and Alarc\'{o}n, M\'{o}nica Pinto and Fern\'{a}ndez, Lidia Fuentes},
title = {Component and aspect-based service product line for pervasive systems},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304736.2304757},
doi = {10.1145/2304736.2304757},
abstract = {Pervasive systems have experienced an increase in demand due to the evolution and popularity of mobile devices and embedded systems. The development of applications for these systems imposes new challenges due to the necessity of adapting these applications both to the changes in the environment and to the resource-constrained devices (e.g. limited battery, memory and CPU) in which they run. These challenges are: (1) the same services are required by most applications for pervasive systems, and thus should be modeled as separate, ready-to-use (re)usable solutions; (2) services need to be customized to the requirements of applications, by generating different versions of the same service containing only the required functionality, and (3) the same service needs to be customized to the different devices in which a same application will run (e.g. with different operating systems, different memory and CPU capacities or different communication technologies). In order to consider all of the above challenges, in this paper we present a software product line approach that permits modelling the variability of these services using feature models, automatically generating different configurations of their software architecture depending on the particular requirements of each application. We use this approach to model typical services of pervasive systems, such as context-awareness and communication, and to evaluate the degree of variability, of reuse and of separation of concerns of these services.},
booktitle = {Proceedings of the 15th ACM SIGSOFT Symposium on Component Based Software Engineering},
pages = {115–124},
numpages = {10},
keywords = {spl, pervasive systems, context-awareness, cbse, aosd},
location = {Bertinoro, Italy},
series = {CBSE '12}
}

@article{10.1016/j.jss.2005.02.028,
author = {Feng, Qian and Lutz, Robyn R.},
title = {Bi-directional safety analysis of product lines},
year = {2005},
issue_date = {November 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {78},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2005.02.028},
doi = {10.1016/j.jss.2005.02.028},
abstract = {As product-line engineering becomes more widespread, more safety-critical software product lines are being built. This paper describes a structured method for performing safety analysis on a software product line, building on standard product-line assets: product-line requirements, architecture, and scenarios. The safety-analysis method is bi-directional in that it combines a forward analysis (from failure modes to effects) with a backward analysis (from hazards to contributing causes). Safety-analysis results are converted to XML files to allow automated consistency checking between the forward and backward analysis results and to support reuse of the safety-analysis results throughout the product line. The paper demonstrates and evaluates the method on a safety-critical product-line subsystem, the Door Control System. Results show that the bi-directional safety-analysis method found both missing and incorrect software safety requirements. Some of the new safety requirements affected all the systems in the product line while others affected only some of the systems in the product line. The results demonstrate that the proposed method can handle the challenges to safety analysis posed by variations within a product line.},
journal = {J. Syst. Softw.},
month = nov,
pages = {111–127},
numpages = {17},
keywords = {XML, Software safety, Software architecture, Reuse, Product lines}
}

@article{10.4018/JITR.2018010104,
author = {Vegendla, Aparna and Duc, Anh Nguyen and Gao, Shang and Sindre, Guttorm},
title = {A Systematic Mapping Study on Requirements Engineering in Software Ecosystems},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {11},
number = {1},
issn = {1938-7857},
url = {https://doi.org/10.4018/JITR.2018010104},
doi = {10.4018/JITR.2018010104},
abstract = {Software ecosystems SECOs and open innovation processes have been claimed as a way forward for the software industry. A proper understanding of requirements is as important for SECOs as for more traditional ones. This article presents a mapping study on the issues of RE and quality aspects in SECOs. Our findings indicate that among the various phases or subtasks of RE, most of the SECO specific research has been accomplished on elicitation, analysis, and modeling. On the other hand, requirement selection, prioritization, verification, and traceability has attracted few published studies. Among the various quality attributes, most of the SECOs research has been performed on security, performance and testability. On the other hand, reliability, safety, maintainability, transparency, usability attracted few published studies. The article provides a review of the academic literature about SECO-related RE activities, modeling approaches, and quality attributes, positions the source publications in a taxonomy of issues and identifies gaps where there has been little research.},
journal = {J. Inf. Technol. Res.},
month = jan,
pages = {49–69},
numpages = {21},
keywords = {Software Ecosystem, Requirements Engineering, Mapping Study}
}

@article{10.1016/j.datak.2006.06.009,
author = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
title = {Managing requirements conflicts in software product lines: A goal and scenario based approach},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {3},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2006.06.009},
doi = {10.1016/j.datak.2006.06.009},
abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed.},
journal = {Data Knowl. Eng.},
month = jun,
pages = {417–432},
numpages = {16},
keywords = {Syntactic and semantic requirements conflict detection, Software product line, Requirements partitioning, Requirements conflicts, Goal and scenario authoring}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured model-based mutation analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, mutation analysis, featured transition systems},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/3432601.3432616,
author = {Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.},
title = {The weakest link: revealing and modeling the architectural patterns of microservice applications},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\'{a}si-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {software vulnerability, microservice, cloud-native application, application topology},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1007/978-3-642-33666-9_34,
author = {Vierhauser, Michael and Gr\"{u}nbacher, Paul and Heider, Wolfgang and Holl, Gerald and Lettner, Daniela},
title = {Applying a consistency checking framework for heterogeneous models and artifacts in industrial product lines},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_34},
doi = {10.1007/978-3-642-33666-9_34},
abstract = {Product line engineering relies on heterogeneous models and artifacts to define and implement the product line's reusable assets. The complexity and heterogeneity of product line artifacts as well as their interdependencies make it hard to maintain consistency during development and evolution, regardless of the modeling approaches used. Engineers thus need support for detecting and resolving inconsistencies within and between the various artifacts. In this paper we present a framework for checking and maintaining consistency of arbitrary product line artifacts. Our approach is flexible and extensible regarding the supported artifact types and the definition of constraints. We discuss tool support developed for the DOPLER product line tool suite. We report the results of applying the approach to sales support applications of industrial product lines.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {531–545},
numpages = {15},
keywords = {sales support, model-based product lines, consistency checking},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1007/s11219-011-9170-7,
author = {Acher, Mathieu and Collet, Philippe and Gaignard, Alban and Lahire, Philippe and Montagnat, Johan and France, Robert B.},
title = {Composing multiple variability artifacts to assemble coherent workflows},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9170-7},
doi = {10.1007/s11219-011-9170-7},
abstract = {The development of scientific workflows is evolving toward the systematic use of service-oriented architectures, enabling the composition of dedicated and highly parameterized software services into processing pipelines. Building consistent workflows then becomes a cumbersome and error-prone activity as users cannot manage such large-scale variability. This paper presents a rigorous and tooled approach in which techniques from Software Product Line (SPL) engineering are reused and extended to manage variability in service and workflow descriptions. Composition can be facilitated while ensuring consistency. Services are organized in a rich catalog which is organized as a SPL and structured according to the common and variable concerns captured for all services. By relying on sound merging techniques on the feature models that make up the catalog, reasoning about the compatibility between connected services is made possible. Moreover, an entire workflow is then seen as a multiple SPL (i.e., a composition of several SPLs). When services are configured within, the propagation of variability choices is then automated with appropriate techniques and the user is assisted in obtaining a consistent workflow. The approach proposed is completely supported by a combination of dedicated tools and languages. Illustrations and experimental validations are provided using medical imaging pipelines, which are representative of current scientific workflows in many domains.},
journal = {Software Quality Journal},
month = sep,
pages = {689–734},
numpages = {46},
keywords = {Software product lines, Scientific workflows, Feature models, Composition}
}

@article{10.1016/j.jss.2019.01.044,
author = {Th\"{u}m, Thomas and Kn\"{u}ppel, Alexander and Kr\"{u}ger, Stefan and Bolle, Stefanie and Schaefer, Ina},
title = {Feature-oriented contract composition},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.01.044},
doi = {10.1016/j.jss.2019.01.044},
journal = {J. Syst. Softw.},
month = jun,
pages = {83–107},
numpages = {25},
keywords = {Formal methods, Deductive verification, Design by contract, Software product lines, Feature-oriented programming}
}

@inproceedings{10.1145/2701319.2701335,
author = {Gamez, Nadia and El Haddad, Joyce and Fuentes, Lidia},
title = {Managing the Variability in the Transactional Services Selection},
year = {2015},
isbn = {9781450332736},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701319.2701335},
doi = {10.1145/2701319.2701335},
abstract = {Web service composition is the capability to recursively construct a value added service by means of picking up existing services. An important step in the composition process is the selection step, which includes choosing services located in repositories. The selection approaches of Web services need to consider their specifics which raises important challenges as the management of the inherent service variability in functionality and implementation and ensuring correct execution termination between others. To realize reliable service compositions, transactional properties of services must be considered during the selection step. We argue that the transactional properties should be considered at the operation level of each service to be composed. However, modelling transactional services composition at the operation level drastically increment the complexity of service selection. In order to overcome this difficulty, in this paper we report on our research in progress on transactional service selection, which follows a Software Product Line approach considering the set of services that provide the same functionality as part of a service family. We model the variable operations of the service families using Feature Models. In this way, the selection process consists of selecting each service from a service family such that the aggregated transactional property satisfies the user preference.},
booktitle = {Proceedings of the 9th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {88–95},
numpages = {8},
keywords = {Transactional Services, Feature Modeling, Discovery and Selection},
location = {Hildesheim, Germany},
series = {VaMoS '15}
}

@article{10.1016/j.scico.2012.06.002,
author = {Th\"{u}m, Thomas and K\"{a}stner, Christian and Benduhn, Fabian and Meinicke, Jens and Saake, Gunter and Leich, Thomas},
title = {FeatureIDE: An extensible framework for feature-oriented software development},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {79},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.002},
doi = {10.1016/j.scico.2012.06.002},
abstract = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
journal = {Sci. Comput. Program.},
month = jan,
pages = {70–85},
numpages = {16},
keywords = {Tool support, Software product lines, Preprocessors, Feature-oriented software development, Feature-oriented programming, Feature modeling, Delta-oriented programming, Aspect-oriented programming}
}

@inproceedings{10.1007/978-3-319-27343-3_1,
author = {Braubach, Lars and Pokahr, Alexander and Kalinowski, Julian and Jander, Kai},
title = {Tailoring Agent Platforms with Software Product Lines},
year = {2015},
isbn = {9783319273426},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-27343-3_1},
doi = {10.1007/978-3-319-27343-3_1},
abstract = {Agent platforms have been conceived traditionally as middleware, helping to deal with various application challenges like agent programming models, remote messaging, and coordination protocols. A\"{\i} \'{z}middleware is typically a bundle of functionalities necessary to execute multi-agent applications. In contrast to this traditional view, nowadays different use cases also for selected agent concepts have emerged requiring also different kinds of functionalities. Examples include a platform for conducting multi-agent simulations, intelligent agent behavior models for controlling non-player characters NPCs in games and a lightweight version suited for mobile devices. A one-size-fits-all software bundle often does not sufficiently match these requirements, because customers and developers want solutions specifically tailored to their needs, i.e. a small but focused solution is frequently preferred over bloated software with extraneous functionality. Software product lines are an approach suitable for creating a series of similar products from a common code base. In this paper we will show how software product line modeling and technology can help creating tailor-made products from multi-agent platforms. Concretely, the Jadex platform will be analyzed and a feature model as well as an implementation path will be presented.},
booktitle = {Revised Selected Papers of the 13th German Conference on Multiagent System Technologies - Volume 9433},
pages = {3–21},
numpages = {19},
location = {Cottbus, Germany},
series = {MATES 2015}
}

@inproceedings{10.1145/568760.568805,
author = {Johansson, Enrico and H\"{o}st, Martin},
title = {Tracking degradation in software product lines through measurement of design rule violations},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568805},
doi = {10.1145/568760.568805},
abstract = {In order to increase reuse, a number of product versions may be developed based on the same software platform. The platform must, however, be managed and updated according to new requirements if it should be reusable in a series of releases. This means that the platform is constantly changed during its lifecycle, and changes can result in degradation of the platform. In this paper, a measurement approach is proposed as a means of tracking the degradation of a software platform and consequently in the product line. The tracking approach is evaluated in a case study where it is applied to a series of different releases of a product. The result of the case study indicates that the presented approach is feasible.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {249–254},
numpages = {6},
keywords = {software product line, software platform, project tracking, graph, design rules, degradation},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.5555/2667025.2667027,
author = {Siegmund, Norbert and Mory, Maik and Feigenspan, Janet and Saake, Gunter and Nykolaychuk, Mykhaylo and Schumann, Marco},
title = {Interoperability of non-functional requirements in complex systems},
year = {2012},
isbn = {9781467318532},
publisher = {IEEE Press},
abstract = {Heterogeneity of embedded systems leads to the development of variable software, such as software product lines. From such a family of programs, stakeholders select the specific variant that satisfies their functional requirements. However, different functionality exposes different non-functional properties of these variants. Especially in the embedded-system domain, non-functional requirements are vital, because resources are scarce. Hence, when selecting an appropriate variant, we have to fulfill also non-functional requirements. Since more systems are interconnected, the challenge is to find a variant that additionally satisfies global nonfunctional (or quality) requirements. In this paper, we advert the problem of achieving interoperability of non-functional requirements among multiple interacting systems using a real-world scenario. Furthermore, we show an approach to find optimal variants for multiple systems that reduces computation effort by means of a stepwise configuration process.},
booktitle = {Proceedings of the Second International Workshop on Software Engineering for Embedded Systems},
pages = {2–8},
numpages = {7},
location = {Zurich, Switzerland},
series = {SEES '12}
}

@inproceedings{10.1145/3180155.3180163,
author = {Guo, Jianmei and Shi, Kai},
title = {To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180163},
doi = {10.1145/3180155.3180163},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1027–1038},
numpages = {12},
keywords = {constraint solving, multi-objective evolutionary algorithms, search-based software engineering, software product lines, validity},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3330204.3330251,
author = {Sorgatto, Doglas W. and Paiva, D\'{e}bora M. B. and Cagnin, Maria Istela},
title = {Requirement Reuse in Business Processes Lines: Reutiliza\c{c}\~{a}o de requisitos em linhas de processos de neg\'{o}cio},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330251},
doi = {10.1145/3330204.3330251},
abstract = {The cost reduction in the Requirement Engineering process finds in business process modeling a way of aligning business goals with software requirements, and for development companies that have development demands in the same domain, greater savings can be found with the adoption of Business Process Lines (BPL). From this perspective, this paper presents the ARReq, which is an approach that allows the elicitation, specification and reuse of requirements from BPLs. It has been defined to provide quality attributes, suggested by ISO/IEC 29.148, to the functional, non-functional requirements and business rules elicited with the support of any elicitation technique applicable to BPMN business process models. A qualitative analysis was carried out and allowed to observe that ARReq is scalable, has low coupling with BPLs management approaches, besides specifying the requirements reused in the formats of user stories and requirements document and to provide a traceability matrix to support the software maintainability.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {41},
numpages = {8},
keywords = {Requirement reuse, Business Processes Line, BPMN},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/1944892.1944899,
author = {Galster, Matthias and Avgeriou, Paris},
title = {The notion of variability in software architecture: results from a preliminary exploratory study},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944899},
doi = {10.1145/1944892.1944899},
abstract = {Context: In the software product line domain, the concept of variability is well recognized. However, variability in the context of software architecture still seems to be poorly understood. Objective: In this paper, we aim at contributing to the development of a basic understanding of the notion of variability in the software architecture domain, beyond the idea of product lines. Method: We perform a preliminary exploratory study which consists of two parts: an expert survey among 11 subjects, and a mini focus group with 4 participants. For both parts, we collect and analyze mostly qualitative data. Results: Our observations indicate that there seems to be no common understanding of "variability" in the context of software architecture. On the other hand, some challenges related to variability in software architecture are similar to challenges identified in the product line domain. Conclusions: Variability in software architecture might require more theoretical foundations in order to establish "variability" as an architectural key concept and first-class quality attribute.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {59–67},
numpages = {9},
keywords = {variability, software architecture, questionnaire, product lines, mini focus group},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1007/s11219-016-9346-2,
author = {Kim, Doohwan and Hong, Jang-Eui and Chung, Lawrence},
title = {Investigating relationships between functional coupling and the energy efficiency of embedded software},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9346-2},
doi = {10.1007/s11219-016-9346-2},
abstract = {Software coupling involves dependencies among pieces of software called modules. Different types of coupling will dictate the manner whereby software modules interact and will result in different approaches to mutual function calls and return values, which can affect software quality attributes. Undoubtedly, coupling has been one of the most critical factors for supporting software modularity because it affects such important software quality attributes as reusability, readability, and maintainability. It is no surprise that coupling can affect energy efficiency. Recently, energy efficiency has increasingly been recognized as a critical software quality attribute, particularly for embedded software, including smartphone applications. Unfortunately, few studies have been conducted to date concerning coupling in developing energy-efficient and modular software, other than general studies on energy consumption and resource overutilization in the context of modularity. In this study, we aim to investigate the relationship between energy consumption and software coupling. In particular, we aim to determine whether it is possible to control energy consumption by applying different types of software coupling and, if so, how this might be done. We have performed a large number of experiments from which we have gained insight, although that insight might not be applicable to all possible types of coupling that are feasible, to help guide software engineers in developing energy-efficient embedded software. From the experimental results, we observe that overall "data" coupling reduces energy consumption when a large amount of data must be passed from one module to another, whereas "common" coupling is preferred when continuous memory references are needed, although energy consumption can also be somewhat dependent upon the operating environment. We describe such insights into the relationship between energy consumption and software coupling.},
journal = {Software Quality Journal},
month = jun,
pages = {491–519},
numpages = {29},
keywords = {Software quality, Software coupling, Experimental investigation, Energy consumption, Design decision}
}

@inproceedings{10.1007/978-3-642-37422-7_26,
author = {Adam, Sebastian and Schmid, Klaus},
title = {Effective requirements elicitation in product line application engineering: an experiment},
year = {2013},
isbn = {9783642374210},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37422-7_26},
doi = {10.1007/978-3-642-37422-7_26},
abstract = {[Context &amp; Motivation] Developing new software systems based on a software product line (SPL) is still a time-consuming task and the benefits of using such an approach are often smaller than expected. One important reason for this are difficulties in systematically mapping customer requirements to characteristics of the SPL. [Question/problem] Even though it has been recognized that the success of reuse strongly depends on how requirements are treated, it remains unclear how to perform this in an optimal way. [Principal ideas/results] In this paper, we present a controlled experiment performed with 26 students that compared two requirements elicitation approaches when instantiating a given SPL. [Contribution] Our findings indicate that a novel, problem-oriented requirements approach that explicitly integrates the reuse of SPL requirements into the elicitation of customer-specific requirements is more effective than a traditional SPL requirements approach, which distinguishes requirements reuse and additional elicitation customer-specific requirements.},
booktitle = {Proceedings of the 19th International Conference on Requirements Engineering: Foundation for Software Quality},
pages = {362–378},
numpages = {17},
location = {Essen, Germany},
series = {REFSQ'13}
}

@article{10.1016/j.infsof.2016.08.011,
author = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed-Hassan},
title = {Automating feature model refactoring},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.011},
doi = {10.1016/j.infsof.2016.08.011},
abstract = {Display Omitted Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult.Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change.Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it.Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model.Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {138–157},
numpages = {20},
keywords = {Model transformation &amp; refactoring, Feature model refactoring}
}

@inproceedings{10.1145/1842752.1842815,
author = {Galster, Matthias},
title = {Describing variability in service-oriented software product lines},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842815},
doi = {10.1145/1842752.1842815},
abstract = {Service-oriented architectures are a standard-based and technology-independent distributed computing paradigm for discovering, binding and assembling loosely-coupled software services. Software product lines on the other hand allow a generic architecture to be configured and deployed in different instances. Product lines facilitate systematic reuse through managing variability. In this paper, we combine ideas from the service domain and the product line domain and investigate what types of variability exist in service-oriented software architectures. Moreover, we suggest a way for representing variability in service-oriented architectures by formalizing the notion of variability. To allow different viewpoints on variability, we define stakeholder roles that occur in the context of service-oriented software architectures. By applying the proposed concepts, we hope to improve variability management at the software architecture level of service-oriented systems.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {344–350},
numpages = {7},
keywords = {variability, service-oriented architectures, modeling},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.1109/QUATIC.2012.14,
author = {Gonzalez-Huerta, Javier and Insfran, Emilio and Abrahao, Silvia},
title = {A Multimodel for Integrating Quality Assessment in Model-Driven Engineering},
year = {2012},
isbn = {9780769547770},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QUATIC.2012.14},
doi = {10.1109/QUATIC.2012.14},
abstract = {The development of complex software systems following the Model-Driven Engineering (MDE) approach relies on the use of different models for describing the system (e.g., structure, behavior). These models should be specified first separately but then their inter-relationship must be established since they represent complementary aspects of the system. Besides, MDE development processes are mostly focused on functionality, and do not give proper support to the quality aspects of the system. In this paper, we present a generic multimodel and the process for its construction, allowing the representation of the different viewpoint models of a software system and the relationships among elements on these viewpoints. This multimodel is a means for integrating a quality viewpoint in MDE processes, allowing the quality attributes to become a decision factor in the choice among design decisions in transformation processes. The feasibility of this approach is illustrated through the use of the multimodel in a specific example for Software Product Line development.},
booktitle = {Proceedings of the 2012 Eighth International Conference on the Quality of Information and Communications Technology},
pages = {251–254},
numpages = {4},
keywords = {Software Product Lines, Quality Atttributes, Model Driven Development},
series = {QUATIC '12}
}

@article{10.1007/s00766-014-0211-1,
author = {Djouab, Rachida and Abran, Alain and Seffah, Ahmed},
title = {An ASPIRE-based method for quality requirements identification from business goals},
year = {2016},
issue_date = {March     2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {1},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-014-0211-1},
doi = {10.1007/s00766-014-0211-1},
abstract = {Quality requirements are the main drivers for modeling and evaluating software quality at an early stage, and ASPIRE is an engineering method designed to elicit and document the quality requirements of embedded systems. This paper proposes an extension to ASPIRE to identify quality requirements from the business goals of the organization and ensure their traceability. This extension includes a set of added components created from the main concepts of the SOQUAREM methodology, including the BMM (business motivation model), derivation rules, the quality attribute utility tree, the quality attribute scenario template, the quality attribute documentation template, and ISO 9126. The applicability of the extended method is illustrated with a wireless plant control system as an example.},
journal = {Requir. Eng.},
month = mar,
pages = {87–106},
numpages = {20},
keywords = {Software quality engineering, Quality requirements (QRs), Quality attributes (QAs), QR elicitation method, Non-functional requirements (NFRs), Business goals (BGs), BMM (business motivation model)}
}

@inproceedings{10.5555/1785246.1785323,
author = {Li, Yi-Yuan and Yin, Jian-wei and Li, Yin and Dong, Jin-Xiang},
title = {Configuration modeling based software product development},
year = {2007},
isbn = {354076836X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product line is an effective way to implement software production for mass customization. How to organize and configure the software artifacts in software product line to rapidly produce customized software product meeting individual demands is one of the key problems. Corresponding to the phases of feature selection and software artifact binding in the process of software production, the feature configuration model and software artifact configuration model are constructed to provide a uniform framework of constraint description for feature model and domain application requirement. The results of problem solving are the sets of feature and software artifact meeting feature constraints and application requirements. The proposed method of configuration modeling and problem solving provide a theoretical foundation to rapidly produce software product on the base of configuration of reusable domain assets.},
booktitle = {Proceedings of the 7th International Conference on Advanced Parallel Processing Technologies},
pages = {624–639},
numpages = {16},
keywords = {software artifact configuration, problem solving, feature configuration, configuration rule},
location = {Guangzhou, China},
series = {APPT'07}
}

@inproceedings{10.1145/1837154.1837157,
author = {Siegmund, Norbert and Feigenspan, Janet and Soffner, Michael and Fruth, Jana and K\"{o}ppen, Veit},
title = {Challenges of secure and reliable data management in heterogeneous environments},
year = {2010},
isbn = {9781605589923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837154.1837157},
doi = {10.1145/1837154.1837157},
abstract = {Ubiquitous computing is getting more important since requirements for complex systems grow fast. In these systems, embedded devices have to fulfill different tasks. They have to monitor the environment, store data, communicate with other devices, and react to user input. In addition to this complexity, quality issues such as security and reliability have to be considered, as well, due to their increasing use in life critical application scenarios. Finally, different devices with different application goals are used, which results in interoperability problems. In this paper, we highlight challenges for interoperability, data management, and security, which arise with complex systems. Furthermore, we present approaches to overcome different problems and how an integrated solution can be realized using software product line techniques.},
booktitle = {Proceedings of the First International Workshop on Digital Engineering},
pages = {17–24},
numpages = {8},
keywords = {software product lines, security, digital engineering, data management},
location = {Magdeburg, Germany},
series = {IWDE '10}
}

@inproceedings{10.5555/645547.658835,
author = {Dobrica, Liliana and Niemel\"{a}, Eila},
title = {Software Architecture Quality Analysis Methods},
year = {2002},
isbn = {3540434836},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The open problem of structural methods is how to take a better advantage of software architectural concepts to analyse software systems for quality attributes in a systematic and repetitive way. Throughout the presentation we try to introduce a way of thinking founded on analysis at the architecture level of the quality attributes with the purpose to initiate and maintain a software product-line considering the quality as the main driver in product line development. This tutorial represents a study that shows the state of the research at this moment, in the quality analysis methods for software architectures, by presenting and discussing the most representative architecture analysis methods. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process.},
booktitle = {Proceedings of the 7th International Conference on Software Reuse: Methods, Techniques, and Tools},
pages = {337–338},
numpages = {2},
series = {ICSR-7}
}

@inproceedings{10.1145/2642937.2642939,
author = {Segura, Sergio and S\'{a}nchez, Ana B. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated variability analysis and testing of an E-commerce site.: an experience report},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642939},
doi = {10.1145/2642937.2642939},
abstract = {In this paper, we report on our experience on the development of La Hilandera, an e-commerce site selling haberdashery products and craft supplies in Europe. The store has a huge input space where customers can place almost three millions of different orders which made testing an extremely difficult task. To address the challenge, we explored the applicability of some of the practices for variability management in software product lines. First, we used a feature model to represent the store input space which provided us with a variability view easy to understand, share and discuss with all the stakeholders. Second, we used techniques for the automated analysis of feature models for the detection and repair of inconsistent and missing configuration settings. Finally, we used test selection and prioritization techniques for the generation of a manageable and effective set of test cases. Our findings, summarized in a set of lessons learnt, suggest that variability techniques could successfully address many of the challenges found when developing e-commerce sites.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {139–150},
numpages = {12},
keywords = {variability, feature modelling, experience report, e-commerce, automated testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1016/j.jss.2011.06.026,
author = {Guo, Jianmei and White, Jules and Wang, Guangxin and Li, Jian and Wang, Yinglin},
title = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.026},
doi = {10.1016/j.jss.2011.06.026},
abstract = {Abstract: Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2208–2221},
numpages = {14},
keywords = {Software product lines, Product derivation, Optimization, Genetic algorithm, Feature models, Configuration}
}

@article{10.4018/ijswis.2014010103,
author = {Ermilov, Timofey and Khalili, Ali and Auer, S\"{o}ren},
title = {Ubiquitous Semantic Applications: A Systematic Literature Review},
year = {2014},
issue_date = {January 2014},
publisher = {IGI Global},
address = {USA},
volume = {10},
number = {1},
issn = {1552-6283},
url = {https://doi.org/10.4018/ijswis.2014010103},
doi = {10.4018/ijswis.2014010103},
abstract = {Recently practical approaches for development of ubiquitous semantic applications have made quite some progress. In particular in the area of the ubiquitous access to the semantic data the authors recently observed a large number of approaches, systems and applications being described in the literature. With this survey the authors aim to provide an overview on the rapidly emerging field of Ubiquitous Semantic Applications (UbiSA). The authors conducted a systematic literature review comprising a thorough analysis of 48 primary studies out of 172 initially retrieved papers. The authors obtained a comprehensive set of quality attributes for UbiSA together with corresponding application features suggested for their realization. The quality attributes include aspects such as mobility, usability, heterogeneity, collaboration, customizability and evolvability. The primary studies were surveyed in the light of these quality attributes and the authors performed a thorough analysis of five ubiquitous semantic applications, six frameworks for UbiSA, three UbiSA specific ontologies, five ubiquitous semantic systems and nine general approaches. The proposed quality attributes facilitate the evaluation of existing approaches and the development of novel, more effective and intuitive UbiSA.},
journal = {Int. J. Semant. Web Inf. Syst.},
month = jan,
pages = {66–99},
numpages = {34},
keywords = {Web Applications, Ubiquitous Device, Ubiquitous Applications, Survey, Semantic Web}
}

@article{10.1109/TSE.2002.1019479,
author = {Dobrica, Liliana and Niemel\"{a}, Eila},
title = {A survey on software architecture analysis methods},
year = {2002},
issue_date = {July 2002},
publisher = {IEEE Press},
volume = {28},
number = {7},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2002.1019479},
doi = {10.1109/TSE.2002.1019479},
abstract = {The purpose of the architecture evaluation of a software system is to analyze the architecture to identify potential risks and to verify that the quality requirements have been addressed in the design. This survey shows the state of the research at this moment, in this domain, by presenting and discussing eight of the most representative architecture analysis methods. The selection of the studied methods tries to cover as many particular views of objective reflections as possible to be derived from the general goal. The role of the discussion is to offer guidelines related to the use of the most suitable method for an architecture assessment process. We will concentrate on discovering similarities and differences between these eight available methods by making classifications, comparision and appropriateness studies.},
journal = {IEEE Trans. Softw. Eng.},
month = jul,
pages = {638–653},
numpages = {16},
keywords = {software architecture, scenarios, quality attributes, analysis techniques and methods}
}

@article{10.1016/j.jss.2015.08.026,
author = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
title = {Evolution of software in automated production systems},
year = {2015},
issue_date = {December 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {110},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.026},
doi = {10.1016/j.jss.2015.08.026},
abstract = {Automated Production Systems (aPS) impose specific requirements regarding evolution.We present a classification of how Automated Production Systems evolve.We discuss the state of art and research needs for the development phases of aPS.Model-driven engineering and Variability Management are key issues.Cross-discipline analysis of (non)-functional requirements must be improved. Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. Display Omitted},
journal = {J. Syst. Softw.},
month = dec,
pages = {54–84},
numpages = {31},
keywords = {Software engineering, Evolution, Automation, Automated production systems}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Reverse engineering, Multi-objective evolutionary algorithms, Feature models, Empirical evaluation}
}

@article{10.1007/s11219-015-9273-7,
author = {Grbac, Tihana Galinac and Runeson, Per and Huljeni\'{c}, Darko},
title = {A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-015-9273-7},
doi = {10.1007/s11219-015-9273-7},
abstract = {Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification.},
journal = {Software Quality Journal},
month = dec,
pages = {967–995},
numpages = {29},
keywords = {Unit verification, Software metrics, Software fault distributions, Replication, Empirical research}
}

@inproceedings{10.1007/11741060_6,
author = {Lohmann, Daniel and Schr\"{o}der-Preikschat, Wolfgang and Spinczyk, Olaf},
title = {The design of application-tailorable operating system product lines},
year = {2005},
isbn = {3540336893},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11741060_6},
doi = {10.1007/11741060_6},
abstract = {System software for deeply embedded devices has to cope with a broad variety of requirements and platforms, but especially with strict resource constraints. To compete against proprietary systems (and thereby to facilitate reuse), an operating system product line for deeply embedded systems has to be highly configurable and tailorable. It is therefore crucial that all selectable and configurable features can be encapsulated into fine-grained, exchangeable and reusable implementation components. However, the encapsulation of non-functional properties is often limited, due to their cross-cutting character. Fundamental system policies, like synchronization or activation points for the scheduler, have typically to be reflected in many points of the operating system component code. The presented approach is based on feature modeling, C++ class composition and overcomes the above mentioned problems by means of aspect-oriented programming (AOP). It facilitates a fine-grained encapsulation and configuration of even non-functional properties in system software.},
booktitle = {Proceedings of the Second International Conference on Construction and Analysis of Safe, Secure, and Interoperable Smart Devices},
pages = {99–117},
numpages = {19},
location = {Nice, France},
series = {CASSIS'05}
}

@inproceedings{10.5555/1758398.1758458,
author = {Zhang, Hongyu and Jarzabek, Stan and Yang, Bo},
title = {Quality prediction and assessment for product lines},
year = {2003},
isbn = {3540404422},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In recent years, software product lines have emerged as a promising approach to improve software development productivity in IT industry. In the product line approach, we identify both commonalities and variabilities in a domain, and build generic assets for an organization. Feature diagrams are often used to model common and variant product line requirements and can be considered part of the organizational assets. Despite their importance, quality attributes (or non-functional requirements, NFRs) such as performance and security have not been sufficiently addressed in product line development. A feature diagram alone does not tell us how to select a configuration of variants to achieve desired quality attributes of a product line member. There is a lack of an explicit model that can represent the impact of variants on quality attributes. In this paper, we propose a Bayesian Belief Network (BBN) based approach to quality prediction and assessment for a software product line. A BBN represents domain experts' knowledge and experiences accumulated from the development of similar projects. It helps us capture the impact of variants on quality attributes, and helps us predict and assess the quality of a product line member by performing quantitative analysis over it. For developing specific systems, members of a product line, we reuse the expertise captured by a BBN instead of working from scratch. We use examples from the Computer Aided Dispatch (CAD) product line project to illustrate our approach.},
booktitle = {Proceedings of the 15th International Conference on Advanced Information Systems Engineering},
pages = {681–695},
numpages = {15},
location = {Klagenfurt, Austria},
series = {CAiSE'03}
}

@inproceedings{10.1109/ASE.2011.6100096,
author = {Oster, Zachary J. and Santhanam, Ganesh Ram and Basu, Samik},
title = {Automating analysis of qualitative preferences in goal-oriented requirements engineering},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100096},
doi = {10.1109/ASE.2011.6100096},
abstract = {In goal-oriented requirements engineering, a goal model graphically represents relationships between the required goals (functional requirements), tasks (realizations of goals), and optional goals (non-functional properties) involved in designing a system. It may, however, be impossible to find a design that fulfills all required goals and all optional goals. In such cases, it is useful to find designs that provide the required functionality while satisfying the most preferred set of optional goals under the goal model's constraints. We present an approach that considers expressive qualitative preferences over optional goals, as these can model interacting and/or mutually exclusive subgoals. Our framework employs a model checking-based method for reasoning with qualitative preferences to identify the most preferred alternative(s). We evaluate our approach using existing goal models from the literature.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {448–451},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1145/1944892.1944897,
author = {Gilson, Fabian and Englebert, Vincent},
title = {Towards handling architecture design, variability and evolution with model transformations},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944897},
doi = {10.1145/1944892.1944897},
abstract = {Software systems have to face evolving requirements from information system stakeholders, infrastructure modifications, and evolving rationales about the implementation. This increases the rate of migration and redeployment of systems. Recent approaches intend to abstract architectural element specifications from the implementing technology and manage software design through model transformations. Based on an Architecture Description Language integrating infrastructure modelling facilities and a requirement modelling language, the present work manages architecturally significant requirements and infrastructure evolutions by model transformations. Our approach offers support for evolution and variability management tasks as it makes explicit the rationales concerning requirements, infrastructure and implementation alternatives that guide both the software architecture and the infrastructure definition.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {39–48},
numpages = {10},
keywords = {model transformation, infrastructure constraint, architecture variability, architecture description language, architecturally significant requirement},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@inproceedings{10.5555/1885639.1885651,
author = {Nolan, Andy J. and Abrah\~{a}o, Silvia},
title = {Dealing with cost estimation in software product lines: experiences and future directions},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {After 5 years invested in developing accurate cost estimation tools, Rolls-Royce has learnt about the larger potential of the tools to shape many aspects of the business. A good estimation tool is a "model" of a project and is usually used to estimate cost and schedule, but it can also estimate and validate risks and opportunities. Estimation tools have unified engineering, project and business needs. The presence of good estimation tools has driven higher performance and stability in the business. It was evident we needed this capability to underpin decisions in our new Software Product Line strategy. The objective of this paper is twofold. First, we report the experiences gained in the past on the use of estimation tools. Second, we describe the current efforts and future directions on the development of an estimation tool for Software Product Lines. At the heart of the Product Line estimation tool is a simple representation of the product - represented as the number of Lines Of Code (LOC). The next generation of tool, will need to consider wider aspects of product quality in order to create more accurate estimates and support better decisions about our products.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {121–135},
numpages = {15},
keywords = {software product lines, industrial experiences, cost estimation},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.5555/2050167.2050171,
author = {Nunes, Ingrid and Cowan, Donald and Cirilo, Elder and De Lucena, Carlos J. P.},
title = {A case for new directions in agent-oriented software engineering},
year = {2010},
isbn = {9783642226359},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The state-of-the-art of Agent-oriented Software Engineering (AOSE) is insufficiently reflected in the state-of-practice in developing complex distributed systems. This paper discusses software engineering (SE) areas that have not been widely addressed in the context of AOSE, leading to a lack of mechanisms that support the development of Multiagent Systems (MASs) based on traditional SE principles, such as modularity, reusability and maintainability. This discussion is based on an exploratory study of the development of a family of buyer agents following the belief-desire-intention model and using a Software Product Line architecture. Based on the discussion presented in this paper, we hope to encourage the AOSE community to address particular SE issues on the development of MAS that have not yet been (widely) considered.},
booktitle = {Proceedings of the 11th International Conference on Agent-Oriented Software Engineering},
pages = {37–61},
numpages = {25},
keywords = {software reuse, software product lines, software architectures, multi-agent systems, agent-oriented software engineering},
location = {Toronto, Canada},
series = {AOSE'10}
}

@inproceedings{10.5555/2337223.2337468,
author = {Colanzi, Thelma Elita},
title = {Search based design of software product lines architectures},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {The Product-Line Architecture (PLA) is the main artifact of a Software Product Line (SPL). However, obtaining a modular, extensible and reusable PLA is a people-intensive and non-trivial task, related to different and possible conflicting factors. Hence, the PLA design is a hard problem and to find the best architecture can be formulated as an optimization problem with many factors. Similar Software Engineering problems have been efficiently solved by search-based algorithms in the field known as Search-based Software Engineering. The existing approaches used to optimize software architecture are not suitable since they do not encompass specific characteristics of SPL. To easy the SPL development and to automate the PLA design this work introduces a multi-objective optimization approach to the PLA design. The approach is now being implemented by using evolutionary algorithms. Empirical studies will be performed to validate the neighborhood operators, SPL measures and search algorithms chosen. Finally, we intend to compare the results of the proposed approach with PLAs designed by human architects.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1507–1510},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1109/ICSE.2019.00112,
author = {Kaltenecker, Christian and Grebhahn, Alexander and Siegmund, Norbert and Guo, Jianmei and Apel, Sven},
title = {Distance-based sampling of software configuration spaces},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00112},
doi = {10.1109/ICSE.2019.00112},
abstract = {Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1084–1094},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/1454268.1454275,
author = {Bertoncello, Ivo Augusto and Dias, Marcelo Oliveira and Brito, Patrick H. S. and Rubira, Cec\'{\i}lia M. F.},
title = {Explicit exception handling variability in component-based product line architectures},
year = {2008},
isbn = {9781605582290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454268.1454275},
doi = {10.1145/1454268.1454275},
abstract = {Separation of concerns is one of the overarching goals of exception handling in order to keep separate normal and exceptional behaviour of a software system. In the context of a software product line (SPL), this separation of concerns is also important for designing software variabilities related to different exception handling strategies, such as the choice of different handlers depending on the set of selected features. This paper presents a method for refactoring object-oriented product line architecture in order to separate explicitly their normal and exceptional behaviour into different software components. The new component-based software architecture includes variation points related to different choices of exception handlers that can be selected during product instantiations, thus facilitating the evolution of the exceptional behaviour. The feasibility of the proposed approach is assessed through a SPL of mobile applications.},
booktitle = {Proceedings of the 4th International Workshop on Exception Handling},
pages = {47–54},
numpages = {8},
keywords = {software architecture, exceptional behaviour, exception handling, component-based software development},
location = {Atlanta, Georgia},
series = {WEH '08}
}

@article{10.1145/3011286.3011291,
author = {Galster, Matthias and Zdun, Uwe and Weyns, Danny and Rabiser, Rick and Zhang, Bo and Goedicke, Michael and Perrouin, Gilles},
title = {Variability and Complexity in Software Design: Towards a Research Agenda},
year = {2017},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/3011286.3011291},
doi = {10.1145/3011286.3011291},
abstract = {Many of today's software systems accommodate different usage and deployment scenarios. Intentional and unintentional variability in functionality or quality attributes (e.g., performance) of software significantly increases the complexity of the problem and design space of those systems. The complexity caused by variability becomes increasingly difficult to handle due to the increasing size of software systems, new and emerging application domains, dynamic operating conditions under which software systems have to operate, fast moving and highly competitive markets, and more powerful and versatile hardware. This paper reports results of the first International Workshop on Variability and Complexity in Software Design that brought together researchers and engineers interested in the topic of complexity and variability. It also outlines directions the field might move in the future},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {27–30},
numpages = {4},
keywords = {software design, complexity, Variability}
}

@inproceedings{10.1007/11424758_6,
author = {Kim, Soo Dong and Chang, Soo Ho and La, Hyun Jung},
title = {A systematic process to design product line architecture},
year = {2005},
isbn = {3540258604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11424758_6},
doi = {10.1007/11424758_6},
abstract = {Product Line Engineering is being accepted as a representative software reuse methodology by using core assets and product line architecture is known as a key element of core assets. However, current research on product line engineering has room to provide specific and detailed guidelines of designing product line architectures and reflecting variability in the architecture. In this paper, we present a reference model and a process to design the architecture with detailed instructions. Especially architectural variability is codified by describing decision model representing variation.},
booktitle = {Proceedings of the 2005 International Conference on Computational Science and Its Applications - Volume Part I},
pages = {46–56},
numpages = {11},
location = {Singapore},
series = {ICCSA'05}
}

@article{10.1109/MC.2013.335,
author = {Seceleanu, Cristina and Crnkovic, Ivica},
title = {Component Models for Reasoning},
year = {2013},
issue_date = {November 2013},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {46},
number = {11},
issn = {0018-9162},
url = {https://doi.org/10.1109/MC.2013.335},
doi = {10.1109/MC.2013.335},
abstract = {The world of component-based systems is as appealing as it is challenging. Components, as first-class citizens of component-based systems, serve as the main units of encapsulated functionality and also units of composition, with the intention to improve development efficiency and software quality through reusability, extensibility and analyzability of software. These benefits are obtained especially when the components are understood by means of a formally well-defined component model, amenable to effective reasoning on functional and extra-functional properties at unit- as well as system-level. In this article we present the basic concepts of component-based design, emphasizing the characteristics of different types of component compositions, which dictate particular trade-offs between the degree of assurance and design flexibility. We show that rich and semantically well-defined component models with encapsulated reasoning information enable prediction of the system behavior and in general the system functional and non-functional properties. In this way, the development process is significantly simplified. We illustrate the concept by giving a short overview of the ProCom component model that is designed for enabling predictability in the embedded and real-time systems domain.},
journal = {Computer},
month = nov,
pages = {40–47},
numpages = {8},
keywords = {software engineering, reasoning, predictability, component models, Software engineering, Software architecture, Prediction methods, Computational modeling}
}

@article{10.1145/2897760,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
title = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2897760},
doi = {10.1145/2897760},
abstract = {A feature model specifies the sets of features that define valid products in a software product line. Recent work has considered the problem of choosing optimal products from a feature model based on a set of user preferences, with this being represented as a many-objective optimization problem. This problem has been found to be difficult for a purely search-based approach, leading to classical many-objective optimization algorithms being enhanced either by adding in a valid product as a seed or by introducing additional mutation and replacement operators that use an SAT solver. In this article, we instead enhance the search in two ways: by providing a novel representation and by optimizing first on the number of constraints that hold and only then on the other objectives. In the evaluation, we also used feature models with realistic attributes, in contrast to previous work that used randomly generated attribute values. The results of experiments were promising, with the proposed (SIP) method returning valid products with six published feature models and a randomly generated feature model with 10,000 features. For the model with 10,000 features, the search took only a few minutes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = apr,
articleno = {17},
numpages = {39},
keywords = {Product selection}
}

@inproceedings{10.5555/3505326.3505356,
author = {Ravari, Yaser Norouzzadeh and Spronck, Pieter and Sifa, Rafet and Drachen, Anders},
title = {Predicting victory in a hybrid online competitive game: the case of Destiny},
year = {2017},
isbn = {978-1-57735-791-9},
publisher = {AAAI Press},
abstract = {Competitive multi-player game play is a common feature in major commercial titles, and has formed the foundation for esports. In this paper, the question whether it is possible to predict match outcomes in First Person Shooter-type multiplayer competitive games with mixed genres is addressed. The case employed is Destiny, which forms a hybrid title combining Massively Multi-player Online Role-Playing game features and First-Person Shooter games. Destiny provides the opportunity to investigate prediction of the match outcome, as well as the influence of performance metrics on the match results in a hybrid multi-player major commercial title. Two groups of models are presented for predicting match results: One group predicts match results for each individual game mode and the other group predicts match results in general, without considering specific game modes. Models achieve a performance between 63% and 99% in terms of average precision, with a higher performance recorded for the models trained on specific multi-player game modes, of which Destiny has several. We also analyzed performance metrics and their influence for each model. The results show that many key shooter performance metrics such as Kill/Death ratio are relevant across game modes, but also that some performance metrics are mainly important for specific competitive game modes. The results indicate that reliable match prediction is possible in FPS-type esports games.},
booktitle = {Proceedings of the Thirteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
articleno = {30},
numpages = {7},
location = {Little Cottonwood Canyon, Utah, USA},
series = {AIIDE'17}
}

@inproceedings{10.1145/1944892.1944894,
author = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Th\"{u}m, Thomas and Saake, Gunter},
title = {Multi-dimensional variability modeling},
year = {2011},
isbn = {9781450305709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944892.1944894},
doi = {10.1145/1944892.1944894},
abstract = {The variability of a software product line (SPL)is often described with a feature model. To avoid highly complex models, stakeholders usually try to separate different variability dimensions, such as domain variability and implementation variability. This results in distinct variability models, which are easier to handle than one large model. On the other hand, it is sometimes required to analyze the variability dimensions of an SPL in combination using a single model only. To combine separate modeling and integrated analysis of variability, we present Velvet, a language for multi-dimensional variability modeling. Velvet allows stakeholders to model each variability dimension of an SPL separately and to compose the separated dimensions on demand. This improves reuse of feature models and supports independent modeling variability dimensions. Furthermore, Velvet integrates feature modeling and configuration in a single language. The combination of both concepts creates further reuse opportunities and allows stakeholders to independently configure variability dimensions.},
booktitle = {Proceedings of the 5th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {11–20},
numpages = {10},
keywords = {variability modeling, separation of concerns, feature models},
location = {Namur, Belgium},
series = {VaMoS '11}
}

@article{10.1007/s11761-014-0161-y,
author = {Huergo, Rosane S. and Pires, Paulo F. and Delicato, Flavia C. and Costa, Bruno and Cavalcante, Everton and Batista, Thais},
title = {A systematic survey of service identification methods},
year = {2014},
issue_date = {September 2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {8},
number = {3},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-014-0161-y},
doi = {10.1007/s11761-014-0161-y},
abstract = {One of the major challenges for the adoption of the service-oriented architecture (SOA) is the service identification phase that aims to determine which services are appropriate to be implemented. In the last decade, several service identification methods (SIMs) were proposed. However, the service identification phase still remains a challenge to organizations due to the lack of systematic methods and comprehensive approaches that support the examination of the businesses from multiple perspectives and consider service quality attributes. This work aims to provide an overview of existing SIMs by detailing which service's perspectives, stated as relevant by the industry, are addressed by the SIMs and also by synthesizing the identification techniques used by them. We have performed a systematic survey over publications about SIMs from 2002 to June 2013, and 105 studies were selected. A detailed investigation on the analyzed SIMs revealed that the identification techniques applied by them have a correlation on how they address many of the service's perspectives. In addition, they are supporting the SOA adoption by handling many perspectives of the OASIS' reference architecture for SOA. However, most of them do not explicitly address service quality attributes and few studies support the evaluation of both. Therefore, future research should follow the direction toward hybrid methods with mechanisms to elicit business and service's quality attributes.},
journal = {Serv. Oriented Comput. Appl.},
month = sep,
pages = {199–219},
numpages = {21},
keywords = {Systematic survey, Service-oriented architecture, Service identification method, SOA, SIM}
}

@inproceedings{10.1145/3302333.3302350,
author = {Garc\'{\i}a, Sergio and Str\"{u}ber, Daniel and Brugali, Davide and Di Fava, Alessandro and Schillinger, Philipp and Pelliccione, Patrizio and Berger, Thorsten},
title = {Variability Modeling of Service Robots: Experiences and Challenges},
year = {2019},
isbn = {9781450366489},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302333.3302350},
doi = {10.1145/3302333.3302350},
abstract = {Sensing, planning, controlling, and reasoning, are human-like capabilities that can be artificially replicated in an autonomous robot. Such a robot implements data structures and algorithms devised on a large spectrum of theories, from probability theory, mechanics, and control theory to ethology, economy, and cognitive sciences. Software plays a key role in the development of robotic systems, as it is the medium to embody intelligence in the machine. During the last years, however, software development is increasingly becoming the bottleneck of robotic systems engineering due to three factors: (a) the software development is mostly based on community efforts and it is not coordinated by key stakeholders; (b) robotic technologies are characterized by a high variability that makes reuse of software a challenging practice; and (c) robotics developers are usually not specifically trained in software engineering. In this paper, we illustrate our experiences from EU, academic, and industrial projects in identifying, modeling, and managing variability in the domain of service robots. We hope to raise awareness for the specific variability challenges in robotics software engineering and to inspire other researchers to advance this field.},
booktitle = {Proceedings of the 13th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {8},
numpages = {6},
location = {Leuven, Belgium},
series = {VaMoS '19}
}

@article{10.1504/IJAOSE.2008.016800,
author = {Verstraete, Paul and Germain, Bart Saint and Valckenaers, Paul and Brussel, Hendrik Van and Belle, Jan Van and Hadeli},
title = {Engineering manufacturing control systems using PROSA and delegate MAS},
year = {2008},
issue_date = {January 2008},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {2},
number = {1},
issn = {1746-1375},
url = {https://doi.org/10.1504/IJAOSE.2008.016800},
doi = {10.1504/IJAOSE.2008.016800},
abstract = {This paper presents a systematic description of a reusable software architecture for multiagent systems in the domain of manufacturing control. The architectural description consolidates the authors' expertise in this area. Until now, the research has taken a manufacturing control perspective of multiagent systems. The research team has focused on providing benefits to the manufacturing control domain by designing a novel type of control system. This paper takes a software architectural perspective of multiagent manufacturing control. The systematic description specifies a software product line architecture for manufacturing control. The paper describes the assets of the software product line architecture and how these assets can be combined.},
journal = {Int. J. Agent-Oriented Softw. Eng.},
month = jan,
pages = {62–89},
numpages = {28},
keywords = {software reuse, software architecture, multi-agent systems, manufacturing control, agent-based systems, MASs}
}

@article{10.1007/s00607-018-0646-1,
author = {Galindo, Jos\'{e} A. and Benavides, David and Trinidad, Pablo and Guti\'{e}rrez-Fern\'{a}ndez, Antonio-Manuel and Ruiz-Cort\'{e}s, Antonio},
title = {Automated analysis of feature models: Quo vadis?},
year = {2019},
issue_date = {May       2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {101},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-018-0646-1},
doi = {10.1007/s00607-018-0646-1},
abstract = {Feature models have been used since the 90s to describe software product lines as a way of reusing common parts in a family of software systems. In 2010, a systematic literature review was published summarizing the advances and settling the basis of the area of automated analysis of feature models (AAFM). From then on, different studies have applied the AAFM in different domains. In this paper, we provide an overview of the evolution of this field since 2010 by performing a systematic mapping study considering 423 primary sources. We found six different variability facets where the AAFM is being applied that define the tendencies: product configuration and derivation; testing and evolution; reverse engineering; multi-model variability-analysis; variability modelling and variability-intensive systems. We also confirmed that there is a lack of industrial evidence in most of the cases. Finally, we present where and when the papers have been published and who are the authors and institutions that are contributing to the field. We observed that the maturity is proven by the increment in the number of journals published along the years as well as the diversity of conferences and workshops where papers are published. We also suggest some synergies with other areas such as cloud or mobile computing among others that can motivate further research in the future.},
journal = {Computing},
month = may,
pages = {387–433},
numpages = {47},
keywords = {Variability-intensive systems, Software product lines, Feature models, Automated analysis, 68T35}
}

@inproceedings{10.1145/2721956.2721977,
author = {Kajtazovic, Nermin and Preschern, Christopher and H\"{o}ller, Andrea and Kreiner, Christian},
title = {Towards pattern-based reuse in safety-critical systems},
year = {2014},
isbn = {9781450334167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2721956.2721977},
doi = {10.1145/2721956.2721977},
abstract = {Challenges such as time-to-market, reduced costs for change and maintenance have radically influenced development of today's safety-critical systems. Many domains have already adopted their system's engineering to support modular and component-based architectures. With the component-based design paradigm, the system engineering is utilized allowing to distribute development among different development teams, however, with the price that there is no full trust in independently developed parts, which makes their reuse challenging. Until now, many approaches that address reuse, on conceptual or detailed level, have been proposed. A very important aspect addressed here is to document the information flow between system parts in detail, i.e. from higher abstraction levels down to the implementation details, in order to put more trust into independently developed parts of the system.In this paper, we describe a compact pattern system with the aim to establish a link between high level concepts for reuse and detailed description of the behavior of system parts. The main goal is to document these details up to the higher levels of abstraction in more systematic way.},
booktitle = {Proceedings of the 19th European Conference on Pattern Languages of Programs},
articleno = {33},
numpages = {15},
location = {Irsee, Germany},
series = {EuroPLoP '14}
}

@inproceedings{10.1145/3251104,
author = {Langdon, William B. and Petke, Justyna and White, David R.},
title = {Session details: Genetic Improvement 2015 Workshop},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3251104},
doi = {10.1145/3251104},
abstract = {It is our great pleasure to welcome you to the first international workshop on the Genetic Improvement of Software -- GI-2015, held at GECCO'15. Our goal was to bring together research from across the globe to exchange ideas on using optimisation techniques, particularly evolutionary computation such as genetic programming, to improve existing software. We invited short position papers to encourage the discussion of new ideas and recent work in addition to longer and more concrete submissions. The call for participation invited GI work on automatic bug-fixing; improving functionality; improving non-functional properties such as efficiency, memory and energy consumption; "plastic surgery" by transplanting functionality from other existing code to host software; and automatically specialising generic software for dedicated tasks. As you will see, we have accepted papers in most of these areas as well as papers on improving the nascent genetic improvement tools in use, improving parallel code, GI's relationship with software product lines (SPL), improving security and GI for embedded systems.We had submissions from Asia, Europe and both North and South America. They were exactly evenly split between full-length submissions (8) and two page position papers (8).},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1016/j.jss.2014.12.041,
author = {Pascual, Gustavo G. and Lopez-Herrejon, Roberto E. and Pinto, M\'{o}nica and Fuentes, Lidia and Egyed, Alexander},
title = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.12.041},
doi = {10.1016/j.jss.2014.12.041},
abstract = {Mobile applications require to self-adapt their behavior to context changes.We propose a DSPL approach to manage variability at runtime.Configurations are generated using multiobjective evolutionary algorithms.We apply a fix operator to generate only valid configurations at runtime.We demonstrate that this approach is suitable for mobile environments. Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
journal = {J. Syst. Softw.},
month = may,
pages = {392–411},
numpages = {20},
keywords = {Evolutionary algorithms, Dynamic reconfiguration, DSPL}
}

@article{10.1016/j.eswa.2013.12.028,
author = {Segura, Sergio and Parejo, Jos\'{e} A. and Hierons, Robert M. and Benavides, David and Ruiz-Cort\'{e}s, Antonio},
title = {Automated generation of computationally hard feature models using evolutionary algorithms},
year = {2014},
issue_date = {June, 2014},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {41},
number = {8},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.12.028},
doi = {10.1016/j.eswa.2013.12.028},
abstract = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {3975–3992},
numpages = {18},
keywords = {Software product lines, Search-based testing, Performance testing, Feature models, Evolutionary algorithms, Automated analysis}
}

@article{10.1016/j.infsof.2019.01.004,
author = {Souza, Eric and Moreira, Ana and Goul\~{a}o, Miguel},
title = {Deriving architectural models from requirements specifications: A systematic mapping study},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.004},
doi = {10.1016/j.infsof.2019.01.004},
journal = {Inf. Softw. Technol.},
month = may,
pages = {26–39},
numpages = {14},
keywords = {Literature review, Mapping study, Software architecture}
}

@article{10.1016/j.infsof.2008.04.002,
author = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
title = {Variability assessment in software product families},
year = {2009},
issue_date = {January, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.04.002},
doi = {10.1016/j.infsof.2008.04.002},
abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {195–218},
numpages = {24},
keywords = {Variability, Software product families, Evolution, Assessment}
}

@article{10.1109/TNET.2021.3056772,
author = {Ruby, Rukhsana and Zhong, Shuxin and ElHalawany, Basem M. and Luo, Hanjiang and Wu, Kaishun},
title = {SDN-Enabled Energy-Aware Routing in Underwater Multi-Modal Communication Networks},
year = {2021},
issue_date = {June 2021},
publisher = {IEEE Press},
volume = {29},
number = {3},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3056772},
doi = {10.1109/TNET.2021.3056772},
abstract = {Despite extensive research efforts, underwater sensor networks (UWSNs) still suffer from serious performance issues due to their inefficient and uncoordinated channel access and resource management. For example, due to the lack of holistic knowledge on the network resources, existing decentralized routing protocols fail to provide globally optimal performance. On the other hand, Software Defined Networking (SDN), as a promising paradigm to provide prominent centralized solutions, can be employed to address the aforementioned issues in UWSNs. Indeed, SDN brings unprecedented opportunities to improve the network performance through the development of advanced algorithms at controllers. In this paper, we study the routing problem in such a network with new features including centralized route decision, global network-state awareness, seamless route discovery while considering the optimization of several long-term global performance metrics. We formulate the entire routing problem of a multi-modal UWSN as an optimization problem while considering the interference phenomenon of ad hoc scenarios and some long-term global performance metrics of an ideal routing protocol. Our formulated problem nicely captures all possible flexibilities of a sensor node no matter it has the full-duplex or half-duplex functionality. Upon the formulation, we recognize the NP-hard nature of the problem for all possible scenarios. We adopt a rounding technique based on the convex programming relaxation concept to solve the formulated routing problem that considers full-duplex scenarios, whereas we solve the problem for half-duplex scenarios using a greedy method upon interpreting it as a submodular function maximization problem. Through extensive simulation via our Python-based in-house simulator, we verify that our proposed globally optimal routing scheme always outperforms three existing decentralized routing protocols (each of these protocols are selected from each of three prominent protocol types, i.e., flooding, cross-layer information and adaptive machine learning based, respectively) in terms of reliability, latency, energy efficiency, lifetime and fairness.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {965–978},
numpages = {14}
}

@inproceedings{10.1109/ICSEA.2008.80,
author = {Kim, Kangtae and Kim, Hyungrok and Kim, Sundeok and Chang, Gihun},
title = {A Case Study on SW Product Line Architecture Evaluation: Experience in the Consumer Electronics Domain},
year = {2008},
isbn = {9780769533728},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSEA.2008.80},
doi = {10.1109/ICSEA.2008.80},
abstract = {A well-executed software architecture is one of the most critical factors for achieving the intended effectiveness of a software product line as a holistic picture of a system. Today, many organizations are investing in architecture and its quality attributes for productivity, time to market and etc. In this paper, we show an evaluation framework named which provides criteria for measuring execution(designing and implementing) of an architecture and guideline for improvement based on measurement, analysis and improvement principle. We focused on software architecture design itself because we are mainly concerned with design and implementation issues thus concentrating on the architecture criteria. The contribution of the paper is to identify and demonstrate a architecture analysis and improvement process with practical experimentation results. The framework is based on design attributes of product line architecture and static analysis of architecture implementation.},
booktitle = {Proceedings of the 2008 The Third International Conference on Software Engineering Advances},
pages = {192–197},
numpages = {6},
keywords = {product line, software architecture evaluation},
series = {ICSEA '08}
}

@inproceedings{10.5555/645882.672251,
author = {Yacoub, Sherif M.},
title = {Performance Analysis of Component-Based Applications},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Performance analysis is a software engineering activity that involves analyzing a software application with respect to performance quality attributes such as response and execution times. Performance analysis tools provide the necessary support for the analyst to monitor program execution, record and analyze performance data, and locate and understand areas of poor performance. Performance analysis methods and techniques are highly dependent on the properties of the software system to be analyzed. Product line engineering applications possess some special properties that impose constraints on the selection of the performance analysis techniques to be applied and the tools to be used. The development of a component-based reference architecture is crucial to the success of a true product line. The component-based nature facilitates the integration of components and the replacement of a component with another to meet the requirements of an instance application of the product line. In this paper, we discuss performance analysis of component-based software systems and its automation. We discuss how component-based system properties influence the selection of methods and tools used to obtain and analyze performance measures. We use a case study of the document content remastering product line to illustrate the application of a performance analysis method to component-based applications.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {299–315},
numpages = {17},
keywords = {performance analysis, component-based software engineering (CBSE), application and component profiling, and performance tools},
series = {SPLC 2}
}

@inproceedings{10.1145/2361999.2362027,
author = {Galster, Matthias and Avgeriou, Paris and Weyns, Danny and Becker, Martin},
title = {Second international workshop on variability in software architecture},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362027},
doi = {10.1145/2361999.2362027},
abstract = {Variability is the ability of a software system or artifact to be adapted for specific contexts, in a preplanned manner. Many of today's software systems are built with variability in mind, e.g., product lines and families, self-adaptive systems, open platforms, or service-based systems that support dynamic runtime composition of web services. Variability is reflected in and facilitated through the software architecture. Also, as the software architecture is a reference point for many development activities and for achieving quality attributes, variability should be treated as a first-class and cross-cutting concern in software architecture. Therefore, the Second International Workshop on Variability in Software Architecture (VARSA 2012) aims at identifying critical challenges and progressing the state-of-the-art on variability in software architecture. VARSA 2012 is a follow-up of the First International Workshop on Variability in Software Architecture (VARSA 2011), held at WICSA 2011.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {132–134},
numpages = {3},
keywords = {variability, software architecture, VARSA},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/2577080.2577095,
author = {Dubslaff, Clemens and Kl\"{u}ppelholz, Sascha and Baier, Christel},
title = {Probabilistic model checking for energy analysis in software product lines},
year = {2014},
isbn = {9781450327725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2577080.2577095},
doi = {10.1145/2577080.2577095},
abstract = {In a software product line (SPL), a collection of software products is defined by their commonalities in terms of features rather than explicitly specifying all products one-by-one. Several verification techniques were adapted to establish temporal properties of SPLs. Symbolic and family-based model checking have been proven to be successful for tackling the combinatorial blow-up arising when reasoning about several feature combinations. However, most formal verification approaches for SPLs presented in the literature focus on the static SPLs, where the features of a product are fixed and cannot be changed during runtime. This is in contrast to dynamic SPLs, allowing to adapt feature combinations of a product dynamically after deployment.The main contribution of the paper is a compositional modeling framework for dynamic SPLs, which supports probabilistic and nondeterministic choices and allows for quantitative analysis. We specify the feature changes during runtime within an automata-based coordination component, enabling to reason over strategies how to trigger dynamic feature changes for optimizing various quantitative objectives, e.g., energy or monetary costs and reliability. For our framework there is a natural and conceptually simple translation into the input language of the prominent probabilistic model checker PRISM. This facilitates the application of PRISM's powerful symbolic engine to the operational behavior of dynamic SPLs and their family-based analysis against various quantitative queries. We demonstrate feasibility of our approach by a case study issuing an energy-aware bonding network device.},
booktitle = {Proceedings of the 13th International Conference on Modularity},
pages = {169–180},
numpages = {12},
keywords = {software product lines, probabilistic model checking, energy analysis, dynamic features},
location = {Lugano, Switzerland},
series = {MODULARITY '14}
}

@inproceedings{10.1145/1562860.1562864,
author = {Siegmund, Norbert and Pukall, Mario and Soffner, Michael and K\"{o}ppen, Veit and Saake, Gunter},
title = {Using software product lines for runtime interoperability},
year = {2009},
isbn = {9781605585482},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1562860.1562864},
doi = {10.1145/1562860.1562864},
abstract = {Today, often small, heterogeneous systems have to cooperate in order to fulfill a certain task. Interoperability between these systems is needed for their collaboration. However, achieving this interoperability raises several problems. For example, embedded systems might induce a higher probability for a system failure due to constrained power supply. Nevertheless, interoperability must be guaranteed even in scenarios where embedded systems are used. To overcome this problem, we use services to abstract the functionality from the system which realizes it. We outline how services can be generated using software product line techniques to bridge the heterogeneity of cooperating systems. Additionally, we address runtime changes of already deployed services to overcome system failures. In this paper, we show the runtime adaption process of these changes which includes the following two points. First, we outline why feature-oriented programming is appropriate in such scenarios. Second, we describe the runtime adaption process of services with feature-oriented programming.},
booktitle = {Proceedings of the Workshop on AOP and Meta-Data for Software Evolution},
articleno = {4},
numpages = {7},
keywords = {software product lines, runtime adaption, interoperability},
location = {Genova, Italy},
series = {RAM-SE '09}
}

@inproceedings{10.1145/3330204.3330212,
author = {Silva, Thiciane Suely Couto and Rocha, Fabio Gomes and dos Santos, Rodrigo Pereira},
title = {Resource Demand Management in Java Ecosystem},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330212},
doi = {10.1145/3330204.3330212},
abstract = {The evolution of Software Ecosystems (SECO) requires to satisfy community's needs based on platform's and applications' requirements, known as SECO demands. Such SECO demands must be evaluated, approved and subsequently translated into platform resources (e.g., API, framework, library). In this context, this work applies a research approach based on primary and secondary studies to investigate how resource demands are managed in Java SECO. To do so, we conducted a systematic mapping study on the existing models, methods and conditioning factors for supporting emergence and/or inclusion of SECO platforms' resources. Based on the results, we planned and executed a survey research with members of a real SECO, more especifically Java Community Process (JCP) Committee members. From the studies, we identified seven methods for collecting data related to the emergence of new APIs, including a method for architecture assessment based on quality attributes, and finally ten conditioning factors for API management that can support SECO evolution. The results also allowed us to organize a process for managing demands of resources in Java SECO platform. We found out that the lack of standardization for management of SECO demands can hinder platforms/applications management. Therefore, this field is worth of further in-depth studies.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {3},
numpages = {8},
keywords = {Software Ecosystem, Resource Demand Management, Java},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Systematic mapping study, Software product lines, Evolution}
}

@phdthesis{10.5555/2049065,
author = {Sun, Hongyu},
advisor = {Lutz, Robyn R.},
title = {Quantifiable non-functional requirements modeling and static verification for web service compositions},
year = {2010},
isbn = {9781124430300},
publisher = {Iowa State University},
address = {USA},
abstract = {As service oriented architectures have become more widespread in industry, many complex web services are assembled using independently developed modular services from different vendors. Although the functionalities of the composite web services are ensured during the composition process, the non-functional requirements (NFRs) are often ignored in this process. Since quality of services plays a more and more important role in modern service-based systems, there is a growing need for effective approaches to verifying that a composite web service not only offers the required functionality but also satisfies the desired NFRs. Current approaches to verifying NFRs of composite services (as opposed to individual services) remain largely ad-hoc and informal in nature. This is especially problematic for high-assurance composite web services. High-assurance composite web services are those composite web services with special concern on critical NFRs such as security, safety and reliability. Examples of such applications include traffic control, medical decision support and the coordinated response systems for civil emergencies. The latter serves to motivate and illustrate the work described here.In this dissertation we develop techniques for ensuring that a composite service meets the user-specified NFRs expressible as hard constraints, e.g., “the messages of particular operations must be authenticated.” We introduce an automata-based framework for verifying that a composite service satisfies the desired NFRs based on the known guarantees regarding the non-functional properties of the component services. This automata-based model is able to represent NFRs that are hard, quantitative constraints on the composite web services. This model addresses two issues previously not handled in the modeling and verification of NFRs for composite web services: (1) the scope of the NFRs and (2) consistency checking of multiple NFRs.A scope of a NFR on a web service composition is the effective range of the NFR on the sub-workflows and modular services of the web service composition. It allows more precise description of a NFR constraint and more efficient verification. When multiple NFRs exist and overlap in their scopes, consistency checking is necessary to avoid wasted verification efforts on conflicting constraints. The approach presented here captures scope information in the model and uses it to check the consistency of multiple NFRs prior to the static verification of web service compositions. We illustrate how our approach can be used to verify security requirements for an Emergency Management System. We then focus on families of highly-customizable, composed web services where repeated verification of similar sets of NFRs can waste computation resources. We introduce a new approach to extend software product line engineering techniques to the web service composition domain. The resulting technique uses a partitioning similar to that between domain engineering and application engineering in the product-line context. It specifies the options that the user can select and constructs the resulting web service compositions. By first creating a web-service composition search space that satisfies the common requirements and then querying the search space as the user makes customization decisions, the technique provides a more efficient way to verify customizable web services. A decision model, illustrated with examples from the emergency-response application, is created to interact with the customers and ensure the consistency of their specifications. The capability to reuse the composition search space is shown to improve the quality of the composite services and reduce the cost of reverifying the same compositions. By distinguishing the commonalities and the variabilities of the web services, we divide the web composition into two stages: the preparation stage (to construct all commonalities) and the customization stage (to choose optional and alternative features). We thus draw most of the computation overhead into the first stage during the design in order to enable improved runtime efficiency during the second stage.A simulation platform was constructed to conduct experiments on the two verification approaches and three strategies introduced in this dissertation. The results of these experiments were analyzed to show the advantage of our automaton-based model in its verification efficiency with scoping information. We have shown how to choose the most efficient verification strategy from the three strategies of verifying multiple NFRs introduced in this dissertation under different circumstances. The results indicate that the software product line approach has significant efficiency improvement over traditional on-demand verification for highly customizable web service compositions.},
note = {AAI3438737}
}

@inproceedings{10.1007/978-3-662-45234-9_10,
author = {Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kit, Micha\l{} and Marek, Luk\'{a}\v{s} and T\r{u}ma, Petr},
title = {Towards Performance-Aware Engineering of Autonomic Component Ensembles},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_10},
doi = {10.1007/978-3-662-45234-9_10},
abstract = {Ensembles of autonomic components are a novel software engineering paradigm for development of open-ended distributed highly dynamic software systems e.g. smart cyber-physical systems. Recent research centered around the concept of ensemble-based systems resulted in design and development models that aim to systematize and simplify the engineering process of autonomic components and their ensembles. These methods highlight the importance of covering both the functional concepts and the non-functional properties, specifically performance-related aspects of the future systems. In this paper we propose an integration of the emerging techniques for performance assessment and awareness into different stages of the development process. Our goal is to aid both designers and developers of autonomic component ensembles with methods providing performance awareness throughout the entire development life cycle including runtime.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {131–146},
numpages = {16},
keywords = {performance engineering, ensemble-based systems, component systems}
}

@inproceedings{10.1007/11554844_17,
author = {Trew, Tim},
title = {Enabling the smooth integration of core assets: defining and packaging architectural rules for a family of embedded products},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_17},
doi = {10.1007/11554844_17},
abstract = {One of the remaining challenges in product line engineering is how to establish the quality of the reusable assets so that we can be confident that they can be configured and composed reliably. This is desirable, both to avoid having to completely re-test each product and to avoid integration faults only being detected late in product development. One of the diversity mechanisms of Philips’ high-end TV product line is the selection and com position of sub-systems, so different sub-system variants must integrate reli ably if the aims of the product line are to be realized. An earlier study of in tegration testing obligations in Philips products concluded that certain design policies must be imposed if integration testing is to be feasible, but it did not describe how relevant policies could be identified at the earliest stages of de sign. This paper addresses how a set of architectural rules were established for the TV product line through a root-cause analysis of problem reports, and packaged so that developers can recognize when they should be applied. The approach builds on other work on the impact of design choices on non-func-tional requirements to ensure that all quality attributes are addressed.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {137–149},
numpages = {13},
location = {Rennes, France},
series = {SPLC'05}
}

@article{10.1109/L-CA.2007.12,
author = {August, David and Chang, Jonathan and Girbal, Sylvain and Gracia-Perez, Daniel and Mouchard, Gilles and Penry, David A. and Temam, Olivier and Vachharajani, Neil},
title = {UNISIM: An Open Simulation Environment and Library for Complex Architecture Design and Collaborative Development},
year = {2007},
issue_date = {July 2007},
publisher = {IEEE Computer Society},
address = {USA},
volume = {6},
number = {2},
issn = {1556-6056},
url = {https://doi.org/10.1109/L-CA.2007.12},
doi = {10.1109/L-CA.2007.12},
abstract = {Simulator development is already a huge burden for many academic and industry research groups; future complex or heterogeneous multi-cores, as well as the multiplicity of performance metrics and required functionality, will make matters worse. We present a new simulation environment, called UNISIM, which is designed to rationalize simulator development by making it possible and efficient to distribute the overall effort over multiple research groups, even without direct cooperation. UNISIM achieves this goal with a combination of modular software development, distributed communication protocols, multi-level abstract modeling, interoperability capabilities, a set of simulator services APIs, and an open library/repository for providing a consistent set of simulator modules.},
journal = {IEEE Comput. Archit. Lett.},
month = jul,
pages = {45–48},
numpages = {4},
keywords = {Processor Architectures, Performance and Reliability}
}

@proceedings{10.1145/2304736,
title = {CBSE '12: Proceedings of the 15th ACM SIGSOFT symposium on Component Based Software Engineering},
year = {2012},
isbn = {9781450313452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 15th ACM SIGSOFT Symposium on Component Based Software Engineering (CBSE 2012) marks a milestone in the research on using components to build software systems in an efficient way. Over the years, this symposium has established a track record of bringing together researchers and practitioners from a variety of disciplines to promote a better understanding of CBSE from diverse perspectives, and to engage in active discussion and debate.Component-based software engineering continues to attract interest and evolve as a discipline for the rapid assembly of flexible software systems. CBSE combines elements of software requirements engineering, architecture, design, verification, testing, configuration and deployment. The role of and need for CBSE in industrial application remains critical.New trends in global services, distributed systems architectures, dynamically adaptable systems, and large-scale software systems often cross organizational boundaries and push the limits of established component-based methods, tools, and platforms. Innovative solutions from diverse paradigms (e.g., service-, aspect-, and agent-oriented) are needed to address these emerging trends. Topics of interest for CBSE 2012 therefore include, but are not limited to, the following:  Specification, architecture, and design of component models and component-based systems Software quality assurance for component-based engineering Verification, testing and certification of component-based systems Component composition, binding, and dynamic adaptation Component-based engineering with agents, aspects, or services Component-based product line engineering Non-functional properties (quality of service attributes) in component-based engineering Patterns and frameworks for component-based engineering Tools and methods for component-based engineering Industrial experience using component-based software development Empirical studies in component-based software engineering Teaching component-based software engineeringIn addition to the above, this year we have a special theme: Components for Achieving Long-Lived Systems. Many industrial systems have very strict requirements for uninterrupted operation. There are examples of systems that have aimed to provide continuous operation for more than 15 years (that is, since the first CBSE!). Such requirements place significant demands on the underlying architecture, mandating that the architecture be very well understood and carefully designed. In turn, the architecture, if implemented correctly, forms a foundation for achieving critical quality attributes such as dependability, robustness, usability, and flexibility. The principles of component-based software engineering offer a promise for achieving effective architectures for long-lived systems. This is especially so since this approach natively provides the ability to add, remove, replace, and/or modify components during operation. A related class of approaches deals with self-management in component-based systems in order to ensure continuous operation.CBSE 2012 received 50 submissions, each of which received at least three independent reviews by the CBSE Program Committee. The careful review process included an on-line discussion, after which 11 papers (22%) have been accepted for publication in this volume as full papers. Eleven more submissions have been selected as short papers. We have assembled an exciting program that shows that this is still a very vibrant and relevant community. We hope to see you at CBSE 2012 in Bertinoro!This year, CBSE is again part of the federated event CompArch, together with "QoSA 2012: 8th International ACM SIGSOFT Conference on the Quality of Software Architectures," "ISARCS 2012: 3rd International ACM SIGSOFT Symposium on Architecting Critical Systems," "WCOP 2012: 17th International Doctoral Symposium on Components and Architecture," and "ROSS 2012: Workshop on Reusing Open-Source Components."},
location = {Bertinoro, Italy}
}

@article{10.1016/j.jss.2016.09.045,
author = {Parejo, Jos\'{e} A. and S\'{a}nchez, Ana B. and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio and Lopez-Herrejon, Roberto E. and Egyed, Alexander},
title = {Multi-objective test case prioritization in highly configurable systems},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.09.045},
doi = {10.1016/j.jss.2016.09.045},
abstract = {A multi-objective test case prioritization real-world case study is presented.Seven objective functions based on functional and non-functional data are proposed.Comparison of the effectiveness of 63 combinations of up to three objectives.NSGA-II evolutionary algorithm to solve the multi-objective prioritization problem.Multi-objective prioritization is more effective than mono-objective approaches. Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non-functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
journal = {J. Syst. Softw.},
month = dec,
pages = {287–310},
numpages = {24},
keywords = {Variability, Test case prioritization, Highly-configurable systems, Automated software testing}
}

@article{10.1016/j.infsof.2016.08.005,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {Requirements monitoring frameworks},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.08.005},
doi = {10.1016/j.infsof.2016.08.005},
abstract = {Analyze the characteristics and application areas of monitoring approaches proposed in different domains.Systematically identify frameworks supporting requirements monitoring.Analyze to what extent the monitoring frameworks support requirements monitoring in SoS. ContextSoftware systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS. ObjectiveThe aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS. MethodWe performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS. ResultsWe identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson's requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS. ConclusionsWe conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {89–109},
numpages = {21},
keywords = {Systems of systems, Systematic literature review, Requirements monitoring}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Systematic mapping study, Source code metrics, Software metrics, Object-oriented metrics, Feature-oriented metrics, Aspect-oriented metrics}
}

@inproceedings{10.1145/2110147.2110160,
author = {Schroeter, Julia and Cech, Sebastian and G\"{o}tz, Sebastian and Wilke, Claas and A\ss{}mann, Uwe},
title = {Towards modeling a variable architecture for multi-tenant SaaS-applications},
year = {2012},
isbn = {9781450310581},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2110147.2110160},
doi = {10.1145/2110147.2110160},
abstract = {A widespread business model in cloud computing is to offer software as a service (SaaS) over the Internet. Such applications are often multi-tenant aware, which means that multiple tenants share hardware and software resources of the same application instance. However, SaaS stakeholders have different or even contradictious requirements and interests: For a user, the application's quality and non-functional properties have to be maximized (e.g., choosing the fastest available algorithm for a computation at runtime). In contrast, a resource or application provider is interested in minimizing the operating costs while maximizing his profit. Finally, tenants are interested in offering a customized functionality to their users. To identify an optimal compromise for all these objectives, multiple levels of variability have to be supported by reference architectures for multi-tenant SaaS applications. In this paper, we identify requirements for such a runtime architecture addressing the individual interests of all involved stakeholders. Furthermore, we show how our existing architecture for dynamically adaptive applications can be extended for the development and operation of multi-tenant applications.},
booktitle = {Proceedings of the 6th International Workshop on Variability Modeling of Software-Intensive Systems},
pages = {111–120},
numpages = {10},
keywords = {variability modeling, software-as-a-service, self-optimization, multi-tenancy, auto-tuning},
location = {Leipzig, Germany},
series = {VaMoS '12}
}

@inproceedings{10.5555/1947545.1947579,
author = {Schlegel, Christian and Steck, Andreas and Brugali, Davide and Knoll, Alois},
title = {Design abstraction and processes in robotics: from code-driven to model-driven engineering},
year = {2010},
isbn = {3642173187},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Advanced software engineering is the key factor in the design of future complex cognitive robots. It will decide about their robustness, (run-time) adaptivity, cost-effectiveness and usability.We present a novel overall vision of a model-driven engineering approach for robotics that fuses strategies for robustness by design and robustness by adaptation. It enables rigid definitions of quality-of-service, re-configurability and physics-based simulation as well as for seamless system level integration of disparate technologies and resource awareness.We report on steps towards implementing this idea driven by a first robotics meta-model with first explications of non-functional properties. A model-driven toolchain provides the model transformation and code generation steps. It also provides design time analysis of resource parameters (e.g. schedulability analysis of realtime tasks) as step towards resource awareness in the development of integrated robotic systems.},
booktitle = {Proceedings of the Second International Conference on Simulation, Modeling, and Programming for Autonomous Robots},
pages = {324–335},
numpages = {12},
location = {Darmstadt, Germany},
series = {SIMPAR'10}
}

@inproceedings{10.5555/1768904.1768909,
author = {Gallina, Barbara and Guelfi, Nicolas},
title = {A template for requirement elicitation of dependable product lines},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Engineering software quickly and at a low cost, while preserving quality, is a well-known objective that has not been reached. Reducing the development time can be achieved by reusing software components, as proposed in the software product line development approach. Dependability may be one of the most important attributes concerning quality, due to negative consequences (health, cost, time, etc.) induced by non-dependable software. Our proposal, presented in this article, is to offer a means to elicit the requirements of a product line, such that the dependability attribute would be explicitly considered, and such that reuse would be achieved by differentiating commonalities and variabilities between products. The proposed semi-formal template includes product commonality and variability elicitation, as well as elicitation of normal, misuse and recovery scenarios. Furthermore, we allow the elicitation of the advanced transactional nature of scenarios, since it provides us with a way to elicit fault tolerance requirements, which is our targeted means to achieving dependability.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {63–77},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {software product lines, modeling principles, Feature models},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/648114.748898,
author = {Svahnberg, Mikael and Mattsson, Michael},
title = {Conditions and Restrictions for Product Line Generation Migration},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper we describe a case study of a company in the domain of automatic guided vehicles (AGVs) that is in the process of migrating from a previous generation of software product line, which has mainly been centered around hardware, into a new product line generation, which will be software-centered. We describe the issues motivating this transition, and the factors that complicate it. Moreover, we present a three stage process for migrating into a new software product line. This process is currently initiated in collaboration with the aforementioned company.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {143–154},
numpages = {12},
series = {PFE '01}
}

@inproceedings{10.1007/11554844_20,
author = {Etxeberria, Leire and Sagardui, Goiuria},
title = {Product-line architecture: new issues for evaluation},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_20},
doi = {10.1007/11554844_20},
abstract = {In the product-line context, where a lack or mismatch in a quality attribute is potentially replicated among all products, product-line evaluation could detect problems before concrete products are developed. The life span of a software product-line architecture is much longer than the one of an ordinary software product and it serves as a basis for a set of related systems. Therefore, the product-line architecture should be adaptable to evolution as well as support a number of different products. All these characteristics set new requirements to the product-line architecture evaluation. This paper highlights the new issues that can arise when evaluating a product-line architecture versus evaluating a single-system architecture, including classifications of relevant attributes in product-line architecture evaluation, new evaluation moments and techniques. These issues are used as components of a framework to survey product-line architecture evaluation methods and metrics.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {174–185},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.5555/1870926.1871144,
author = {Neukirchner, Moritz and Stein, Steffen and Schrom, Harald and Ernst, Rolf},
title = {A software update service with self-protection capabilities},
year = {2010},
isbn = {9783981080162},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Integration of system components is a crucial challenge in the design of embedded real-time systems, as complex non-functional interdependencies may exist. We propose a software update service with self-protection capabilities against unverified system updates - thus solving the integration problem in-system.As modern embedded systems may evolve through software updates, component replacement or even self-optimization, possible system configurations are hard to predict. Thus the designer of system updates does not know the exact system configuration. This turns the proof of system feasibility into a critical challenge. This paper presents the architecture of a framework and associated protocols enabling updates in embedded systems while ensuring safe operation w.r.t. non-functional properties. The proposed process employs contract based principles at the interfaces towards applications to perform an in-system verification. Practical feasibility of our approach is demonstrated by an implementation of the update process, which is analzed w.r.t. the memory consumption overhead and execution time.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {903–908},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '10}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@inproceedings{10.1109/WI-IAT.2014.170,
author = {Louati, Amine and Haddad, Joyce El and Pinson, Suzanne},
title = {A Multilevel Agent-Based Approach for Trustworthy Service Selection in Social Networks},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.170},
doi = {10.1109/WI-IAT.2014.170},
abstract = {The growing number of services available within social applications (viz. Social networks) raises a new and challenging search issue: selecting desired services from social networks. Traditional discovery and selection approaches, which are registry-based (e.g., UDDI, ebXML), have manifested their limitations as they often fall behind users' expectations. This is because registries fail to (i) take into consideration non functional properties such as QoS and trust and (ii) capitalize on the information resulting from the previous experiences between agents. To address these shortcomings, we use software agents as they support interactions and offer well-developed capabilities to formally express and interpret semantic information useful to evaluate trust. Trust in a service is a multi-aspect concept that includes a social-based aspect such as judging whether the provider is worthwhile pursuing before using his services (viz. Trust in sociability), expert-based aspect such as estimating whether the service behaves well and as expected (viz. Trust in expertise) and, recommender-based aspect such as assessing whether an agent is reliable and we can rely on its recommendations (viz. Trust in recommendation).},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 03},
pages = {214–221},
numpages = {8},
keywords = {Trust, Social Networks, Service Selection, Referral Systems, Multi-Agent Systems},
series = {WI-IAT '14}
}

@inproceedings{10.1109/ISPAW.2011.69,
author = {Kwon, Jagun and Hailes, Stephen},
title = {A Lightweight, Component-Based Approach to Engineering Reconfigurable Embedded Real-Time Control Software},
year = {2011},
isbn = {9780769544298},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISPAW.2011.69},
doi = {10.1109/ISPAW.2011.69},
abstract = {The cost of poor or repeat engineering in complex control systems is extremely high, and flexibility in software design and implementation is one of the key factors in staying competitive in the market. Complexity can be managed most effectively if the underlying software systems support structured, standardised, high-level abstraction layers that encapsulate unnecessary details behind well-defined interfaces. Moreover, since the costs of software maintenance are often as high as that of initial development, the ease with which it is possible flexibly to reconfigure, re-engineer, and replace software components in operational systems is also critical. In this paper, we present a lightweight, component-based approach to engineering embedded real-time control software, which is realized in the form of a middleware system named MIREA. The middleware supports dynamic reconfiguration of components written in C/C++, and addresses variability management in relation to non-functional properties, such as quality-of-service (QoS) and real-time scheduling. Users are allowed to componentize existing libraries easily, such as the standard NIST 4D/Real-time Control Systems (RCS) library, which has been successfully used in many U.S government-driven intelligent control projects, and to reuse them as dynamically reconfigurable components. A realistic illustration is provided showing how control systems are structured and reconfigured using our approach. In fact, we discuss our approach to control using a fusion of NIST RCS as a means of architecting a real time control system and MIREA as a means of realising that architecture. Our progress to date suggests that MIREA is indeed well suited as a middleware facilitating the construction of efficient, lightweight, and scalable real-time embedded control systems.},
booktitle = {Proceedings of the 2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications Workshops},
pages = {361–366},
numpages = {6},
series = {ISPAW '11}
}

@article{10.1007/s10664-020-09912-w,
author = {Damasceno, Carlos Diego Nascimento and Mousavi, Mohammad Reza and Simao, Adenilso da Silva},
title = {Learning by sampling: learning behavioral family models from software product lines},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09912-w},
doi = {10.1007/s10664-020-09912-w},
abstract = {Family-based behavioral analysis operates on a single specification artifact, referred to as family model, annotated with feature constraints to express behavioral variability in terms of conditional states and transitions. Family-based behavioral modeling paves the way for efficient model-based analysis of software product lines. Family-based behavioral model learning incorporates feature model analysis and model learning principles to efficiently unify product models into a family model and integrate the behavior of various products into a behavioral family model. Albeit reasonably effective, the exhaustive analysis of product lines is often infeasible due to the potentially exponential number of valid configurations. In this paper, we first present a family-based behavioral model learning techniques, called FFSMDiff. Subsequently, we report on our experience on learning family models by employing product sampling. Using 105 products of six product lines expressed in terms of Mealy machines, we evaluate the precision of family models learned from products selected from different settings of the T-wise product sampling criterion. We show that product sampling can lead to models as precise as those learned by exhaustive analysis and hence, reduce the costs for family model learning.},
journal = {Empirical Softw. Engg.},
month = jan,
numpages = {46},
keywords = {T-wise sampling, Family model, Model learning, Software product lines}
}

@inproceedings{10.1007/11751113_3,
author = {Caporuscio, Mauro and Muccini, Henry and Pelliccione, Patrizio and Di Nisio, Ezio},
title = {Rapid system development via product line architecture implementation},
year = {2005},
isbn = {3540340637},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11751113_3},
doi = {10.1007/11751113_3},
abstract = {Software Product Line (SPL) engineering allows designers to reason about an entire family of software applications, instead of a single product, with a strategic importance for the rapid development of new applications. While much effort has been spent so far in understanding and modeling SPLs and their architectures, very little attention has been given on how to systematically enforce SPL architectural decisions into the implementation step.In this paper we propose a methodological approach and an implementation framework, based on a plugin component-based development, which allows us to move from an architectural specification of the SPL to its implementation in a systematic way. We show the suitability of this framework through its application to the TOOL one case study SPL.},
booktitle = {Proceedings of the Second International Conference on Rapid Integration of Software Engineering Techniques},
pages = {18–33},
numpages = {16},
location = {Heraklion, Crete, Greece},
series = {RISE'05}
}

@inproceedings{10.1145/3127041.3127059,
author = {Scheerer, Max and Busch, Axel and Koziolek, Anne},
title = {Automatic evaluation of complex design decisions in component-based software architectures},
year = {2017},
isbn = {9781450350938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127041.3127059},
doi = {10.1145/3127041.3127059},
abstract = {The quality of modern industrial plants depends on the quality of the hardware used, as well as software. While the impact on quality is comparably well understood by making decisions about the choice of hardware components, this is less true for the decisions on software components. The quality of the resulting software system is strongly influenced by its software architecture. Especially in early project phases a software architect has to make many design decisions. Each design decision highly influences the software architecture and thus, the resulting software quality. However, the impact on the resulting quality of architecture design decisions is hard to estimate in advance. For instance, a software architect could decide to deploy software components on a dedicated server in order to improve the system performance. However, such a decision may increase the network overhead as side-effect. Model-driven approaches have been shown as promising techniques enabling design-time quality prediction for different quality attributes such as performance or reliability. However, such approaches are limited in their automated decision support to simple design decisions like the exchange of one single component. In this paper, we present an approach that automatically evaluates complex design decisions in software architecture models. Such design decisions require the reuse of subsystems with many involved components coming with inhomogeneous architectures. We evaluate our approach using a real-world example system demonstrating the benefits of our approach.},
booktitle = {Proceedings of the 15th ACM-IEEE International Conference on Formal Methods and Models for System Design},
pages = {67–76},
numpages = {10},
keywords = {software quality, software architecture, reuse, model-driven engineering, decision support, component-based},
location = {Vienna, Austria},
series = {MEMOCODE '17}
}

@inproceedings{10.1145/1233901.1233908,
author = {Navarro, Luis Daniel Benavides and Schwanninger, Christa and Sobotzik, Robert and S\"{u}dholt, Mario},
title = {ATOLL: aspect-oriented toll system},
year = {2007},
isbn = {9781595936578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233901.1233908},
doi = {10.1145/1233901.1233908},
abstract = {Product line development places emphasis on quality attributes like understandability, maintainability, reusability and variability. Better modularization techniques like aspect-oriented programming are supposed to improve these attributes.In the context of an industrial case study in the domain of infrastructure software for toll systems from Siemens AG, Germany, we have investigated how OO designs can be enhanced using AO techniques. We have explored, in particular, how sequential crosscutting concerns can be modularized using AspectJ and how distributed ones can be modularized using AWED, a system that features aspects with explicit distribution. Concretely, we show how sequential and distributed aspects improve the implementation of the charge calculation functionality that is central to real-world tolling systems.},
booktitle = {Proceedings of the 6th Workshop on Aspects, Components, and Patterns for Infrastructure Software},
pages = {7–es},
keywords = {software product lines, aspect-oriented software development},
location = {Vancouver, British Columbia, Canada},
series = {ACP4IS '07}
}

@inproceedings{10.1007/978-3-030-25332-5_30,
author = {Zhivkov, Tsvetan and Schneider, Eric and Sklar, Elizabeth},
title = {MRComm: Multi-Robot Communication Testbed},
year = {2019},
isbn = {978-3-030-25331-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-25332-5_30},
doi = {10.1007/978-3-030-25332-5_30},
abstract = {This work demonstrates how dynamic robot behaviour that responds to different types of network disturbances can improve communication and mission performance in a Multi-Robot Team (MRT). A series of experiments are conducted which show how two different network perturbations (i.e. packet loss and signal loss) and two different network types (i.e. wireless local area network and ad-hoc network) impact communication. Performance is compared using two MRT behaviours: a baseline versus a novel dynamic behaviour that adapts to fluctuations in communication quality. Experiments are carried out on a known map with tasks assigned to a robot team at the start of a mission. During each experiment, a number of performance metrics are recorded. A novel dynamic Leader-Follower (LF) behaviour enables continuous communication through two key functions: the first reacts to the network type by using signal strength to determine if the robot team must commit to grouping together to maintain communication; and the second employs a special task status messaging function that guarantees a message is communicated successfully to the team members. The results presented in this work are significant for real-world multi-robot system applications that require continuous communication amongst team members.},
booktitle = {Towards Autonomous Robotic Systems: 20th Annual Conference, TAROS 2019, London, UK, July 3–5, 2019, Proceedings, Part II},
pages = {346–357},
numpages = {12},
keywords = {Dynamic roles, Behaviour-based control, Multi-robot team},
location = {London, United Kingdom}
}

@inproceedings{10.1145/378795.378860,
author = {Xiong, Jianxin and Johnson, Jeremy and Johnson, Robert and Padua, David},
title = {SPL: a language and compiler for DSP algorithms},
year = {2001},
isbn = {1581134142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/378795.378860},
doi = {10.1145/378795.378860},
abstract = {We discuss the design and implementation of a compiler that translates formulas representing signal processing transforms into efficient C or Fortran programs. The formulas are represented in a language that we call SPL, an acronym from Signal Processing Language. The compiler is a component of the SPIRAL system which makes use of formula transformations and intelligent search strategies to automatically generate optimized digital signal processing (DSP) libraries. After a discussion of the translation and optimization techniques implemented in the compiler, we use SPL formulations of the fast Fourier transform (FFT) to evaluate the compiler. Our results show that SPIRAL, which can be used to implement many classes of algorithms, produces programs that perform as well as “hard-wired” systems like FFTW.},
booktitle = {Proceedings of the ACM SIGPLAN 2001 Conference on Programming Language Design and Implementation},
pages = {298–308},
numpages = {11},
location = {Snowbird, Utah, USA},
series = {PLDI '01}
}

@inproceedings{10.1109/ECBS.2011.19,
author = {Galster, Matthias and Eberlein, Armin},
title = {Identifying Potential Core Assets in Service-Based Systems to Support the Transition to Service-Oriented Product Lines},
year = {2011},
isbn = {9780769543796},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ECBS.2011.19},
doi = {10.1109/ECBS.2011.19},
abstract = {Variability in service-oriented architectures (SOA) is usually addressed through loose coupling of services and dynamic retrieval, composition and binding of services. Facilitating variability can lead to different instances of one SOA. These instances share commonalities, but vary in certain aspects (e.g., in functionality or quality attributes). Current service-based development approaches do not adequately address variability and the management of different SOA instances. To handle different instances and to support systematic variability management, different instances of a service-based system may be treated as members of a product line. Therefore, we present a light-weight method to decide on what services to add to service-based systems to facilitate the transition from individual systems to a service-oriented product line. When adding services to service-based systems, the structural stability of these system decreases. We argue that a decrease in structural stability must be justified by additional value provided by the enhanced service-based systems. Based on the enhanced systems, our method then identifies potential core asset services for a service-oriented product line, taking into account common services within the different systems. Here, core asset services are reusable services that occur in any instance of the SOA. Thus, our method helps with the transition from individual products to a product line. A case study is included to illustrate our method.},
booktitle = {Proceedings of the 2011 18th IEEE International Conference and Workshops on Engineering of Computer-Based Systems},
pages = {179–186},
numpages = {8},
keywords = {variability, value, stability, service-oriented architectures, service-based, product lines, core assets, SOA},
series = {ECBS '11}
}

@article{10.1016/j.jss.2011.01.053,
author = {Clemente, Pedro J. and Hern\'{a}ndez, Juan and Conejero, Jos\'{e} M. and Ortiz, Guadalupe},
title = {Managing crosscutting concerns in component based systems using a model driven development approach},
year = {2011},
issue_date = {June, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {6},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.01.053},
doi = {10.1016/j.jss.2011.01.053},
abstract = {Abstract: In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure MDD philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach's benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two - stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages.},
journal = {J. Syst. Softw.},
month = jun,
pages = {1032–1053},
numpages = {22},
keywords = {Transformation models, Model Driven Development (MDD), Crosscutting concerns, Component Based Software Development (CBSD), CORBA Component Model (CCM), Aspect Oriented Software Development (AOSD)}
}

@inproceedings{10.1145/2739482.2768422,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Assun\c{c}\~{a}o, Wesley K.G. and Fischer, Stefan and Vergilio, Silvia R. and Egyed, Alexander},
title = {Genetic Improvement for Software Product Lines: An Overview and a Roadmap},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768422},
doi = {10.1145/2739482.2768422},
abstract = {Software Product Lines (SPLs) are families of related software systems that provide different combinations of features. Extensive research and application attest to the significant economical and technological benefits of employing SPL practices. However, there are still several challenges that remain open. Salient among them is reverse engineering SPLs from existing variants of software systems and their subsequent evolution. In this paper, we aim at sketching connections between research on these open SPL challenges and ongoing work on Genetic Improvement. Our hope is that by drawing such connections we can spark the interest of both research communities on the exciting synergies at the intersection of these subject areas.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {823–830},
numpages = {8},
keywords = {variability, software product lines, genetic programming, genetic improvement, evolutionary algorithms},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

@article{10.1007/s11219-012-9185-8,
author = {Bagheri, Ebrahim and Ga\v{s}evi\'{c}, Dragan},
title = {Foreword to the special issue on quality engineering for software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9185-8},
doi = {10.1007/s11219-012-9185-8},
journal = {Software Quality Journal},
month = sep,
pages = {421–424},
numpages = {4}
}

@article{10.1007/s11219-012-9193-8,
author = {Mets\"{a}, Jani and Maoz, Shahar and Katara, Mika and Mikkonen, Tommi},
title = {Using aspects for testing of embedded software: experiences from two industrial case studies},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9193-8},
doi = {10.1007/s11219-012-9193-8},
abstract = {Aspect-oriented software testing is emerging as an important alternative to conventional procedural and object-oriented testing techniques. This paper reports experiences from two case studies where aspects were used for the testing of embedded software in the context of an industrial application. In the first study, we used code-level aspects for testing non-functional properties. The methodology we used for deriving test aspect code was based on translating high-level requirements into test objectives, which were then implemented using test aspects in AspectC++. In the second study, we used high-level visual scenario-based models for the test specification, test generation, and aspect-based test execution. To specify scenario-based tests, we used a UML2-compliant variant of live sequence charts. To automatically generate test code from the models, a modified version of the S2A Compiler, outputting AspectC++ code, was used. Finally, to examine the results of the tests, we used the Tracer, a prototype tool for model-based trace visualization and exploration. The results of the two case studies show that aspects offer benefits over conventional techniques in the context of testing embedded software; these benefits are discussed in detail. Finally, towards the end of the paper, we also discuss the lessons learned, including the technological and other barriers to the future successful use of aspects in the testing of embedded software in industry.},
journal = {Software Quality Journal},
month = jun,
pages = {185–213},
numpages = {29},
keywords = {Software testing, Embedded software, Case studies, Aspect-oriented programming}
}

@article{10.1145/2413038.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {January 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2413038.2382768},
doi = {10.1145/2413038.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {46–49},
numpages = {4},
keywords = {variability, software architecture, VARSA}
}

@article{10.1145/2382756.2382768,
author = {Brahmasani, Siva and Selvakumar, Subramanian and Sivasankar, E.},
title = {Prevention of XSS attacks using STCD: Server side tagging and client side differentiation},
year = {2013},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382768},
doi = {10.1145/2382756.2382768},
abstract = {Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture &amp; 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–9},
numpages = {9},
keywords = {variability, software architecture, VARSA}
}

@article{10.1016/j.jss.2007.04.011,
author = {Kim, Jintae and Park, Sooyong and Sugumaran, Vijayan},
title = {DRAMA: A framework for domain requirements analysis and modeling architectures in software product lines},
year = {2008},
issue_date = {January, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.04.011},
doi = {10.1016/j.jss.2007.04.011},
abstract = {One of the benefits of software product line approach is to improve time-to-market. The changes in market needs cause software requirements to be flexible in product lines. Whenever software requirements are changed, software architecture should be evolved to correspond with them. Therefore, domain architecture should be designed based on domain requirements. It is essential that there is traceability between requirements and architecture, and that the structure of architecture is derived from quality requirements. The purpose of this paper is to provide a framework for modeling domain architecture based on domain requirements within product lines. In particular, we focus on the traceable relationship between requirements and architectural structures. Our framework consists of processes, methods, and a supporting tool. It uses four basic concepts, namely, goal based domain requirements analysis, Analytical Hierarchy Process, Matrix technique, and architecture styles. Our approach is illustrated using HIS (Home Integration System) product line. Finally, industrial examples are used to validate DRAMA.},
journal = {J. Syst. Softw.},
month = jan,
pages = {37–55},
numpages = {19},
keywords = {Traceability, Quality attribute, Domain requirements, Domain architecture}
}

@inproceedings{10.1145/2993412.3011881,
author = {da Silva Amorim, Simone and McGregor, John D. and de Almeida, Eduardo Santana and von Flach G. Chavez, Christina},
title = {Software ecosystems architectural health: challenges x practices},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3011881},
doi = {10.1145/2993412.3011881},
abstract = {Over time many software ecosystems have achieved success. Several organizations are opening their software projects for external businesses, creating an multi-organizational government to development their software platform The software architecture has an important participation in this success. In this context, there are some studies describing architectural challenges for software ecosystems, but little research is investigating how these challenges are being faced by software ecosystems organizations. This paper presents an initial investigation how open source software (OSS) ecosystems have faced several architectural challenges. We conducted interviews with three architects of different OSS ecosystems and gathered some architectural practices to lead with challenges. We also analyzed how these architectural practices have influenced the software ecosystem health, introducing the concept of Software Ecosystems Architectural Health.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {4},
numpages = {7},
keywords = {software improvement, software ecosystems, software architecture},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.5555/648033.744227,
author = {Anastasopoulos, Michalis and Atkinson, Colin and Muthig, Dirk},
title = {A Concrete Method for Developing and Applying Product Line Architectures},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development organizations are often deterred from introducing product line architectures by the lack of simple, ready-touse methods for developing and applying them. The well-known, published product-line-engineering methods tend to focus on the early stages of the software life cycle and address product line issues at a high-level of abstraction. Connecting product-line concepts with established implementation technologies is thus largely left to the user.This paper introduces a method, known as the KobrA method, which addresses this problem by enabling product line concerns to be coupled with regular (nonproduct line) architectural artifacts, and thus introduced incrementally. By explaining how the method can be understood as a concrete instantiation of the well-established PuLSE-DSSA product-line architecture approach, the paper clarifies the product line features of the KobrA method and illustrates how they can be used in tandem with established, general-purpose product line methods.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {294–312},
numpages = {19},
series = {NODe '02}
}

@article{10.1016/j.infsof.2012.07.014,
author = {Ciccozzi, Federico and Cicchetti, Antonio and Sj\"{o}din, Mikael},
title = {Round-trip support for extra-functional property management in model-driven engineering of embedded systems},
year = {2013},
issue_date = {June, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.07.014},
doi = {10.1016/j.infsof.2012.07.014},
abstract = {Context: In order for model-driven engineering to succeed, automated code generation from models through model transformations has to guarantee that extra-functional properties specified at design level are preserved at code level. Objective: The goal of this research work is to provide a full round-trip engineering approach in order to evaluate quality attributes of the embedded system by code execution monitoring as well as code static analysis and then provide back-propagation of the resulting values to modelling level. In this way, properties that can only be roughly estimated statically are evaluated against observed values and this consequently allows to refine the design models for ensuring preservation of analysed extra-functional properties at code level. Method: Following the model-driven engineering vision, (meta-) models and transformations are used as main artefacts for the realisation of the round-trip support which is finally validated against an industrial case study. Result: This article presents an approach to support the whole round-trip process starting from the generation of source code for a target platform, passing through the monitoring of selected system quality attributes at code level, and finishing with the back-propagation of observed values to modelling level. The technique is validated against an industrial case study in the telecommunications applicative domain. Conclusion: Preservation of extra-functional properties through appropriate description, computation and evaluation makes it possible to reduce final product verification and validation effort and costs by generating correct-by-construction code. The proposed round-trip support aids a model-driven component-based development process in ensuring a desired level of extra-functional properties preservation from the source modelling artefacts to the generated code.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {1085–1100},
numpages = {16},
keywords = {Traceability, Model-driven engineering, Model transformations, Extra-functional properties, Code generation, Back-propagation}
}

@article{10.5555/1629036.1629068,
author = {Hunt, John M. and McGregor, John D.},
title = {Building software that is predictable by Construction},
year = {2009},
issue_date = {December 2009},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {25},
number = {2},
issn = {1937-4771},
abstract = {Predictability by Construction is an effort to provide predictions for quality attributes such as performance during the design process in the same way that type systems allow predictions about aspects of correctness. This workshop introduces participants to the PACC Starter Kit (see: http://www.sei.cmu.edu/pacc/), which supports a state machine paradigm. Tools in the kit support computational theories that can be used to make predictions during design and to analyze the code to determine whether the predictions are correct. We will present classroom-tested exercises that use the kit in software engineering, formal methods, operating system, programming languages and theory of computation courses.},
journal = {J. Comput. Sci. Coll.},
month = dec,
pages = {203–204},
numpages = {2}
}

@inproceedings{10.1145/2000229.2000248,
author = {Borde, Etienne and Carlson, Jan},
title = {Towards verified synthesis of ProCom, a component model for real-time embedded systems},
year = {2011},
isbn = {9781450307239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000229.2000248},
doi = {10.1145/2000229.2000248},
abstract = {To take advantage of component-based software engineering, software designers need a component framework that automates the assemblage and integration of developed components. It is then of prime importance to ensure that the synthesized code respects the definition of the component model's semantics. This is all the more difficult in the domain of embedded systems since the considered semantics usually aims at characterizing both functional properties (e.g. data and control dependencies) and non-functional properties such as timing and memory consumption.The component model considered in this paper, called ProCom, relies on an asynchronous operational semantics and a formal hypothesis of atomic and instantaneous interactions between components. The asynchronous approach targets higher exibility in the deployment and analysis process, while the formal hypothesis helps in reducing the combinatory problems of formal verification.In this paper, we present a code generation strategy to synthesize ProCom components, and a formalization of this generated code. This formalization extends the verification possibilities of ProCom architectures, and constitutes a step toward the verification that the produced code respects the operational semantics of ProCom.},
booktitle = {Proceedings of the 14th International ACM Sigsoft Symposium on Component Based Software Engineering},
pages = {129–138},
numpages = {10},
keywords = {verification, synthesis, real-time, embedded systems, component basedmodel},
location = {Boulder, Colorado, USA},
series = {CBSE '11}
}

@inproceedings{10.1109/FOSE.2007.2,
author = {Issarny, Valerie and Caporuscio, Mauro and Georgantas, Nikolaos},
title = {A Perspective on the Future of Middleware-based Software Engineering},
year = {2007},
isbn = {0769528295},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FOSE.2007.2},
doi = {10.1109/FOSE.2007.2},
abstract = {Middleware is a software layer that stands between the networked operating system and the application and provides well known reusable solutions to frequently encountered problems like heterogeneity, interoperability, security, dependability. Further, with networks becoming increasingly pervasive, middleware appears as a major building block for the development of future software systems. Starting with the impact of pervasive networking on computing models, manifested by now common grid and ubiquitous computing, this paper surveys related challenges for the middleware and related impact on the software development. Indeed, future applications will need to cope with advanced non-functional properties such as contextawareness and mobility, for which adequate middleware support must be devised together with accompanying software development notations, methods and tools. This leads us to introduce our view on next generation middleware, considering both technological advances in the networking area but also the need for closer integration with software engineering best practices, to ultimately suggest middleware-based software processes.},
booktitle = {2007 Future of Software Engineering},
pages = {244–258},
numpages = {15},
series = {FOSE '07}
}

@article{10.1016/j.cie.2015.10.006,
author = {Vidoni, Melina C. and Vecchietti, Aldo R.},
title = {A systemic approach to define and characterize Advanced Planning Systems (APS)},
year = {2015},
issue_date = {December 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0360-8352},
url = {https://doi.org/10.1016/j.cie.2015.10.006},
doi = {10.1016/j.cie.2015.10.006},
abstract = {A Software Engineering perspective using several international standards to analyze Advanced Planning and System (APS).Functional Requirements for an APS are proposed.Quality Attributes linked to the Functional Requirements based on SQUARE model.Reference model architecture for an APS based on ISO/IEC 42010. Advanced Planning Systems (APS) have become an important tool for manufacturing and production companies that require a specific system to optimize production, logistic, material and human resources, etc. with the goal of improving the economy of the companies and offer a good customer service. An APS must be integrated to the Enterprise's System (such as an ERP), but this task usually lacks of a specific methodology to be performed and is generally made ad-hoc. With the ultimate objective to provide an approach to facilitate this integration, this work presents a characterization of the APS from a systemic point of view, using standardized Software Engineering concepts. The idea is to provide a definition and characterization of Advanced Planning Systems, by establishing the main goals of this type of system, and considering Functional Requirements, Quality Attributes and a reference model for the architecture. The selected choices are established on the base of several international standards from the Software Engineering area, such as the SEBoK (System Engineering Body of Knowledge) and the SQuaRE (Software product Quality Requirements and Evaluation) model, among others, and aim to serve as a base line for the general concept of APS.},
journal = {Comput. Ind. Eng.},
month = dec,
pages = {326–338},
numpages = {13},
keywords = {Requirements, ERP, Architecture, Advanced Planning System, APS}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@article{10.1016/j.infsof.2009.11.008,
author = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
title = {Knowledge based quality-driven architecture design and evaluation},
year = {2010},
issue_date = {June, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.11.008},
doi = {10.1016/j.infsof.2009.11.008},
abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {577–601},
numpages = {25},
keywords = {Tool, Software architecture, Quality attribute, Ontology, Model-driven development, Evaluation}
}

@inproceedings{10.5555/648033.744208,
author = {Muthig, Dirk and Patzke, Thomas},
title = {Generic Implementation of Product Line Components},
year = {2002},
isbn = {3540007377},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {An argument pro component-based software development is the idea of constructing software systems by assembling preexisting components instead of redeveloping similar or identical functionality always from scratch. Unfortunately, integrating existing components practically means adaptation and use rather than use only, which makes an ideal component-based development hard to realize in practice. Product line engineering, however, tackles this problem by making components as generic as needed for a particular product family and thus allows component reuse. Such a component covers variabilities and thus its implementation must consider variabilities as well.In this paper, we describe a process for implementing generic product line components and give an overview of variability mechanisms at the implementation level, illustrated by a running example, a generic test component.},
booktitle = {Revised Papers from the International Conference NetObjectDays on Objects, Components, Architectures, Services, and Applications for a Networked World},
pages = {313–329},
numpages = {17},
series = {NODe '02}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9013399,
author = {Ramirez-Espinosa, Pablo and Moualeu, Jules M. and da Costa, Daniel Benevides and Lopez-Martinez, F. Javier},
title = {The alpha-k-µ Shadowed Fading Distribution: Statistical Characterization and Applications},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9013399},
doi = {10.1109/GLOBECOM38437.2019.9013399},
abstract = {We introduce the /spl alpha/-/spl kappa/-/spl mu/ shadowed (/spl alpha/-KMS) fading distribution as a natural generalization of the versatile /spl alpha/-/spl kappa/-/spl mu/ and /spl alpha/-/spl eta/-/spl mu/distributions. The /spl alpha/-KMS fading distribution unifies a wide set of fading distributions, as it includes the /spl alpha/-/spl kappa/-/spl mu/, /spl alpha/-/spl eta/-/spl mu/, /spl alpha/-/spl mu/, Weibull, /spl kappa/-/spl mu/ shadowed, Rician shadowed, /spl kappa/-/spl mu/ and /spl eta/-/spl mu/ distributions as special cases, together with classical models like Rice, Nakagami-m, Hoyt, Rayleigh and one-sided Gaussian. Notably, the /spl alpha/-KMS distribution reduces to a finite mixture of /spl alpha/-/spl mu/ distributions when the fading parameters /spl mu/ and m take positive integer values, so that performance analysis over /spl alpha/- KMS fading channels can be tackled by leveraging previous (existing) results in the literature for the simpler /spl alpha/-spl mu/ case. As application examples, important performance metrics like the outage probability and average channel capacity are analyzed},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1109/ICSEA.2009.59,
author = {Bartholdt, Joerg and Medak, Marcel and Oberhauser, Roy},
title = {Integrating Quality Modeling with Feature Modeling in Software Product Lines},
year = {2009},
isbn = {9780769537771},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSEA.2009.59},
doi = {10.1109/ICSEA.2009.59},
abstract = {Due to the large number of possible variants in typical Software Product Lines (SPLs), the modeling of, explicit knowledge of, and predictability of the quality tradeoffs inherent in certain feature selections are critical to the future viability of SPLs. This paper presents IQ-SPLE, an integrated tool-supported approach that considers both qualitative and quantitative quality attributes without imposing hierarchical structural constraints. The approach is demonstrated with an eHealth SPL scenario, with the results showing that this approach is promising for effectively addressing the integration of quality attributes in SPL Engineering (SPLE).},
booktitle = {Proceedings of the 2009 Fourth International Conference on Software Engineering Advances},
pages = {365–370},
numpages = {6},
keywords = {variability, software product lines, quality modeling, feature modeling},
series = {ICSEA '09}
}

@article{10.1145/3284971.3284975,
author = {Lazreg, Sami and Collet, Philippe and Mosser, S\'{e}bastien},
title = {Functional feasibility analysis of variability-intensive data flow-oriented applications over highly-configurable platforms},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3284971.3284975},
doi = {10.1145/3284971.3284975},
abstract = {Data-flow oriented embedded systems, such as automotive systems used to render HMI (e.g., instrument clusters, infotainments), are increasingly built from highly variable specifications while targeting different constrained hardware platforms configurable in a fine-grained way. These variabilities at two different levels lead to a huge number of possible embedded system solutions, which functional feasibility is extremely complex and tedious to predetermine. In this paper, we propose a tooled approach that capture high level specifications as variable dataflows, and targeted platforms as variable component models. Dataflows can then be mapped onto platforms to express a specification of such variability-intensive systems. The proposed solution transforms this specification into structural and behavioral variability models and reuses automated reasoning techniques to explore and assess the functional feasibility of all variants in a single run. We also report on the validation of the proposed approach. A qualitative evaluation has been conducted on an industrial case study of automotive instrument cluster, while a quantitative one is reported on large generated datasets.},
journal = {SIGAPP Appl. Comput. Rev.},
month = oct,
pages = {32–48},
numpages = {17},
keywords = {variability modeling, feature model, embedded system design engineering, behavioral product lines model checking}
}

@article{10.1145/3412816.3412818,
author = {Brings, Jennifer and Daun, Marian and Weyer, Thorsten and Pohl, Klaus},
title = {Analyzing goal variability in cyber-physical system networks},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3412816.3412818},
doi = {10.1145/3412816.3412818},
abstract = {Networks of collaborative cyber-physical systems can achieve goals individual systems are incapable of achieving on their own. However, which goals such a network can achieve depends, in part, on the networks current configuration, i.e. its composition of partaking individual systems. As networks of collaborative cyber-physical systems are of a dynamic nature, the composition of such a network can change during runtime, leading to a plethora of often similar, albeit slightly different configurations. Due to the huge number of possible configurations and their various dependencies to the different goals of the network, it is infeasible to handle this amount of information manually. Hence, to provide support for reasoning about dependencies between different configurations and the goals they can achieve, this paper contributes an automated model-based reasoning approach using view generations. Our approach allows for exploring which goals can be fulfilled by which configurations and which goals cannot be fulfilled by these configurations. We evaluated the approach using an industrial case study which shows the applicability of the approach and a controlled experiment which shows the benefits of the approach.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jul,
pages = {19–35},
numpages = {17},
keywords = {goal modeling, cyber-physical systems, cardinality-based feature models, GRL}
}

@inproceedings{10.1145/337180.337455,
author = {Gannod, Gerald C. and Lutz, Robyn R.},
title = {An approach to architectural analysis of product lines},
year = {2000},
isbn = {1581132069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/337180.337455},
doi = {10.1145/337180.337455},
abstract = {This paper addresses the issue of how to perform architectural analysis on an existing product line architecture. The con tribution of the paper is to identify and demonstrate a repeatable product line architecture analysis process. The approach defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. It then analyzes the architecture against these criteria by both manual and tool-supported methods. The phased approach described in this paper provides a structured analysis of an existing product line architecture using (1) formal specification of the high-level architecture, (2) manual analysis of scenarios to exercise the architecture's support for required variabilities, and (3) model checking of critical behaviors at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain and evaluate the approach.},
booktitle = {Proceedings of the 22nd International Conference on Software Engineering},
pages = {548–557},
numpages = {10},
keywords = {software archtecture, software architecture analysis, product lines, interferometry software},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1007/11554844_6,
author = {Kang, Kyo Chul and Kim, Moonzoo and Lee, Jaejoon and Kim, Byungkil},
title = {Feature-oriented re-engineering of legacy systems into product line assets: a case study},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_6},
doi = {10.1007/11554844_6},
abstract = {Home service robots have a wide range of potential applications, such as home security, patient caring, cleaning, etc. The services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Thus, robot applications need to evolve both quickly and flexibly adopting frequently changing requirements. This makes software product line framework ideal for the domain of home service robots. Unfortunately, however, robot manufacturers often focus on developing technical components (e.g., vision recognizer and speech processor) and then attempt to develop robots by integrating these components in an ad-hoc way. This practice produces robot applications that are hard to re-use and evolve when requirements change. We believe that re-engineering legacy robot applications into product line assets can significantly enhance reusability and evolvability.In this paper, we present our experience of re-engineering legacy home service robot applications into product line assets through feature modeling and analysis. First, through reverse engineering, we recovered architectures and components of the legacy applications. Second, based on the recovered information and domain knowledge, we reconstructed a feature model for the legacy applications. Anticipating changes in business opportunities or technologies, we restructured and refined the feature model to produce a feature model for the product line. Finally, based on the refined feature model and engineering principles we adopted for asset development, we designed a new architecture and components for robot applications.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {45–56},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.5555/645882.672395,
author = {Kang, Kyo Chul and Donohoe, Patrick and Koh, Eunman and Lee, Jaejoon and Lee, Kwanwoo},
title = {Using a Marketing and Product Plan as a Key Driver for Product Line Asset Development},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The product line engineering paradigm has emerged recently to address the need to minimize the development cost and the time to market in this highly competitive global market. Product line development consists of product line asset development and product development using the assets. Product line requirements are essential inputs to product line asset development. These inputs, although critical, are not sufficient to develop product line assets. A marketing and product plan, which includes plans on what features are to be packaged in products, how these features will be delivered to customers (e.g., feature binding time), and how the products will evolve in the future, also drives product line asset development; thus this paper explores design issues from the marketing perspective and presents key design drivers that are tightly coupled with the marketing strategy. An elevator control software example is used to illustrate how product line asset development is related to marketing and product plans.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {366–382},
numpages = {17},
series = {SPLC 2}
}

@inproceedings{10.1145/2420942.2420943,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Ga\v{s}evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {The Fourth International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML2012)},
year = {2012},
isbn = {9781450318075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420942.2420943},
doi = {10.1145/2420942.2420943},
abstract = {The International Workshop on Non-functional System Properties in Domain Specific Modeling Languages (NFPinDSML) series traditionally takes place as part of the Satellite Events of the ACM/IEEE International Conference on Model Driven Engineering Languages and Systems (MODELS). Traditionally, NFPinDSML gathers researchers and practitioners interested in the estimation and evaluation of system quality and their integration in Domain Specific Modeling Languages and Model Driven Engineering in general. This paper is the summary of the fourth NFPinDSML workshop which was affiliated with MODELS 2012.},
booktitle = {Proceedings of the Fourth International Workshop on Nonfunctional System Properties in Domain Specific Modeling Languages},
articleno = {1},
numpages = {2},
location = {Innsbruck, Austria},
series = {NFPinDSML '12}
}

@article{10.1016/j.future.2016.02.006,
author = {Tibermacine, Chouki and Sadou, Salah and Ton That, Minh Tu and Dony, Christophe},
title = {Software architecture constraint reuse-by-composition},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {61},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2016.02.006},
doi = {10.1016/j.future.2016.02.006},
abstract = {Architecture constraints are specifications which enable developers to formalize design rules that architectures should respect, like the topological conditions of a given architecture pattern or style. These constraints can serve as a documentation to better understand an existing architecture description, or can serve as invariants that can be checked after the application of an architecture change to see whether design rules still hold. Like any specifications, architecture constraints are frequently subject to reuse. Besides, these constraints are specified and checked during architecture design time, when component descriptions are specified or selected from repositories, then instantiated and connected together to define architecture descriptions. These two facts (being subject to reuse and instantiation/connection) make architecture constraints good candidates for component-based design within a unified environment. In this paper, we propose a component model for specifying architecture constraints. This model has been implemented as an extension to an ADL that we have developed, which is called CLACS. The obtained process advocates the idea of specifying architecture constraints using the same paradigm of component-based development as for architecture description. To evaluate the component model, we conducted an experiment with a catalog of constraints formalizing the topological conditions of architecture patterns. The results of this experiment showed that constraint specification is improved by this reuse-by-composition model. Design choices in architecture description can be formalized as constraints.Architecture constraints are reusable and customizable specifications.A component model for enabling reusability in constraint specification.Specifying architecture models &amp; constraints with the same concept: component.An empirical evaluation of the model by considering architecture patterns.},
journal = {Future Gener. Comput. Syst.},
month = aug,
pages = {37–53},
numpages = {17},
keywords = {Software component, OCL, Architecture description, Architecture constraint}
}

@article{10.1016/j.infsof.2012.11.001,
author = {Schmid, Klaus and De Almeida, Eduardo Santana and Kishi, Tomoji},
title = {Editorial: Guest Editors' Introduction: Special Issue on Software Reuse and Product Lines},
year = {2013},
issue_date = {March, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.001},
doi = {10.1016/j.infsof.2012.11.001},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {489–490},
numpages = {2}
}

@article{10.1016/j.infsof.2019.06.012,
author = {Balera, Juliana Marino and Santiago J\'{u}nior, Valdivino Alexandre de},
title = {A systematic mapping addressing Hyper-Heuristics within Search-based Software Testing},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.06.012},
doi = {10.1016/j.infsof.2019.06.012},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {176–189},
numpages = {14},
keywords = {Meta-heuristics, Genetic Algorithms, Evolutionary Algorithms, Systematic Mapping, Hyper-heuristics, Search-based Software Testing}
}

@inproceedings{10.5555/2666064.2666068,
author = {Li, Dong and Yang, Ye},
title = {Enhance value by building trustworthy software-reliant system of systems from software product lines},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {Ever growing and expanding mission-critical domains generate ever emerging and more serious challenges. Complexity which may greatly reduce the trustworthiness of a system is the key reason. Based on the practice of FISCAN in the security inspection domain, we believe a software-reliant system of systems (srSoS) built from software product lines (SPL) is a viable solution to address these issues. In this paper, we present a framework of SPL-to-srSoS to extend SPL practices to srSoS and argue its value-enhancement effects by rapidly meeting increased complexity and emergent behaviors of System of Systems (SoS). The framework employs a basic principle of system hiding, and consists of a SPL-to-srSoS process model and an initially conceived value model. A successfully deployed FISCAN srSoS at Airports provides demonstration for the discussion throughout the paper.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {13–16},
numpages = {4},
keywords = {value model, trustworthy, system of systems, system hiding, software-reliant, software product lines, SPL-to-srSOS},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1145/2020976.2020978,
author = {Galster, Matthias and Avgeriou, Paris and Weyns, Danny and M\"{a}nnist\"{o}, Tomi},
title = {Variability in software architecture: current practice and challenges},
year = {2011},
issue_date = {September 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2020976.2020978},
doi = {10.1145/2020976.2020978},
abstract = {Variability in software-intensive systems is usually understood as the ability of a software artifact to be changed in order to fit different contexts, environments, or purposes. Software architecture on the other hand determines the structure of a software system, and is described in an architecture description. This description includes the major stakeholders of a software system and their concerns. Variability is reflected in and facilitated through the software architecture. The First International Workshop on Variability in Software Architecture (VARSA) was held jointly with WICSA 2011 in Boulder, Colorado. The goal of the workshop was to explore and advance the state-of-the art in variability in software architecture. It featured four research paper presentations, two invited talks, and three working groups that discussed specific topics. This report summarizes the themes of the workshop, presents the results of the working group discussions, and suggests topics for further research.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {30–32},
numpages = {3}
}

@article{10.1007/s11042-019-7498-3,
author = {Kaur, Taranjit and Saini, Barjinder Singh and Gupta, Savita},
title = {An adaptive fuzzy K-nearest neighbor approach for MR brain tumor image classification using parameter free bat optimization algorithm},
year = {2019},
issue_date = {Aug 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {15},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-019-7498-3},
doi = {10.1007/s11042-019-7498-3},
abstract = {This paper presents an automatic diagnosis system for the tumor grade classification through magnetic resonance imaging (MRI). The diagnosis system involves a region of interest (ROI) delineation using intensity and edge magnitude based multilevel thresholding algorithm. Then the intensity and the texture attributes are extracted from the segregated ROI. Subsequently, a combined approach known as Fisher+ Parameter-Free BAT (PFreeBAT) optimization is employed to derive the optimal feature subset. Finally, a novel learning approach dubbed as PFree BAT enhanced fuzzy K-nearest neighbor (FKNN) is proposed by combining FKNN with PFree BAT for the classification of MR images into two categories: High and Low-Grade. In PFree BAT enhanced FKNN, the model parameters, i.e., neighborhood size k and the fuzzy strength parameter m are adaptively specified by the PFree BAT optimization approach. Integrating PFree BAT with FKNN enhances the classification capability of the FKNN. The diagnostic system is rigorously evaluated on four MR images datasets including images from BRATS 2012 database and the Harvard repository using classification performance metrics. The empirical results illustrate that the diagnostic system reached to ceiling level of accuracy on the test MR image dataset via 5-fold cross-validation mechanism. Additionally, the proposed PFree BAT enhanced FKNN is evaluated on the Parkinson dataset (PD) from the UCI repository having the pre-extracted feature space. The proposed PFree BAT enhanced FKNN reached to an average accuracy of 98% and 97.45%. with and without feature selection on PD dataset. Moreover, solely to contrast, the performance of the proposed PFree BAT enhanced FKNN with the existing FKNN variants the experimentations were also done on six other standard datasets from KEEL repository. The results indicate that the proposed learning strategy achieves the best value of accuracy in contrast to the existing FKNN variants.},
journal = {Multimedia Tools Appl.},
month = aug,
pages = {21853–21890},
numpages = {38},
keywords = {Model parameters, Diagnosis system, PFree BAT optimization, Fuzzy K-nearest neighbor}
}

@article{10.1016/j.jss.2014.09.042,
author = {Zhi, Junji and Garousi-Yusifo\u{g}lu, Vahid and Sun, Bo and Garousi, Golara and Shahnewaz, Shawn and Ruhe, Guenther},
title = {Cost, benefits and quality of software development documentation},
year = {2015},
issue_date = {January 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {99},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.09.042},
doi = {10.1016/j.jss.2014.09.042},
abstract = {38% of papers propose new techniques and 29% contribute empirical evidences.Validation research papers and solution proposals are dominating types.Agile development is only mentioned in six studies.Design, code and requirement are dominating document types.Formatted text, models and tool support are dominating document forms.Most papers had only one SUS whose sizes do not increase strictly over time.The average number of participants in survey-based studies was 106.17% of papers focus on cost, in contrast to benefit (54%) and quality (72%).Only six studies (8%) discuss the development or production cost.Documentation's main usage includes maintenance aid and program comprehension.Most discussed quality attributes are completeness, consistency and accessibility.Only 10% of papers are affiliated with industry. ContextSoftware documentation is an integral part of any software development process. Researchers and practitioners have expressed concerns about costs, benefits and quality of software documentation in practice. On the one hand, there is a lack of a comprehensive model to evaluate the quality of documentation. On the other hand, researchers and practitioners need to assess whether documentation cost outweighs its benefit. ObjectivesIn this study, we aim to summarize the existing literature and provide an overview of the field of software documentation cost, benefit and quality. MethodWe use the systematic-mapping methodology to map the existing body of knowledge related to software documentation cost, benefit and quality. To achieve our objectives, 11 Research Questions (RQ) are raised. The primary papers are carefully selected. After applying the inclusion and exclusion criteria, our study pool included a set of 69 papers from 1971 to 2011. A systematic map is developed and refined iteratively. ResultsWe present the results of a systematic mapping covering different research aspects related to software documentation cost, benefit and quality (RQ 1-11). Key findings include: (1) validation research papers are dominating (27 papers), followed by solution proposals (21 papers). (2) Most papers (61 out of 69) do not mention the development life-cycle model explicitly. Agile development is only mentioned in 6 papers. (3) Most papers include only one "System under Study" (SUS) which is mostly academic prototype. The average number of participants in survey-based papers is 106, the highest one having approximately 1000 participants. (4) In terms of focus of papers, 50 papers focused on documentation quality, followed by 37 papers on benefit, and 12 papers on documentation cost. (5) The quality attributes of documentation that appear in most papers are, in order: completeness, consistency and accessibility. Additionally, improved meta-models for documentation cost, benefit and quality are also presented. Furthermore, we have created an online paper repository of the primary papers analyzed and mapped during this study. ConclusionOur study results show that this research area is emerging but far from mature. Firstly, documentation cost aspect seems to have been neglected in the existing literature and there are no systematic methods or models to measure cost. Also, despite a substantial number of solutions proposed during the last 40 years, more and stronger empirical evidences are still needed to enhance our understanding of this area. In particular, what we expect includes (1) more validation or evaluation studies; (2) studies involving large-scale development projects, or from large number of study participants of various organizations; (3) more industry-academia collaborations; (4) more estimation models or methods to assess documentation quality, benefit and, especially, cost.},
journal = {J. Syst. Softw.},
month = jan,
pages = {175–198},
numpages = {24},
keywords = {Systematic mapping, Software documentation, Documentation benefit}
}

@inproceedings{10.1145/3023956.3023963,
author = {Halin, Axel and Nuttinck, Alexandre and Acher, Mathieu and Devroey, Xavier and Perrouin, Gilles and Heymans, Patrick},
title = {Yo variability! JHipster: a playground for web-apps analyses},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023963},
doi = {10.1145/3023956.3023963},
abstract = {Though variability is everywhere, there has always been a shortage of publicly available cases for assessing variability-aware tools and techniques as well as supports for teaching variability-related concepts. Historical software product lines contains industrial secrets their owners do not want to disclose to a wide audience. The open source community contributed to large-scale cases such as Eclipse, Linux kernels, or web-based plugin systems (Drupal, WordPress). To assess accuracy of sampling and prediction approaches (bugs, performance), a case where all products can be enumerated is desirable. As configuration issues do not lie within only one place but are scattered across technologies and assets, a case exposing such diversity is an additional asset. To this end, we present in this paper our efforts in building an explicit product line on top of JHipster, an industrial open-source Web-app configurator that is both manageable in terms of configurations (≈ 163,000) and diverse in terms of technologies used. We present our efforts in building a variability-aware chain on top of JHipster's configurator and lessons learned using it as a teaching case at the University of Rennes. We also sketch the diversity of analyses that can be performed with our infrastructure as well as early issues found using it. Our long term goal is both to support students and researchers studying variability analysis and JHipster developers in the maintenance and evolution of their tools.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {44–51},
numpages = {8},
keywords = {web-apps, variability-related analyses, case study},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@article{10.1145/1968587.1968607,
author = {Geetha, D. Evangelin and Kumar, T.V. Suresh and Kanth, K. Rajani},
title = {Framework for hybrid performance prediction process model: use case performance engineering approach},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1968587.1968607},
doi = {10.1145/1968587.1968607},
abstract = {The dynamic behavior of distributed systems requires that their performance characteristics be determined rigorously, preferably in the early stages of software engineering process. Evaluation of the performance at the end of software development leads to increase in the cost of design change. To compare design alternatives or to identify system bottlenecks, quantitative system analysis must be carried out from the early stages of the software development life cycle. In this paper we describe a process model, Hybrid Performance Prediction Process Model that allows modeling and evaluating distributed systems with the explicit goal of assessing performance of the software system during feasibility study. The use case performance engineering approach proposed in this paper exploits use case model and provides flexibility to integrate the software performance prediction process with software engineering process. We use an e-parking application to demonstrate various elements in our framework. The performance metrics are obtained and analyzed by considering two software architectures. Sensitivity analysis on the behavior of resources is carried out. This analysis helps to determine the capacity of the execution environment to obtain the defined performance objectives.},
journal = {SIGSOFT Softw. Eng. Notes},
month = may,
pages = {1–15},
numpages = {15},
keywords = {use case performance engineering, unified modeling language, software performance prediction, simulation model, multitier architecture, hybrid performance prediction process model, 4+1 view model}
}

@inproceedings{10.1007/978-3-642-33182-4_11,
author = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and de Almeida Maia, Marcelo},
title = {A quantitative assessment of aspectual feature modules for evolving software product lines},
year = {2012},
isbn = {9783642331817},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33182-4_11},
doi = {10.1007/978-3-642-33182-4_11},
abstract = {Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines of how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from a SPL developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity stability were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns.},
booktitle = {Proceedings of the 16th Brazilian Conference on Programming Languages},
pages = {134–149},
numpages = {16},
keywords = {variability mechanisms, software product lines, feature-oriented programming, aspectual feature modules, aspect-oriented programming},
location = {Natal, Brazil},
series = {SBLP'12}
}

@inproceedings{10.1007/978-3-540-30587-3_27,
author = {van der Meulen, Meine and Riddle, Steve and Strigini, Lorenzo and Jefferson, Nigel},
title = {Protective wrapping of off-the-shelf components},
year = {2005},
isbn = {3540245480},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30587-3_27},
doi = {10.1007/978-3-540-30587-3_27},
abstract = {System designers using off-the-shelf components (OTSCs), whose internals they cannot change, often use add-on “wrappers” to adapt the OTSCs' behaviour as required. In most cases, wrappers are used to change “functional” properties of the components they wrap. In this paper we discuss instead protective wrapping, the use of wrappers to improve the dependability – i.e., “non-functional” properties like availability, reliability, security, and/or safety – of a component and thus of a system. Wrappers can improve dependability by adding fault tolerance, e.g. graceful degradation, or error recovery mechanisms. We discuss the rational specification of such protective wrappers in view of system dependability requirements, and highlight some of the design trade-offs and uncertainties that affect system design with OTSCs and wrappers, and that differentiate it from other forms of fault-tolerant design.},
booktitle = {Proceedings of the 4th International Conference on COTS-Based Software Systems},
pages = {168–177},
numpages = {10},
location = {Bilbao, Spain},
series = {ICCBSS'05}
}

@article{10.1016/j.jss.2014.01.021,
author = {Walraven, Stefan and Van Landuyt, Dimitri and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
title = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.021},
doi = {10.1016/j.jss.2014.01.021},
abstract = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
journal = {J. Syst. Softw.},
month = may,
pages = {48–62},
numpages = {15},
keywords = {Variability, SaaS, Multi-tenancy}
}

@article{10.1155/2018/2497352,
author = {Khan, Zeeshan Najam and Khan, Shayan Ali and Shakeel, Sobia and Greneche, Jean M.},
title = {Incorporation of Ge on High K Dielectric Material for Different Fabrication Technologies (HBT, CMOS) and Their Impact on Electrical Characteristics of the Device},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-4110},
url = {https://doi.org/10.1155/2018/2497352},
doi = {10.1155/2018/2497352},
abstract = {The paper is composed of distinct reviews on various fabrication technologies of the CMOS family and the characterization of MOS capacitors. The initial part of the article essentially presents a systemic review on an already conducted work on different fabrication technologies such as Si MOSFET, SiGe HBT, and InP HBT. Device and circuit-level performance for broadband and tuned millimetre-wave applications is discussed in detail relative to the underlying CMOS technologies. The comparison is made for various performance metrics for 180 nm, 130 nm, and 90 nm n-MOSFET devices for SiGe and InP HBTs. In the latter part of the study, a comprehensive review on a previously conducted research on electrical and physical characterization of metal-oxide-semiconductor (MOS) capacitors fabricated on a 2.5 μm epitaxial germanium layer grown on (100) silicon substrate is undertaken. The focus and crux of the study is the influence of germanium surface preparation on MOS electrical characteristics. It is observed that predielectric (HfO) deposition annealing in NH3 ambience results in the performance upgradation in critical and key parameters such as equivalent oxide thickness and the gate leakage current.},
journal = {J. Nanomaterials},
month = jan,
numpages = {7}
}

@inproceedings{10.1145/3377024.3377026,
author = {Kenner, Andy and Dassow, Stephan and Lausberger, Christian and Kr\"{u}ger, Jacob and Leich, Thomas},
title = {Using variability modeling to support security evaluations: virtualizing the right attack scenarios},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377026},
doi = {10.1145/3377024.3377026},
abstract = {A software system's security is constantly threatened by vulnerabilities that result from faults in the system's design (e.g., unintended feature interactions) and which can be exploited with attacks. While various databases summarize information on vulnerabilities and other security issues for many software systems, these databases face severe limitations. For example, the information's quality is unclear, often only semi-structured, and barely connected to other information. Consequently, it can be challenging for any security-related stakeholder to extract and understand what information is relevant, considering that most systems exist in different variants and versions. To tackle this problem, we propose to design vulnerability feature models that represent the vulnerabilities of a system and enable developers to virtualize corresponding attack scenarios. In this paper, we report a first case study on Mozilla Firefox for which we extracted vulnerabilities and used them to virtualize vulnerable instances in Docker. To this end, we focused on extracting information from available databases and on evaluating the usability of the results. Our findings indicate several problems with the extraction that complicate modeling, understanding, and testing of vulnerabilities. Nonetheless, the databases provide a valuable foundation for our technique, which we aim to extend with automatic synthesis and analyses of feature models, as well as virtualization for attack scenarios in future work.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {10},
numpages = {9},
keywords = {vulnerability, variability model, software architecture, feature model, exploit, docker-container, attack scenarios},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.5555/381473.381482,
author = {Bosch, Jan},
title = {Software product lines: organizational alternatives},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Software product lines enjoy increasingly wide adoption in the software industry. Most authors focus on the technical and process aspects and assume an organizational model consisting of a domain engineering unit and several application engineering units. In our cooperation with several software development organizations applying software product line principles, we have identified several other organizational models that are employed as well. In this article, we present a number of organizational alternatives, organized around four main models, i.e. development department, business units, domain engineering unit and hierarchical domain engineering units. For each model, its characteristics, applicability and advantages and disadvantages are discussed, as well as an example. Based on an analysis of these models, we present three factors that influence the choice of the organizational model, i.e. product-line assets, the responsibility levels and the type of organizational units.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {91–100},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@article{10.4018/IJCSSA.2015070103,
author = {Kolagari, Ramin Tavakoli and Chen, DeJiu and Lanusse, Agnes and Librino, Renato and L\"{o}nn, Henrik and Mahmud, Nidhal and Mraidha, Chokri and Reiser, Mark-Oliver and Torchiaro, Sandra and Tucci-Piergiovanni, Sara and W\"{a}gemann, Tobias and Yakymets, Nataliya},
title = {Model-Based Analysis and Engineering of Automotive Architectures with EAST-ADL: Revisited},
year = {2015},
issue_date = {July 2015},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {2},
issn = {2166-7292},
url = {https://doi.org/10.4018/IJCSSA.2015070103},
doi = {10.4018/IJCSSA.2015070103},
abstract = {Modern cars have turned into complex high-technology products, subject to strict safety and timing requirements, in a short time span. This evolution has translated into development processes that are not as efficient, flexible and agile as they could or should be. This paper presents the main aspects and capabilities of a rich model-based design framework, founded on EAST-ADL. EAST-ADL is an architecture description language specific to the automotive domain and complemented by a methodology compliant with the functional safety standard for the automotive domain ISO26262. The language and the methodology are used to develop an information model in the sense of a conceptual model, providing the engineer the basis for specifying the various aspects of the system. Inconsistencies, redundancies, and partly even missing system description aspects can be found automaticlally by advanced analyses and optimization capabilities to effectively improve development processes of modern cars.},
journal = {Int. J. Concept. Struct. Smart Appl.},
month = jul,
pages = {25–70},
numpages = {46},
keywords = {Timing Modelling, Software Product Lines, Optimization, Model-Based Software Development, ISO 26262, Functional Safety, EAST-ADL, Dependability, Automotive Software Development, AUTOSAR}
}

@article{10.1016/j.sysarc.2019.02.012,
author = {Brings, Jennifer and Daun, Marian and Bandyszak, Torsten and Stricker, Vanessa and Weyer, Thorsten and Mirzaei, Elham and Neumann, Martin and Zernickel, Jan Stefan},
title = {Model-based documentation of dynamicity constraints for collaborative cyber-physical system architectures: Findings from an industrial case study},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {97},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2019.02.012},
doi = {10.1016/j.sysarc.2019.02.012},
journal = {J. Syst. Archit.},
month = aug,
pages = {153–167},
numpages = {15},
keywords = {Dynamicity constraints, Dynamic morphology, CPS network, Exploratory case study, Cyber-physical systems, Cardinality-based feature models}
}

@inproceedings{10.1145/3194760.3194761,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Integrating reuse into the rapid, continuous software engineering cycle through test-driven search},
year = {2018},
isbn = {9781450357456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194760.3194761},
doi = {10.1145/3194760.3194761},
abstract = {Today's advanced agile practices such as Continuous Integration and Test-Driven Development support a wide range of software development activities to facilitate the rapid delivery of high-quality software. However, the reuse of pre-existing, third-party software components is not one of them. Software reuse is still primarily perceived as a time-consuming, unsystematic and ultimately, "discontinuous" activity even though it aims to deliver the same basic benefits as continuous software engineering - namely, a reduction in the time and effort taken to deliver quality software. However, the increasingly central role of testing in continuous software engineering offers a way of addressing this problem by exploiting the new generation of test-driven search engines that can harvest components based on tests. This search technology not only exploits artifacts that have already been created as part of the continuous testing process to harvest components, it returns results that have a high likelihood of being fit for purpose and thus of being worth reusing. In this paper, we propose to augment continuous software engineering with the rapid, continuous reuse of software code units by integrating the test-driven mining of software artifact repositories into the continuous integration process. More specifically, we propose to use tests written as part of the Test-First Development approach to perform test-driven searches for matching functionality while developers are working on their normal development activities. We discuss the idea of rapid, continuous code reuse based on recent advances in our test-driven search platform and elaborate on scenarios for its application in the future.},
booktitle = {Proceedings of the 4th International Workshop on Rapid Continuous Software Engineering},
pages = {8–11},
numpages = {4},
keywords = {test-driven search, test-driven reuse, test-driven development, rapid continuous integration, rapid continuous code reuse},
location = {Gothenburg, Sweden},
series = {RCoSE '18}
}

@inproceedings{10.1109/ICCBSS.2007.25,
author = {Bhattacharya, Sutirtha and Perry, Dewayne E.},
title = {Predicting Emergent Properties of Component Based Systems},
year = {2007},
isbn = {076952785X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCBSS.2007.25},
doi = {10.1109/ICCBSS.2007.25},
abstract = {Software Product Lines (SPL), Component Based Software Engineering (CBSE) and Commercial Off the Shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from prebuilt components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, the System Architect could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed. In this paper we propose a model for predicting architectural styles, and hence the quality attributes, based on use cases that need to be satisfied by a system configuration. Our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift.},
booktitle = {Proceedings of the Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems},
pages = {41–50},
numpages = {10},
series = {ICCBSS '07}
}

@article{10.1016/j.jss.2012.08.031,
author = {Kulesza, Uir\'{a} and Soares, S\'{e}Rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F. and Delicato, Fl\'{a}Via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot\'{a}Vio and Mendon\c{c}A, Nabor and Batista, Thais and Bonif\'{a}Cio, Rodrigo and Cacho, N\'{e}Lio and Silva, Lyrene and Von Staa, Arndt and Silveira, F\'{a}Bio and Valente, Marco T\'{u}Lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec\'{\i}Lia},
title = {The crosscutting impact of the AOSD Brazilian research community},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.08.031},
doi = {10.1016/j.jss.2012.08.031},
abstract = {Background: Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil. Aims: In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community. Method: Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results: Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.},
journal = {J. Syst. Softw.},
month = apr,
pages = {905–933},
numpages = {29},
keywords = {Research impact, Modularity, Aspect-Oriented Software Development}
}

@inproceedings{10.1145/3368089.3409675,
author = {Siegmund, Norbert and Ruckel, Nicolai and Siegmund, Janet},
title = {Dimensions of software configuration: on the configuration context in modern software development},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409675},
doi = {10.1145/3368089.3409675},
abstract = {With the rise of containerization, cloud development, and continuous integration and delivery, configuration has become an essential aspect not only to tailor software to user requirements, but also to configure a software system’s environment and infrastructure. This heterogeneity of activities, domains, and processes blurs the term configuration, as it is not clear anymore what tasks, artifacts, or stakeholders are involved and intertwined. However, each re- search study and each paper involving configuration places their contributions and findings in a certain context without making the context explicit. This makes it difficult to compare findings, translate them to practice, and to generalize the results. Thus, we set out to evaluate whether these different views on configuration are really distinct or can be summarized under a common umbrella. By interviewing practitioners from different domains and in different roles about the aspects of configuration and by analyzing two qualitative studies in similar areas, we derive a model of configuration that provides terminology and context for research studies, identifies new research opportunities, and allows practitioners to spot possible challenges in their current tasks. Although our interviewees have a clear view about configuration, it substantially differs due to their personal experience and role. This indicates that the term configuration might be overloaded. However, when taking a closer look, we see the interconnections and dependencies among all views, arriving at the conclusion that we need to start considering the entire spectrum of dimensions of configuration.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {338–349},
numpages = {12},
keywords = {variability, developer study, configuration management and life cycle, Dimensions of software configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.jss.2012.11.044,
author = {Souza, Iuri Santos and Da Silva Gomes, Gecynalda Soares and Da Mota Silveira Neto, Paulo Anselmo and Do Carmo Machado, Ivan and De Almeida, Eduardo Santana and De Lemos Meira, Silvio Romero},
title = {Evidence of software inspection on feature specification for software product lines},
year = {2013},
issue_date = {May, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.11.044},
doi = {10.1016/j.jss.2012.11.044},
abstract = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.},
journal = {J. Syst. Softw.},
month = may,
pages = {1172–1190},
numpages = {19},
keywords = {Software quality control, Software product lines, Software inspection, Empirical study}
}

@article{10.1016/j.infsof.2011.10.004,
author = {Mond\'{e}jar, Rub\'{e}n and Garc\'{\i}a-L\'{o}pez, Pedro and Pairot, Carles and Pamies-Juarez, Lluis},
title = {Damon: A distributed AOP middleware for large-scale scenarios},
year = {2012},
issue_date = {March, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.10.004},
doi = {10.1016/j.infsof.2011.10.004},
abstract = {Context: The development of distributed applications in large-scale environments has always been a complex task. In order to guarantee non-functional properties like scalability or availability, developers are usually faced with the same problems over and over again. These problems can be separated in distributed concerns, as for example, distribution, load-balancing or replication, just to name a few. Nevertheless, none of the current solutions in adaptive middleware area, like Aspect-Oriented Programming (AOP), is capable of implementing these distributed concerns transparently. Objective: In this article, we present a distributed AOP middleware for large-scale development, called Damon. Its main goal is to implement true distributed aspects, which enables the use of distributed concerns in applications that were not specifically designed for distributed or large-scale scenarios. Method: Our middleware comprises two main layers: a distributed composition model and a scalable deployment platform. The distributed composition model envisages separation of distributed aspects, taking the necessary features from component models, like distribution facilities and connectors, and from computational reflection, like introspection and meta-levels. This recursive and fully distributed model provides its own Architecture Description Language (ADL), and thus allows low dependency and high cohesion among distributed aspects. Additionally, our model is built on top of a deployment platform where distributed aspects are disseminated and activated in individual or grouped hosts. This platform benefits from peer-to-peer and dynamic AOP substrates to implement its services in a decentralized, decoupled, and efficient way. Results: Therefore, our middleware solution reduces the complexity of distributed application development, managing separated functionalities, and enabling necessary services like transparent reconfiguration and deployment at runtime. Finally, we have implemented a functional prototype and we conducted several experiments using the PlanetLab testbed. Conclusion: Our distributed AOP approach fulfills the large-scale scenarios requirements, and represents a solid building block for future distributed transparent infrastructures.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {317–330},
numpages = {14},
keywords = {Peer-to-peer, Middleware, Distributed AOP, Composition, ADL}
}

@article{10.1016/j.scico.2014.03.006,
author = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and Maia, Marcelo de Almeida},
title = {A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines},
year = {2014},
issue_date = {December 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {96},
number = {P2},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2014.03.006},
doi = {10.1016/j.scico.2014.03.006},
abstract = {Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from two SPLs developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the SPL structure. Variability mechanisms are systematically evaluated in the evolution of SPLs.FOP and AFM have shown better adherence to the Open-Closed Principle than CC.When crosscutting concerns are present, AFM are recommended over FOP.Refactoring at component level has important impact in AFM and FOP.CC compilation should be avoided when modular design is an important requirement.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {230–253},
numpages = {24},
keywords = {Variability mechanisms, Software product lines, Feature-oriented programming, Aspectual feature modules, Aspect-oriented programming}
}

@article{10.1016/j.jss.2016.06.102,
author = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
title = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.102},
doi = {10.1016/j.jss.2016.06.102},
abstract = {We proposed a novel software architecture-level adaptation approach.We adopted known architectural patterns in distributed and concurrent systems.We developed a framework to support the self-adaptive mechanism.We developed and evaluated five adaptive policies.Our approach improved performance and increased reliability in our experiments. More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
journal = {J. Syst. Softw.},
month = nov,
pages = {311–328},
numpages = {18},
keywords = {Software architecture, Reliability, Performance, Patterns, Elastic computing, Distributed and concurrent architecture, Autonomic computing}
}

@inbook{10.5555/1985668.1985673,
author = {Kazhamiakin, Raman and Benbernou, Salima and Baresi, Luciano and Plebani, Pierluigi and Uhlig, Maike and Barais, Olivier},
title = {Adaptation of service-based systems},
year = {2010},
isbn = {3642175988},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Service Research Challenges and Solutions for the Future Internet: S-Cube - towards Engineering, Managing and Adapting Service-Based Systems},
pages = {117–156},
numpages = {40}
}

@inproceedings{10.1109/SEAMS.2019.00018,
author = {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and Kehrer, Timo and Weyns, Danny and Calinescu, Radu and Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A. and M\"{u}ller, Hausi A. and Nenzi, Laura and Nuseibeh, Bashar and Pasquale, Liliana and Reisig, Wolfgang and Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
title = {Modelling and analysing resilient cyber-physical systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00018},
doi = {10.1109/SEAMS.2019.00018},
abstract = {From smart buildings to medical devices to smart nations, software systems increasingly integrate computation, networking, and interaction with the physical environment. These systems are known as Cyber-Physical Systems (CPS). While these systems open new opportunities to deliver improved quality of life for people and reinvigorate computing, their engineering is a difficult problem given the level of heterogeneity and dynamism they exhibit. While progress has been made, we argue that complexity is now at a level such that existing approaches need a major re-think to define principles and associated techniques for CPS. In this paper, we identify research challenges when modelling, analysing and engineering CPS. We focus on three key topics: theoretical foundations of CPS, self-adaptation methods for CPS, and exemplars of CPS serving as a research vehicle shared by a larger community. For each topic, we present an overview and suggest future research directions, thereby focusing on selected challenges. This paper is one of the results of the Shonan Seminar 118 on Modelling and Analysing Resilient Cyber-Physical Systems, which took place in December 2018.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {70–76},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@article{10.1007/s10009-014-0340-3,
author = {Hendriks, Martijn and Basten, Twan and Verriet, Jacques and Brass\'{e}, Marco and Somers, Lou},
title = {A blueprint for system-level performance modeling of software-intensive embedded systems},
year = {2016},
issue_date = {Feb 2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-014-0340-3},
doi = {10.1007/s10009-014-0340-3},
abstract = {Exploration of design alternatives and estimation of their key performance metrics such as latency and energy consumption is essential for making the proper design decisions in the early phases of system development. Often, high-level models of the dynamic behavior of the system are used for the analysis of design alternatives. Our work presents a blueprint for building efficient and re-usable models for this purpose. It builds on the well-known Y-chart pattern in that it gives more structure for the proper modeling of interaction on shared resources that plays a prominent role in software-intensive embedded systems. We show how the blueprint can be used to model a small yet illustrative example system with the Uppaal tool, and with the Java general-purpose programming language, and reflect on their respective strengths and weaknesses. The Java-based approach has resulted in a very flexible and fast discrete-event simulator with many re-usable components. It currently is used by TNO-ESI and Oc\'{e}-Technologies B.V. for early model-based performance analysis that supports the design process for professional printing systems.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = feb,
pages = {21–40},
numpages = {20},
keywords = {System-level modeling, Simulation, Performance analysis, Embedded system, Design space exploration}
}

@article{10.1016/j.infsof.2010.12.003,
author = {da Mota Silveira Neto, Paulo Anselmo and Carmo Machado, Ivan do and McGregor, John D. and de Almeida, Eduardo Santana and de Lemos Meira, Silvio Romero},
title = {A systematic mapping study of software product lines testing},
year = {2011},
issue_date = {May, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.003},
doi = {10.1016/j.infsof.2010.12.003},
abstract = {ContextIn software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. ObjectiveThis paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. MethodA systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. ResultsAlthough several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. ConclusionThe results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {407–423},
numpages = {17},
keywords = {Software testing, Software product lines, Mapping study}
}

@inproceedings{10.5555/1856821.1857004,
author = {Baras, John S. and Tabatabaee, Vahid and Jain, Kaustubh},
title = {Component based modeling for cross-layer analysis of 802.11 MAC and OLSR routing protocols in ad-hoc networks},
year = {2009},
isbn = {9781424452385},
publisher = {IEEE Press},
abstract = {We present a complete scenario driven component based analytic model of 802.11 MAC and OLSR routing protocols in MANETs. We use this model to provide a systematic approach to study the network performance and cross-layer analysis and design of routing, scheduling, MAC and PHY layer protocols. The routing protocol is divided into multiple components. Componentization is a standard methodology for analysis and synthesis of complex systems. To provide a component based design methodology, we have to develop a component based model of the wireless network that considers cross-layer dependency of performance. The component based model enable us to study the effect of each component on the overall performance of the wireless network, and to design each component separately. For the MAC layer, we use a fixed point loss model of 802.11 protocol that considers effects of hidden nodes and finite retransmission attempts. We have also considered simple models for PHY and scheduling. The main focus of this paper is on integration of these models to obtain a complete model for wireless networks.In several scenario driven studies, with user-specified topologies and traffic demands, we study the performance metrics-throughput and delay. By analyzing the performances under varying network scenarios, we are able to identify a few sources of performance degradation. We also study the effect of certain design parameters on the network performance. Thus, demonstrating the ability of the model to quickly identify problem components and try alternative design parameters.},
booktitle = {Proceedings of the 28th IEEE Conference on Military Communications},
pages = {1240–1246},
numpages = {7},
location = {Boston, Massachusetts, USA},
series = {MILCOM'09}
}

@inproceedings{10.5555/2008503.2008518,
author = {Bo\v{s}kovi\'{c}, Marko and Mussbacher, Gunter and Bagheri, Ebrahim and Amyot, Daniel and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
title = {Aspect-oriented feature models},
year = {2010},
isbn = {9783642212093},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software Product Lines (SPLs) have emerged as a prominent approach for software reuse. SPLs are sets of software systems called families that are usually developed as a whole and share many common features. Feature models are most typically used as a means for capturing commonality and managing variability of the family. A particular product from the family is configured by selecting the desired features of that product. Typically, feature models are considered monolithic entities that do not support modularization well. As industrial feature models tend to be large, their modularization has become an important research topic lately. However, existing modularization approaches do not support modularization of crosscutting concerns. In this paper, we introduce Aspect-oriented Feature Models (AoFM) and argue that using aspect-oriented techniques improves the manageability and reduces the maintainability effort of feature models. Particularly, we advocate an asymmetric approach that allows for the modularization of basic and crosscutting concerns in feature models.},
booktitle = {Proceedings of the 2010 International Conference on Models in Software Engineering},
pages = {110–124},
numpages = {15},
keywords = {software product lines, feature models, aspect-oriented modeling},
location = {Oslo, Norway},
series = {MODELS'10}
}

@inproceedings{10.1145/1321631.1321711,
author = {Botterweck, Goetz and O'Brien, Liam and Thiel, Steffen},
title = {Model-driven derivation of product architectures},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321711},
doi = {10.1145/1321631.1321711},
abstract = {Product Derivation is one of the central activities in Software Product Lines (SPL). One of the main challenges of the process of product derivation is dealing with complexity, which is caused by the large number of artifacts and dependencies between them. Another major challenge is maximizing development efficiency and reducing time-to-market, while at the same time producing high quality products. One approach to overcome these challenges is to automate the derivation process. To this end, this paper focuses on one particular activity of the derivation process; the derivation of the product-specific architecture and describes how this activity can be automated using a model-driven approach. The approach derives the product-specific architecture by selectively copying elements from the product-line architecture. The decision, which elements are included in the derived architecture, is based on a product-specific feature configuration. We present a prototype that implements the derivation as a model transformation described in the Atlas Transformation Language (ATL). We conclude with a short overview of related work and directions for future research},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {469–472},
numpages = {4},
keywords = {software product lines, software architectures, product derivation, model-driven approaches, model transformation, ATL},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {sustainability dimensions, software sustainability, empirical study},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1016/j.jss.2015.09.019,
author = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and Silveira Neto, Paulo Anselmo da Mota and Cavalcanti, Yguarat\~{a} Cerqueira and Meira, Silvio Romero de Lemos},
title = {Twenty-eight years of component-based software engineering},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {111},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.09.019},
doi = {10.1016/j.jss.2015.09.019},
abstract = {We defined more precisely the identification of the gaps.We also defined more precisely the incentives for further research.In Section 4.3 we made explicit connection to the Fig. 15 and identified gaps.All pointed typos were fixed. The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
journal = {J. Syst. Softw.},
month = jan,
pages = {128–148},
numpages = {21},
keywords = {Systematic mapping study, Software component, Component-based software engineering, Component-based software development}
}

@inproceedings{10.1145/3382494.3410677,
author = {Shu, Yangyang and Sui, Yulei and Zhang, Hongyu and Xu, Guandong},
title = {Perf-AL: Performance Prediction for Configurable Software through Adversarial Learning},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410677},
doi = {10.1145/3382494.3410677},
abstract = {Context: Many software systems are highly configurable. Different configuration options could lead to varying performances of the system. It is difficult to measure system performance in the presence of an exponential number of possible combinations of these options.Goal: Predicting software performance by using a small configuration sample.Method: This paper proposes Perf-AL to address this problem via adversarial learning. Specifically, we use a generative network combined with several different regularization techniques (L1 regularization, L2 regularization and a dropout technique) to output predicted values as close to the ground truth labels as possible. With the use of adversarial learning, our network identifies and distinguishes the predicted values of the generator network from the ground truth value distribution. The generator and the discriminator compete with each other by refining the prediction model iteratively until its predicted values converge towards the ground truth distribution.Results: We argue that (i) the proposed method can achieve the same level of prediction accuracy, but with a smaller number of training samples. (ii) Our proposed model using seven real-world datasets show that our approach outperforms the state-of-the-art methods. This help to further promote software configurable performance.Conclusion: Experimental results on seven public real-world datasets demonstrate that PERF-AL outperforms state-of-the-art software performance prediction methods.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {16},
numpages = {11},
keywords = {regularization, configurable systems, adversarial learning, Software performance prediction},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1109/ICSE.2019.00113,
author = {Ha, Huong and Zhang, Hongyu},
title = {DeepPerf: performance prediction for configurable software with deep sparse neural network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00113},
doi = {10.1109/ICSE.2019.00113},
abstract = {Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1095–1106},
numpages = {12},
keywords = {sparsity regularization, software performance prediction, highly configurable systems, deep sparse feedforward neural network},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-way Program Merging for Facilitating Family-based Analyses of Variant-rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {13},
numpages = {59},
keywords = {variability encoding, quality assurance, model matching, control flow automata, Program merging}
}

@article{10.1016/j.specom.2015.07.005,
author = {Ullmann, Raphael and Bourlard, Herv\'{e}},
title = {Predicting the intrusiveness of noise through sparse coding with auditory kernels},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {76},
number = {C},
issn = {0167-6393},
url = {https://doi.org/10.1016/j.specom.2015.07.005},
doi = {10.1016/j.specom.2015.07.005},
abstract = {We propose a perceptual model using sparse sound representations with auditory kernels.We show that the number of kernels models perceptual properties of background noise.The achieved average correlation to subjective noise intrusiveness scores exceeds 95%. This paper presents a novel approach to predicting the intrusiveness of background noises in speech signals as it is perceived by human listeners. This problem is of particular interest in telephony, where the recently widened range of transmitted audio frequencies has increased the importance of appropriate background noise reduction strategies. Current approaches predict the average noise intrusiveness score that would be obtained in a subjective listening test by combining different signal features related to physical properties (e.g., signal energy, spectral distribution) or psychoacoustic estimations (e.g., loudness) of noise. The combination and/or implementation of such features requires expert knowledge or the availability of training data. We present a novel approach that is based on a model of efficient sound coding, using a sparse spike coding representation of noise. We show that the sparsity of these representations implicitly models several factors in the perception of noise, and yields predictions of noise intrusiveness scores that compare to or outperform traditional features, without the use of training data. Our evaluation datasets and used performance metrics are based on standardized methods for the evaluation of quality prediction models.},
journal = {Speech Commun.},
month = feb,
pages = {186–200},
numpages = {15},
keywords = {Sparse spike coding, Perceptual models, Objective quality assessment, Noise reduction, Noise intrusiveness}
}

@article{10.1016/j.jss.2007.08.033,
author = {Sadat-Mohtasham, S. Hossein and Ghorbani, Ali A.},
title = {A language for high-level description of adaptive web systems},
year = {2008},
issue_date = {July, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.08.033},
doi = {10.1016/j.jss.2007.08.033},
abstract = {Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on users' behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWL's design decisions and their benefits are explained.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1196–1217},
numpages = {22},
keywords = {Domain-specific programming language, Aspect-oriented programming, Adaptive web system}
}

@article{10.1016/j.jss.2016.03.068,
author = {Triantafyllidis, Konstantinos and Aslam, Waqar and Bondarev, Egor and Lukkien, Johan J. and de With, Peter H.N.},
title = {ProMARTES},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.068},
doi = {10.1016/j.jss.2016.03.068},
abstract = {Cycle-accurate performance analysis open-source tool for CBRTDS.Fusion of network and processing analysis methods.Support both analytical and simulation analysis techniques.Application of the proposed framework to an autonomously navigating robot system.Proposal of an effective combination of scheduling and simulation analysis techniques. This paper proposes a cycle-accurate performance analysis method for real-time component-based distributed systems (CB-RTDS). The method involves the following phases: (a) profiling SW components at cycle execution level and modeling the obtained performance measurements in MARTE-compatible component resource models, (b) guided composition of the system architecture from available SW and HW components, (c) automated generation of a system model, specifying both computation and network loads, and (d) performance analysis (scheduling, simulation and network analysis) of the composed system model. The method is demonstrated for a real-world case study of 3 autonomously navigating robots with advanced sensing capabilities. The case study is challenging because of the SW/HW mapping, real-time requirements and data synchronization among multiple nodes. This case-study proved that, thanks to the adopted low-level performance metrics, we are able to obtain accurate performance predictions of both computation and network delays. Moreover, the combination of analytical and simulation analysis methods enables the computation of both the guaranteed Worst Case Execution Time (WCET) and the detailed execution time-line data for real-time tasks. As a result, the analysis yields the identification of an optimal architecture, with respect to real-time deadlines, robustness and system costs. The paper main contributions are the cycle-accurate performance analysis workflow and supportive open-source ProMARTES tool-chain, both incorporating a network prediction model in all the performance analysis phases.},
journal = {J. Syst. Softw.},
month = jul,
pages = {450–470},
numpages = {21},
keywords = {Simulation, Scheduling, Real-time, Performance, Distributed, Analysis}
}

@inproceedings{10.1145/3239372.3239397,
author = {Ballar\'{\i}n, Manuel and Marc\'{e}n, Ana C. and Pelechano, Vicente and Cetina, Carlos},
title = {Measures to report the Location Problem of Model Fragment Location},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239397},
doi = {10.1145/3239372.3239397},
abstract = {Model Fragment Location (MFL) aims at identifying model elements that are relevant to a requirement, feature, or bug. Many MFL approaches have been introduced in the last few years to address the identification of the model elements that correspond to a specific functionality. However, there is a lack of detail when the measurements about the search space (models) and the measurements about the solution to be found (model fragment) are reported. Generally, the only reported measure is the model size. In this paper, we propose using five measurements (size, volume, density, multiplicity, and dispersion) to report the location problems. These measurements are the result of analyzing 1,308 MFLs in a family of industrial models over the last four years. Using two MFL approaches, we emphasize the importance of these measurements in order to compare results. Our work not only proposes improving the reporting of the location problem, but it also provides real measurements of location problems that are useful to other researchers in the design of synthetic location problems.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {189–199},
numpages = {11},
keywords = {Traceability Link Recovery, Model Fragment Location, Feature Location, Bug Location},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/2361999.2362032,
author = {Nakagawa, Elisa Yumi},
title = {Reference architectures and variability: current status and future perspectives},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362032},
doi = {10.1145/2361999.2362032},
abstract = {Reference architectures refer to a special type of software architecture that achieves well-recognized understanding of specific domains, promoting reuse of design expertise and facilitating the development, standardization, and evolution of software systems. Designed for various domains and purpose, they have increasingly impacted important aspects of system development, such as productivity and quality of such systems. In another perspective, variability has been considered in several research topics as a mechanism that facilitates software development and evolution. In this context, the main contribution of this paper is to present the current status regarding variability in the reference architecture engineering. It is also presented future research perspectives that could be conducted, providing new directions to the reference architecture engineering in order to become existing and new reference architectures more effective elements to the development and evolution of software-intensive systems.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {159–162},
numpages = {4},
keywords = {variability, reference architecture},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@article{10.1016/j.future.2019.04.032,
author = {Mousa, Afaf and Bentahar, Jamal and Alam, Omar},
title = {Context-aware composite SaaS using feature model},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {99},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.04.032},
doi = {10.1016/j.future.2019.04.032},
journal = {Future Gener. Comput. Syst.},
month = oct,
pages = {376–390},
numpages = {15}
}

@inproceedings{10.1145/3442391.3442409,
author = {G\"{o}ttmann, Hendrik and Bacher, Isabelle and Gottwald, Nicolas and Lochau, Malte},
title = {Static Analysis Techniques for Efficient Consistency Checking of Real-Time-Aware DSPL Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442409},
doi = {10.1145/3442391.3442409},
abstract = {Dynamic Software Product Lines (DSPL) have recently gained momentum as integrated engineering methodology for (self-)adaptive software. DSPL enhance statically configurable software by enabling run-time reconfiguration to facilitate continuous adaptations to changing environmental contexts. In a previous work, we presented a model-based methodology for specifying and automatically analyzing real-time constraints of reconfiguration decisions in a feature-oriented and compositional way. Internally, we translate real-time-aware DSPL specifications into timed automata serving as input for off-the-shelf model checkers like Uppaal for automatically checking semantic consistency properties. However, due to the very high computational complexity of model checking timed automata, those consistency checks suffer from scalability problems thus obstructing practical applications of the proposed approach. In this paper, we tackle this issue by investigating various kinds of static-analysis techniques that (1) aim to avoid expensive model checker calls by statically detecting certain classes of inconsistencies beforehand and otherwise (2) perform model reduction by detecting and merging equivalence states prior to model checker calls. The results of our experimental evaluation show very promising performance improvements achievable by those techniques, especially by the model-reduction approach.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {17},
numpages = {9},
keywords = {Timed Automata, Reconfiguration Decisions, Dynamic Software Product Lines},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1109/MiSE.2019.00018,
author = {Sch\"{o}ttle, Matthias and Kienzle, J\"{o}rg},
title = {On the difficulties of raising the level of abstraction and facilitating reuse in software modelling: the case for signature extension},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00018},
doi = {10.1109/MiSE.2019.00018},
abstract = {Reuse is central to improving the software development process, increasing software quality and decreasing time-to-market. Hence it is of paramount importance that modelling languages provide features that enable the specification and modularization of reusable artefacts, as well as their subsequent reuse. In this paper we outline several difficulties caused by the finality of method signatures that make it hard to specify and use reusable artefacts encapsulating several variants. The difficulties are illustrated with a running example. To evaluate whether these difficulties can be observed at the programming level, we report on an empirical study conducted on the Java Platform API as well as present workarounds used in various programming languages to deal with the rigid nature of signatures. Finally, we outline signature extension as an approach to overcome these problems at the modelling level.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {71–77},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@article{10.1016/j.jss.2014.01.051,
author = {Galster, Matthias and Avgeriou, Paris and M\"{a}nnist\"{o}, Tomi and Weyns, Danny},
title = {Editorial: Variability in software architecture - State of the art},
year = {2014},
issue_date = {May, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {91},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2014.01.051},
doi = {10.1016/j.jss.2014.01.051},
abstract = {Many of today's software systems are built with variability in mind, such as software product families, self-adaptive systems, and open platforms. The architecture of a software system is the reference point for all development activities and the earliest point where significant design decisions are taken. We appeal to the software architecture community to continue the work on variability in software architecture as the needs for variability will only grow in the future, and the more we learn the more open questions we come across. This guarantees both challenging and rewarding times ahead.},
journal = {J. Syst. Softw.},
month = may,
pages = {1–2},
numpages = {2}
}

@inproceedings{10.1145/2851613.2851758,
author = {Bombonatti, Denise and Gralha, Catarina and Moreira, Ana and Ara\'{u}jo, Jo\~{a}o and Goul\~{a}o, Miguel},
title = {Usability of requirements techniques: a systematic literature review},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851758},
doi = {10.1145/2851613.2851758},
abstract = {The usability of requirements engineering (RE) techniques has been recognised as a key factor for their successful adoption by industry. RE techniques must be accessible to stakeholders with different backgrounds, so they can be empowered to effectively and efficiently contribute to building successful systems. When selecting an appropriate requirements engineering technique for a given context, one should consider the usability supported by each of the candidate techniques. The first step towards achieving this goal is to gather the best evidence available on the usability of RE approaches by performing a systematic literature review, to answer one research question: How is the usability of requirements engineering techniques and tools addressed? We systematically review articles published in the Requirements Engineering Journal, one of the main sources for mature work in RE, to motivate a research roadmap to make RE approaches more accessible to stakeholders with different backgrounds.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1270–1275},
numpages = {6},
keywords = {requirements engineering approaches, systematic literature review, usability},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2578128.2578237,
author = {de Andrade, Hugo Sica and Almeida, Eduardo and Crnkovic, Ivica},
title = {Architectural bad smells in software product lines: an exploratory study},
year = {2014},
isbn = {9781450325233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578128.2578237},
doi = {10.1145/2578128.2578237},
abstract = {The Software Product Lines (SPL) paradigm has arisen for taking advantage of existing common aspects between different products, while also considering product-specific features. The architecture of a SPL comprises a model that will result in product architectures, and may include solutions leading to bad (architectural) design. One way to assess such design decisions is through the identification of architectural bad smells, which are properties that prejudice the overall software quality, but are not necessarily faulty or errant. In this paper, we conduct an exploratory study that aims at characterizing bad smells in the context of product line architectures. We analyzed an open source SPL project and extracted its architecture to investigate the occurrence or absence of four smells initially studied in single systems. In addition, we propose a smell specific to the SPL context and discuss possible causes and implications of having those smells in the architecture of a product line. The results indicate that the granularity of the SPL features may influence on the occurrence of smells.},
booktitle = {Proceedings of the WICSA 2014 Companion Volume},
articleno = {12},
numpages = {6},
keywords = {software product lines, exploratory study, evaluation, architecture, architectural bad smells},
location = {Sydney, Australia},
series = {WICSA '14 Companion}
}

@article{10.1016/j.future.2014.12.002,
author = {Weinreich, Rainer and Groher, Iris and Miesbauer, Cornelia},
title = {An expert survey on kinds, influence factors and documentation of design decisions in practice},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.12.002},
doi = {10.1016/j.future.2014.12.002},
abstract = {Support for capturing architectural knowledge has been identified as an important research challenge. As the basis for an approach to recovering design decisions and capturing their rationale, we performed an expert survey in practice to gain insights into the different kinds, influence factors, and sources for design decisions and also into how they are currently captured in practice. The survey was conducted with 25 software architects, software team leads, and senior developers from 22 different companies in 10 different countries with more than 13 years of experience in software development on average. The survey confirms earlier work by other authors on design decision classification and influence factors, and also identifies additional kinds of decisions and influence factors not mentioned in previous work. In addition, we gained insight into the practice of capturing, the relative importance of different decisions and influence factors, and into potential sources for recovering decisions. We present results of a qualitative expert survey on design decisions in practice.We examine design decision classification, documentation, and influence factors.We collect architects' experiences in decision making and documentation.We provide recommendations for potential improvements and research directions based on the results of our study.Results are compared to literature and similar studies.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {145–160},
numpages = {16},
keywords = {Software architecture knowledge management, Design decisions, Design decision influence factors, Design decision documentation, Design decision classification}
}

@inproceedings{10.1145/1083183.1083188,
author = {Kr\"{u}ger, Ingolf H. and Mathew, Reena and Meisinger, Michael},
title = {From scenarios to aspects: exploring product lines},
year = {2005},
isbn = {1581139632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083183.1083188},
doi = {10.1145/1083183.1083188},
abstract = {Software product lines are gaining importance because they allow improvements in time to market, cost, productivity and quality of software products. Architecture evaluation is one important aspect in the development of product lines for large-scale distributed systems. It is desirable to evaluate and compare architectures for functionality and quality attributes before implementing or changing the whole system. Often, the effort required for the thorough evaluation of alternatives using prototypes is prohibitive. In this paper, we present an approach for cost-efficient software architecture evaluation, based on scenario-oriented software specifications, modeling the system services. We show how to map the same set of services to several possible target architectures and give a procedure to generate evaluation prototypes using aspect-oriented programming techniques. This significantly reduces the effort required to explore architectural alternatives. We explain our approach using the Center TRACON Automation System as an example.},
booktitle = {Proceedings of the Fourth International Workshop on Scenarios and State Machines: Models, Algorithms and Tools},
pages = {1–6},
numpages = {6},
location = {St. Louis, Missouri},
series = {SCESM '05}
}

@book{10.5555/2671146,
author = {Mistrik, Ivan and Bahsoon, Rami and Kazman, Rick and Zhang, Yuanyuan},
title = {Economics-Driven Software Architecture},
year = {2014},
isbn = {0124104649},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Economics-driven Software Architecture presents a guide for engineers and architects who need to understand the economic impact of architecture design decisions: the long term and strategic viability, cost-effectiveness, and sustainability of applications and systems. Economics-driven software development can increase quality, productivity, and profitability, but comprehensive knowledge is needed to understand the architectural challenges involved in dealing with the development of large, architecturally challenging systems in an economic way. This book covers how to apply economic considerations during the software architecting activities of a project. Architecture-centric approaches to development and systematic evolution, where managing complexity, cost reduction, risk mitigation, evolvability, strategic planning and long-term value creation are among the major drivers for adopting such approaches. It assists the objective assessment of the lifetime costs and benefits of evolving systems, and the identification of legacy situations, where architecture or a component is indispensable but can no longer be evolved to meet changing needs at economic cost. Such consideration will form the scientific foundation for reasoning about the economics of nonfunctional requirements in the context of architectures and architecting. Familiarizes readers with essential considerations in economic-informed and value-driven software design and analysis Introduces techniques for making value-based software architecting decisions Provides readers a better understanding of the methods of economics-driven architecting}
}

@inproceedings{10.5555/2820656.2820664,
author = {de Jesus Souza, Magno Lu\~{a} and Santos, Alcemir Rodrigues and de Almeida, Eduardo Santana},
title = {Towards the selection of modeling techniques for dynamic software product lines},
year = {2015},
publisher = {IEEE Press},
abstract = {Emerging domains such as smart homes and more recently smart cities represent a big challenge to software engineering. In such context, the need of runtime self-adaptations to cope with both user needs and environmental changes brings Dynamic Software Product Lines (DSPL) as a suitable solution. However, DSPL implementation itself is challenging, which demands a proper modeling. In this sense, the literature still lacks of means of choosing the modeling technique that best fits a given domain. This paper tackles such problem by defining a criteria for rank such techniques, which is used for ranking a set DSPL modeling techniques found in the literature.},
booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
pages = {19–22},
numpages = {4},
keywords = {modeling techniques, dynamic variability, dynamic software product lines},
location = {Florence, Italy},
series = {PLEASE '15}
}

@inproceedings{10.1145/1596495.1596500,
author = {Peper, Christian and Schneider, Daniel},
title = {On runtime service quality models in adaptive ad-hoc systems},
year = {2009},
isbn = {9781605586816},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596495.1596500},
doi = {10.1145/1596495.1596500},
abstract = {Ad-hoc computer systems can automatically realize higher services when at least two distributed and communicating (embedded) devices come together. For this purpose, they must able to manage appearance and loss of devices and resources, and they have to adapt to changes in requirements and environment. Based on a component-oriented approach for adaptive ad-hoc systems, this paper suggests a high-level service quality reference model to advocate further research on the quality matching problem between service provider and client components.},
booktitle = {Proceedings of the 2009 ESEC/FSE Workshop on Software Integration and Evolution @ Runtime},
pages = {11–18},
numpages = {8},
keywords = {ubiquitous computing, quality-of-service, distributed systems, component-orientation, ambient intelligence, adaptivity, ad-hoc systems},
location = {Amsterdam, The Netherlands},
series = {SINTER '09}
}

@article{10.1145/3204459,
author = {Chen, Tao and Li, Ke and Bahsoon, Rami and Yao, Xin},
title = {FEMOSAA: Feature-Guided and Knee-Driven Multi-Objective Optimization for Self-Adaptive Software},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3204459},
doi = {10.1145/3204459},
abstract = {Self-Adaptive Software (SAS) can reconfigure itself to adapt to the changing environment at runtime, aiming to continually optimize conflicted nonfunctional objectives (e.g., response time, energy consumption, throughput, cost, etc.). In this article, we present Feature-guided and knEe-driven Multi-Objective optimization for Self-Adaptive softwAre (FEMOSAA), a novel framework that automatically synergizes the feature model and Multi-Objective Evolutionary Algorithm (MOEA) to optimize SAS at runtime. FEMOSAA operates in two phases: at design time, FEMOSAA automatically transposes the engineers’ design of SAS, expressed as a feature model, to fit the MOEA, creating new chromosome representation and reproduction operators. At runtime, FEMOSAA utilizes the feature model as domain knowledge to guide the search and further extend the MOEA, providing a larger chance for finding better solutions. In addition, we have designed a new method to search for the knee solutions, which can achieve a balanced tradeoff. We comprehensively evaluated FEMOSAA on two running SAS: One is a highly complex SAS with various adaptable real-world software under the realistic workload trace; another is a service-oriented SAS that can be dynamically composed from services. In particular, we compared the effectiveness and overhead of FEMOSAA against four of its variants and three other search-based frameworks for SAS under various scenarios, including three commonly applied MOEAs, two workload patterns, and diverse conflicting quality objectives. The results reveal the effectiveness of FEMOSAA and its superiority over the others with high statistical significance and nontrivial effect sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {5},
numpages = {50},
keywords = {self-adaptive system, search-based software engineering, performance engineering, multi-objective optimization, multi-objective evolutionary algorithm, Feature model}
}

@inproceedings{10.5555/338283.338361,
author = {Blair, Gordon S. and Blair, Lynne and Issarny, Val\'{e}ie and Tuma, Petr and Zarras, Apostolos},
title = {The role of software architecture in constraining adaptation incomponent-based middleware platforms},
year = {2000},
isbn = {3540673520},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Future middleware platforms will need to be more configurable in order to meet the demands of a wide variety of application domains. Furthermore, we believe that such platforms will also need to be re-configurable, for example to enable systems to adapt to changes in the underlying systems infrastructure. A number of technologies are emerging to support this level of configurability and re-configurability, most notably middleware platforms based on the concepts of open implementation and reflection. One problem with this general approach is that widespread changes can often be made to the middleware platform, potentially jeopardizing the integrity of the overall system. This paper discusses the role of software architecture in maintaining the overall integrity of the system in such an environment. More specifically, the paper discusses extensions to the Aster framework to support the re-configuration of a reflective (component-based) middleware platform in a constrained manner. The approach is based on i) the formal specification of a range of possible component configurations, ii) the systematic selection of configurations based on a given set of non-functional properties, and iii) the orderly re-configuration between configurations, again based on formally specified rules.},
booktitle = {IFIP/ACM International Conference on Distributed Systems Platforms},
pages = {164–184},
numpages = {21},
location = {New York, New York, USA},
series = {Middleware '00}
}

@inproceedings{10.1109/WICSA.2005.50,
author = {Bhattacharya, Sutirtha and Perry, Dewayne E.},
title = {Predicting Architectural Styles from Component Specifications},
year = {2005},
isbn = {0769525482},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WICSA.2005.50},
doi = {10.1109/WICSA.2005.50},
abstract = {Software Product Lines (SPL), Component Based Software Engineering (CBSE) and Commercial Off The Shelf (COTS) components provide a rich supporting base for creating software architectures. Further, they promise significant improvements in the quality of software configurations that can be composed from pre-built components. Software architectural styles provide a way for achieving a desired coherence for such component-based architectures. This is because the different architectural styles enforce different quality attributes for a system. If the architectural style of an emergent system could be predicted in advance, a System Integrator could make necessary changes to ensure that the quality attributes dictated by the system requirements were satisfied before the actual system was deployed and tested. In this paper we propose a model for predicting architectural styles based on use cases that need to be met by a system configuration. Moreover, our technique can be used to determine stylistic conformance and hence indicate the presence or absence of architectural drift},
booktitle = {Proceedings of the 5th Working IEEE/IFIP Conference on Software Architecture},
pages = {231–232},
numpages = {2},
keywords = {System Composition, Reuse, Component Based Software Engineering, Architectural Style},
series = {WICSA '05}
}

@inproceedings{10.1145/2993236.2993246,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Collet, Philippe and Alam, Omar},
title = {Delaying decisions in variable concern hierarchies},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993246},
doi = {10.1145/2993236.2993246},
abstract = {Concern-Oriented Reuse (CORE) proposes a new way of structuring model-driven software development, where models of the system are modularized by domains of abstraction within units of reuse called concerns. Within a CORE concern, models are further decomposed and modularized by features. This paper extends CORE with a technique that enables developers of high-level concerns to reuse lower-level concerns without unnecessarily committing to a specific feature selection. The developer can select the functionality that is minimally needed to continue development, and reexpose relevant alternative lower-level features of the reused concern in the reusing concern's interface. This effectively delays decision making about alternative functionality until the higher-level reuse context, where more detailed requirements are known and further decisions can be made. The paper describes the algorithms for composing the variation (i.e., feature and impact models), customization, and usage interfaces of a concern, as well as the concern's realization models and finally an entire concern hierarchy, as is necessary to support delayed decision making in CORE.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {93–103},
numpages = {11},
keywords = {Reuse Hierarchies, Model-Driven Engineering, Model Reuse, Model Interfaces, Delaying of Decisions},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.1109/ICSE.2019.00090,
author = {Lillack, Max and St\u{a}nciulescu, \c{S}tefan and Hedman, Wilhelm and Berger, Thorsten and W\k{a}sowski, Andrzej},
title = {Intention-based integration of software variants},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00090},
doi = {10.1109/ICSE.2019.00090},
abstract = {Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional software merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants.In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions---domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the proposed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {831–842},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1007/s10664-019-09763-0,
author = {Kr\"{u}ger, Jacob and Lausberger, Christian and von Nostitz-Wallwitz, Ivonne and Saake, Gunter and Leich, Thomas},
title = {Search. Review. Repeat? An empirical study of threats to replicating SLR searches},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09763-0},
doi = {10.1007/s10664-019-09763-0},
abstract = {A systematic literature review (SLR) is an empirical method used to provide an overview of existing knowledge and to aggregate evidence within a domain. For computer science, several threats to the completeness of such reviews have been identified, leading to recommendations and guidelines on how to improve their quality. However, few studies address to what extent researchers can replicate an SLR. To conduct a replication, researchers have to first understand how the set of primary studies has been identified in the original study, and can ideally retrieve the same set when following the reported protocol. In this article, we focus on this initial step of a replication and report a two-fold empirical study: Initially, we performed a tertiary study using a sample of SLRs in computer science and identified what information that is needed to replicate the searches is reported. Based on the results, we conducted a descriptive, multi-case study on digital libraries to investigate to what extent these allow replications. The results reveal two threats to replications of SLRs: First, while researchers have improved the quality of their reports, relevant details are still missing—we refer to a reporting threat. Second, we found that some digital libraries are inconsistent in their query results—we refer to a searching threat. While researchers conducting a review can only overcome the first threat and the second may not be an issue for all kinds of replications, researchers should be aware of both threats when conducting, reviewing, and building on SLRs.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {627–677},
numpages = {51},
keywords = {Digital library, Replication, Threats to validity, Software engineering, Systematic literature review, Tertiary study}
}

@article{10.1016/j.jss.2017.03.005,
author = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
title = {Automotive software engineering},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.005},
doi = {10.1016/j.jss.2017.03.005},
abstract = {A comprehensive survey of literature on Automotive Software Engineering (ASE).679 primary studies were identified, classified and analyzed with respect to five dimensions.Three most investigated areas include software architecture &amp; design, testing and reuse.ASE seems to have high industrial relevance but is relatively lower in its scientific rigor.Validation &amp; comparative studies are less represented and literature lacks practitioner-oriented guidelines. The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
journal = {J. Syst. Softw.},
month = jun,
pages = {25–55},
numpages = {31},
keywords = {Systematic mapping study, Software-intensive systems, Literature survey, Embedded systems, Automotive systems, Automotive software engineering}
}

@article{10.1016/j.infsof.2019.03.015,
author = {Borg, Markus and Chatzipetrou, Panagiota and Wnuk, Krzysztof and Al\'{e}groth, Emil and Gorschek, Tony and Papatheocharous, Efi and Shah, Syed Muhammad Ali and Axelsson, Jakob},
title = {Selecting component sourcing options: A survey of software engineering’s broader make-or-buy decisions},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.015},
doi = {10.1016/j.infsof.2019.03.015},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {18–34},
numpages = {17},
keywords = {Survey, Decision making, Software architecture, Sourcing, Component-based software engineering}
}

@inproceedings{10.5555/2018027.2018039,
author = {Johnsen, Andreas and Lundqvist, Kristina},
title = {Developing dependable software-intensive systems: AADL vs. EAST-ADL},
year = {2011},
isbn = {9783642213373},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Dependable software-intensive systems, such as embedded systems for avionics and vehicles are often developed under severe quality, schedule and budget constraints. As the size and complexity of these systems dramatically increases, the architecture design phase becomes more and more significant in order to meet these constraints. The use of Architecture Description Languages (ADLs) provides an important basis for mutual communication, analysis and evaluation activities. Hence, selecting an ADL suitable for such activities is of great importance. In this paper we compare and investigate the two ADLs - AADL and EASTADL. The level of support provided to developers of dependable software-intensive systems is compared, and several critical areas of the ADLs are highlighted. Results of using an extended comparison framework showed many similarities, but also one clear distinction between the languages regarding the perspectives and the levels of abstraction in which systems are modeled.},
booktitle = {Proceedings of the 16th Ada-Europe International Conference on Reliable Software Technologies},
pages = {103–117},
numpages = {15},
keywords = {software-intensive systems, dependable systems, architecture description languages, EAST-ADL, AADL},
location = {Edinburgh, UK},
series = {Ada-Europe'11}
}

@inproceedings{10.5555/1756269.1756282,
author = {Avgeriou, Paris and Retalis, Simos and Skordalakis, Manolis},
title = {An architecture for open learning management systems},
year = {2001},
isbn = {9783540075448},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {There exists an urgent demand on defining architectures for Learning Management Systems, so that high-level frameworks for understanding these systems can be discovered, and quality attributes like portability, interoperability, reusability and modifiability can be achieved. In this paper we propose a prototype architecture aimed to engineer Open Learning Management Systems, that professes state-of the-art software engineering techniques such as layered structure and component-based nature. Our work is based upon standards and practices from international standardization bodies, on the empirical results of designing, developing and evaluating Learning Management Systems and on the practices of well-established software engineering techniques.},
booktitle = {Proceedings of the 8th Panhellenic Conference on Informatics},
pages = {183–200},
numpages = {18},
location = {Nicosia, Cyprus},
series = {PCI'01}
}

@article{10.1016/j.scico.2019.102344,
author = {Basile, Davide and ter Beek, Maurice H. and Degano, Pierpaolo and Legay, Axel and Ferrari, Gian-Luigi and Gnesi, Stefania and Di Giandomenico, Felicita},
title = {Controller synthesis of service contracts with variability},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {187},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.102344},
doi = {10.1016/j.scico.2019.102344},
journal = {Sci. Comput. Program.},
month = feb,
numpages = {23},
keywords = {Behavioural variability, Variability, Service orchestrations, Contract automata, Supervisory control theory}
}

@inproceedings{10.1145/1370175.1370249,
author = {Avgeriou, Paris and Lago, Patricia and Kruchten, Philippe},
title = {Third international workshop on sharing and reusing architectural knowledge (SHARK 2008)},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370249},
doi = {10.1145/1370175.1370249},
abstract = {The shift of the software architecture community towards architectural knowledge has brought along some promising research directions. In this workshop we discuss the issues that lead to the application of architectural knowledge in research and industrial practice; ongoing research and new ideas to advance the field. In its previous editions we examined the state of the art and practice, future challenges and trends. This third edition will discuss, among others, architectural knowledge as perceived by different research communities, including requirements engineering, service-oriented computing and international standardization.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {1065–1066},
numpages = {2},
keywords = {architectural knowledge},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@inproceedings{10.1109/CHASE.2009.5071420,
author = {Unphon, Hataichanok and Dittrich, Yvonne and Hubaux, Arnaud},
title = {Taking care of cooperation when evolving socially embedded systems: The PloneMeeting case},
year = {2009},
isbn = {9781424437122},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CHASE.2009.5071420},
doi = {10.1109/CHASE.2009.5071420},
abstract = {This paper proposes a framework to (i) analyse the contexts of socially embedded systems and (ii) support the understanding of change during their evolutions. Our finding is based on a co-operative project with a government agency developing a partially-automated variability configurator for an open source software product family. By employing our framework, we realised that the way variations and their management are implemented have to accommodate work practices from the use context as well as development practice, and here especially the cooperation within the development team and between users and developers. The empirical evidence has confirmed our understanding of what is relevant when estimating the evolvability of socially embedded systems. We propose to use our framework in architecture-level design and evaluation in order to take these cooperative relationships into account early in the evolution cycle.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Cooperative and Human Aspects on Software Engineering},
pages = {96–103},
numpages = {8},
series = {CHASE '09}
}

@inproceedings{10.1007/11693017_22,
author = {Min, Hyun Gi and Kim, Soo Dong},
title = {A technique to represent and generate components in MDA/PIM for automation},
year = {2006},
isbn = {3540330933},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11693017_22},
doi = {10.1007/11693017_22},
abstract = {Component-Based Development (CBD) is an effective approach to develop software effectively and economically through reuse of software components. Model Driven Architecture (MDA) is a new software development paradigm where software is generated by a series of model transformations. By combing essential features of CBD and MDA, both benefits of software reusability and development automation can be achieved in a single framework. In this paper, we propose a UML profile for specifying component-based design in MDA framework. The profile consists of UML extensions, notations, and related instructions to specify elements of CBD in MDA constructs. Once components are specified with our profile at the level of PIM, they can be automatically transformed into PSM and eventually source code implementation.},
booktitle = {Proceedings of the 9th International Conference on Fundamental Approaches to Software Engineering},
pages = {293–307},
numpages = {15},
location = {Vienna, Austria},
series = {FASE'06}
}

@article{10.1007/s11276-018-1837-6,
author = {Yildirim, Ahmet and Zeydan, Engin and Yigit, Ibrahim Onuralp},
title = {A statistical comparative performance analysis of mobile network operators},
year = {2020},
issue_date = {Feb 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {2},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1837-6},
doi = {10.1007/s11276-018-1837-6},
abstract = {Mobile telephony is one of the most widely utilized technologies in the modern world. Records of the usage behaviour of mobile users can provide valuable information for understanding the behaviour of networks for Mobile Network Operators (MNOs). For different reasons, MNOs are interested in knowing how their competitors’ performance varies based on location, phone category, phone Operating System (OS) for various cellular network technology (CNT). This can help MNOs to invest intelligently in locations where they operate with inferior performance. Therefore, Key Performance Indicator (KPI) comparisons among MNOs are of interest for all MNOs. In this article, we investigate cellular network performance statistical comparisons of major Mobile Network Operators (MNOs) in Turkey using a large scale real-world proprietary mobile traffic dataset over a period of 18 months. Focusing our approach on different dimensions of crowd-sourced dataset allows us: (i) to know end-to-end nationwide network performance comparisons of MNOs using real-world measurement data, (ii) to calculate Confidence Intervals (CIs) for the mean difference of KPIs (such as downlink speed, latency, jitter and packet loss) for obtaining useful comparative statistical information of MNO performances and (iii) to observe the existence of significant performance differences between MNOs depending on the region which they are operating, phone category, phone OS as well as CNTs.},
journal = {Wirel. Netw.},
month = feb,
pages = {1105–1124},
numpages = {20},
keywords = {Cellular, KPIs, Comparisons, Performance, MNOs, Data analytics}
}

@article{10.1007/s00450-011-0202-0,
author = {Drago, Mauro Luigi and Ghezzi, Carlo and Mirandola, Raffaela},
title = {A quality driven extension to the QVT-relations transformation language},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {1},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-011-0202-0},
doi = {10.1007/s00450-011-0202-0},
abstract = {An emerging approach to software development is Model Driven Software Development 				(MDSD). It shifts the focus from source code to models, aims at cost reduction, risk 				mitigation, and eases the engineering of complex applications. System models can be 				used in the early development stages to verify certain relevant properties, such as 				performance, before source code is available and problems become hard and costly to 				solve. The present status of Model Driven Engineering (MDE) is still far from this 				ideal situation. A well-known problem is feedback provisioning, which arises when 				different solutions for the same design problem exist. An approach for feedback 				provisioning automation leverages model transformations, which glue together models 				in an MDSD setting, encapsulate the design rationale, and promote knowledge reuse 				and solutions otherwise available only to experienced engineers. In this article we 				present QVTR2, our solution to the feedback problem. 					QVTR2 is an extension of the QVT-Relations language 				with constructs to express design alternatives, their impact on non-functional 				metrics, and how to evaluate them and guide the engineers in the selection of the 				most appropriate solution. We demonstrate the effectiveness of our solution by using 				the QVTR2 engine to perform a modified version of the 				standard UML-to-RDBMS transformation in the 				context of a real e-commerce application, and by showing how we can guide a 				non-expert engineer in the selection of a solution that satisfies given performance 				requirements.},
journal = {Comput. Sci.},
month = feb,
pages = {1–20},
numpages = {20},
keywords = {Model transformations, Model driven software development, Model driven quality prediction, Feedback provisioning}
}

@article{10.1145/3229048,
author = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
title = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3229048},
doi = {10.1145/3229048},
abstract = {Architecture-centric development addresses the increasing complexity and variability of software systems by focusing on architectural models, which are generally easier to understand and manipulate than source code. It requires a mechanism that can maintain architecture-implementation conformance during architectural development and evolution. The challenge is twofold. There is an abstraction gap between software architecture and implementation, and both may evolve. Existing approaches are deficient in support for both change mapping and product line architecture. This article presents a novel approach named 1.x-way mapping and its extension, 1.x-line mapping to support architecture-implementation mapping in single system development and in product line development, respectively. They specifically address mapping architecture changes to code, maintaining variability conformance between product line architecture and code, and tracing architectural implementation. We built software tools named xMapper and xLineMapper to realize the two approaches, and conducted case studies with two existing open-source systems to evaluate the approaches. The result shows that our approaches are applicable to the implementation of a real software system and are capable of maintaining architecture-implementation conformance during system evolution.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {8},
numpages = {52},
keywords = {variability conformance, architecture-centric feature traceability, architecture-centric development, architectural evolution, Architecture-implementation mapping}
}

@inproceedings{10.1145/3229345.3229419,
author = {Oliveira, Joyce Aline and Vargas, Matheus and Rodrigues, Roni},
title = {SOA Reuse: Systematic Literature Review Updating and Research Directions},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229419},
doi = {10.1145/3229345.3229419},
abstract = {Service Oriented Architecture (SOA) reuse has been used strategically in organizations to reduce development costs and increase the quality of applications. This article analyzes a systematic literature review in order to identify concepts, goals, strategies, and metrics of SOA reuse. The results show that the main goal of SOA reuse is to decrease development costs. The factor that most negatively influences SOA reuse is the existence of legacy systems. The strategy used most to potentialize SOA reuse is business process management. Metrics proposed by studies to measure SOA reuse are related to modularity and adaptability indicators. The study is relevant because it increases the body of knowledge of the area. Additionally, a set of gaps to be addressed by researchers and reuse practitioners was identified.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {71},
numpages = {8},
keywords = {systematic literature review, Service Oriented Architecture, SOA reuse},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@inproceedings{10.1007/978-3-642-30982-3_7,
author = {Petriu, Dorina C. and Alhaj, Mohammad and Tawhid, Rasha},
title = {Software performance modeling},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_7},
doi = {10.1007/978-3-642-30982-3_7},
abstract = {Ideally, a software development methodology should include both the ability to specify non-functional requirements and to analyze them starting early in the lifecycle; the goal is to verify whether the system under development would be able to meet such requirements. This chapter considers quantitative performance analysis of UML software models annotated with performance attributes according to the standard "UML Profile for Modeling and Analysis of Real-Time and Embedded Systems" (MARTE). The chapter describes a model transformation chain named PUMA (Performance by Unified Model Analysis) that enables the integration of performance analysis in a UML-based software development process, by automating the derivation of performance models from UML+MARTE software models, and by facilitating the interoperability of UML tools and performance tools. PUMA uses an intermediate model called "Core Scenario Model" (CSM) to bridge the gap between different kinds of software models accepted as input and different kinds of performance models generated as output. Transformation principles are described for transforming two kinds of UML behaviour representation (sequence and activity diagrams) into two kinds of performance models (Layered Queueing Networks and stochastic Petri nets). Next, PUMA extensions are described for two classes of software systems: service-oriented architecture (SOA) and software product lines (SPL).},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {219–262},
numpages = {44},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@inproceedings{10.1145/2866614.2866628,
author = {Th\"{u}m, Thomas and Winkelmann, Tim and Schr\"{o}ter, Reimar and Hentschel, Martin and Kr\"{u}ger, Stefan},
title = {Variability Hiding in Contracts for Dependent Software Product Lines},
year = {2016},
isbn = {9781450340199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2866614.2866628},
doi = {10.1145/2866614.2866628},
abstract = {Software product lines are used to efficiently develop and verify similar software products. While they focus on reuse of artifacts between products, a product line may also be reused itself in other product lines. A challenge with such dependent product lines is evolution; every change in a product line may influence all dependent product lines. With variability hiding, we aim to hide certain features and their artifacts in dependent product lines. In prior work, we focused on feature models and implementation artifacts. We build on this by discussing how variability hiding can be extended to specifications in terms of method contracts. We illustrate variability hiding in contracts by means of a running example and share our insights with preliminary experiments on the benefits for formal verification. In particular, we find that not every change in a certain product line requires a re-verification of other dependent product lines.},
booktitle = {Proceedings of the 10th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {97–104},
numpages = {8},
keywords = {method contracts, deductive verification, Multi product line},
location = {Salvador, Brazil},
series = {VaMoS '16}
}

@inproceedings{10.1145/2745802.2745815,
author = {Zhou, You and Zhang, He and Huang, Xin and Yang, Song and Babar, Muhammad Ali and Tang, Hao},
title = {Quality assessment of systematic reviews in software engineering: a tertiary study},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745815},
doi = {10.1145/2745802.2745815},
abstract = {Context: The quality of an Systematic Literature Review (SLR) is as good as the quality of the reviewed papers. Hence, it is vital to rigorously assess the papers included in an SLR. There has been no tertiary study aimed at reporting the state of the practice of quality assessment used in SLRs in Software Engineering (SE).Objective: We aimed to study the practices of quality assessment of the papers included in SLRs in SE.Method: We conducted a tertiary study of the SLRs that have performed quality assessment of the reviewed papers.Results: We identified and analyzed different aspects of the quality assessment of the papers included in 127 SLRs.Conclusion: Researchers use a variety of strategies for quality assessment of the papers reviewed, but report little about the justification for the used criteria. The focus is creditability but not relevance aspect of the papers. Appropriate guidelines are required for devising quality assessment strategies.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {14},
numpages = {14},
keywords = {systematic (literature) review, software engineering, quality assessment},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1007/978-3-662-45234-9_22,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {Deployment Variability in Delta-Oriented Models},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_22},
doi = {10.1007/978-3-662-45234-9_22},
abstract = {Software engineering increasingly emphasizes variability by developing families of products for a range of application contexts or user requirements. ABS is a modeling language which supports variability in the formal modeling of software by using feature selection to transform a delta-oriented base model into a concrete product model. ABS also supports deployment models, with a separation of concerns between execution cost and server capacity. This allows the model-based assessment of deployment choices on a product's quality of service. This paper combines deployment models with the variability concepts of ABS, to model deployment choices as features when designing a family of products.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {304–319},
numpages = {16}
}

@inproceedings{10.5555/2486788.2486853,
author = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
title = {On the value of user preferences in search-based software engineering: a case study in software product lines},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Software design is a process of trading off competing objectives. If the user objective space is rich, then we should use optimizers that can fully exploit that richness. For example, this study configures software product lines (expressed as feature maps) using various search-based software engineering methods. As we increase the number of optimization objectives, we find that methods in widespread use (e.g. NSGA-II, SPEA2) perform much worse than IBEA (Indicator-Based Evolutionary Algorithm). IBEA works best since it makes most use of user preference knowledge. Hence it does better on the standard measures (hypervolume and spread) but it also generates far more products with 0% violations of domain constraints. Our conclusion is that we need to change our methods for search-based software engineering, particularly when studying complex decision spaces.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {492–501},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/1782814.1782819,
author = {Altintas, N. Ilker and Cetin, Semih and Dogru, Ali H.},
title = {Industrializing software development: the "factory automation" way},
year = {2006},
isbn = {3540759115},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Improving the productivity by means of systematic reuse has been a major challenge particularly for the last decade in software industry. Following the individual techniques like Architecture-Based Development, Model-Driven Development and Software Product Lines, Software Factories have eventually come to the stage as an umbrella solution to software productivity problem by assembling the applications with frameworks, patterns, models and tools. While this theoretically seems quite suitable, it still needs practical guidance at certain points such as defining and orchestrating reusable assets for setting up distinct software factories. This paper proposes a methodical way for such difficulties in establishing software factories as the way other manufacturing industries have been doing for several decades, which is known to be "factory automation". We articulate the "software factory automation" for managing reusable assets across distinct software product lines based on an architecture-driven software factory meta-model and tailoring them to form directly executable software assets.},
booktitle = {Proceedings of the 2nd International Conference on Trends in Enterprise Application Architecture},
pages = {54–68},
numpages = {15},
location = {Berlin, Germany},
series = {TEAA'06}
}

@inproceedings{10.1145/3474624.3476010,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Implementing Search-Based Software Engineering Approaches with Nautilus},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476010},
doi = {10.1145/3474624.3476010},
abstract = {Search-Based Software Engineering (SBSE) approaches adopt search-based techniques to solve Software Engineering (SE) optimization problems. Among these techniques, evolutionary algorithms are the most popular and successfully used, such as multi-objective evolutionary algorithms. However, some challenges still need to be addressed. Firstly, SE problems are complex and commonly impacted by many conflicting factors. In this context, the use of many-objective algorithms is necessary. Secondly, the users very often do not recognise the found solutions as feasible because these solutions are usually not generated considering the users’ needs and preferences. Thus, to deal properly with this situation, preference-based algorithms should be applied. Moreover, there are some practical issues regarding the choice of operators, evaluation of algorithms and visualization of solutions. Existing frameworks do not provide support to address these challenges. To overcome these limitations, we present Nautilus, an open-source Java web-platform tool that works with plugins to ease the addition of new problem instances, implementation of search operators and different multi and many-objective optimization algorithms, guided (or not) by human participation. This paper describes Nautilus-NRP, an extension implemented to address the Next Release Problem (NRP). NRP refers to the selection of requirements to be implemented in the next release of a software and is used to illustrate Nautilus’ main functionalities and how it can be extended to solve a SE problem. Link for the video: https://youtu.be/2dbwslTrvhg.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {303–308},
numpages = {6},
keywords = {preference-based algorithms, next release problem, many-objective optimization},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1016/j.cl.2018.01.003,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {Personalized recommender systems for product-line configuration processes},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.003},
doi = {10.1016/j.cl.2018.01.003},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {451–471},
numpages = {21},
keywords = {Personalized recommendations, Recommender systems, Product-line configuration, Feature model, Product lines}
}

@article{10.1016/j.jss.2012.10.013,
author = {Nakagawa, Elisa Y. and Antonino, Pablo O. and Becker, Martin and Maldonado, Jos\'{e} C. and Storf, Holger and Villela, Karina B. and Rombach, Dieter},
title = {Relevance and perspectives of AAL in Brazil},
year = {2013},
issue_date = {April, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.10.013},
doi = {10.1016/j.jss.2012.10.013},
abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
journal = {J. Syst. Softw.},
month = apr,
pages = {985–996},
numpages = {12},
keywords = {Reference architecture, Population aging, Ambient Assisted Living (AAL), AAL platform}
}

@article{10.1016/j.asoc.2016.08.030,
author = {Saeed, Aneesa and Ab Hamid, Siti Hafizah and Mustafa, Mumtaz Begum},
title = {The experimental applications of search-based techniques for model-based testing},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.030},
doi = {10.1016/j.asoc.2016.08.030},
abstract = {Graphical abstractDisplay Omitted HighlightsA systematic review of applications of search-based techniques for model-based testing is provided.Four taxonomies are proposed to classify the applications based on the purpose, problems, solutions and evaluations.The applications are analyzed based on the proposed taxonomies.The development of search-based techniques for model-based testing is discussed.Limitations and potential research directions are summarized. ContextModel-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. ObjectiveThe objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research. MethodWe conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. ResultsThe results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT. ConclusionThis extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1094–1117},
numpages = {24},
keywords = {Test case generation, Taxonomy, Systematic literature review, Software testing, Search-based techniques, Model-based testing}
}

@article{10.1016/j.eswa.2016.03.038,
author = {G Reina, D. and Ciobanu, R.I. and Toral, S.L. and Dobre, C.},
title = {A multi-objective optimization of data dissemination in delay tolerant networks},
year = {2016},
issue_date = {September 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {57},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2016.03.038},
doi = {10.1016/j.eswa.2016.03.038},
abstract = {Data dissemination in DTNs as a multi-objective ptimization problemOptimization of the hit rate, delivery cost, and delivery latency metricsSet of non-dominated solutions given by the Pareto frontSelection of solutions from the Pareto front using decision treesSocial strength is the parameter with the highest influence on the balanced results Data dissemination in delay tolerant networks is an important issue due to the complexity of a multi-hop network formed by a high number of nodes with limited resources, variable and unpredicted mobility conditions. Nodes have to act as expert systems and make suitable forwarding decisions based on local knowledge on the fly. Most of the proposed algorithms rely on adjusting a range of decision variables related to social and topological aspects of the network. Adjusting such parameters is still an open issue since many of them are interrelated. To solve this problem, we propose a multi-objective evolutionary simulation framework for optimizing in terms of delivery hit, delivery cost and latency, a probabilistic data dissemination algorithm based on well-known and widely used social and topological parameters such as centrality, similarity, social strength, friendship, and trust. The proposed multi-objective based optimization framework provides many advantages with respect to existing approaches based on single objective optimization. Primarily, it allows the network designer to have a complete view of the possible outcomes of the data dissemination algorithm through the Pareto front (non-dominated solutions). Furthermore, we propose a decision tree-based selection to obtain under which values of the decision variables we can find a set of solutions that meet a target performance. We validate this selection mechanism by providing the conditions under which we can find balanced solutions in the considered simulation scenarios. The solutions provided by the proposed approach have significant implications for the design of new data dissemination algorithms in DTNs.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {178–191},
numpages = {14},
keywords = {Multi-objective optimization, Genetic algorithm, Delay tolerant networks, Decision tree, Data dissemination}
}

@article{10.1007/s10664-017-9573-6,
author = {Guo, Jianmei and Yang, Dingyu and Siegmund, Norbert and Apel, Sven and Sarkar, Atrisha and Valov, Pavel and Czarnecki, Krzysztof and Wasowski, Andrzej and Yu, Huiqun},
title = {Data-efficient performance learning for configurable systems},
year = {2018},
issue_date = {Jun 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9573-6},
doi = {10.1007/s10664-017-9573-6},
abstract = {Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1826–1867},
numpages = {42},
keywords = {Parameter tuning, Model selection, Regression, Configurable systems, Performance prediction}
}

@inproceedings{10.5555/1987684.1987695,
author = {Juszczyk, Lukasz and Schall, Daniel and Mietzner, Ralph and Dustdar, Schahram and Leymann, Frank},
title = {CAGE: customizable large-scale SOA testbeds in the cloud},
year = {2010},
isbn = {9783642193934},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Large-scale and complex distributed systems are increasingly implemented as SOAs. These comprise diverse types of components, e.g., Web services, registries, workflow engines, and services buses, that interact with each others to establish composite functionality. The drawback of this trend is that testing of complex SOAs becomes a challenging task. During the development phase, testers must verify the system's correct functionality, but often do not have access to adequate testbeds. In this paper, we present an approach for solving this issue. We combine the Genesis2 testbed generator, that emulates SOA environments, with Cafe, a framework for provisioning of component-based applications in the cloud. Our approach allows to model large-scale service-based testbed infrastructures, to specify their behavior, and to deploy these automatically in the cloud. As a result, testers can emulate required environments on-demand for evaluating SOAs at runtime.},
booktitle = {Proceedings of the 2010 International Conference on Service-Oriented Computing},
pages = {76–87},
numpages = {12},
location = {San Francisco, CA},
series = {ICSOC'10}
}

@article{10.1145/3444689,
author = {Zhao, Liping and Alhoshan, Waad and Ferrari, Alessio and Letsholo, Keletso J. and Ajagbe, Muideen A. and Chioasca, Erol-Valeriu and Batista-Navarro, Riza T.},
title = {Natural Language Processing for Requirements Engineering: A Systematic Mapping Study},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3444689},
doi = {10.1145/3444689},
abstract = {Natural Language Processing for Requirements Engineering (NLP4RE) is an area of research and development that seeks to apply natural language processing (NLP) techniques, tools, and resources to the requirements engineering (RE) process, to support human analysts to carry out various linguistic analysis tasks on textual requirements documents, such as detecting language issues, identifying key domain concepts, and establishing requirements traceability links. This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field. Following the guidance of systematic review, the mapping study is directed by five research questions, cutting across five aspects of NLP4RE research, concerning the state of the literature, the state of empirical research, the research focus, the state of tool development, and the usage of NLP technologies. Our main results are as follows: (i) we identify a total of 404 primary studies relevant to NLP4RE, which were published over the past 36 years and from 170 different venues; (ii) most of these studies (67.08%) are solution proposals, assessed by a laboratory experiment or an example application, while only a small percentage (7%) are assessed in industrial settings; (iii) a large proportion of the studies (42.70%) focus on the requirements analysis phase, with quality defect detection as their central task and requirements specification as their commonly processed document type; (iv) 130 NLP4RE tools (i.e., RE specific NLP tools) are extracted from these studies, but only 17 of them (13.08%) are available for download; (v) 231 different NLP technologies are also identified, comprising 140 NLP techniques, 66 NLP tools, and 25 NLP resources, but most of them—particularly those novel NLP techniques and specialized tools—are used infrequently; by contrast, commonly used NLP technologies are traditional analysis techniques (e.g., POS tagging and tokenization), general-purpose tools (e.g., Stanford CoreNLP and GATE) and generic language lexicons (WordNet and British National Corpus). The mapping study not only provides a collection of the literature in NLP4RE but also, more importantly, establishes a structure to frame the existing literature&nbsp;through categorization, synthesis and conceptualization of the main theoretical concepts and relationships that encompass&nbsp;both RE and NLP aspects. Our work thus produces a conceptual framework of NLP4RE. The framework is used to identify research gaps and directions, highlight technology transfer needs, and encourage more synergies between the RE community, the NLP one, and the software&nbsp;and systems&nbsp;practitioners. Our results can be used as a starting point to frame future studies according to a well-defined terminology and can be expanded as new technologies and novel solutions emerge.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {55},
numpages = {41},
keywords = {systematic review, systematic mapping study, software engineering (SE), natural language processing (NLP), Requirements engineering (RE)}
}

@inproceedings{10.1145/3377024.3377036,
author = {Sprey, Joshua and Sundermann, Chico and Krieter, Sebastian and Nieke, Michael and Mauro, Jacopo and Th\"{u}m, Thomas and Schaefer, Ina},
title = {SMT-based variability analyses in FeatureIDE},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377036},
doi = {10.1145/3377024.3377036},
abstract = {Handling configurable systems with thousands of configuration options is a challenging problem in research and industry. One of the most common approaches to manage the configuration options of large systems is variability modelling. The verification and configuration process of large variability models is manually infeasible. Hence, they are usually assisted by automated analyses based on solving satisfiability problems (SAT). Recent advances in satisfiability modulo theories (SMT) could prove SMT solvers as a viable alternative to SAT solvers. However, SMT solvers are typically not utilized for variability analyses. A comparison for SAT and SMT could help to estimate SMT solvers potential for the automated analysis. We integrated two SMT solvers into FeatureIDE and compared them against a SAT solver on analyses for feature models, configurations, and realization artifacts. We give an overview of all variability analyses in FeatureIDE and present the results of our empirical evaluation for over 122 systems. We observed that SMT solvers are generally faster in generating explanations of unsatisfiable requests. However, the evaluated SAT solver outperformed SMT solvers for other analyses.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {6},
numpages = {9},
keywords = {variability analysis, preprocessor analysis, feature models, feature model analysis, feature attributes, configuration analysis, attribute optimization, SMT analysis, SMT, SAT vs SMT, SAT analysis, SAT},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@article{10.1145/2088883.2088900,
author = {Tekinerdogan, Bedir and Cetin, Semih and Babar, Muhammad Ali and Lago, Patricia and M\"{a}ki\"{o}, Juho},
title = {Architecting in global software engineering},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2088883.2088900},
doi = {10.1145/2088883.2088900},
abstract = {This paper summarizes the results of the First Workshop on Arc-hitecting in Global Software Engineering (GSE), which was or-ganized in conjunction with the 6th International Conference on Global Software Engineering (ICGSE 2011). The workshop aimed to bring together researchers and practitioners for defining and advancing the state-of-the-art and state-of-the practice in architecture design of global software development systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–7},
numpages = {7},
keywords = {workshop, software architecture, global software engineering}
}

@article{10.1016/j.infsof.2012.09.011,
author = {Galster, Matthias and Avgeriou, Paris and Tofan, Dan},
title = {Constraints for the design of variability-intensive service-oriented reference architectures - An industrial case study},
year = {2013},
issue_date = {February, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.09.011},
doi = {10.1016/j.infsof.2012.09.011},
abstract = {Context: Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help. Objective: The design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints. Method: An exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings. Results: We identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain. Conclusions: Our results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {428–441},
numpages = {14},
keywords = {e-Government, Variability, Service-oriented architecture, SOA, Reference architectures, Case study}
}

@inproceedings{10.1145/1101908.1101972,
author = {Fredriksson, Johan and Tivoli, Massimo and Crnkovic, Ivica},
title = {A component-based development framework for supporting functional and non-functional analysis in control system design},
year = {2005},
isbn = {1581139934},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101908.1101972},
doi = {10.1145/1101908.1101972},
abstract = {The use of component-based development (CBD) is growing in the software engineering community and it has been successfully applied in many engineering domains such as office applications and in web-based distributed applications. Recently, the need of CBD is growing also in other domains related to dependable and embedded systems, namely, in the control engineering domain. However, the widely used commercial component technologies are unable to provide solutions to the requirements of embedded systems as they require too much resources and they do not provide methods and tools for developing predictable and analyzable embedded systems. There is a need for new component-based technologies appropriate to development of embedded systems. In this paper we briefly present a component-based development framework called SAVEComp. SAVEComp is developed for safety-critical real-time systems. One of the main characteristics of SAVEComp is syntactic and semantic simplicity which enables a high analyzability of properties important for embedded systems. We discuss how SAVEComp is able to provide an efficient support for designing and implementing embedded control systems by mainly focusing on simplicity and analyzability of functional requirements and of real-time and dependability quality attributes. In particular we discuss the typical solutions of control systems in which feedback loops are used and which significantly complicate the design process. We provide a solution for increasing design abstraction level and still being able to reason about system properties using SAVEComp approach. Finally, we discuss an extension of SAVEComp with dynamic run-time property checking by utilizing run-time spare capacity that is normally induced by real-time analysis.},
booktitle = {Proceedings of the 20th IEEE/ACM International Conference on Automated Software Engineering},
pages = {368–371},
numpages = {4},
keywords = {real time embedded systems, non functional analysis, functional analysis, control systems},
location = {Long Beach, CA, USA},
series = {ASE '05}
}

@article{10.1016/j.micpro.2019.05.013,
author = {Pomante, Luigi and Muttillo, Vittoriano and K\v{r}ena, Bohuslav and Vojnar, Tom\'{a}\v{s} and Veljkovi\'{c}, Filip and Magnin, Pac\^{o}me and Matschnig, Martin and Fischer, Bernhard and Martinez, Jabier and Gruber, Thomas},
title = {The AQUAS ECSEL Project Aggregated Quality Assurance for Systems: Co-Engineering Inside and Across the Product Life Cycle},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2019.05.013},
doi = {10.1016/j.micpro.2019.05.013},
journal = {Microprocess. Microsyst.},
month = sep,
pages = {54–67},
numpages = {14},
keywords = {Product life-cycle, Co-engineering, Performance, Security, Safety, Cyber-physical systems}
}

@inproceedings{10.1007/978-3-319-31854-7_74,
author = {Su, Jian and Han, Lu and Li, Yue and Liu, Shufen and Yao, Zhilin and Bao, Tie},
title = {Research on Building and Analysis for Attribute Model in Quality Evaluation of Domain Software},
year = {2016},
isbn = {9783319318530},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-31854-7_74},
doi = {10.1007/978-3-319-31854-7_74},
abstract = {This paper looks into how to select attributes for domain software quality evaluation and put forward a method to build the attribute model of domain software. It also conducts an analysis into the correlation between the change in attributes and the change in software quality evaluation based on the model. Domain software pays more attention to domain features of the software. Based on quality evaluation need, a structured attribute model was built by selection of general, domain and application attributes. Based on the attribute model, an analysis on the impact of change in attributes on software quality evaluation can be conducted. At last, a case is used to demonstrate the process of analyzing how the change in the weight of a single attribute would impact on software quality evaluation. The building method of attribute model put forward in this paper focuses on general and domain attributes of domain software meanwhile looks into the trend of software quality evaluation variation based on the attribute model, thus provides better support for domain software quality evaluation.},
booktitle = {Revised Selected Papers of the Second International Conference on Human Centered Computing - Volume 9567},
pages = {752–758},
numpages = {7},
keywords = {Software engineering, Quality evaluation, Impact analysis, Attribute model},
location = {Colombo, Sri Lanka},
series = {HCC 2016}
}

@article{10.1007/s10270-021-00870-5,
author = {Font, Jaime and Arcega, Lorena and Haugen, \O{}ystein and Cetina, Carlos},
title = {Handling nonconforming individuals in search-based model-driven engineering: nine generic strategies for feature location in the modeling space of the meta-object facility},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-021-00870-5},
doi = {10.1007/s10270-021-00870-5},
abstract = {Lately, the model-driven engineering community has been paying more attention to the techniques offered by the search-based software engineering community. However, even though the conformance of models and metamodels is a topic of great interest for the modeling community, the works that address model-related problems through the use of search metaheuristics are not taking full advantage of the strategies for handling nonconforming individuals. The search space can be huge when searching in model artifacts (magnitudes of around 10150 for models of 500 elements). By handling the nonconforming individuals, the search space can be drastically reduced. In this work, we present a set of nine generic strategies for handling nonconforming individuals that are ready to be applied to model artifacts. The strategies are independent from the application domain and only include constraints derived from the meta-object facility. In addition, we evaluate the strategies with two industrial case studies using an evolutionary algorithm to locate features in models. The results show that the use of the strategies presented can reduce the number of generations needed to reach the solution by 90% of the original value. Generic strategies such as the ones presented in this work could lead to the emergence of more complex fitness functions for searches in models or even new applications for the search metaheuristics in model-related problems.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1653–1688},
numpages = {36},
keywords = {Evolutionary algorithm (EA), Feature location (FL), Search-based software engineering (SBSE), Model-driven engineering (MDE)}
}

@inproceedings{10.1145/3168365.3168372,
author = {Acher, Mathieu and Temple, Paul and J\'{e}z\'{e}quel, Jean-Marc and Galindo, Jos\'{e} A. and Martinez, Jabier and Ziadi, Tewfik},
title = {VaryLATEX: Learning Paper Variants That Meet Constraints},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168372},
doi = {10.1145/3168365.3168372},
abstract = {How to submit a research paper, a technical report, a grant proposal, or a curriculum vitae that respect imposed constraints such as formatting instructions and page limits? It is a challenging task, especially when coping with time pressure. In this work, we present VaryLATEX, a solution based on variability, constraint programming, and machine learning techniques for documents written in LATEX to meet constraints and deliver on time. Users simply have to annotate LATEX source files with variability information, e.g., (de)activating portions of text, tuning figures' sizes, or tweaking line spacing. Then, a fully automated procedure learns constraints among Boolean and numerical values for avoiding non-acceptable paper variants, and finally, users can further configure their papers (e.g., aesthetic considerations) or pick a (random) paper variant that meets constraints, e.g., page limits. We describe our implementation and report the results of two experiences with VaryLATEX.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {83–88},
numpages = {6},
keywords = {variability modelling, technical writing, machine learning, generators, constraint programming, LATEX},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@article{10.1016/j.jss.2015.08.054,
author = {Capilla, Rafael and Jansen, Anton and Tang, Antony and Avgeriou, Paris and Babar, Muhammad Ali},
title = {10 years of software architecture knowledge management},
year = {2016},
issue_date = {June 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {116},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.08.054},
doi = {10.1016/j.jss.2015.08.054},
abstract = {A retrospective analysis of state of the art of AKM.AKM practice from models to tools around three different generations.Our results of AK practice in industry, barriers and remedies.Use of AK in different software development contexts.A comparison of extended capabilities of AKM tools. The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using AK for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in AK management (AKM). Capturing, sharing, and using AK has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of AKM approaches, there is a need to identify the successes and shortcomings of current AK approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the AK research tools in the early stages where AK became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote AK use in modern software development practices.},
journal = {J. Syst. Softw.},
month = jun,
pages = {191–205},
numpages = {15},
keywords = {Architectural knowledge management, Architectural design decisions, Agile development}
}

@inproceedings{10.1145/2361999.2362029,
author = {Kabbedijk, J. and Jansen, S.},
title = {The role of variability patterns in multi-tenant business software},
year = {2012},
isbn = {9781450315685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2361999.2362029},
doi = {10.1145/2361999.2362029},
abstract = {Within the business software domain it is crucial for a software vendor to comply to different customer requirements. Traditionally this could be done by offering different products to different customers, but because multi-tenant business software deployments use one software product to serve all customers, this is no longer possible. Software vendors have to make sure that one instance of a software product is variable enough to support all different requirements from all different customers. This ability is defined as tenant-based variability.Within this paper a conceptual model is presented, explaining the role software patterns play in solving variability implementation problems in multi-tenant business software. Different important aspects of patterns are explained, like forces and consequences and are linked to concepts in the problem domain. The paper suggests that variability patterns play a large role in addressing variability in multi-tenant business software and provide a valuable vocabulary for researching, reporting, thinking and communicating about variability solutions in online software products.},
booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
pages = {143–146},
numpages = {4},
keywords = {variability, software patterns, online business software, multi-tenancy, SaaS},
location = {Helsinki, Finland},
series = {WICSA/ECSA '12}
}

@inproceedings{10.1145/2601248.2601257,
author = {H\"{a}ser, Florian and Felderer, Michael and Breu, Ruth},
title = {Software paradigms, assessment types and non-functional requirements in model-based integration testing: a systematic literature review},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601257},
doi = {10.1145/2601248.2601257},
abstract = {Context: In modern systems, like cyber-physical systems, where software and physical services are interacting, safety, security or performance play an important role. In order to guarantee the correct interoperability of such systems, with respect to functional and non-functional requirements, integration testing is an effective measure to achieve this. Model-based testing moreover not only enables early definition and validation, but also test automation. This makes it a good choice to overcome urgent challenges of integration testing. Objective: Many publications on model-based integration testing (MBIT) approaches can be found. Nevertheless, a study giving a systematic overview on the underlying software paradigms, measures for guiding the integration testing process as well as non-functional requirements they are suitable for, is missing. The aim of this paper is to find and synthesize the relevant primary studies to gain a comprehensive understanding of the current state of model-based integration testing. Method: For synthesizing the relevant studies, we conducted a systematic literature review (SLR) according to the guidelines of Kitchenham. Results: The systematic search and selection retrieved 83 relevant studies from which data has been extracted. Our review identified three assessment criteria for guiding the testing process, namely static metrics, dynamic metrics and stochastic &amp;random. In addition it shows that just a small fraction considers non-functional requirements. Most approaches are for component-oriented systems. Conclusion: Results from the SLR show that there are two major research gaps. First, there is an accumulated need for approaches in the MBIT field that support non-functional requirements, as they are gaining importance. Second, means for steering the integration testing process, especially together with automation, need to evolve.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {29},
numpages = {10},
keywords = {systematic literature review, non-functional requirements, model-based integration testing, assessment types},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.1007/s10270-014-0405-5,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Applying UML/MARTE on industrial projects: challenges, experiences, and guidelines},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-014-0405-5},
doi = {10.1007/s10270-014-0405-5},
abstract = {Modeling and Analysis of Real-Time and Embedded Systems (MARTE) is a Unified Modeling Language (UML) profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In the last 5 years, we have applied UML/MARTE to three distinct industrial problems in three industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experience of solving these problems by applying UML/MARTE on four industrial case studies. We highlight the challenges we faced with respect to the industrial adoption of MARTE. Based on our combined experience, we derive a framework to guide practitioners for future applications of UML/MARTE in an industrial context. The framework provides a set of detailed guidelines that help reduce the gap between the modeling notations and real-world industrial application needs.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1367–1385},
numpages = {19},
keywords = {UML, Real-Time Embedded Systems, Model-based Testing, MARTE, Industrial Case Studies, Architecture Modeling}
}

@article{10.1016/j.infsof.2017.03.004,
author = {Dey, Sangeeta and Lee, Seok-Won},
title = {REASSURE},
year = {2017},
issue_date = {July 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {87},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.004},
doi = {10.1016/j.infsof.2017.03.004},
abstract = {ContextSocio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users cognitive level would be able to support the inherent complexity of variability acquisition. ObjectiveOur main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users preferences. MethodWe explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. ResultsThe result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in users requirements than the traditional practices. ConclusionWe envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain.},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {160–179},
numpages = {20},
keywords = {Socio-technical systems, Requirements elicitation, Repertory grid, Adaptive systems}
}

@article{10.1016/j.infsof.2017.02.002,
author = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Genana N. and Carvalho, Hervaldo},
title = {Building reliable and maintainable Dynamic Software Product Lines},
year = {2017},
issue_date = {June 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {86},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.02.002},
doi = {10.1016/j.infsof.2017.02.002},
abstract = {Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN).Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain.Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist.Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30ms, notification and adaptation plan update time over the network around 5s, and space consumption around 5 MB.Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patientthus providing early detection of serious health issues and the possibility of proactive treatmentand a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {54–70},
numpages = {17},
keywords = {Reliability, Maintainability, Dynamic Software Product Lines, Context-awareness, Body Sensor Network, Adaptiveness}
}

@inproceedings{10.1007/978-3-642-29645-1_22,
author = {Mussbacher, Gunter and Al Abed, Wisam and Alam, Omar and Ali, Shaukat and Beugnard, Antoine and Bonnet, Valentin and Br\ae{}k, Rolv and Capozucca, Alfredo and Cheng, Betty H. C. and Fatima, Urooj and France, Robert and Georg, Geri and Guelfi, Nicolas and Istoan, Paul and J\'{e}z\'{e}quel, Jean-Marc and Kienzle, J\"{o}rg and Klein, Jacques and L\'{e}zoray, Jean-Baptiste and Malakuti, Somayeh and Moreira, Ana and Phung-Khac, An and Troup, Lucy},
title = {Comparing six modeling approaches},
year = {2011},
isbn = {9783642296444},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-29645-1_22},
doi = {10.1007/978-3-642-29645-1_22},
abstract = {While there are many aspect-oriented modeling (AOM) approaches, from requirements to low-level design, it is still difficult to compare them and know under which conditions different approaches are most applicable. This comparison, however, is crucially important to unify existing AOM and more traditional object-oriented modeling (OOM) approaches and to generalize individual approaches into a comprehensive end-to-end method. Such a method does not yet exist. This paper reports on work done at the inaugural Comparing Modeling Approaches (CMA) workshop towards the goal of identifying potential comprehensive methodologies: (i) a common, focused case study for six modeling approaches, (ii) a set of criteria applied to each of the six approaches, and (iii) the assessment results.},
booktitle = {Proceedings of the 2011th International Conference on Models in Software Engineering},
pages = {217–243},
numpages = {27},
keywords = {object-oriented modeling, comparison criteria, case study, aspect-oriented modeling},
location = {Wellington, New Zealand},
series = {MODELS'11}
}

@inproceedings{10.1145/3084226.3084253,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio},
title = {Evaluating Software Architecture Evaluation Methods: An Internal Replication},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084253},
doi = {10.1145/3084226.3084253},
abstract = {Context: The size and complexity of software systems along with the demand for ensuring quality requirements have fostered the interest in software architecture evaluation methods. Although several empirical studies have been reported, the actual body of knowledge is still insufficient. To address this concern, we presented a family of four controlled experiments that compares a recently proposed method, the Quality-Driven Architecture Derivation and Improvement (QuaDAI) method against the well-known Architecture Tradeoff Analysis Method (ATAM).Objective: To provide further evidence on the efficiency, effectiveness, and perceived satisfaction of participants using these two software architecture evaluation methods. We report the results of a differentiated internal replication study.Method: The same materials used in the baseline experiments were employed in this replication but the participants were sixteen practitioners. In addition, we used a simpler design to reduce the treatments' application sequences.Results: The participants obtained architectures with better quality when applying QuaDAI, and they found this method to be more useful and likely to be used than ATAM, but no difference in terms of efficiency and perceived ease of use were found.Conclusions: The results are in line with the baseline experiments and support the hypothesis that QuaDAI achieve better results than ATAM when performing architectural evaluations; however, further work is need to improve the methods usability.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {144–153},
numpages = {10},
keywords = {Software Architecture Evaluation, Experiment Replication},
location = {Karlskrona, Sweden},
series = {EASE '17}
}

@article{10.1504/IJHPCN.2006.010204,
author = {Sridhar, K. N. and Jacob, Lillykutty},
title = {Performance evaluation and enhancement of a link stability based routing protocol for MANETs},
year = {2006},
issue_date = {July 2006},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {4},
number = {1/2},
issn = {1740-0562},
url = {https://doi.org/10.1504/IJHPCN.2006.010204},
doi = {10.1504/IJHPCN.2006.010204},
abstract = {In this paper, first we present a performance comparison of Associativity Based Routing (ABR) and Ad hoc On-demand Distance Vector (AODV) routing protocols. Delay, throughput and energy consumption are the performance metrics that are used for the comparison. We also carry out scenario based performance evaluation of ABR. We propose a technique to reduce the packet size by changing the mechanics of the protocol, and evaluate the proposed technique considering delay, throughput and energy consumption. We also propose and study a scheduling mechanism (neighbour-state dependent scheduling) that uses the overhead information in ABR routing packets to improve the performance.},
journal = {Int. J. High Perform. Comput. Netw.},
month = jul,
pages = {66–77},
numpages = {12},
keywords = {wireless communications, throughput, scheduling, routing protocols, performance evaluation, performance enhancement, mobility models, mobile networks, link stability, energy consumption, delay, associativity based routing, ad hoc on-demand distance vector, ad hoc networks, MANET, AODV, ABR routing packets}
}

@article{10.1145/3176644,
author = {Xiang, Yi and Zhou, Yuren and Zheng, Zibin and Li, Miqing},
title = {Configuring Software Product Lines by Combining Many-Objective Optimization and SAT Solvers},
year = {2018},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3176644},
doi = {10.1145/3176644},
abstract = {A feature model (FM) is a compact representation of the information of all possible products from software product lines. The optimal feature selection involves the simultaneous optimization of multiple (usually more than three) objectives in a large and highly constrained search space. By combining our previous work on many-objective evolutionary algorithm (i.e., VaEA) with two different satisfiability (SAT) solvers, this article proposes a new approach named SATVaEA for handling the optimal feature selection problem. In SATVaEA, an FM is simplified with the number of both features and constraints being reduced greatly. We enhance the search of VaEA by using two SAT solvers: one is a stochastic local search--based SAT solver that can quickly repair infeasible configurations, whereas the other is a conflict-driven clause-learning SAT solver that is introduced to generate diversified products. We evaluate SATVaEA on 21 FMs with up to 62,482 features, including two models with realistic values for feature attributes. The experimental results are promising, with SATVaEA returning 100% valid products on almost all FMs. For models with more than 10,000 features, the search in SATVaEA takes only a few minutes. Concerning both effectiveness and efficiency, SATVaEA significantly outperforms other state-of-the-art algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {14},
numpages = {46},
keywords = {vector angle--based evolutionary algorithm (VaEA), satisfiability (SAT) solvers, many-objective optimization, Optimal feature selection}
}

@inproceedings{10.1145/2889160.2889273,
author = {da Silva Sousa, Leonardo},
title = {Spotting design problems with smell agglomerations},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889273},
doi = {10.1145/2889160.2889273},
abstract = {Design problems are structures that indicate violations of key design principles or rules. The main difficulty to identify them in the source code is due to the fact they are scattered through several code elements. Thus, code smells - microstructures in the program - have been used to reveal surface indications of a design problem. However, individually, each code smell represents only a partial embodiment of a design problem. Since design problem is scattered through several program elements, we are investigating a strategy to select a group of code smells that is likely to help developers to find design problems. We call agglomeration this group of code smells. Our main goal is to summarize smell agglomerations that better indicate the occurrence of design problems. Our strategy to derive agglomerations is based on capturing semantic relations among closely-related code smells. We will assess to what extent code smell agglomerations help developers to locate and prioritize design problems.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {863–866},
numpages = {4},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.jss.2009.06.051,
author = {Bosch, Jan and Bosch-Sijtsema, Petra},
title = {From integration to composition: On the impact of software product lines, global development and ecosystems},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.051},
doi = {10.1016/j.jss.2009.06.051},
abstract = {Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability.},
journal = {J. Syst. Softw.},
month = jan,
pages = {67–76},
numpages = {10},
keywords = {Software product lines, Software integration, Software ecosystems, Software composition, Global development}
}

@inproceedings{10.1145/2591062.2591179,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {A case study on testing, commissioning, and operation of very-large-scale software systems},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591179},
doi = {10.1145/2591062.2591179},
abstract = {An increasing number of software systems today are very-large-scale software systems (VLSS) with system-of-systems (SoS) architectures. Due to their heterogeneity and complexity VLSS are difficult to understand and analyze, which results in various challenges for development and evolution. Existing software engineering processes, methods, and tools do not sufficiently address the characteristics of VLSS. Also, there are only a few empirical studies on software engineering for VLSS. We report on results of an exploratory case study involving engineers and technical project managers of an industrial automation VLSS for metallurgical plants. The paper provides empirical evidence on how VLSS are tested, commissioned, and operated in practice. The paper discusses practical challenges and reports industrial requirements regarding process and tool support. In particular, software processes and tools need to provide general guidance at the VLSS level as well as specific methods and tools for systems that are part of the VLSS. Processes and tools need to support multi-disciplinary engineering across system boundaries. Furthermore, managing variability and evolution is success-critical in VLSS verification and validation.},
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {125–134},
numpages = {10},
keywords = {Very-Large-Scale Software Systems, Verification and Validation, Case Study},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.5555/2041790.2041822,
author = {Acher, Mathieu and Cleve, Anthony and Collet, Philippe and Merle, Philippe and Duchien, Laurence and Lahire, Philippe},
title = {Reverse engineering architectural feature models},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Reverse engineering the variability of an existing system is a challenging activity. The architect knowledge is essential to identify variation points and explicit constraints between features, for instance in feature models (FMs), but the manual creation of FMs is both time-consuming and error-prone. On a large scale, it is very difficult for an architect to guarantee that the resulting FM is consistent with the architecture it is associated with. In this paper, we present a comprehensive, tool supported process for reverse engineering architectural FMs. We develop automated techniques to extract and combine different variability descriptions of an architecture. Then, alignment and reasoning techniques are applied to integrate the architect knowledge and reinforce the extracted FM. We illustrate the process when applied to a representative software system and we report on our experience in this context.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {220–235},
numpages = {16},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1016/j.infsof.2011.06.002,
author = {Breivold, Hongyu Pei and Crnkovic, Ivica and Larsson, Magnus},
title = {A systematic review of software architecture evolution research},
year = {2012},
issue_date = {January, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.002},
doi = {10.1016/j.infsof.2011.06.002},
abstract = {Context: Software evolvability describes a software system's ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research. Objective: In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice. Method: The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process. Results: Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented. Conclusion: The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {16–40},
numpages = {25},
keywords = {Systematic review, Software evolvability, Software architecture, Evolvability analysis, Architecture evolution, Architecture analysis}
}

@article{10.1007/s00450-014-0273-9,
author = {Goltz, Ursula and Reussner, Ralf H. and Goedicke, Michael and Hasselbring, Wilhelm and M\"{a}rtin, Lukas and Vogel-Heuser, Birgit},
title = {Design for future: managed software evolution},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {30},
number = {3–4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-014-0273-9},
doi = {10.1007/s00450-014-0273-9},
abstract = {Innovative software engineering methodologies, concepts and tools which focus on supporting the ongoing evolution of complex software, in particular regarding its continuous adaptation to changing functional and quality requirements as well as platforms over a long period are required. Supporting such a co-evolution of software systems along with their environment represents a very challenging undertaking, as it requires a combination or even integration of approaches and insights from different software engineering disciplines. To meet these challenges, the Priority Programme 1593 Design for Future--Managed Software Evolution has been established, funded by the German Research Foundation, to develop fundamental methodologies and a focused approach for long-living software systems, maintaining high quality and supporting evolution during the whole life cycle. The goal of the priority programme is integrated and focused research in software engineering to develop methods for the continuous evolution of software and software/hardware systems for making systems adaptable to changing requirements and environments. For evaluation, we focus on two specific application domains: information systems and production systems in automation engineering. In particular two joint case studies from these application domains promote close collaborations among the individual projects of the priority programme. We consider several research topics that are of common interest, for instance co-evolution of models and implementation code, of models and tests, and among various types of models. Another research topic of common interest are run-time models to automatically synchronise software systems with their abstract models through continuous system monitoring. Both concepts, co-evolution and run-time models contribute to our vision to which we refer to as knowledge carrying software. We consider this as a major need for a long life of such software systems.},
journal = {Comput. Sci.},
month = aug,
pages = {321–331},
numpages = {11},
keywords = {maintenance and operation, Software life cycle, Legacy systems, Knowledge carrying software, Design, Co-evolution}
}

@inproceedings{10.1145/2576768.2598305,
author = {Lopez-Herrejon, Roberto Erick and Javier Ferrer, Javier and Chicano, Francisco and Haslinger, Evelyn Nicole and Egyed, Alexander and Alba, Enrique},
title = {A parallel evolutionary algorithm for prioritized pairwise testing of software product lines},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598305},
doi = {10.1145/2576768.2598305},
abstract = {Software Product Lines (SPLs) are families of related software systems, which provide different feature combinations. Different SPL testing approaches have been proposed. However, despite the extensive and successful use of evolutionary computation techniques for software testing, their application to SPL testing remains largely unexplored. In this paper we present the Parallel Prioritized product line Genetic Solver (PPGS), a parallel genetic algorithm for the generation of prioritized pairwise testing suites for SPLs. We perform an extensive and comprehensive analysis of PPGS with 235 feature models from a wide range of number of features and products, using 3 different priority assignment schemes and 5 product prioritization selection strategies. We also compare PPGS with the greedy algorithm prioritized-ICPL. Our study reveals that overall PPGS obtains smaller covering arrays with an acceptable performance difference with prioritized-ICPL.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1255–1262},
numpages = {8},
keywords = {software product lines, pairwise testing, feature models, combinatorial interaction testing},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@article{10.1007/s10009-012-0253-y,
author = {Schaefer, Ina and Rabiser, Rick and Clarke, Dave and Bettini, Lorenzo and Benavides, David and Botterweck, Goetz and Pathak, Animesh and Trujillo, Salvador and Villela, Karina},
title = {Software diversity: state of the art and perspectives},
year = {2012},
issue_date = {October   2012},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {5},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-012-0253-y},
doi = {10.1007/s10009-012-0253-y},
abstract = {Diversity is prevalent in modern software systems to facilitate adapting the software to customer requirements or the execution environment. Diversity has an impact on all phases of the software development process. Appropriate means and organizational structures are required to deal with the additional complexity introduced by software variability. This introductory article to the special section "Software Diversity--Modeling, Analysis and Evolution" provides an overview of the current state of the art in diverse systems development and discusses challenges and potential solutions. The article covers requirements analysis, design, implementation, verification and validation, maintenance and evolution as well as organizational aspects. It also provides an overview of the articles which are part of this special section and addresses particular issues of diverse systems development.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = oct,
pages = {477–495},
numpages = {19},
keywords = {Variability, Software product lines, Software diversity}
}

@article{10.1007/s10270-018-00712-x,
author = {Bencomo, Nelly and G\"{o}tz, Sebastian and Song, Hui},
title = {Models@run.time: a guided tour of the state of the art and research challenges},
year = {2019},
issue_date = {October   2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-00712-x},
doi = {10.1007/s10270-018-00712-x},
abstract = {More than a decade ago, the research topic models@run.time was coined. Since then, the research area has received increasing attention. Given the prolific results during these years, the current outcomes need to be sorted and classified. Furthermore, many gaps need to be categorized in order to further develop the research topic by experts of the research area but also newcomers. Accordingly, the paper discusses the principles and requirements of models@run.time and the state of the art of the research line. To make the discussion more concrete, a taxonomy is defined and used to compare the main approaches and research outcomes in the area during the last decade and including ancestor research initiatives. We identified and classified 275 papers on models@run.time, which allowed us to identify the underlying research gaps and to elaborate on the corresponding research challenges. Finally, we also facilitate sustainability of the survey over time by offering tool support to add, correct and visualize data.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {3049–3082},
numpages = {34},
keywords = {Systematic literature review, Self-reflection, Models@run.time, Causal connection}
}

@inproceedings{10.1007/978-3-319-35122-3_9,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Alam, Omar and Sch\"{o}ttle, Matthias and Belloir, Nicolas and Collet, Philippe and Combemale, Benoit and Deantoni, Julien and Klein, Jacques and Rumpe, Bernhard},
title = {VCU: The Three Dimensions of Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_9},
doi = {10.1007/978-3-319-35122-3_9},
abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces VCU. A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {122–137},
numpages = {16},
keywords = {Variability, Usage, Reuse, Interfaces, Extension, Customization, Configuration, Concern-oriented reuse},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2031759.2031771,
author = {Dobrica, Liliana and Ovaska, Eila},
title = {Analysis of a cross-domain reference architecture using change scenarios},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031771},
doi = {10.1145/2031759.2031771},
abstract = {The content of this paper addresses the issue of how to perform analysis of a cross domain reference architecture. The cross domain reference architecture is designed based on the domains requirements and features modeling. The definition of a cross domain reference architecture is based on well known concepts from software architecture description, service orientation and product line. We apply a method based on change scenarios to analyze variability at the architectural level. In order to handle complexity in analysis we propose categories of change scenarios to be derived from each problem domain and we provide informal guidelines for each step of the analysis method.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {10},
numpages = {9},
keywords = {variability, service, scenarios, quality, cross domain reference architecture, analysis methods},
location = {Essen, Germany},
series = {ECSA '11}
}

@inproceedings{10.5555/1565362.1565386,
author = {Agarwal, Ankur and Shankar, Ravi and Pandya, A. S.},
title = {Embedding Intelligence into EDA Tools},
year = {2006},
isbn = {1586036750},
publisher = {IOS Press},
address = {NLD},
abstract = {Multiprocessor system on chip (MpSoC) platform has set a new innovative trend for the system-on-chip (SoC) design. Demanding Quality of Service (QOS) and performance metrics are leading to the adoption of a new design methodology for MpSoC. These will have to be built around highly scalable and reusable architectures that yield high speed at low cost and high energy efficiency for a variety of demanding applications. Designing such a system, in the presence of such aggressive QOS and Performance requirements, is an NP-complete problem. In this paper, we present the application of genetic algorithms to system level design flow to provide best effort solutions for two specific tasks, viz.., performance tradeoff and task partitioning.},
booktitle = {Proceedings of the 2006 Conference on Integrated Intelligent Systems for Engineering Design},
pages = {389–408},
numpages = {20},
keywords = {System Level Design, Non-Recurring Expense (Nre), Network On Chip, Intelligent Algorithm, Genetic Algorithms, Electronic Design Automation (Eda), Data Mining}
}

@article{10.1007/s10270-017-0592-y,
author = {Ross, Jordan A. and Murashkin, Alexandr and Liang, Jia Hui and Antkiewicz, Micha\l{} and Czarnecki, Krzysztof},
title = {Synthesis and exploration of multi-level, multi-perspective architectures of automotive embedded systems},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0592-y},
doi = {10.1007/s10270-017-0592-y},
abstract = {In industry, evaluating candidate architectures for automotive embedded systems is routinely done during the design process. Today's engineers, however, are limited in the number of candidates that they are able to evaluate in order to find the optimal architectures. This limitation results from the difficulty in defining the candidates as it is a mostly manual process. In this work, we propose a way to synthesize multi-level, multi-perspective candidate architectures and to explore them across the different layers and perspectives. Using a reference model similar to the EAST-ADL domain model but with a focus on early design, we explore the candidate architectures for two case studies: an automotive power window system and the central door locking system. Further, we provide a comprehensive set of question templates, based on the different layers and perspectives, that engineers can ask to synthesize only the candidates relevant to their task at hand. Finally, using the modeling language Clafer, which is supported by automated backend reasoners, we show that it is possible to synthesize and explore optimal candidate architectures for two highly configurable automotive sub-systems.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {739–767},
numpages = {29},
keywords = {Multi-perspective architectures, Multi-level architectures, Early design, E/E architecture, Candidate architectures, Architecture synthesis, Architecture optimization}
}

@article{10.1145/3487921,
author = {Hezavehi, Sara M. and Weyns, Danny and Avgeriou, Paris and Calinescu, Radu and Mirandola, Raffaela and Perez-Palacin, Diego},
title = {Uncertainty in Self-adaptive Systems: A Research Community Perspective},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487921},
doi = {10.1145/3487921},
abstract = {One of the primary drivers for self-adaptation is ensuring that systems achieve their goals regardless of the uncertainties they face during operation. Nevertheless, the concept of uncertainty in self-adaptive systems is still insufficiently understood. Several taxonomies of uncertainty have been proposed, and a substantial body of work exists on methods to tame uncertainty. Yet, these taxonomies and methods do not fully convey the research community’s perception on what constitutes uncertainty in self-adaptive systems and on the key characteristics of the approaches needed to tackle uncertainty. To understand this perception and learn from it, we conducted a survey comprising two complementary stages in which we collected the views of 54 and 51 participants, respectively. In the first stage, we focused on current research and development, exploring how the concept of uncertainty is understood in the community and how uncertainty is currently handled in the engineering of self-adaptive systems. In the second stage, we focused on directions for future research to identify potential approaches to dealing with unanticipated changes and other open challenges in handling uncertainty in self-adaptive systems. The key findings of the first stage are: (a) an overview of uncertainty sources considered in self-adaptive systems, (b) an overview of existing methods used to tackle uncertainty in concrete applications, (c) insights into the impact of uncertainty on non-functional requirements, (d) insights into different opinions in the perception of uncertainty within the community and the need for standardised uncertainty-handling processes to facilitate uncertainty management in self-adaptive systems. The key findings of the second stage are: (a) the insight that over 70% of the participants believe that self-adaptive systems can be engineered to cope with unanticipated change, (b) a set of potential approaches for dealing with unanticipated change, (c) a set of open challenges in mitigating uncertainty in self-adaptive systems, in particular in those with safety-critical requirements. From these findings, we outline an initial reference process to manage uncertainty in self-adaptive systems. We anticipate that the insights on uncertainty obtained from the community and our proposed reference process will inspire valuable future research on self-adaptive systems.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {10},
numpages = {36},
keywords = {survey, uncertainty challenges, unanticipated change, uncertainty methods, uncertainty models, uncertainty, Self-adaptation}
}

@inproceedings{10.1145/2025113.2025177,
author = {Mori, Marco},
title = {A software lifecycle process for context-aware adaptive systems},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025177},
doi = {10.1145/2025113.2025177},
abstract = {It is increasingly important for computing systems to evolve their behavior at run-time because of resources uncertainty, system failures and emerging user needs. Our approach supports software engineers to analyze and develop context-aware adaptive applications. The software lifecycle process we propose supports static and dynamic decision making mechanisms, run-time consistent evolution and it is amenable to be automated.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {412–415},
numpages = {4},
keywords = {software lifecycle process, feature engineering, context-aware adaptive systems, consistent evolution},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@article{10.1016/j.jss.2018.07.014,
author = {Makki, Majid and Van Landuyt, Dimitri and Lagaisse, Bert and Joosen, Wouter},
title = {A comparative study of workflow customization strategies: Quality implications for multi-tenant SaaS},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.014},
doi = {10.1016/j.jss.2018.07.014},
journal = {J. Syst. Softw.},
month = oct,
pages = {423–438},
numpages = {16},
keywords = {Software quality, Functional customization, Workflow automation, Software-as-a-Service, Multi-tenancy}
}

@inproceedings{10.1145/2602576.2602585,
author = {Etxeberria, Leire and Trubiani, Catia and Cortellessa, Vittorio and Sagardui, Goiuria},
title = {Performance-based selection of software and hardware features under parameter uncertainty},
year = {2014},
isbn = {9781450325769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602576.2602585},
doi = {10.1145/2602576.2602585},
abstract = {Configurable software systems allow stakeholders to derive variants by selecting software and/or hardware features. Performance analysis of feature-based systems has been of large interest in the last few years, however a major research challenge is still to conduct such analysis before achieving full knowledge of the system, namely under a certain degree of uncertainty. In this paper we present an approach to analyze the correlation between selection of features embedding uncertain parameters and system performance. In particular, we provide best and worst case performance bounds on the basis of selected features and, in cases of wide gaps among these bounds, we carry on a sensitivity analysis process aimed at taming the uncertainty of parameters. The application of our approach to a case study in the e-health domain demonstrates how to support stakeholders in the identification of system variants that meet performance requirements.},
booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {23–32},
numpages = {10},
keywords = {uncertainty, software architectures, performance analysis, feature selection},
location = {Marcq-en-Bareul, France},
series = {QoSA '14}
}

@article{10.1504/ijwgs.2019.100837,
author = {Bani-Ismail, Basel and Baghdadi, Youcef},
title = {Migrating two legacy systems to SOA: a new approach for service selection based on data flow diagram},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {15},
number = {3},
issn = {1741-1106},
url = {https://doi.org/10.1504/ijwgs.2019.100837},
doi = {10.1504/ijwgs.2019.100837},
abstract = {There are many service identification methods (SIMs) to simplify service identification in SOA lifecycle. These SIMs vary in terms of their features (e.g., input artefact, technique). Due to this diversity, few evaluation frameworks have been proposed to guide organisations in selecting a suitable SIM based on their available input artefacts (e.g., source code, business process). This research concerns with SIMs that consider data flow diagram (DFD) as an input artefact, in order to migrate two legacy systems, modelled with DFD, to SOA. Only two SIMs are found in the literature to identify services based on DFD. However, these SIMs do not provide a way to select among the services identified to be implemented as web services. Therefore, this paper aims to bridge this gap by proposing a new approach for service selection based on DFD to assist organisations in speeding up the process of migrating their legacy systems to SOA.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {251–281},
numpages = {30},
keywords = {DFD, data flow diagram, evaluation framework, service quality, service selection, SIM, service identification method, service identification, SOA, service-oriented architecture}
}

@inproceedings{10.1109/MISE.2009.5069898,
author = {Nguyen, Quyen L.},
title = {Non-functional requirements analysis modeling for software product lines},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069898},
doi = {10.1109/MISE.2009.5069898},
abstract = {In most IT projects, software developers usually pay attention to functional requirements that satisfy business needs of the system. Non-functional requirements (NFR) such as performance, usability, security, etc. are usually handled ad-hoc during the system testing phase, when it is late and costly to fix problems. Due to the importance and criticality of NFR, I study the problem of modeling NFR for Software Product Lines (SPL), which adds yet an additional dimension of complexity. This paper will survey the software engineering literature, in search of a systematic way to analyze and design NFR, from the perspectives of the concept of commonality and variability of SPL and the characteristics of NFR. Finally, I will propose a methodology based on the extension of Product Line UML-Based Software Engineering (PLUS) techniques, for a unified and automated method to model NFR throughout all phases of SPL engineering.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {56–61},
numpages = {6},
series = {MISE '09}
}

@article{10.1016/j.jss.2010.02.017,
author = {White, J. and Benavides, D. and Schmidt, D. C. and Trinidad, P. and Dougherty, B. and Ruiz-Cortes, A.},
title = {Automated diagnosis of feature model configurations},
year = {2010},
issue_date = {July, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.02.017},
doi = {10.1016/j.jss.2010.02.017},
abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1094–1107},
numpages = {14},
keywords = {Software product-lines, Optimization, Diagnosis, Constraint satisfaction, Configuration}
}

@inproceedings{10.1145/3196398.3196442,
author = {Nair, Vivek and Agrawal, Amritanshu and Chen, Jianfeng and Fu, Wei and Mathew, George and Menzies, Tim and Minku, Leandro and Wagner, Markus and Yu, Zhe},
title = {Data-driven search-based software engineering},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196442},
doi = {10.1145/3196398.3196442},
abstract = {This paper introduces Data-Driven Search-based Software Engineering (DSE), which combines insights from Mining Software Repositories (MSR) and Search-based Software Engineering (SBSE). While MSR formulates software engineering problems as data mining problems, SBSE reformulate Software Engineering (SE) problems as optimization problems and use meta-heuristic algorithms to solve them. Both MSR and SBSE share the common goal of providing insights to improve software engineering. The algorithms used in these two areas also have intrinsic relationships. We, therefore, argue that combining these two fields is useful for situations (a) which require learning from a large data source or (b) when optimizers need to know the lay of the land to find better solutions, faster.This paper aims to answer the following three questions: (1) What are the various topics addressed by DSE?, (2) What types of data are used by the researchers in this area?, and (3) What research approaches do researchers use? The paper briefly sets out to act as a practical guide to develop new DSE techniques and also to serve as a teaching resource.This paper also presents a resource (tiny.cc/data-se) for exploring DSE. The resource contains 89 artifacts which are related to DSE, divided into 13 groups such as requirements engineering, software product lines, software processes. All the materials in this repository have been used in recent software engineering papers; i.e., for all this material, there exist baseline results against which researchers can comparatively assess their new ideas.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {341–352},
numpages = {12},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.1007/s10664-019-09705-w,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Apel, Sven},
title = {On the relation of control-flow and performance feature interactions: a case study},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09705-w},
doi = {10.1007/s10664-019-09705-w},
abstract = {Detecting feature interactions is imperative for accurately predicting performance of highly-configurable systems. State-of-the-art performance prediction techniques rely on supervised machine learning for detecting feature interactions, which, in turn, relies on time-consuming performance measurements to obtain training data. By providing information about potentially interacting features, we can reduce the number of required performance measurements and make the overall performance prediction process more time efficient. We expect that information about potentially interacting features can be obtained by analyzing the source code of a highly-configurable system, which is computationally cheaper than performing multiple performance measurements. To this end, we conducted an in-depth qualitative case study on two real-world systems (mbedTLS and SQLite), in which we explored the relation between internal (precisely control-flow) feature interactions, detected through static program analysis, and external (precisely performance) feature interactions, detected by performance-prediction techniques using performance measurements. We found that a relation exists that can potentially be exploited to predict performance interactions.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2410–2437},
numpages = {28},
keywords = {Feature-interaction prediction, Feature interaction, Feature, Control-flow feature interaction, Variability, Performance feature interaction, Highly configurable software system}
}

@article{10.1016/j.comnet.2013.02.025,
author = {Apel, Sven and Von Rhein, Alexander and Th\"{u}m, Thomas and K\"{a}stner, Christian},
title = {Feature-interaction detection based on feature-based specifications},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {12},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2013.02.025},
doi = {10.1016/j.comnet.2013.02.025},
abstract = {Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification.},
journal = {Comput. Netw.},
month = aug,
pages = {2399–2409},
numpages = {11},
keywords = {Feature interaction, Feature orientation, Feature-based specification, Modularity, Software product lines}
}

@inproceedings{10.1109/WI.2006.173,
author = {Zhou, Jiehan and Niemela, Eila},
title = {Toward Semantic QoS Aware Web Services: Issues, Related Studies and Experience},
year = {2006},
isbn = {0769527477},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI.2006.173},
doi = {10.1109/WI.2006.173},
abstract = {Semantic QoSaware Web services incorporating the emerging Web services in the QoSaware system development are promoting ServiceOriented Software Engineering (SOSE). To identify the steps toward semantic quality of service (QoS)aware Web services, this paper examines previous studies related to semantic QoSaware Web services, including QoSaware Web service architectures, QoS classification, QoS ontology, QoS specification languages, and Web service creation tools. Moreover, a case study is presented to discuss the gaps between our current quality driven software development approach and the semantic QoSaware Web services.},
booktitle = {Proceedings of the 2006 IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {553–557},
numpages = {5},
series = {WI '06}
}

@inproceedings{10.1145/2593770.2593781,
author = {Ramaswamy, Arunkumar and Monsuez, Bruno and Tapus, Adriana},
title = {Model-driven software development approaches in robotics research},
year = {2014},
isbn = {9781450328494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593770.2593781},
doi = {10.1145/2593770.2593781},
abstract = {Recently, there is an encouraging trend in adopting model-driven engineering approaches for software development in robotics research. In this paper, currently available model-driven techniques in robotics are analyzed with respect to the domain-specific requirements. A conceptual overview of our software development approach called 'Self Adaptive Framework for Robotic Systems (SafeRobots)' is explained and we also try to position our approach within this model ecosystem.},
booktitle = {Proceedings of the 6th International Workshop on Modeling in Software Engineering},
pages = {43–48},
numpages = {6},
keywords = {Model-driven software development, Robotics},
location = {Hyderabad, India},
series = {MiSE 2014}
}

@inbook{10.5555/1768283.1768287,
author = {Cuenot, Philippe and Chen, DeJiu and G\'{e}rard, S\'{e}bastien and L\"{o}nn, Henrik and Reiser, Mark-Oliver and Servat, David and Kolagari, Ramin Tavakoli and T\"{o}rngren, Martin and Weber, Matthias},
title = {Towards improving dependability of automotive systems by using the EAST-ADL architecture description language},
year = {2007},
isbn = {9783540740339},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The complexity of embedded automotive systems calls for a more rigorous approach to system development compared to current state of practice. A critical issue is the management of the engineering information that defines the embedded system. Development time, cost efficiency, quality and most importantly, dependability, all benefit from appropriate information management. System modeling based on an architecture description language is a way to keep the engineering information in one information structure. The EAST-ADL was developed in the EAST-EEA project (www.east-eea.org) and is an architecture description language for automotive embedded systems. It is currently refined in the ATESST project (www.atesst.org). This chapter describes how dependability is addressed in the EAST-ADL. The engineering process defined in the EASIS project (www.easis-online.org) is used as an example to illustrate the support for engineering processes in EAST-ADL.},
booktitle = {Architecting Dependable Systems IV},
pages = {39–65},
numpages = {27}
}

@article{10.1016/j.jss.2017.09.033,
author = {Badampudi, Deepika and Wnuk, Krzysztof and Wohlin, Claes and Franke, Ulrik and Smite, Darja and Cicchetti, Antonio},
title = {A decision-making process-line for selection of software asset origins and components},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.033},
doi = {10.1016/j.jss.2017.09.033},
abstract = {Presents a process-line for selecting software asset origins and components.Process-line helps decision-makers to build their decisions-making process.The process-line is evaluated through five case studies in three companies.The practitioners did not perceive any activity to be missing in the process-line.A sub-set of activities were followed by the companies without any specific order. Selecting sourcing options for software assets and components is an important process that helps companies to gain and keep their competitive advantage. The sourcing options include: in-house, COTS, open source and outsourcing. The objective of this paper is to further refine, extend and validate a solution presented in our previous work. The refinement includes a set of decision-making activities, which are described in the form of a process-line that can be used by decision-makers to build their specific decision-making process. We conducted five case studies in three companies to validate the coverage of the set of decision-making activities. The solution in our previous work was validated in two cases in the first two companies. In the validation, it was observed that no activity in the proposed set was perceived to be missing, although not all activities were conducted and the activities that were conducted were not executed in a specific order. Therefore, the refinement of the solution into a process-line approach increases the flexibility and hence it is better in capturing the differences in the decision-making processes observed in the case studies. The applicability of the process-line was then validated in three case studies in a third company.},
journal = {J. Syst. Softw.},
month = jan,
pages = {88–104},
numpages = {17},
keywords = {Case study, Component-based software engineering, Decision-making}
}

@article{10.1007/s10270-021-00909-7,
author = {Abrah\~{a}o, Silvia and Insfran, Emilio and Slu\"{y}ters, Arthur and Vanderdonckt, Jean},
title = {Model-based intelligent user interface adaptation: challenges and future directions},
year = {2021},
issue_date = {Oct 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-021-00909-7},
doi = {10.1007/s10270-021-00909-7},
abstract = {Adapting the user interface of a software system to the requirements of the context of use continues to be a major challenge, particularly when users become more demanding in terms of adaptation quality. A considerable number of methods have, over the past three decades, provided some form of modelling with which to support user interface adaptation. There is, however, a crucial issue as regards in analysing the concepts, the underlying knowledge, and the user experience afforded by these methods as regards comparing their benefits and shortcomings. These methods are so numerous that positioning a new method in the state of the art is challenging. This paper, therefore, defines a conceptual reference framework for intelligent user interface adaptation containing a set of conceptual adaptation properties that are useful for model-based user interface adaptation. The objective of this set of properties is to understand any method, to compare various methods and to generate new ideas for adaptation. We also analyse the opportunities that machine learning techniques could provide for data processing and analysis in this context, and identify some open challenges in order to guarantee an appropriate user experience for end-users. The relevant literature and our experience in research and industrial collaboration have been used as the basis on which to propose future directions in which these challenges can be addressed.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1335–1349},
numpages = {15},
keywords = {Context of use, Intelligent user interface, Machine learning, Model-based software engineering, Model-driven engineering, User interface adaptation, Conceptual reference framework}
}

@inproceedings{10.1145/2000259.2000263,
author = {Koziolek, Heiko},
title = {Sustainability evaluation of software architectures: a systematic review},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000263},
doi = {10.1145/2000259.2000263},
abstract = {Long-living software systems are sustainable if they can be cost-efficiently maintained and evolved over their entire life-cycle. The quality of software architectures determines sustainability to a large extent. Scenario-based software architecture evaluation methods can support sustainability analysis, but they are still reluctantly used in practice. They are also not integrated with architecture-level metrics when evaluating implemented systems, which limits their capabilities. Existing literature reviews for architecture evaluation focus on scenario-based methods, but do not provide a critical reflection of the applicability of such methods for sustainability evaluation. Our goal is to measure the sustainability of a software architecture both during early design using scenarios and during evolution using scenarios and metrics, which is highly relevant in practice. We thus provide a systematic literature review assessing scenario-based methods for sustainability support and categorize more than 40 architecture-level metrics according to several design principles. Our review identifies a need for further empirical research, for the integration of existing methods, and for the more efficient use of formal architectural models.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {3–12},
numpages = {10},
keywords = {architectural metric, evolution scenario, software architecture, survey, sustainability},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1007/978-3-642-41533-3_37,
author = {Alam, Omar and Kienzle, J\"{o}rg and Mussbacher, Gunter},
title = {Concern-Oriented Software Design},
year = {2013},
isbn = {9783642415326},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-41533-3_37},
doi = {10.1007/978-3-642-41533-3_37},
abstract = {There exist many solutions to solve a given design problem, and it is difficult to capture the essence of a solution and make it reusable for future designs. Furthermore, many variations of a given solution may exist, and choosing the best alternative depends on application-specific high-level goals and non-functional requirements. This paper proposes Concern-Oriented Software Design, a modelling technique that focuses on concerns as units of reuse. A concern groups related models serving the same purpose, and provides three interfaces to facilitate reuse. The variation interface presents the design alternatives and their impact on non-functional requirements. The customization interface of the selected alternative details how to adapt the generic solution to a specific context. Finally, the usage interface specifies the provided behaviour. We illustrate our approach by presenting the concern models of variations of the Observer design pattern, which internally depends on the Association concern to link observers and subjects.},
booktitle = {Proceedings of the 16th International Conference on Model-Driven Engineering Languages and Systems - Volume 8107},
pages = {604–621},
numpages = {18}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00107,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-box performance-influence models: a profiling and learning approach (replication package)},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00107},
doi = {10.1109/ICSE-Companion52605.2021.00107},
abstract = {These artifacts refer to the study and implementation of the paper 'White-Box Performance-Influence Models: A Profiling and Learning Approach'. In this document, we describe the idea and process of how to build white-box performance models for configurable software systems. Specifically, we describe the general steps and tools that we have used to implement our approach, the data we have obtained, and the evaluation setup. We further list the available artifacts, such as raw measurements, configurations, and scripts at our software heritage repository.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {232–233},
numpages = {2},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A variability-based testing approach for synthesizing video sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = {A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms.},
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Combinatorial testing, Variability, Video analysis},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1007/11554844_19,
author = {Helferich, Andreas and Herzwurm, Georg and Schockert, Sixten},
title = {QFD-PPP: product line portfolio planning using quality function deployment},
year = {2005},
isbn = {3540289364},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11554844_19},
doi = {10.1007/11554844_19},
abstract = {In today’s competitive business environment, it is extremely important to offer customers exactly the products they want. Software product lines have the potential to enable companies to offer a large variety of products while still being able to manage the complexity caused by this increased number of products. But offering a large range of variants does not necessarily mean increased profits, as many manufacturing companies had to notice in the early 1990ies. The task of Product Portfolio Planning is the development of a product portfolio that optimally satisfies customer demands and at the same time restricts the number of products offered. Quality Function Deployment (QFD) is a well-known and successfully used Quality Management method that can help companies to identify true customer needs and the features needed to fulfil these needs. This paper demonstrates how QFD can be used for Product Portfolio Planning, thus offering potentially great benefits.},
booktitle = {Proceedings of the 9th International Conference on Software Product Lines},
pages = {162–173},
numpages = {12},
location = {Rennes, France},
series = {SPLC'05}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1007/s11219-018-9405-y,
author = {Vale, Gustavo and Fernandes, Eduardo and Figueiredo, Eduardo},
title = {On the proposal and evaluation of a benchmark-based threshold derivation method},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9405-y},
doi = {10.1007/s11219-018-9405-y},
abstract = {Software-intensive systems have been growing in both size and complexity. Consequently, developers need better support for measuring and controlling the software quality. In this context, software metrics aim at quantifying different software quality aspects. However, the effectiveness of measurement depends on the definition of reliable metric thresholds, i.e., numbers that characterize a metric value as critical given a quality aspect. In fact, without proper metric thresholds, it might be difficult for developers to indicate problematic software components for correction, for instance. Based on a literature review, we have found several existing methods for deriving metric thresholds and observed their evolution. Such evolution motivated us to propose a new method that incorporates the best of the existing methods. In this paper, we propose a novel benchmark-based method for deriving metric thresholds. We assess our method, called Vale's method, using a set of metric thresholds derived with the support of our method, aimed at composing detection strategies for two well-known code smells, namely god class and lazy class. For this purpose, we analyze three benchmarks composed of multiple software product lines. In addition, we demonstrate our method in practice by applying it to a benchmark composed of 103 Java open-source software systems. In the evaluation, we compare Vale's method to two state-of-the-practice threshold derivation methods selected as a baseline, which are Lanza's method and Alves' method. Our results suggest that the proposed method provides more realistic and reliable thresholds, with better recall and precision in the code smell detection, when compared to both baseline methods.},
journal = {Software Quality Journal},
month = mar,
pages = {275–306},
numpages = {32},
keywords = {Benchmark, Code smell, Software metric, Software product lines, Threshold}
}

@inproceedings{10.1007/978-3-030-66030-7_2,
author = {Wonjiga, Amir Teshome and Lacoste, Marc},
title = {Towards an Extensible Security Monitoring Architecture for Vehicular Networks},
year = {2020},
isbn = {978-3-030-66029-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-66030-7_2},
doi = {10.1007/978-3-030-66030-7_2},
abstract = {Extensibility for security monitoring in 5G vehicular networks remains largely unexplored despite strong requirements for interoperability, to support multiple properties (e.g., security, privacy, trust, sustainability) and to reach trade-offs. We discuss ITS security monitoring challenges and propose an extensible monitoring architecture to meet them. We design and implement a sample security monitoring probe for CAM and DENM and demonstrate on simulations the probe capabilities on a cooperative collision detection use case.},
booktitle = {Communication Technologies for Vehicles: 15th International Workshop, Nets4Cars/Nets4Trains/Nets4Aircraft 2020, Bordeaux, France, November 16–17, 2020, Proceedings},
pages = {15–24},
numpages = {10},
keywords = {Extensible security monitoring, ITS, CAM, DENM},
location = {Bordeaux, France}
}

@inproceedings{10.1145/2593882.2593886,
author = {Garlan, David},
title = {Software architecture: a travelogue},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593886},
doi = {10.1145/2593882.2593886},
abstract = {Over the past two and a half decades software architecture has emerged as an important subfield of software engineering. During that time there has been considerable progress in developing the technological and methodological base for treating architectural design as an engineering discipline. However, much still remains to be done to achieve that. Moreover, the changing face of technology raises a number of challenges for software architecture. This travelogue recounts the history of the field, its current state of practice and research, and speculates on some of the important emerging trends, challenges, and aspirations.},
booktitle = {Future of Software Engineering Proceedings},
pages = {29–39},
numpages = {11},
keywords = {Software architecture, architecture and agility, architecture description languages, architecture styles, architecture trends, software frame-works, software product lines},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@inproceedings{10.1145/3387940.3391474,
author = {Brings, Jennifer and Daun, Marian},
title = {Towards automated safety analysis for architectures of dynamically forming networks of cyber-physical systems},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391474},
doi = {10.1145/3387940.3391474},
abstract = {Dynamically forming networks of cyber-physical systems are becoming increasingly widespread in manufacturing, transportation, automotive, avionics and more domains. The emergence of future internet technology and the ambition for ever closer integration of different systems leads to highly collaborative cyber-physical systems. Such cyber-physical systems form networks to provide additional functions, behavior, and benefits the individual systems cannot provide on their own. As safety is a major concern of systems from these domains, there is a need to provide adequate support for safety analyses of these collaborative cyber-physical systems. This support must explicitly consider the dynamically formed networks of cyber-physical systems. This is a challenging task as the configurations of these cyber-physical system networks (i.e. the architecture of the super system the individual system joins) can differ enormously depending on the actual systems joining a cyber-physical system network. Furthermore, the configuration of the network heavily impacts the adaptations performed by the individual systems and thereby impacting the architecture not only of the system network but of all individual systems involved. As existing safety analysis techniques, however, are not meant for supporting such an array of potential system network configurations the individual system will have to be able to cope with at runtime, we propose automated support for safety analysis for these systems that considers the configuration of the system network. Initial evaluation results from the application to industrial case examples show that the proposed support can aid in the detection of safety defects.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {258–265},
numpages = {8},
keywords = {cyber-physical system, safety analysis, system architecture},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3023956.3023959,
author = {Ochoa, Lina and Pereira, Juliana Alves and Gonz\'{a}lez-Rojas, Oscar and Castro, Harold and Saake, Gunter},
title = {A survey on scalability and performance concerns in extended product lines configuration},
year = {2017},
isbn = {9781450348119},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3023956.3023959},
doi = {10.1145/3023956.3023959},
abstract = {Product lines have been employed as a mass customisation method that reduces production costs and time-to-market. Multiple product variants are represented in a product line, however the selection of a particular configuration depends on stakeholders' functional and non-functional requirements. Methods like constraint programming and evolutionary algorithms have been used to support the configuration process. They consider a set of product requirements like resource constraints, stakeholders' preferences, and optimization objectives. Nevertheless, scalability and performance concerns start to be an issue when facing large-scale product lines and runtime environments. Thus, this paper presents a survey that analyses strengths and drawbacks of 21 approaches that support product line configuration. This survey aims to: i) evidence which product requirements are currently supported by studied methods; ii) how scalability and performance is considered in existing approaches; and iii) point out some challenges to be addressed in future research.},
booktitle = {Proceedings of the 11th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {5–12},
numpages = {8},
keywords = {configuration, literature review, performance, product line, product requirements, scalability, survey},
location = {Eindhoven, Netherlands},
series = {VaMoS '17}
}

@inproceedings{10.1007/978-3-319-35122-3_16,
author = {Braga, Rosana T. and Feloni, Daniel and Pacini, Karen and Filho, Domenico Schettini and Gottardi, Thiago},
title = {AIRES: An Architecture to Improve Software Reuse},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_16},
doi = {10.1007/978-3-319-35122-3_16},
abstract = {Among the several challenges still faced by Software Engineering, software reuse can be listed as a potential solution towards improving productivity and quality, through the utilization of previously produced artifacts that can leverage development activities. Among these artifacts we can mention not only code, but also requirements' documents, analysis and design models, test cases, documentation, and even development processes that achieved success in the past and could be reused again and again. However, the diversity of methods, processes and tools for software engineering make it difficult to turn reuse into a systematic activity. Considering this context, the present paper aims at presenting an architectural model that encompasses the main elements needed to support software reuse in a large scale. This model, named AIRES, allows reuse to be realized intrinsically to the development process life cycle, providing mechanisms to facilitate a variety of processes and artifacts representation and a Service-Oriented Architecture SOA to make assets available to other software engineering environments or tools. The AIRES model is being implemented using open source platforms and will be available within the cloud.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {231–246},
numpages = {16},
keywords = {Reuse environments, Reuse tools, Software reuse},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/2833258.2833284,
author = {Bien, Ngo Huy and Thu, Tran Dan},
title = {Graphical User Interface Variability Architecture Pattern},
year = {2015},
isbn = {9781450338431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2833258.2833284},
doi = {10.1145/2833258.2833284},
abstract = {Designing software applications for multiple tenants is challenging. The task is even harder when designing pure multi-tenancy applications that must support different customers using a single codebase and data store. One of the most common problems when developing these systems is to support different graphical user interface not only for different users but also tenants. This critical requirement also applies to context-aware applications in which different graphical user interface should be presented to each user according to the user's context or software components and platforms that should allow developers easily to create different looks and feel for their applications. In this paper, we propose an architecture pattern for modeling graphical user interface that support different customizations and configurations. The modularity, the reusability and the maintainability of the pattern were evaluated by qualitative analysis based on well-known patterns used in the proposed architecture pattern. Real world systems were built to validate the applicability, the correctness, the security and the performance of the pattern. We believe that our pattern will be useful for software providers as well as normal organizations when building software components or software systems for different customers or creating multi-tenancy applications in a cloud-based environment or building context-aware applications.},
booktitle = {Proceedings of the 6th International Symposium on Information and Communication Technology},
pages = {304–311},
numpages = {8},
keywords = {Graphical user interface, PaaS, SaaS, architecture pattern, enterprise systems, multi-tenancy, software architecture, software variability, variability pattern},
location = {Hue City, Viet Nam},
series = {SoICT '15}
}

@inproceedings{10.1145/2304696.2304705,
author = {Alebrahim, Azadeh and Heisel, Maritta},
title = {Supporting quality-driven design decisions by modeling variability},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304705},
doi = {10.1145/2304696.2304705},
abstract = {Design decisions should take quality characteristics into account. To support such decisions, we capture various solution artifacts with different levels of satisfying quality requirements as variabilities in the solution space and provide them with rationales for selecting suitable variants. We present a UML-based approach to modeling variability in the problem and the solution space by adopting the notion of feature modeling. It provides a mapping of requirements variability to design solution variability to be used as a part of a general process for generating design alternatives. Our approach supports the software engineer in the process of decision-making for selecting suitable solution variants, reflecting quality concerns, and reasoning about it.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {43–48},
numpages = {6},
keywords = {decision-making, design alternatives, feature modeling, quality requirements, variability modeling},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.1007/978-3-030-45989-5_12,
author = {Sun, Chang-ai and Wang, Jing and Guo, Jing and Wang, Zhen and Duan, Li},
title = {A Reconfigurable Microservice-Based Migration Technique for IoT Systems},
year = {2019},
isbn = {978-3-030-45988-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-45989-5_12},
doi = {10.1007/978-3-030-45989-5_12},
abstract = {An Internet of Things (IoT) system is often an integration of a large number of hardware and software modules, which are expected to be easily replaced or reconfigured in order to cater for quickly-changing environments and requirements. With the popularity of microservices, people have attempted to introduce the microservice architecture to IoT systems, while paid little attention to the connectivity between the decomposed microservices, resulting in poor reconfigurability of the resulting system. In this paper, we propose a reconfigurable microservice-based migration technique for IoT systems, which first decomposes an IoT system as a set of microservices and then introduces variation contexts to make the decomposed microservices reconfigurable. We have conducted a case study on an open-source real-life unmanned aerial vehicle (UAV) system. The results demonstrate that the migrated UAV system can be dynamically reconfigured to handle various run-time changes.},
booktitle = {Service-Oriented Computing – ICSOC 2019 Workshops: WESOACS, ASOCA, ISYCC, TBCE, and STRAPS, Toulouse, France, October 28–31, 2019, Revised Selected Papers},
pages = {142–155},
numpages = {14},
keywords = {Internet of Things (IoT), Microservices, Migration techniques, Service compositions, Reconfigurable systems},
location = {Toulouse, France}
}

@article{10.1145/3178315.3178328,
author = {Wang, Huaimin},
title = {Harnessing the crowd wisdom for software trustworthiness},
year = {2018},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3178315.3178328},
doi = {10.1145/3178315.3178328},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {1–6},
numpages = {6}
}

@inproceedings{10.1145/2984043.2998537,
author = {D\"{u}rschmid, Tobias},
title = {Design pattern builder: a concept for refinable reusable design pattern libraries},
year = {2016},
isbn = {9781450344371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2984043.2998537},
doi = {10.1145/2984043.2998537},
abstract = {Reuse is one of the core principles in professional software engineering. Design patterns provide a reusable solution for common design problems. But their implementations are generally not reusable as they are often well tailored to a specific context. We introduce a concept, that facilitates the reuse of their implementations by defining an abstract design pattern definition that can be instantiated with specialized design decisions. This approach is a meta-level Builder constructing design patterns as first-class citizens. It simplifies the application of design patterns by providing a pattern library and still being able to adjust it to the concrete context.},
booktitle = {Companion Proceedings of the 2016 ACM SIGPLAN International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
pages = {45–46},
numpages = {2},
keywords = {AOP, Design patterns, modularity, reusability},
location = {Amsterdam, Netherlands},
series = {SPLASH Companion 2016}
}

@article{10.1016/j.knosys.2020.106660,
author = {Liu, Zhen and Feng, Xiaodong and Wang, Yecheng and Zuo, Wenbo},
title = {Self-paced learning enhanced neural matrix factorization for noise-aware recommendation},
year = {2021},
issue_date = {Feb 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {213},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2020.106660},
doi = {10.1016/j.knosys.2020.106660},
journal = {Know.-Based Syst.},
month = feb,
numpages = {12},
keywords = {Recommendation, Deep learning, Noisy and outlier corruption, Instance weighting, Self-paced learning}
}

@inproceedings{10.1007/978-3-030-26250-1_3,
author = {Ne\v{s}i\'{c}, Damir and Nyberg, Mattias},
title = {Modular Safety Cases for Product Lines Based on Assume-Guarantee Contracts},
year = {2019},
isbn = {978-3-030-26249-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-26250-1_3},
doi = {10.1007/978-3-030-26250-1_3},
abstract = {Safety cases are recommended, and in some cases required, by a number of standards. In the product line context, unlike for single systems, safety cases are inherently complex because they must argue about the safety of a family of products that share various types of engineering assets. Safety case modularization has been proposed to reduce safety case complexity by separating concerns, modularizing tightly coupled arguments, and localizing effects of changes to particular modules. Existing modular safety-case approaches for product lines propose a feature-based modularization, which is too coarse to modularize the claims of different types, at different levels of abstraction. To overcome these limitation, a novel, modular safety-case architecture is presented. The modularization is based on a contract-based specification product-line model, which jointly captures the component-based architecture of systems and corresponding safety requirements as assume-guarantee contracts. The proposed safety-case architecture is analyzed against possible product-line changes and it is shown that it is robust both with respect to fine and coarse-grained, and also product and implementation-level changes. The proposed modular safety case is exemplified on a simplified, but real automotive system.},
booktitle = {Computer Safety, Reliability, and Security: SAFECOMP 2019 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, Turku, Finland, September 10, 2019, Proceedings},
pages = {28–40},
numpages = {13},
keywords = {Modular safety case, Assume-guarantee contract, Product line},
location = {Turku, Finland}
}

@article{10.1016/j.jss.2016.02.026,
author = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
title = {A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.026},
doi = {10.1016/j.jss.2016.02.026},
abstract = {We propose feature-driven crossover to improve feature modularization in PLA design.We compare the performance of a search algorithm with and without that crossover.Empirical results show the feature-driven crossover provides benefits to PLA design.Such crossover allows achieving better solutions and greater diversity of solutions.Such crossover operator also contributes to improve basic design principles. The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized PLA design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in PLA design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential PLA designs), with higher feature-based cohesion, and less feature scattering and tangling.},
journal = {J. Syst. Softw.},
month = nov,
pages = {126–143},
numpages = {18},
keywords = {Crossover operator, Empirical study, Multi-objective genetic algorithm, Product line architecture design}
}

@article{10.1016/j.infsof.2016.09.007,
author = {Weinreich, Rainer and Groher, Iris},
title = {Software architecture knowledge management approaches and their support for knowledge management activities},
year = {2016},
issue_date = {December 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {80},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.09.007},
doi = {10.1016/j.infsof.2016.09.007},
abstract = {Context: Numerous approaches for Software Architecture Knowledge Management (SAKM) have been developed by the research community over the last decade. Still, these approaches have not yet found widespread use in practice.Objective: This work identifies existing approaches to SAKM and analyzes them in terms of their support for central architecture knowledge management activities, i.e., capturing, using, maintaining, sharing, and reuse of architectural knowledge, along with presenting the evidence provided for this support.Method: A systematic literature review has been conducted for identifying and analyzing SAKM approaches, covering work published between January 2004 and August 2015. We identified 56 different approaches to SAKM based on 115 studies. We analyzed each approach in terms of its focus and support for important architecture knowledge management activities and in terms of the provided level of evidence for each supported activity.Results: Most of the developed approaches focus on using already-captured knowledge. Using is also the best-validated activity. The problem of efficient capturing is still not sufficiently addressed, and only a few approaches specifically address reuse, sharing, and, especially, maintaining.Conclusions: Without adequate support for other core architecture knowledge management activities besides using, the adoption of SAKM in practice will remain an elusive target. The problem of efficient capturing is still unsolved, as is the problem of maintaining captured knowledge over the long term. We also need more case studies and replication studies providing evidence for the usefulness of developed support for SAKM activities, as well as better reporting on these case studies.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {265–286},
numpages = {22},
keywords = {Software architecture, Software architecture knowledge management, Software architecture knowledge management activities, Software architecture knowledge management approaches, Systematic literature review}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1007/978-3-642-27216-5_11,
author = {Nunes, Ingrid and Barbosa, Simone Diniz Junqueira and de Lucena, Carlos J. P.},
title = {Dynamically adapting BDI agents based on high-level user specifications},
year = {2011},
isbn = {9783642272158},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-27216-5_11},
doi = {10.1007/978-3-642-27216-5_11},
abstract = {Users are facing an increasing challenge of managing information and being available anytime anywhere, as the web exponentially grows. As a consequence, assisting them in their routine tasks has become a relevant issue to be addressed. In this paper, we introduce an adaptation mechanism that is responsible for dynamically adapting a BDI agent-based running system in order to support software customisation for users. This mechanism is used within a software framework for supporting the development of Personal Assistance Software (PAS), which relies on the idea of exposing a high-level user model to empower users to manage it as well as increase user trust in the task delegation process.},
booktitle = {Proceedings of the 10th International Conference on Advanced Agent Technology},
pages = {139–163},
numpages = {25},
keywords = {BDI, framework, personal assistance software, software adaptation, user modeling},
location = {Taipei, Taiwan},
series = {AAMAS'11}
}

@article{10.1016/j.jss.2017.09.025,
title = {Managing architectural technical debt},
year = {2018},
issue_date = {January 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {135},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.09.025},
doi = {10.1016/j.jss.2017.09.025},
abstract = {A systematic literature review on Architectural Technical Debt (ATD).One key contribution is our novel model of ATD.The model provides an holistic overview of ATD.The model illustrates information for managing and raising awareness about ATD. Large Software Companies need to support the continuous and fast delivery of customer value in both the short and long term. However, this can be impeded if the evolution and maintenance of existing systems is hampered by what has been recently termed Technical Debt (TD). Specifically, Architectural TD has received increased attention in the last few years due to its significant impact on system success and, left unchecked, it can cause expensive repercussions. It is therefore important to understand the underlying factors of architectural TD. With this as background, there is a need for a descriptive model to illustrate and explain different architectural TD issues. The aim of this study is to synthesize and compile research efforts with the goal of creating new knowledge with a specific interest in the architectural TD field. The contribution of this paper is the presentation of a novel descriptive model, providing a comprehensive interpretation of the architectural TD phenomenon. This model categorizes the main characteristics of architectural TD and reveals their relations. The results show that, by using this model, different stakeholders could increase the system's success rate, and lower the rate of negative consequences, by raising awareness about architectural TD.},
journal = {J. Syst. Softw.},
month = jan,
pages = {1–16},
numpages = {16}
}

@article{10.1016/j.jss.2007.01.047,
author = {Liu, Jing and Dehlinger, Josh and Lutz, Robyn},
title = {Safety analysis of software product lines using state-based modeling},
year = {2007},
issue_date = {November, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.01.047},
doi = {10.1016/j.jss.2007.01.047},
abstract = {The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results.},
journal = {J. Syst. Softw.},
month = nov,
pages = {1879–1892},
numpages = {14},
keywords = {Model-based development, Product lines, Safety-critical systems, State-based modeling}
}

@article{10.1007/s10257-014-0251-6,
author = {Lee, Chien-Hsiang and Hwang, San-Yih and Yen, I-Ling and Yu, Tao-Kang},
title = {A service pattern model for service composition with flexible functionality},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2},
issn = {1617-9846},
url = {https://doi.org/10.1007/s10257-014-0251-6},
doi = {10.1007/s10257-014-0251-6},
abstract = {A key feature with service-oriented-architecture is to allow flexible composition of services into a business process. Although previous works related to service composition have paved the way for automatic composition, the techniques have limited applicability when it comes to composing complex workflows based on functional requirements, partly due to the large search space of the available services. In this paper, we propose a novel concept, the prospect service. Unlike existing abstract services which possess fixed service interfaces, a prospect service has a flexible interface to allow functional flexibility. Furthermore, we define a meta-model to specify service patterns with prospect services and adaptable workflow constructs to model flexible and adaptable process templates. An automated instantiation method is introduced to instantiate concrete processes with different functionalities from a service pattern. Since the search space for automatically instantiating a process from a service pattern is greatly reduced compared to that for automatically composing a process from scratch, the proposed approach significantly improve the feasibility of automated composition. Empirical study of the service pattern shows that the use of the proposed model significantly outperforms manual composition in terms of composition time and accuracy, and simulation results demonstrate that the proposed automated instantiation method is efficient.},
journal = {Inf. Syst. E-Bus. Manag.},
month = may,
pages = {235–265},
numpages = {31},
keywords = {Meta-model, Service pattern, Variability modeling, Web service composition}
}

@article{10.1007/s10270-020-00823-4,
author = {Kretschmer, Roland and Khelladi, Djamel Eddine and Lopez-Herrejon, Roberto Erick and Egyed, Alexander},
title = {Consistent change propagation within models},
year = {2021},
issue_date = {Apr 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00823-4},
doi = {10.1007/s10270-020-00823-4},
abstract = {Developers change models with clear intentions—e.g., for refactoring, defects removal, or evolution. However, in doing so, developers are often unaware of the consequences of their changes. Changes to one part of a model may affect other parts of the same model and/or even other models, possibly created and maintained by other developers. The consequences are incomplete changes and with it inconsistencies within or across models. Extensive works exist on detecting and repairing inconsistencies. However, the literature tends to focus on inconsistencies as errors in need of repairs rather than on incomplete changes in need of further propagation. Many changes are non-trivial and require a series of coordinated model changes. As developers start changing the model, intermittent inconsistencies arise with other parts of the model that developers have not yet changed. These inconsistencies are cues for incomplete change propagation. Resolving these inconsistencies should be done in a manner that is consistent with the original changes. We speak of consistent change propagation. This paper leverages classical inconsistency repair mechanisms to explore the vast search space of change propagation. Our approach not only suggests changes to repair a given inconsistency but also changes to repair inconsistencies caused by the aforementioned repair. In doing so, our approach follows the developer’s intent where subsequent changes may not contradict or backtrack earlier changes. We argue that consistent change propagation is essential for effective model-driven engineering. Our approach and its tool implementation were empirically assessed on 18 case studies from industry, academia, and GitHub to demonstrate its feasibility and scalability. A comparison with two versioned models shows that our approach identifies actual repair sequences that developers had chosen. Furthermore, an experiment involving 22 participants shows that our change propagation approach meets the workflow of how developers handle changes by always computing the sequence of repairs resulting from the change propagation.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {539–555},
numpages = {17},
keywords = {Model-driven engineering, Inconsistency repair, Change propagation, Consistency detection}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00014,
author = {Idowu, Samuel and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Asset management in machine learning: a survey},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00014},
doi = {10.1109/ICSE-SEIP52600.2021.00014},
abstract = {Machine Learning (ML) techniques are becoming essential components of many software systems today, causing an increasing need to adapt traditional software engineering practices and tools to the development of ML-based software systems. This need is especially pronounced due to the challenges associated with the large-scale development and deployment of ML systems. Among the most commonly reported challenges during the development, production, and operation of ML-based systems are experiment management, dependency management, monitoring, and logging of ML assets. In recent years, we have seen several efforts to address these challenges as witnessed by an increasing number of tools for tracking and managing ML experiments and their assets. To facilitate research and practice on engineering intelligent systems, it is essential to understand the nature of the current tool support for managing ML assets. What kind of support is provided? What asset types are tracked? What operations are offered to users for managing those assets? We discuss and position ML asset management as an important discipline that provides methods and tools for ML assets as structures and the ML development activities as their operations. We present a feature-based survey of 17 tools with ML asset management support identified in a systematic search. We overview these tools' features for managing the different types of assets used for engineering ML-based systems and performing experiments. We found that most of the asset management support depends on traditional version control systems, while only a few tools support an asset granularity level that differentiates between important ML assets, such as datasets and models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {51–60},
numpages = {10},
keywords = {SE4AI, asset management, machine learning},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1016/j.sysarc.2018.06.002,
author = {Chen, Bo and Li, Xi and Zhou, Xuehai},
title = {Model checking of MARTE/CCSL time behaviors using timed I/O automata},
year = {2018},
issue_date = {Aug 2018},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {88},
number = {C},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2018.06.002},
doi = {10.1016/j.sysarc.2018.06.002},
journal = {J. Syst. Archit.},
month = aug,
pages = {120–125},
numpages = {6},
keywords = {UML/MARTE/CCSL, Timed IO automata, Timing behaviors}
}

@inproceedings{10.1145/2797433.2797445,
author = {Portocarrero, Jes\'{u}s M. T. and Delicato, Fl\'{a}via C. and Pires, Paulo F. and Nakagawa, Elisa Y. and Oquendo, Flavio},
title = {Self-Adaptive Middleware for Wireless Sensor Networks: A Reference Architecture},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797445},
doi = {10.1145/2797433.2797445},
abstract = {Wireless Sensor Networks (WSN) are networks composed by tiny devices equipped with sensing, processing, storage, and wireless communication capabilities. WSN are used in highly dynamic environments. Applications for WSN should have an autonomous behavior to adapt their operation and achieve the best network performance. Such adaptation should preferably be performed by a middleware layer tailored to the limited resources of WSN. In this paper, we introduce a Reference Architecture (RA) of a self-adaptive middleware for WSN to contribute for the development of solutions enabling autonomic behavior in WSN. Our RA follows an autonomic computing model (MAPE-K) proposed by IBM and it was specified using a formal description language (pi-ADL) that enables the specification of dynamic architectures. ProSA-RA was used to systematize the design, representation and evaluation of our RA.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {12},
numpages = {8},
keywords = {Autonomic computing, pi-ADL, reference architecture, self-adaptive system, wireless sensor network},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

@article{10.1016/j.jss.2021.111037,
author = {Echeverr\'{\i}a, Jorge and Font, Jaime and P\'{e}rez, Francisca and Cetina, Carlos},
title = {Comparison of search strategies for feature location in software models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {181},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111037},
doi = {10.1016/j.jss.2021.111037},
journal = {J. Syst. Softw.},
month = nov,
numpages = {21},
keywords = {Feature location in models, Search strategies}
}

@inproceedings{10.1145/3319008.3319015,
author = {Fu, Changlan and Zhang, He and Huang, Xin and Zhou, Xin and Li, Zhi},
title = {A Review of Meta-ethnographies in Software Engineering},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319015},
doi = {10.1145/3319008.3319015},
abstract = {Context: Data synthesis is one of the most significant tasks in Systematic Literature Review (SLR). Software Engineering (SE) researchers have adopted a variety of methods of synthesizing data that originated in other disciplines. One of the qualitative data synthesis methods is meta-ethnography, which is being used in SE SLRs. Objective: We aim at studying the adoption of meta-ethnography in SE SLRs in order to understand how this method has been used in SE. Method: We conducted a tertiary study of the use of meta-ethnography by reviewing sixteen SLRs. We carried out an empirical inquiry by integrating SLR and confirmatory email survey. Results: There is a general lack of knowledge, or even awareness, of different aspects of meta-ethnography and/or how to apply it. Conclusion: There is a need of investment in gaining in-depth knowledge and skills of correctly applying meta-ethnography in order to increase the quality and reliability of the findings generated from SE SLRs. Our study reveals that meta-ethnography is a suitable method to SE research. We discuss challenges and propose recommendations of adopting meta-ethnography in SE. Our effort also offers a preliminary checklist of the systematic considerations for doing meta-ethnography in SE and improving the quality of meta-ethnographic research in SE.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {68–77},
numpages = {10},
keywords = {meta-ethnography, qualitative research synthesis, systematic (literature) review},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1016/j.jss.2015.07.008,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul and Seyerlehner, Klaus and Wallner, Stefan and Zeisel, Helmut},
title = {ReMinds},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {112},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.07.008},
doi = {10.1016/j.jss.2015.07.008},
abstract = {The identification of challenges for monitoring system-of-systems architectures.The flexible and extensible REMINDS framework for developing domain-specific monitoring applications.An evaluation of the framework by applying it to concrete systems of an industrial system of systems including performance analyses of the framework. Many software-intensive systems today can be characterized as systems of systems (SoS) comprising complex, interrelated, and heterogeneous systems. The behavior of SoS often only emerges at runtime due to complex interactions between the involved systems and their environment. It is thus necessary to determine unexpected behavior by monitoring SoS at runtime, i.e., collecting and analyzing events and data at different layers and levels of granularity. Existing monitoring approaches are often limited to individual systems, particular architectural styles, or technologies. In this paper we thus derive challenges for monitoring SoS based on an industrial case. We present a flexible framework adaptable to different system architectures and technologies. We discuss its capabilities for instrumenting systems, collecting and persisting events and data, checking constraints on events and data, and visualizing the systems' behavior to users. We demonstrate the framework's flexibility by tailoring and applying it to an industrial SoS and assessing its performance and scalability. Our results show that the framework is flexible and scalable for monitoring an industrial SoS with realistic event loads.},
journal = {J. Syst. Softw.},
month = feb,
pages = {123–136},
numpages = {14},
keywords = {Framework, Runtime monitoring, System-of-systems architectures}
}

@article{10.1016/j.jss.2016.06.068,
author = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
title = {Cloud migration process-A survey, evaluation framework, and open challenges},
year = {2016},
issue_date = {October 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.06.068},
doi = {10.1016/j.jss.2016.06.068},
abstract = {The relevant approaches for migrating legacy applications to the cloud are surveyed.An extensive analysis of existing approaches on the basis of a set of important criteria/features.Important cloud migration activities, techniques, and concerns that need to be properly addressed in a typical cloud migration process are delineated.Existing open issues and future research opportunities on the cloud migration research area are discussed. Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
journal = {J. Syst. Softw.},
month = oct,
pages = {31–69},
numpages = {39},
keywords = {Cloud computing, Cloud migration, Evaluation framework, Legacy application, Migration methodology, Process model}
}

@article{10.1016/j.scico.2021.102694,
author = {Liebrenz, Timm and Herber, Paula and Glesner, Sabine},
title = {Service-oriented decomposition and verification of hybrid system models using feature models and contracts},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {211},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102694},
doi = {10.1016/j.scico.2021.102694},
journal = {Sci. Comput. Program.},
month = nov,
numpages = {25},
keywords = {Hybrid systems, Compositional verification, Theorem proving, Model-driven development}
}

@article{10.1504/IJMSO.2014.063133,
author = {Preschern, Christopher and Kajtazovic, Nermin and Kreiner, Christian},
title = {Efficient development and reuse of domain-specific languages for automation systems},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {3},
issn = {1744-2621},
url = {https://doi.org/10.1504/IJMSO.2014.063133},
doi = {10.1504/IJMSO.2014.063133},
abstract = {Domain-Specific Languages DSLs help to decrease system development costs by providing developers with an effective way to construct systems for a specific domain. However, for DSL construction, a developer has to invest some upfront investment. If this investment is smaller than the benefit in terms of more effective development for domain-specific systems, then the construction of a DSL pays off. In order to decrease the initial effort to construct DSLs for a specific domain the automation domain, we present an efficient DSL architecture which allows structured reuse within the automation domain. With this DSL architecture, it is easier to build an initial automation DSL and when building multiple automation DSLs, significant parts of the DSL can be reused across different automation domains. We present the DSL architecture and discuss its benefits and drawbacks. Furthermore, we present and evaluate three automation DSL case studies which apply the described architecture.},
journal = {Int. J. Metadata Semant. Ontologies},
month = jul,
pages = {215–226},
numpages = {12}
}

@inproceedings{10.1145/2430502.2430522,
author = {von Rhein, Alexander and Apel, Sven and K\"{a}stner, Christian and Th\"{u}m, Thomas and Schaefer, Ina},
title = {The PLA model: on the combination of product-line analyses},
year = {2013},
isbn = {9781450315418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2430502.2430522},
doi = {10.1145/2430502.2430522},
abstract = {Product-line analysis has received considerable attention in the last decade. As it is often infeasible to analyze each product of a product line individually, researchers have developed analyses, called variability-aware analyses, that consider and exploit variability manifested in a code base. Variability-aware analyses are often significantly more efficient than traditional analyses, but each of them has certain weaknesses regarding applicability or scalability. We present the Product-Line-Analysis model, a formal model for the classification and comparison of existing analyses, including traditional and variability-aware analyses, and lay a foundation for formulating and exploring further, combined analyses. As a proof of concept, we discuss different examples of analyses in the light of our model, and demonstrate its benefits for systematic comparison and exploration of product-line analyses.},
booktitle = {Proceedings of the 7th International Workshop on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {8},
keywords = {PLA model, product-line analysis, software product lines},
location = {Pisa, Italy},
series = {VaMoS '13}
}

@article{10.1016/j.jss.2019.110428,
author = {Sobhy, Dalia and Minku, Leandro and Bahsoon, Rami and Chen, Tao and Kazman, Rick},
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110428},
doi = {10.1016/j.jss.2019.110428},
journal = {J. Syst. Softw.},
month = jan,
numpages = {28},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity}
}

@inproceedings{10.1109/WI-IAT.2009.363,
author = {Li, Mu and Huai, JinPeng and Guo, HuiPeng},
title = {An Adaptive Web Services Selection Method Based on the QoS Prediction Mechanism},
year = {2009},
isbn = {9780769538013},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2009.363},
doi = {10.1109/WI-IAT.2009.363},
abstract = {In recent years, many QoS-based web service selection methods have been proposed. However, as QoS changes dynamically, the atomic services of a composite web service could be replaced with other ones that have better quality. The performance of a composite web service will be decreased if this replacement happens frequently in runtime. Predicting the change of QoS accurately in select phase can effectively reduce this web services “thrash”. In this paper, we propose a web service selection algorithm GFS (Goodness-Fit Selection algorithm) based on QoS prediction mechanism in dynamic environments. We use structural equation to model the QoS measurement of web services. By taking the advantage of the prediction mechanism of structural equation model, we can quantitatively predict the change of quality of service dynamically. Optimal web service is selected based on the predicted results. Simulation results show that in dynamic environments, GFS provides higher selection accuracy than previous selection methods.},
booktitle = {Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {395–402},
numpages = {8},
keywords = {QoS, Structural Equation Modeling, prediction, web service selection},
series = {WI-IAT '09}
}

@inproceedings{10.5555/2337223.2337243,
author = {Siegmund, Norbert and Kolesnikov, Sergiy S. and K\"{a}stner, Christian and Apel, Sven and Batory, Don and Rosenm\"{u}ller, Marko and Saake, Gunter},
title = {Predicting performance via automated feature-interaction detection},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Customizable programs and program families provide user-selectable features to allow users to tailor a program to an application scenario. Knowing in advance which feature selection yields the best performance is difficult because a direct measurement of all possible feature combinations is infeasible. Our work aims at predicting program performance based on selected features. However, when features interact, accurate predictions are challenging. An interaction occurs when a particular feature combination has an unexpected influence on performance. We present a method that automatically detects performance-relevant feature interactions to improve prediction accuracy. To this end, we propose three heuristics to reduce the number of measurements required to detect interactions. Our evaluation consists of six real-world case studies from varying domains (e.g., databases, encoding libraries, and web servers) using different configuration techniques (e.g., configuration files and preprocessor flags). Results show an average prediction accuracy of 95%.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {167–177},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/3412841.3442106,
author = {Liu, Tianen and Khuri, Natalia},
title = {Classification of drug prescribing information using long short-term memory networks},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442106},
doi = {10.1145/3412841.3442106},
abstract = {Information about drug's safety and efficacy is publicly available to healthcare providers and consumers in the United States. Yet, it remains challenging to find this information for special populations of patients. These populations include pregnant, lactating, nursing women, elderly, and pediatric patients. Motivated by the unmet need for the accurate and efficient extraction of information, we trained a multi-class Long Short-Term Memory classifier with over 90,000 semi-structured labeled texts. The classifier achieved excellent performance when tested on an unseen dataset of 20,000 texts, reaching between 95% to 99% accuracy for the five classes. The classifier significantly outperformed the baseline model trained using Na\"{\i}ve Bayes algorithm, especially in the classification of texts containing information relevant to nursing mothers.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1086–1089},
numpages = {4},
keywords = {biomedical text classification, drug labels, long short-term memory network, na\"{\i}ve bayes classifier},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1007/978-3-030-27455-9_4,
author = {Colanzi, Thelma Elita and Assun\c{c}\~{a}o, Wesley Klewerton Guez and Farah, Paulo Roberto and Vergilio, Silvia Regina and Guizzo, Giovani},
title = {A Review of Ten Years of the Symposium on Search-Based Software Engineering},
year = {2019},
isbn = {978-3-030-27454-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27455-9_4},
doi = {10.1007/978-3-030-27455-9_4},
abstract = {The year 2018 marked the tenth anniversary of the Symposium on Search Based Software Engineering (SSBSE). In order to better understand the characteristics and evolution of papers published in SSBSE, this work reports results from a mapping study targeting the ten proceedings of SSBSE. Our goal is to identify and to analyze authorship collaborations, the impact and relevance of SSBSE in terms of citations, the software engineering areas commonly studied as well as the new problems recently solved, the computational intelligence techniques preferred by authors and the rigour of experiments conducted in the papers. Besides this analysis, we list some recommendations to new authors who envisage to publish their work in SSBSE. Despite of existing mapping studies on SBSE, our contribution in this work is to provide information to researchers and practitioners willing to enter the SBSE field, being a source of information to strengthen the symposium, guide new studies, and motivate new collaboration among research groups.},
booktitle = {Search-Based Software Engineering: 11th International Symposium, SSBSE 2019, Tallinn, Estonia, August 31 – September 1, 2019, Proceedings},
pages = {42–57},
numpages = {16},
keywords = {Systematic mapping, SBSE, Bibliometric analysis},
location = {Tallinn, Estonia}
}

@inproceedings{10.1145/1842752.1842810,
author = {Hilliard, Rich},
title = {On representing variation},
year = {2010},
isbn = {9781450301794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1842752.1842810},
doi = {10.1145/1842752.1842810},
abstract = {Although primarily studied in the context of product lines, variability is a key fact about most systems and therefore a concern for the architectures of those systems. Thus it is essential for the Architect to have suitable tools for representing, managing and reasoning about variation. Most work on product line variation has focused on the variability of components and their connectors within an architecture. Meanwhile, Architects today often use multiple viewpoints to frame diverse stakeholders' concerns for an architecture. How can variation be expressed within the representational paradigm of multiple viewpoints? This paper uses a simplified model of variation reflecting current practice and explores the consequences of that model for the representation of variation as a part of architecture description, using the conceptual foundation of ISO/IEC 42010 (the revision of IEEE 1471:2000) and poses a number of questions for discussion at the VARI-ARCH workshop.},
booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
pages = {312–315},
numpages = {4},
keywords = {architecture description, concerns, features, variation},
location = {Copenhagen, Denmark},
series = {ECSA '10}
}

@inproceedings{10.5555/2819009.2819038,
author = {Schroeder, Jan and Holzner, Daniela and Berger, Christian and Hoel, Carl-Johan and Laine, Leo and Magnusson, Anders},
title = {Design and evaluation of a customizable multi-domain reference architecture on top of product lines of self-driving heavy vehicles: an industrial case study},
year = {2015},
publisher = {IEEE Press},
abstract = {Self-driving vehicles for commercial use cases like logistics or overcast mines increase their owners' economic competitiveness. Volvo maintains, evolves, and distributes a vehicle control product line for different brands like Volvo Trucks, Renault, and Mack in more than 190 markets world-wide. From the different application domains of their customers originates the need for a multi-domain reference architecture concerned with transport mission planning, execution, and tracking on top of the vehicle control product line. This industrial case study is the first of its kind reporting about the systematic process to design such a reference architecture involving all relevant external and internal stakeholders, development documents, low-level artifacts, and literature. Quantitative and qualitative metrics were applied to evaluate non-functional requirements on the reference architecture level before a concrete variant was evaluated using a Volvo FMX truck in an exemplary construction site setting.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {189–198},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.5555/2664360.2664386,
author = {Lobato, Luanna Lopes and Silveira Neto, Paulo Anselmo da Mota and Machado, Ivan do Carmo and de Alemida, Eduardo Santana and Meira, Silvio Romero de Lemos},
title = {Risk management in software product lines: an industrial case study},
year = {2012},
isbn = {9781467323529},
publisher = {IEEE Press},
abstract = {Software Product Lines (SPL) adoption can affect several aspects of an organization and it involves significant investment and risk. This way, SPL risk management is a crucial activity of SPL adoption. This study aims to identify SPL risks during the scoping and requirement disciplines to provide information to better understand risk management in SPL. In order to achieve the previous stated goal, a case study research was applied in an industrial project in the medical information management domain. Using the captured risks, a classification scheme was built and risk mitigation strategies were identified. We spent five months, totaling 79 hours, performing risk management (RM) in the scoping discipline and twelve months, totaling 148 hours, performing RM on the requirements discipline. We identified 32 risks during the scoping discipline and 20 risks during the requirements discipline, 14 risks occurred in both disciplines. Some identified risks are not particular to SPL development, however, they have their impact increased due to the SPL characteristic. All the study results and lessons learned are useful for all project managers and researchers who are considering the introduction of SPL risk management in industry or academia.},
booktitle = {Proceedings of the International Conference on Software and System Process},
pages = {180–189},
numpages = {10},
keywords = {case study, risk management, software engineering, software product lines},
location = {Zurich, Switzerland},
series = {ICSSP '12}
}

@article{10.1186/s13677-020-00195-6,
author = {Koo, Jahwan and Faseeh Qureshi, Nawab Muhammad and Siddiqui, Isma Farah and Abbas, Asad and Bashir, Ali Kashif},
title = {IoT-enabled directed acyclic graph in spark cluster},
year = {2020},
issue_date = {Dec 2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {9},
number = {1},
issn = {2192-113X},
url = {https://doi.org/10.1186/s13677-020-00195-6},
doi = {10.1186/s13677-020-00195-6},
abstract = {Real-time data streaming fetches live sensory segments of the dataset in the heterogeneous distributed computing environment. This process assembles data chunks at a rapid encapsulation rate through a streaming technique that bundles sensor segments into multiple micro-batches and extracts into a repository, respectively. Recently, the acquisition process is enhanced with an additional feature of exchanging IoT devices’ dataset comprised of two components: (i) sensory data and (ii) metadata. The body of sensory data includes record information, and the metadata part consists of logs, heterogeneous events, and routing path tables to transmit micro-batch streams into the repository. Real-time acquisition procedure uses the Directed Acyclic Graph (DAG) to extract live query outcomes from in-place micro-batches through MapReduce stages and returns a result set. However, few bottlenecks affect the performance during the execution process, such as (i) homogeneous micro-batches formation only, (ii) complexity of dataset diversification, (iii) heterogeneous data tuples processing, and (iv) linear DAG workflow only. As a result, it produces huge processing latency and the additional cost of extracting event-enabled IoT datasets. Thus, the Spark cluster that processes Resilient Distributed Dataset (RDD) in a fast-pace using Random access memory (RAM) defies expected robustness in processing IoT streams in the distributed computing environment. This paper presents an IoT-enabled Directed Acyclic Graph (I-DAG) technique that labels micro-batches at the stage of building a stream event and arranges stream elements with event labels. In the next step, heterogeneous stream events are processed through the I-DAG workflow, which has non-linear DAG operation for extracting queries’ results in a Spark cluster. The performance evaluation shows that I-DAG resolves homogeneous IoT-enabled stream event issues and provides an effective stream event heterogeneous solution for IoT-enabled datasets in spark clusters.},
journal = {J. Cloud Comput.},
month = sep,
numpages = {15},
keywords = {Apache spark, Internet of Things (IoT), Directed acyclic graph, MapReduce, Micro-batch stream}
}

@inproceedings{10.5555/2041790.2041818,
author = {Nakagawa, Elisa Yumi and Antonino, Pablo Oliveira and Becker, Martin},
title = {Reference architecture and product line architecture: a subtle but critical difference},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Currently, the size and complexity of software systems, as well as critical time to market, demand new approaches from Software Engineering discipline for building such systems. In this context, the use of reference architectures and product line architectures is becoming a common practice. However, both of these concepts are sometimes mistakenly seen as the same thing; it is also not clearly established how they can be explored in a complementary way in order to contribute to software development. The main contribution of this paper is to make a clear differentiation between these architectures, by investigating and establishing definitions for each of them. Based on this, we also propose the use of reference architectures as a basis for product line architectures. As a result, a better understanding of both reference architectures and product line architectures, as well as an understanding of how to explore them jointly, can contribute to promoting more effective reuse in the development of software systems.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {207–211},
numpages = {5},
location = {Essen, Germany},
series = {ECSA'11}
}

@article{10.1007/s00607-013-0338-9,
author = {Bertolino, Antonia and Inverardi, Paola and Muccini, Henry},
title = {Software architecture-based analysis and testing: a look into achievements and future challenges},
year = {2013},
issue_date = {August    2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {95},
number = {8},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-013-0338-9},
doi = {10.1007/s00607-013-0338-9},
journal = {Computing},
month = aug,
pages = {633–648},
numpages = {16}
}

@inproceedings{10.1109/ASE.2013.6693104,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Scalable product line configuration: a straw to break the camel's back},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693104},
doi = {10.1109/ASE.2013.6693104},
abstract = {Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a "seed" in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {465–474},
numpages = {10},
keywords = {SMT solvers, automated configuration, evolutionary algorithms, multiobjective optimization, variability models},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@inproceedings{10.1007/978-3-319-26844-6_32,
author = {Brink, Christopher and Heisig, Philipp and Sachweh, Sabine},
title = {Using Cross-Dependencies During Configuration of System Families},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_32},
doi = {10.1007/978-3-319-26844-6_32},
abstract = {Nowadays, the automotive industry uses software product lines to support the management and maintenance of software variants. However, the development of mechatronic systems includes not merely software, but also other system parts like operating system, hardware or even mechanical parts. We call a combination of these system parts a system family SF. This combination raises the question how different variable system parts can be modeled and used for a combined configuration in a flexible way. We argue that a modeling process should combine all of these system parts, while the product configuration has to consider dependencies between them. Based on our previous work, we address this question and discuss dependencies between different system parts.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {439–452},
numpages = {14},
keywords = {Dependencies, Feature models, Hardware/software, Product lines, System families, Systems},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.5555/2666064.2666067,
author = {Annosi, Maria Carmela and Di Penta, Massimiliano and Tortora, Genny},
title = {Managing and assessing the risk of component upgrades},
year = {2012},
isbn = {9781467317511},
publisher = {IEEE Press},
abstract = {This paper describes the experience, carried out by Ericsson Telecomunicazioni S.p.A (Italy), in managing the migration of their legacy products towards a product line approach and, specifically, how the update of third-party software products is handled in such product lines. The paper describes the Ericsson application scenario in the development and evolution of network management products. Then, it provides an overview of how the company adopts (i) an internal toolkit to manage third party software products, with the aim of determining the impact of their updates upon variants of the network management system, and (ii) a risk management framework, which helps the developer to decide whether and when update third-party products.},
booktitle = {Proceedings of the Third International Workshop on Product LinE Approaches in Software Engineering},
pages = {9–12},
numpages = {4},
keywords = {code provenance, software product lines, software reuse, third party components},
location = {Zurich, Switzerland},
series = {PLEASE '12}
}

@article{10.1016/j.scico.2012.06.008,
author = {Eklund, Ulrik and Gustavsson, H\r{a}kan},
title = {Architecting automotive product lines: Industrial practice},
year = {2013},
issue_date = {December, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {12},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.06.008},
doi = {10.1016/j.scico.2012.06.008},
abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures at two internationally well-known automotive companies. The case study shows several interesting results. The process of managing architectural changes as well as the information the architects maintain and update is surprisingly similar between the two companies, despite that one has a strong line organisation and the other a strong project organisation. The architecting process found does not differ from what can be seen in other business domains. What does differ is that the architects studied see themselves interacting much more with other stakeholders than architects in general. The actual architectures are based on similar technology, e.g. CAN, but the network topology, S/W deployment and interfaces are totally different. The results indicate how the company's different core values influence the architects when defining and maintaining the architectures over time. One company maintains four similar architectures in parallel, each at a different stage in their respective life-cycle, while the other has a single architecture for all products since 2002. The organisational belonging of the architects in the former company has been turbulent in contrast to the latter and there is some speculation if this is correlated.},
journal = {Sci. Comput. Program.},
month = dec,
pages = {2347–2359},
numpages = {13},
keywords = {Architecting, Automotive industry, Case study, Process}
}

@inproceedings{10.1007/978-3-642-37057-1_7,
author = {Rubin, Julia and Chechik, Marsha},
title = {Quality of merge-refactorings for product lines},
year = {2013},
isbn = {9783642370564},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-37057-1_7},
doi = {10.1007/978-3-642-37057-1_7},
abstract = {In this paper, we consider the problem of refactoring related software products specified in UML into annotative product line representations. Our approach relies on identifying commonalities and variabilities in existing products and further merging those into product line representations which reduce duplications and facilitate reuse. Varying merge strategies can lead to producing several semantically correct, yet syntactically different refactoring results. Depending on the goal of the refactoring, one result can be preferred to another. We thus propose to capture the goal using a syntactic quality function and use that function to guide the merge strategy. We define and implement a quality-based merge-refactoring framework for UML models containing class and statechart diagrams and report on our experience applying it on three case-studies.},
booktitle = {Proceedings of the 16th International Conference on Fundamental Approaches to Software Engineering},
pages = {83–98},
numpages = {16},
location = {Rome, Italy},
series = {FASE'13}
}

@inproceedings{10.1145/2245276.2231961,
author = {de Oliveira, Andr\'{e} Luiz and Ferrari, Fabiano Cuttigi and Penteado, Ros\^{a}ngela A. Dellosso and de Camargo, Valter Vieira},
title = {Investigating framework product lines},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231961},
doi = {10.1145/2245276.2231961},
abstract = {Frameworks are tools that promote the reuse of pieces of software within specific domains. An intrinsic property of frameworks is the large amount of intertwined code found across its several modules. This configures an architecture whose modules can hardly be decoupled. Consequently, an application derived from a framework usually carries on the full framework architecture, irrespective of the subset of application requirements. This compromises the maintainability, evolution and reusability of both framework and applications derived from it. To deal with this problem, this paper introduces the concept of Framework Product Lines (FPL). In a FPL, each member - or configuration -- is a framework that contains only a subset of the FPL features according to the application requirements and rules that constrain their composition. Thus, this paper presents the framework product lines concept and shows its use for evolving an application framework towards FPL. Results show preliminary gains in terms of reusability and maintainability in both evolved framework and applications derived from it.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1177–1182},
numpages = {6},
keywords = {framework, framework product lines, maintainability, reusability, software architectures, software product lines},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2851613.2851941,
author = {Vidal, Santiago and Guimaraes, Everton and Oizumi, Willian and Garcia, Alessandro and Pace, Andr\'{e}s D\'{\i}az and Marcos, Claudia},
title = {On the criteria for prioritizing code anomalies to identify architectural problems},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851941},
doi = {10.1145/2851613.2851941},
abstract = {Architectural problems constantly affect evolving software projects. When not properly addressed, those problems can hinder the longevity of a software system. Some studies revealed that a wide range of architectural problems are reflected in source code through code anomalies. However, a software project often contains thousands of code anomalies and many of them have no relation to architectural problems. As a consequence, developers struggle to effectively determine which (groups of) anomalies are architecturally relevant. This work proposes criteria for prioritizing groups of code anomalies as indicators of architectural problems in evolving systems.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1812–1814},
numpages = {3},
keywords = {architectural problems, code anomalies, software maintenance},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1007/978-3-642-30982-3_8,
author = {Becker, Steffen},
title = {Model transformations in non-functional analysis},
year = {2012},
isbn = {9783642309816},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30982-3_8},
doi = {10.1007/978-3-642-30982-3_8},
abstract = {The quality assessment of software design models in early development phases can prevent wrong design decisions on the architectural level. As such wrong decisions are usually very cost-intensive to revert in late testing phases, model-driven quality predictions offer early quality estimates to prevent such erroneous decisions. By model-driven quality predictions we refer to analyses which run fully automated based on model-driven methods and tools. In this paper, we give an overview on the process of model-driven quality analyses used today with a special focus on issues that arise in fully automated approaches.},
booktitle = {Proceedings of the 12th International Conference on Formal Methods for the Design of Computer, Communication, and Software Systems: Formal Methods for Model-Driven Engineering},
pages = {263–289},
numpages = {27},
keywords = {MARTE, model-driven quality analyses, palladio component model, performance, reliability},
location = {Bertinoro, Italy},
series = {SFM'12}
}

@article{10.1016/j.jss.2006.08.039,
author = {Kuz, Ihor and Liu, Yan and Gorton, Ian and Heiser, Gernot},
title = {CAmkES: A component model for secure microkernel-based embedded systems},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.08.039},
doi = {10.1016/j.jss.2006.08.039},
abstract = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
journal = {J. Syst. Softw.},
month = may,
pages = {687–699},
numpages = {13},
keywords = {Component architecture, Embedded system, Microkernel}
}

@inproceedings{10.5555/1926743.1926758,
author = {Ovaska, Eila},
title = {Ontology driven piecemeal development of smart spaces},
year = {2010},
isbn = {3642169163},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software development is facing new challenges due to transformation from product based software engineering towards integration and collaboration based software engineering that embodies high degree of dynamism both at design time and run time. Short time-to-markets require cost reduction by maximizing software reuse; openness for new innovations presumes a flexible innovation platform and agile software development; and user satisfaction assumes high quality in a situation based manner. How to deal with these contradictory requirements in software engineering? The main contribution of this paper is a novel approach that is influenced by business innovation, human centered design, model driven development and ontology oriented design. The approach is called Ontology driven Piecemeal Software Engineering (OPSE). OPSE facilitates incremental software development based on software pieces that follow the design principles defined by means of ontologies. Its key elements are abstraction, aggregation and adaptivity. The approach is intended for and applied to the development of smart spaces.},
booktitle = {Proceedings of the First International Joint Conference on Ambient Intelligence},
pages = {148–156},
numpages = {9},
keywords = {MDD, context awareness, interoperability, ontology, smart space},
location = {Malaga, Spain},
series = {AmI'10}
}

@article{10.1007/s10270-018-0662-9,
author = {Kolesnikov, Sergiy and Siegmund, Norbert and K\"{a}stner, Christian and Grebhahn, Alexander and Apel, Sven},
title = {Tradeoffs in modeling performance of highly configurable software systems},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-018-0662-9},
doi = {10.1007/s10270-018-0662-9},
abstract = {Modeling the performance of a highly configurable software system requires capturing the influences of its configuration options and their interactions on the system's performance. Performance-influence models quantify these influences, explaining this way the performance behavior of a configurable system as a whole. To be useful in practice, a performance-influence model should have a low prediction error, small model size, and reasonable computation time. Because of the inherent tradeoffs among these properties, optimizing for one property may negatively influence the others. It is unclear, though, to what extent these tradeoffs manifest themselves in practice, that is, whether a large configuration space can be described accurately only with large models and significant resource investment. By means of 10 real-world highly configurable systems from different domains, we have systematically studied the tradeoffs between the three properties. Surprisingly, we found that the tradeoffs between prediction error and model size and between prediction error and computation time are rather marginal. That is, we can learn accurate and small models in reasonable time, so that one performance-influence model can fit different use cases, such as program comprehension and performance prediction. We further investigated the reasons for why the tradeoffs are marginal. We found that interactions among four or more configuration options have only a minor influence on the prediction error and that ignoring them when learning a performance-influence model can save a substantial amount of computation time, while keeping the model small without considerably increasing the prediction error. This is an important insight for new sampling and learning techniques as they can focus on specific regions of the configuration space and find a sweet spot between accuracy and effort. We further analyzed the causes for the configuration options and their interactions having the observed influences on the systems' performance. We were able to identify several patterns across subject systems, such as dominant configuration options and data pipelines, that explain the influences of highly influential configuration options and interactions, and give further insights into the domain of highly configurable systems.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {2265–2283},
numpages = {19},
keywords = {Feature interactions, Highly configurable software systems, Machine learning, Performance prediction, Performance-influence models, Software product lines, Variability}
}

@article{10.1016/j.infsof.2011.09.003,
author = {Conejero, Jos\'{e} M. and Figueiredo, Eduardo and Garcia, Alessandro and Hern\'{a}ndez, Juan and Jurado, Elena},
title = {On the relationship of concern metrics and requirements maintainability},
year = {2012},
issue_date = {February, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.003},
doi = {10.1016/j.infsof.2011.09.003},
abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {212–238},
numpages = {27},
keywords = {Concern metrics, Crosscutting, Maintainability, Product lines, Requirements engineering, Stability}
}

@inproceedings{10.1007/978-3-642-54804-8_7,
author = {Kowal, Matthias and Schaefer, Ina and Tribastone, Mirco},
title = {Family-Based Performance Analysis of Variant-Rich Software Systems},
year = {2014},
isbn = {9783642548031},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-54804-8_7},
doi = {10.1007/978-3-642-54804-8_7},
abstract = {We study models of software systems with variants that stem from a specific choice of configuration parameters with a direct impact on performance properties. Using UML activity diagrams with quantitative annotations, we model such systems as a product line. The efficiency of a product-based evaluation is typically low because each product must be analyzed in isolation, making difficult the re-use of computations across variants. Here, we propose a family-based approach based on symbolic computation. A numerical assessment on large activity diagrams shows that this approach can be up to three orders of magnitude faster than product-based analysis in large models, thus enabling computationally efficient explorations of large parameter spaces.},
booktitle = {Proceedings of the 17th International Conference on Fundamental Approaches to Software Engineering - Volume 8411},
pages = {94–108},
numpages = {15}
}

@inproceedings{10.5555/2971808.2972065,
author = {Barke, Erich and F\"{u}rtig, Andreas and Gl\"{a}ser, Georg and Grimm, Christoph and Hedrich, Lars and Heinen, Stefan and Hennig, Eckhard and Lee, Hyun-Sek Lukas and Nebel, Wolfgang and Nitsche, Gregor and Olbrich, Markus and Radojicic, Carna and Speicher, Fabian},
title = {Embedded tutorial: analog-/mixed-signal verification methods for AMS coverage analysis},
year = {2016},
isbn = {9783981537062},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Analog-/Mixed-Signal (AMS) design verification is one of the most challenging and time consuming tasks of todays complex system on chip (SoC) designs. In contrast to digital system design, AMS designers have to deal with a continuous state space of conservative quantities, highly nonlinear relationships, non-functional influences, etc. enlarging the number of possibly critical scenarios to infinity. In this special session we demonstrate the verification of functional properties using simulative and formal methods. We combine different approaches including automated abstraction and refinement of mixed-level models, state-space discretization as well as affine arithmetic. To reach sufficient verification coverage with reasonable time and effort, we use enhanced simulation schemes to avoid conventional simulation drawbacks.},
booktitle = {Proceedings of the 2016 Conference on Design, Automation &amp; Test in Europe},
pages = {1102–1111},
numpages = {10},
location = {Dresden, Germany},
series = {DATE '16}
}

@inproceedings{10.1145/1134650.1134678,
author = {Pandey, Raju and Wu, Jeffrey},
title = {BOTS: a constraint-based component system for synthesizing scalable software systems},
year = {2006},
isbn = {159593362X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1134650.1134678},
doi = {10.1145/1134650.1134678},
abstract = {Embedded application developers create applications for a wide range of devices with different resource constraints. Developers want to maximize the use of the limited resources available on the device while still not exceeding the capabilities of the device. To do this, the developer must be able to scale his software for different platforms. In this paper, we present a software engineering methodology that automatically scales software to different platforms. We intend to have the application developer write high level functional specifications of his software and have tools that automatically scale the underlying runtime. These tools will use the functional and non-functional constraints of both the hardware and client application to produce an appropriate runtime. Our initial results show that the proposed approach can scale operating systems and virtual machines that satisfy the constraints of varying hardware/application combinations.},
booktitle = {Proceedings of the 2006 ACM SIGPLAN/SIGBED Conference on Language, Compilers, and Tool Support for Embedded Systems},
pages = {189–198},
numpages = {10},
keywords = {components, constraints, embedded systems, generative programming, runtime systems, wireless sensor networks},
location = {Ottawa, Ontario, Canada},
series = {LCTES '06}
}

@inproceedings{10.1145/3357141.3357147,
author = {Sousa, Bruno L. and Bigonha, Mariza A. S. and Ferreira, Kecia A. M.},
title = {Analysis of Coupling Evolution on Open Source Systems},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357147},
doi = {10.1145/3357141.3357147},
abstract = {Software evolution is an intrinsic process of the software life cycle. The comprehension of this process is a central research topic in Software Engineering. It is widely accepted that as a software system evolves, its internal quality declines, and its complexity increases. However, there is a gap in the comprehension of how this process occurs in a fine-grained view. In this work, we apply a software metric approach to investigate how the internal quality of object-oriented software systems evolves in the aspect of coupling. More specifically, we analyze (i) how the coupling behavior may be described over the software evolution, (ii) how the coupling behavior affects the reusability and complexity of the systems, and (iii) the percentage of classes from the systems that directly impacts on the coupling evolution. The results and observations of this study are compiled in seven properties of coupling evolution, among which stand out: (i) the coupling behavior is better modeled by a cubic function, (ii) the coupling evolution tends to increase the complexity of the systems, (iii) the systems tend to be designed with a high level of complexity, and (iv) the coupling evolution is affected by a small group of classes.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {23–32},
numpages = {10},
keywords = {coupling, object-orientation, open source, software evolution, software metrics, software quality},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/1868688.1868690,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Apel, Sven},
title = {Automating energy optimization with features},
year = {2010},
isbn = {9781450302081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868688.1868690},
doi = {10.1145/1868688.1868690},
abstract = {Mobile devices such as cell phones and notebooks rely on battery power supply. For these systems, optimizing the power consumption is important to increase the system's lifetime. However, this is hard to achieve because energy-saving functions often depend on the hardware, and operating systems. The diversity of hardware components and operating systems makes the implementation time consuming and difficult. We propose an approach to automate energy optimization of programs by implementing energy-saving functionality as modular, separate implementation units (e.g., feature modules or aspects). These units are bundled as energy features into an energy-optimization feature library. Based on aspect-oriented and feature-oriented programming, we discuss different techniques to compose the source code of a client program and the implementation units of the energy features.},
booktitle = {Proceedings of the 2nd International Workshop on Feature-Oriented Software Development},
pages = {2–9},
numpages = {8},
keywords = {energy consumption, feature-oriented programming, software product lines},
location = {Eindhoven, The Netherlands},
series = {FOSD '10}
}

@article{10.1145/1366546.1366547,
author = {G\'{e}rard, S\'{e}bastien and Feiler, Peter and Rolland, Jean-Francois and Filali, Mamoun and Reiser, Mark-Oliver and Delanote, Didier and Berbers, Yolande and Pautet, Laurent and Perseil, Isabelle},
title = {UML&amp;AADL '2007 grand challenges},
year = {2007},
issue_date = {October 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/1366546.1366547},
doi = {10.1145/1366546.1366547},
abstract = {On today's sharply competitive industrial market, engineers must focus on their core competencies to produce ever more innovative products, while also reducing development times and costs. This has further heightened the complexity of the development process. At the same time, industrial systems, and specifically real-time embedded systems, have become increasingly software-intensive. New software development approaches and methods must therefore be found to free engineers from the even more complex technical constraints of development and to enable them to concentrate on their core business specialties. One emerging solution is to foster model-based development by defining modeling artifacts well-suited to their domain concerns instead of asking them to write code. However, model-driven approaches will be solutions to the previous issues only if models evolves from a contemplative role to a productive role within the development processes. In this context, model transformation is a key design paradigm that will foster this revolution. This paper is the result of discussions and exchanges that took place within the second edition of the workshop "UML&amp;AADL" (http://www.artist-embedded.org/artist/Topics.html) that-was hold in 2007 in Auckland, New Zealand, in conjunction with the ICECCS07 conference. The purpose of this workshop was to gather people of both communities from UML (including its domain specific extensions, with a focus on MARTE) and AADL (including its annexes) in order to foster sharing of results and experiments. More specially this year, the focus was on how both standards do subscribe to the model driven engineering paradigm, or to be more precise, how MDE may ease and foster the usage of both sets of standards for developing real-time embedded systems. This paper will show that, even if the work is not yet finished, the current results seems to be already very promising.},
journal = {SIGBED Rev.},
month = oct,
articleno = {1},
numpages = {1},
keywords = {AADL, ADL, MARTE, MDA, MDD, MDE, TLA+, UML, embedded, real-time, xUML}
}

@inproceedings{10.1145/2993236.2993249,
author = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
title = {A feature-based personalized recommender system for product-line configuration},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993236.2993249},
doi = {10.1145/2993236.2993249},
abstract = {Today’s competitive marketplace requires the industry to understand unique and particular needs of their customers. Product line practices enable companies to create individual products for every customer by providing an interdependent set of features. Users configure personalized products by consecutively selecting desired features based on their individual needs. However, as most features are interdependent, users must understand the impact of their gradual selections in order to make valid decisions. Thus, especially when dealing with large feature models, specialized assistance is needed to guide the users in configuring their product. Recently, recommender systems have proved to be an appropriate mean to assist users in finding information and making decisions. In this paper, we propose an advanced feature recommender system that provides personalized recommendations to users. In detail, we offer four main contributions: (i) We provide a recommender system that suggests relevant features to ease the decision-making process. (ii) Based on this system, we provide visual support to users that guides them through the decision-making process and allows them to focus on valid and relevant parts of the configuration space. (iii) We provide an interactive open-source configurator tool encompassing all those features. (iv) In order to demonstrate the performance of our approach, we compare three different recommender algorithms in two real case studies derived from business experience.},
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {120–131},
numpages = {12},
keywords = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
location = {Amsterdam, Netherlands},
series = {GPCE 2016}
}

@inproceedings{10.5555/1885639.1885701,
author = {McGregor, John D.},
title = {The many paths to quality core assets},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {502},
numpages = {1},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.1145/3324884.3416620,
author = {Dorn, Johannes and Apel, Sven and Siegmund, Norbert},
title = {Mastering uncertainty in performance estimations of configurable software systems},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416620},
doi = {10.1145/3324884.3416620},
abstract = {Understanding the influence of configuration options on performance is key for finding optimal system configurations, system understanding, and performance debugging. In prior research, a number of performance-influence modeling approaches have been proposed, which model a configuration option's influence and a configuration's performance as a scalar value. However, these point estimates falsely imply a certainty regarding an option's influence that neglects several sources of uncertainty within the assessment process, such as (1) measurement bias, (2) model representation and learning process, and (3) incomplete data. This leads to the situation that different approaches and even different learning runs assign different scalar performance values to options and interactions among them. The true influence is uncertain, though. There is no way to quantify this uncertainty with state-of-the-art performance modeling approaches. We propose a novel approach, P4, based on probabilistic programming that explicitly models uncertainty for option influences and consequently provides a confidence interval for each prediction of a configuration's performance alongside a scalar. This way, we can explain, for the first time, why predictions may cause errors and which option's influences may be unreliable. An evaluation on 12 real-world subject systems shows that P4's accuracy is in line with the state of the art while providing reliable confidence intervals, in addition to scalar predictions.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {684–696},
numpages = {13},
keywords = {P4, configurable software systems, performance-influence modeling, probabilistic programming},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2884781.2884868,
author = {Oizumi, Willian and Garcia, Alessandro and da Silva Sousa, Leonardo and Cafeo, Bruno and Zhao, Yixue},
title = {Code anomalies flock together: exploring code anomaly agglomerations for locating design problems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884868},
doi = {10.1145/2884781.2884868},
abstract = {Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies -- popularly known as code smells -- may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to "flock together" to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {440–451},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2024436.2024438,
author = {Calinescu, Radu},
title = {When the requirements for adaptation and high integrity meet},
year = {2011},
isbn = {9781450308533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024436.2024438},
doi = {10.1145/2024436.2024438},
abstract = {Two classes of software that are notoriously difficult to develop on their own are rapidly merging into one. This will affect every key service that we rely upon in modern society, yet a successful merge is unlikely to be achievable using software development techniques specific to either class.This paper explains the growing demand for software capable of both self-adaptation and high integrity, and advocates the use of a collection of "@runtime" techniques for its development, operation and management. We summarise early research into the development of such techniques, and discuss the remaining work required to overcome the great challenge of self-adaptive high-integrity software.},
booktitle = {Proceedings of the 8th Workshop on Assurances for Self-Adaptive Systems},
pages = {1–4},
numpages = {4},
keywords = {high-integrity software, model checking, self-adaptive software},
location = {Szeged, Hungary},
series = {ASAS '11}
}

@inproceedings{10.1145/2660190.2660191,
author = {Kolesnikov, Sergiy and Roth, Judith and Apel, Sven},
title = {On the relation between internal and external feature interactions in feature-oriented product lines: a case study},
year = {2014},
isbn = {9781450329804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660190.2660191},
doi = {10.1145/2660190.2660191},
abstract = {The feature-interaction problem has been explored for many years. Still, we lack sufficient knowledge about the interplay of different kinds of interactions in software product lines. Exploring the relations between different kinds of feature interactions will allow us to learn more about the nature of interactions and their causes. This knowledge can then be applied for improving existing approaches for detecting, managing, and resolving feature interactions. We present a framework for studying relations between different kinds of interactions. Furthermore, we report and discuss the results of a preliminary study in which we examined correlations between internal feature interactions (quantified by a set of software measures) and external feature interactions (represented by product-line-specific type errors). We performed the evaluation on a set of 15 feature-oriented, Java-based product lines. We observed moderate correlations between the interactions under discussion. This gives us confidence that we can apply our approach to studying other types of external feature interactions (e.g., performance interactions).},
booktitle = {Proceedings of the 6th International Workshop on Feature-Oriented Software Development},
pages = {1–8},
numpages = {8},
keywords = {feature interactions, feature-oriented software development, software measures},
location = {V\"{a}ster\r{a}s, Sweden},
series = {FOSD '14}
}

@inproceedings{10.1145/3357141.3357142,
author = {Oliveira, Anderson and Sousa, Leonardo and Oizumi, Willian and Garcia, Alessandro},
title = {On the Prioritization of Design-Relevant Smelly Elements: A Mixed-Method, Multi-Project Study},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357142},
doi = {10.1145/3357141.3357142},
abstract = {Software systems are likely to face what is called design problems. Given the typical lack of design documentation, developers have to rely on implementation-level symptoms, the so-called code smells, to identify and remove design problems. A code smell is a microstructure in the program that can indicate the presence of a design problem. Large programs have hundreds or thousands of program elements (e.g., classes) in which a significant proportion may be affected by smells. Consequently, due to time constraints and the large number of elements, developers have to prioritize the designrelevant program elements, i.e., locate a shortlist of elements that are suspects of having design-relevant smells. However, this task is hard and time-consuming. Unfortunately, the literature fails to provide developers with effective heuristics that automate such prioritization task. The objective of this paper is to propose heuristics that effectively locate a shortlist of design-relevant smelly program elements. For this purpose, we report two studies. In the first one, we investigated the criteria that developers used in practice to accurately prioritize design-relevant smelly elements. Based on these criteria, we derived a preliminary suite of prioritization heuristics. Since we do not know if the heuristics are suitable for an effective prioritization across multiple projects, we performed a second study to evaluate the proposed heuristics. We found that two out of nine heuristics reached an average precision higher than 75% for the four projects we analyzed. Thus, our findings suggest these heuristics are promising to support developers in prioritizing design-relevant smelly elements.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {83–92},
numpages = {10},
keywords = {design problems, heuristics, prioritization},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1145/1852786.1852810,
author = {Falessi, Davide and Cantone, Giovanni and Canfora, Gerardo},
title = {A comprehensive characterization of NLP techniques for identifying equivalent requirements},
year = {2010},
isbn = {9781450300391},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1852786.1852810},
doi = {10.1145/1852786.1852810},
abstract = {Though very important in software engineering, linking artifacts of the same type (clone detection) or of different types (traceability recovery) is extremely tedious, error-prone and requires significant effort. Past research focused on supporting analysts with mechanisms based on Natural Language Processing (NLP) to identify candidate links. Because a plethora of NLP techniques exists, and their performances vary among contexts, it is important to characterize them according to the provided level of support. The aim of this paper is to characterize a comprehensive set of NLP techniques according to the provided level of support to human analysts in detecting equivalent requirements. The characterization consists on a case study, featuring real requirements, in the context of an Italian company in the defense and aerospace domain. The major result from the case study is that simple NLP are more precise than complex ones.},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {18},
numpages = {10},
keywords = {case study, natural language processing, requirements},
location = {Bolzano-Bozen, Italy},
series = {ESEM '10}
}

@inproceedings{10.5555/3386691.3386706,
author = {Lu, Sidi and Luo, Bing and Patel, Tirthak and Yao, Yongtao and Tiwari, Devesh and Shi, Weisong},
title = {Making disk failure predictions SMARTer!},
year = {2020},
isbn = {9781939133120},
publisher = {USENIX Association},
address = {USA},
abstract = {Disk drives are one of the most commonly replaced hardware components and continue to pose challenges for accurate failure prediction. In this work, we present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average.},
booktitle = {Proceedings of the 18th USENIX Conference on File and Storage Technologies},
pages = {151–168},
numpages = {18},
location = {Santa Clara, CA, USA},
series = {FAST'20}
}

@article{10.1016/j.eswa.2012.08.026,
author = {Ognjanovi\'{c}, Ivana and Ga\v{s}Evi\'{c}, Dragan and Bagheri, Ebrahim},
title = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
year = {2013},
issue_date = {March, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2012.08.026},
doi = {10.1016/j.eswa.2012.08.026},
abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1094–1115},
numpages = {22},
keywords = {AHP method, Comparative preferences, Conditional preferences, Lexicographic order, S-AHP method, Well-formed preferences}
}

@inproceedings{10.1007/978-3-030-30942-8_22,
author = {Lunel, Simon and Mitsch, Stefan and Boyer, Benoit and Talpin, Jean-Pierre},
title = {Parallel Composition and Modular Verification of Computer Controlled Systems in Differential Dynamic Logic},
year = {2019},
isbn = {978-3-030-30941-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-30942-8_22},
doi = {10.1007/978-3-030-30942-8_22},
abstract = {Computer-Controlled Systems (CCS) are a subclass of hybrid systems where the periodic relation of control components to time is paramount. Since they additionally are at the heart of many safety-critical devices, it is of primary importance to correctly model such systems and to ensure they function correctly according to safety requirements. Differential dynamic logic dL is a powerful logic to model hybrid systems and to prove their correctness. We contribute a component-based modeling and reasoning framework to dL that separates models into components with timing guarantees, such as reactivity of controllers and controllability of continuous dynamics. Components operate in parallel, with coarse-grained interleaving, periodic execution and communication. We present techniques to automate system safety proofs from isolated, modular, and possibly mechanized proofs of component properties parameterized with timing characteristics.},
booktitle = {Formal Methods – The Next 30 Years: Third World Congress, FM 2019, Porto, Portugal, October 7–11, 2019, Proceedings},
pages = {354–370},
numpages = {17},
location = {Porto, Portugal}
}

@inproceedings{10.1109/CCGrid.2014.25,
author = {Almeida, Andr\'{e} and Dantas, Francisco and Cavalcante, Everton and Batista, Thais},
title = {A branch-and-bound algorithm for autonomic adaptation of multi-cloud applications},
year = {2014},
isbn = {9781479927838},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGrid.2014.25},
doi = {10.1109/CCGrid.2014.25},
abstract = {Adaptation is an important concern in cloud-based applications composed of services provided by different cloud providers since cloud services can suffer from Quality of Services (QoS) fluctuations. Other conditions that can also trigger an adaptation process at runtime are the unavailability of services or the violation of user-defined policies. Moreover, the detection and reaction on such changes must be done in an autonomic way, without the need of user intervention. This paper presents a dynamic adaptation approach for multi-cloud applications supported by a Branch-and-Bound (B&amp;B) algorithm in order to optimize the adaptation process itself when selecting the services to be deployed within the application. Computational experiments comparing the B&amp;B algorithm with another algorithm that evaluates all possible configurations for adapting an application showed that the B&amp;B algorithm is faster than the previous version. This new algorithm brings benefits to the scalability of the adaptation process, which can deal with large configurations of multi-cloud applications composed by a plethora of cloud services.},
booktitle = {Proceedings of the 14th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
pages = {315–323},
numpages = {9},
keywords = {branch-and-bound algorithm, dynamic adaptation, multi-cloud applications, optimization, scalability},
location = {Chicago, Illinois},
series = {CCGRID '14}
}

@inproceedings{10.1109/ICSE43902.2021.00028,
author = {Gao, Yanjie and Zhu, Yonghao and Zhang, Hongyu and Lin, Haoxiang and Yang, Mao},
title = {Resource-Guided Configuration Space Reduction for Deep Learning Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00028},
doi = {10.1109/ICSE43902.2021.00028},
abstract = {Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity.In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {175–187},
numpages = {13},
keywords = {AutoML, configurable systems, constraint solving, deep learning},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1007/s11276-018-1718-z,
author = {Mukhlif, Fadhil and Noordin, Kamarul Ariffin Bin and Mansoor, Ali Mohammed and Kasirun, Zarinah Mohd},
title = {Green transmission for C-RAN based on SWIPT in 5G: a review},
year = {2019},
issue_date = {Jul 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {5},
issn = {1022-0038},
url = {https://doi.org/10.1007/s11276-018-1718-z},
doi = {10.1007/s11276-018-1718-z},
abstract = {C-RAN is a promising new design for the next generation, an important aspect of it in the energy efficiency consideration. Hence, it is considering an innovative candidate to use it as an alternative cellular network instead of the traditional. Investigation green transmission of mobile cloud radio access networks based on SWIPT for 5G cellular networks. Especially, with considering SWIPT as a future solution for increasing the lifetime of end-user battery’s, that’s mean this technique will improving energy efficiency (EE). Addressing SWIPT into C-RAN is a challenging and it is needed to developing a new algorithm to use it on the cellular network with many trying to ensure the success of the system performance. C-RAN as a network and SWIPT as a promising technique with the suggesting green wireless network are discussed besides the importance of energy efficiency for the next generation. Furthermore, there was a study on fifth enabling technologies that can be used for 5G with emphasis on two of them (C-RAN and energy efficiency). Lastly, research challenges and future direction that require substantial research efforts are summarized.},
journal = {Wirel. Netw.},
month = jul,
pages = {2621–2649},
numpages = {29},
keywords = {Green transmission, Power transfer, Cloud radio access network, Energy harvesting (EH), Information decoding (ID), Time switching, Power splitting, MIMO}
}

@article{10.1016/j.dss.2010.12.009,
author = {Ribeiro, Rita A. and Moreira, Ana M. and van den Broek, Pim and Pimentel, Afonso},
title = {Hybrid assessment method for software engineering decisions},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {51},
number = {1},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2010.12.009},
doi = {10.1016/j.dss.2010.12.009},
abstract = {During software development, many decisions need to be made to guarantee the satisfaction of the stakeholders' requirements and goals. The full satisfaction of all of these requirements and goals may not be possible, requiring decisions over conflicting human interests as well as technological alternatives, with an impact on the quality and cost of the final solution. This work aims at assessing the suitability of multi-criteria decision making (MCDM) methods to support software engineers' decisions. To fulfil this aim, a HAM (Hybrid Assessment Method) is proposed, which gives its user the ability to perceive the influence different decisions may have on the final result. HAM is a simple and efficient method that combines one single pairwise comparison decision matrix (to determine the weights of criteria) with one classical weighted decision matrix (to prioritize the alternatives). To avoid consistency problems regarding the scale and the prioritization method, HAM uses a geometric scale for assessing the criteria and the geometric mean for determining the alternative ratings.},
journal = {Decis. Support Syst.},
month = apr,
pages = {208–219},
numpages = {12},
keywords = {Aggregation operators, Multi-criteria decision making, Non-functional software requirements, Software engineering}
}

@article{10.1016/j.compeleceng.2017.08.004,
author = {Alfrez, Germn H. and Pelechano, Vicente},
title = {Achieving autonomic Web service compositions with models at runtime},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.08.004},
doi = {10.1016/j.compeleceng.2017.08.004},
abstract = {Several exceptional situations may arise in the complex, heterogeneous, and changing contexts where Web service operations run. For instance, a Web service operation may have greatly increased its execution time or may have become unavailable. The contribution of this article is to provide a tool-supported framework to guide autonomic adjustments of context-aware service compositions using models at runtime. During execution, when problematic events arise in the context, models are used by an autonomic architecture to guide changes of the service composition. Under the closed-world assumption, the possible context events are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts where service compositions run. Therefore, the proposed framework also covers the dynamic evolution of service compositions to deal with unexpected events in the open world. An evaluation demonstrates that our framework is efficient during dynamic adjustments.},
journal = {Comput. Electr. Eng.},
month = oct,
pages = {332–352},
numpages = {21},
keywords = {Autonomic computing, Dynamic adaptation, Dynamic evolution, Dynamic software product lines, Models at runtime, Web service compositions}
}

@inproceedings{10.1145/2088876.2088879,
author = {Merle, Philippe and Rouvoy, Romain and Seinturier, Lionel},
title = {A reflective platform for highly adaptive multi-cloud systems},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088879},
doi = {10.1145/2088876.2088879},
abstract = {Cloud platforms are increasingly used for hosting a broad diversity of services from traditional e-commerce applications to interactive web-based IDEs. However, we observe that the proliferation of offers by Cloud vendors raises several challenges. Developers will not only have to deploy applications for a specific Cloud, but will also have to consider migrating services from one cloud to another, and to manage applications spanning multiple Clouds. In this paper, we therefore report on a first experiment we conducted to build a multi-Cloud system on top of thirteen existing IaaS/PaaS. From this experiment, we advocate for two dimensions of adaptability---design and execution time---that applications for such systems require to exhibit. Finally, we propose a roadmap for future multi-Cloud systems.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {14–21},
numpages = {8},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.5555/1885639.1885700,
author = {John, Isabel and Schwanninger, Christa and Almeida, Eduardo},
title = {The rise and fall of product line architectures},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This panel addresses questions around architecture like: How do you think a good product line architecture should look like? How much up-front design do we need for a product line architecture? What are hot research topics in product line architecture? The panel is organized as a goldfish bowl, where the panelists are in the middle of the audience and panelists change during the panel.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {500–501},
numpages = {2},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@inproceedings{10.5555/523998.851170,
author = {Henry, R. R.},
title = {A multicast ATM switch with slotted ring fabric},
year = {1997},
isbn = {0818681861},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper proposes a scalable, optical fiber, slotted ring ATM switch architecture. The internal switching fabric and transport mechanism is developed, a model formulated, and deterministic performance analysis equations derived. The delay and blocking performance metrics of a 16/spl times/16 switch with 150 Mbps links are given. It is shown that zero blocking performance can be achieved, as can delays as low as several bit times.},
booktitle = {Proceedings of the 6th International Conference on Computer Communications and Networks},
pages = {500},
keywords = {150 Mbit/s, asynchronous transfer mode, delay performance, deterministic performance analysis equations, multicast ATM switch, optical fiber switch, slotted ring ATM switch architecture, slotted ring fabric, switching fabric, zero blocking performance},
series = {IC3N '97}
}

@article{10.1016/j.future.2019.03.037,
author = {Pourmasoumi, Asef and Kahani, Mohsen and Bagheri, Ebrahim},
title = {The evolutionary composition of desirable execution traces from event logs},
year = {2019},
issue_date = {Sep 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {98},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.03.037},
doi = {10.1016/j.future.2019.03.037},
journal = {Future Gener. Comput. Syst.},
month = sep,
pages = {78–103},
numpages = {26},
keywords = {Business process families, Process improvement, Optimization algorithms, Event logs}
}

@article{10.1504/IJWGS.2012.051527,
author = {Charfi, Anis and Schmeling, Benjamin and Mezini, Mira},
title = {An aspect-oriented framework for specification and enforcement of non-functional concerns in WS-BPEL},
year = {2012},
issue_date = {January 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {4},
issn = {1741-1106},
url = {https://doi.org/10.1504/IJWGS.2012.051527},
doi = {10.1504/IJWGS.2012.051527},
abstract = {Web Service processes in WS-BPEL have several non-functional requirements such as security and reliable messaging. Although there are many WS-* specifications that address these concerns, their integration with WS-BPEL is still open. In this paper, we discuss these non-functional requirements and present a survey on the current support for their specification and enforcement in WS-BPEL engines. Moreover, we introduce an aspect-oriented container framework that uses a declarative deployment descriptor to specify the non-functional requirements. For the enforcement, aspects in AO4BPEL 2.0 are generated, which intercept the process execution and call dedicated middleware Web Services.},
journal = {Int. J. Web Grid Serv.},
month = jan,
pages = {386–424},
numpages = {39}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@inproceedings{10.5555/3290281.3290302,
author = {Kalra, Sumit and T, Prabhakar},
title = {Implementation patterns for multi-tenancy},
year = {2017},
isbn = {9781941652060},
publisher = {The Hillside Group},
address = {USA},
abstract = {Recently multi-tenant applications for SaaS in cloud computing are on rise. These applications increase the degree of resource sharing among tenants with various functional and non-functional requirements. However, often it results in higher design complexity. In this work, we discuss various design patterns to build these applications with efficient tenant management. We divided these patterns in the three categories. The categorization is based on their applicability to design, development, and runtime phases during software development life cycle. These patterns make an application tenant aware and enable multi-tenancy without adding much overhead and complexity in its design.},
booktitle = {Proceedings of the 24th Conference on Pattern Languages of Programs},
articleno = {17},
numpages = {16},
keywords = {multi-tenant, tenant operation and management},
location = {Vancouver, British Columbia, Canada},
series = {PLoP '17}
}

@article{10.1016/j.infsof.2019.04.011,
author = {Rodrigues, Arthur and Rodrigues, Gena\'{\i}na Nunes and Knauss, Alessia and Ali, Raian and Andrade, Hugo},
title = {Enhancing context specifications for dependable adaptive systems: A data mining approach},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.011},
doi = {10.1016/j.infsof.2019.04.011},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {115–131},
numpages = {17},
keywords = {Self-adaptive system, Context uncertainty, Data mining, Design time, Goal modelling, Dependability}
}

@article{10.1016/j.ijinfomgt.2015.09.008,
author = {Chang, Victor and Ramachandran, Muthu and Yao, Yulin and Kuo, Yen-Hung and Li, Chung-Sheng},
title = {A resiliency framework for an enterprise cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {1},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2015.09.008},
doi = {10.1016/j.ijinfomgt.2015.09.008},
abstract = {We have presented a resilient framework for an enterprise cloud.We have developed an architecture with four major services to demonstrate resiliency, where the cloud computing adoption framework (CCAF) takes the center role to blend other services.We explain how our work is relevant to business resiliency.We have the support from a large scale survey to ensure that our design and service can meet the large number of user requirements. This paper presents a systematic approach to develop a resilient software system which can be developed as emerging services and analytics for resiliency. While using the resiliency as a good example for enterprise cloud security, all resilient characteristics should be blended together to produce greater impacts. A framework, cloud computing adoption framework (CCAF), is presented in details. CCAF has four major types of emerging services and each one has been explained in details with regard to the individual function and how each one can be integrated. CCAF is an architectural framework that blends software resilience, service components and guidelines together and provides real case studies to produce greater impacts to the organizations adopting cloud computing and security. CCAF provides business alignments and provides agility, efficiency and integration for business competitive edge. In order to validate user requirements and system designs, a large scale survey has been conducted with detailed analysis provided for each major question. We present our discussion and conclude that the use of CCAF framework can illustrate software resilience and security improvement for enterprise security. CCAF framework itself is validated as an emerging service for enterprise cloud computing with analytics showing survey analysis.},
journal = {Int. J. Inf. Manag.},
month = feb,
pages = {155–166},
numpages = {12},
keywords = {- Software resiliency, Cloud computing Adoption Framework (CCAF), Cloud security and software engineering best practice, Resilient software for Enterprise Cloud}
}

@article{10.5555/2747013.2747140,
author = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
title = {Uncertainty handling in goal-driven self-optimization - Limiting the negative effect on adaptation},
year = {2014},
issue_date = {April 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {90},
number = {C},
issn = {0164-1212},
abstract = {Graphical abstractDisplay Omitted HighlightsWe propose techniques to handle contribution uncertainty and effect uncertainty in goal-driven self-optimization.We integrate these uncertainty handling techniques with preference uncertainty handling to a goal-driven self-optimization framework.We demonstrate the effectiveness of our approach with an evaluation on an online shopping system. Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.},
journal = {J. Syst. Softw.},
month = apr,
pages = {114–127},
numpages = {14},
keywords = {Goal-driven self-optimization, Requirements goal models, Uncertainty}
}

@inproceedings{10.5555/978-3-030-45234-6_fm,
title = {Front Matter},
year = {2020},
isbn = {978-3-030-45233-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Fundamental Approaches to Software Engineering: 23rd International Conference, FASE 2020, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2020, Dublin, Ireland, April 25–30, 2020, Proceedings},
pages = {i–xiii},
location = {Dublin, Ireland}
}

@article{10.1007/s11219-019-09489-8,
author = {Al\'{e}groth, Emil and Gorschek, Tony and Petersen, Kai and Mattsson, Michael},
title = {Characteristics that affect preference of decision models for asset selection: an industrial questionnaire survey},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09489-8},
doi = {10.1007/s11219-019-09489-8},
abstract = {Modern software development relies on a combination of development and re-use of technical asset, e.g., software components, libraries, and APIs. In the past, re-use was mostly conducted with internal assets but today external; open source, customer off-the-shelf (COTS), and assets developed through outsourcing are also common. This access to more asset alternatives presents new challenges regarding what assets to optimally chose and how to make this decision. To support decision-makers, decision theory has been used to develop decision models for asset selection. However, very little industrial data has been presented in literature about the usefulness, or even perceived usefulness, of these models. Additionally, only limited information has been presented about what model characteristics determine practitioner preference toward one model over another. The objective of this work is to evaluate what characteristics of decision models for asset selection determine industrial practitioner preference of a model when given the choice of a decision model of high precision or a model with high speed. An industrial questionnaire survey is performed where a total of 33 practitioners, of varying roles, from 18 companies are tasked to compare two decision models for asset selection. Textual analysis and formal and descriptive statistics are then applied on the survey responses to answer the study’s research questions. The study shows that the practitioners had clear preference toward the decision model that emphasized speed over the one that emphasized decision precision. This conclusion was determined to be because one of the models was perceived faster, had lower complexity, was more flexible in use for different decisions, and was more agile on how it could be used in operation, its emphasis on people, its emphasis on “good enough” precision and ability to fail fast if a decision was a failure. Hence, we found seven characteristics that the practitioners considered important for their acceptance of the model. Industrial practitioner preference, which relates to acceptance, of decision models for asset selection is dependent on multiple characteristics that must be considered when developing a model for different types of decisions such as operational day-to-day decisions as well as more critical tactical or strategic decisions. The main contribution of this work are the seven identified characteristics that can serve as industrial requirements for future research on decision models for asset selection.},
journal = {Software Quality Journal},
month = dec,
pages = {1675–1707},
numpages = {33},
keywords = {Decision models, Characteristics, Industrial study, Survey, Model comparison}
}

@article{10.1007/s11219-016-9320-z,
author = {Carvalho, Rainara Maia and Castro Andrade, Rossana Maria and Oliveira, K\'{a}thia Mar\c{c}al and Sousa Santos, Ismayle and Bezerra, Carla Ilane},
title = {Quality characteristics and measures for human---computer interaction evaluation in ubiquitous systems},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9320-z},
doi = {10.1007/s11219-016-9320-z},
abstract = {The advent of ubiquitous systems places even more focus on users, since these systems must support their daily activities in such a transparent way that does not disturb them. Thus, much more attention should be provided to human---computer interaction (HCI) and, as a consequence, to its quality. Dealing with quality issues implies first the identification of the quality characteristics that should be achieved and, then, which software measures should be used to evaluate them in a target system. Therefore, this work aims to identify what quality characteristics and measures have been used for the HCI evaluation of ubiquitous systems. In order to achieve our goal, we performed a large literature review, using a systematic mapping study, and we present our results in this paper. We identified 41 pertinent papers that were deeply analyzed to extract quality characteristics and software measures. We found 186 quality characteristics, but since there were divergences on their definitions and duplicated characteristics, an analysis of synonyms by peer review based on the equivalence of definitions was also done. This analysis allowed us to define a final suitable set composed of 27 quality characteristics, where 21 are generic to any system but are particularized for ubiquitous applications and 6 are specific for this domain. We also found 218 citations of measures associated with the characteristics, although the majority of them are simple definitions with no detail about their measurement functions. Our results provide not only an overview of this area to guide researchers in directing their efforts but also it can help practitioners in evaluating ubiquitous systems using these measures.},
journal = {Software Quality Journal},
month = sep,
pages = {743–795},
numpages = {53},
keywords = {Human---computer interaction, Quality characteristics, Quality model, Software measures, Systematic mapping study, Ubiquitous systems}
}

@article{10.1016/j.infsof.2016.11.009,
author = {Mariani, Thain\'{a} and Vergilio, Silvia Regina},
title = {A systematic review on search-based refactoring},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.009},
doi = {10.1016/j.infsof.2016.11.009},
abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest.Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities.Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation.Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics.Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {14–34},
numpages = {21},
keywords = {Evolutionary algorithms, Refactoring, Search-based software engineering}
}

@inproceedings{10.5555/2663546.2663573,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, California},
series = {SEAMS '13}
}

@inproceedings{10.4108/eai.25-10-2016.2266615,
author = {Distefano, Salvatore and Scarpa, Marco},
title = {Quantitative assessment of workflow performance through PH reduction},
year = {2017},
isbn = {9781631901416},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/eai.25-10-2016.2266615},
doi = {10.4108/eai.25-10-2016.2266615},
abstract = {Workflows are logical abstraction of processes widely adopted in several contexts such as economy and management sciences (business processes), service engineering (service oriented architecture, Web services, BPEL), software engineering (component based systems, UML, flowcharts) distributed computing (Grid, Cloud, Mapreduce). Design and operation of workflows are critical stages in which problems and issues not manifested by the single block arise from compositions. To deal with such issues, proper techniques and tools should be implemented as support for workflow designers and operators. This paper proposes a solution for the evaluation of workflow performance starting from the components’ ones. Based on the stochastic characterization of the workflow tasks, phase type distributions and stochastic workflow reduction rules, the proposed approach allows to overcome the limits of existing solutions, considering general response time distributions while providing parametric analysis on customer usage profiles and design alternatives. To demonstrate the effectiveness of the proposed solution an example taken from literature is evaluated.},
booktitle = {Proceedings of the 10th EAI International Conference on Performance Evaluation Methodologies and Tools on 10th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {117–124},
numpages = {8},
keywords = {design alternatives, non-markovian behaviors, performance, phase type, usage profile, workflow},
location = {Taormina, Italy},
series = {VALUETOOLS'16}
}

@inproceedings{10.1145/3141848.3141850,
author = {Weckesser, Markus and Lochau, Malte and Ries, Michael and Sch\"{u}rr, Andy},
title = {Towards complete consistency checks of Clafer models},
year = {2017},
isbn = {9781450355186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3141848.3141850},
doi = {10.1145/3141848.3141850},
abstract = {Clafer is a general purpose modeling language that combines UML-like class and meta-modeling with feature-oriented variability modeling and first-order logic constraints. The considerable expressiveness of Clafer makes automated reasoning about properties like model consistency (i.e., finding a valid model instance) very challenging. In particular, multiplicity annotations and recursive model structures yield a potentially unbounded number of model instances resulting in an infinite search space. Existing approaches for consistency checking encode Clafer models into finite constraint-satisfaction problems by either manually or heuristically, setting bounds for the search space. Hence, if no valid model instance has been found, it is unknown whether the model is inconsistent, or whether the bounds have been chosen too tight. In this paper, we characterize a restricted sub-language of Clafer with complex inheritance relations that is crucial for facilitating sound and complete model-consistency checking. To this end, we present the idea of a novel technique for automated search-space restriction, by flattening Clafer models and encoding them as Integer Linear Programs (ILP). Our evaluation shows very promising results of our approach in terms of runtime efficiency for both flattening of complex inheritance hierarchies as well as sound and complete consistency checking.},
booktitle = {Proceedings of the 8th ACM SIGPLAN International Workshop on Feature-Oriented Software Development},
pages = {11–20},
numpages = {10},
keywords = {Automated Validation, Clafer, Integer Linear Programming},
location = {Vancouver, BC, Canada},
series = {FOSD 2017}
}

@inproceedings{10.5555/3367032.3367200,
author = {Terra-Neves, Miguel and Lynce, In\^{e}s and Manquinho, Vasco},
title = {Integrating Pseudo-Boolean constraint reasoning in multi-objective evolutionary algorithms},
year = {2019},
isbn = {9780999241141},
publisher = {AAAI Press},
abstract = {Constraint-based reasoning methods thrive in solving problem instances with a tight solution space. On the other hand, evolutionary algorithms are usually effective when it is not hard to satisfy the problem constraints. This dichotomy has been observed in many optimization problems. In the particular case of Multi-Objective Combinatorial Optimization (MOCO), new recently proposed constraint-based algorithms have been shown to outperform more established evolutionary approaches when a given problem instance is hard to satisfy. In this paper, we propose the integration of constraint-based procedures in evolutionary algorithms for solving MOCO. First, a new core-based smart mutation operator is applied to individuals that do not satisfy all problem constraints. Additionally, a new smart improvement operator based on Minimal Correction Subsets is used to improve the quality of the population. Experimental results clearly show that the integration of these operators greatly improves multi-objective evolutionary algorithms MOEA/D and NSGAII. Moreover, even on problem instances with a tight solution space, the newly proposed algorithms outperform the state-of-the-art constraint-based approaches for MOCO.},
booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
pages = {1184–1190},
numpages = {7},
location = {Macao, China},
series = {IJCAI'19}
}

@article{10.1007/s12650-020-00647-w,
author = {Chotisarn, Noptanit and Merino, Leonel and Zheng, Xu and Lonapalawong, Supaporn and Zhang, Tianye and Xu, Mingliang and Chen, Wei},
title = {A systematic literature review of modern software visualization},
year = {2020},
issue_date = {Aug 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {4},
issn = {1343-8875},
url = {https://doi.org/10.1007/s12650-020-00647-w},
doi = {10.1007/s12650-020-00647-w},
journal = {J. Vis.},
month = aug,
pages = {539–558},
numpages = {20},
keywords = {Software visualization, Systematic literature review, Information visualization}
}

@article{10.1016/j.infsof.2006.11.003,
author = {Niemel\"{a}, Eila and Immonen, Anne},
title = {Capturing quality requirements of product family architecture},
year = {2007},
issue_date = {November, 2007},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {49},
number = {11–12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2006.11.003},
doi = {10.1016/j.infsof.2006.11.003},
abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1107–1120},
numpages = {14},
keywords = {Quality requirement, Software architecture, Software product family, Traceability}
}

@inproceedings{10.5555/2487336.2487363,
author = {Fredericks, Erik M. and Ramirez, Andres J. and Cheng, Betty H. C.},
title = {Towards run-time testing of dynamic adaptive systems},
year = {2013},
isbn = {9781467344012},
publisher = {IEEE Press},
abstract = {It is challenging to design, develop, and validate a dynamically adaptive system (DAS) that satisfies requirements, particularly when requirements can change at run time. Testing at design time can help verify and validate that a DAS satisfies its specified requirements and constraints. While offline tests may demonstrate that a DAS is capable of satisfying its requirements before deployment, a DAS may encounter unanticipated system and environmental conditions that can prevent it from achieving its objectives. In working towards a requirements-aware DAS, this paper proposes run-time monitoring and adaptation of tests as another technique for evaluating whether a DAS satisfies, or is even capable of satisfying, its requirements given its current execution context. To this end, this paper motivates the need and identifies challenges for adaptively testing a DAS at run time, as well as suggests possible methods for leveraging offline testing techniques for verifying run-time behavior.},
booktitle = {Proceedings of the 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {169–174},
numpages = {6},
location = {San Francisco, CA, USA},
series = {SEAMS '13}
}

@article{10.1145/1988997.1989008,
author = {Rahmani, M.},
title = {Software modeling &amp; design: UML, use cases, patterns, &amp; software architectures by Hassan Gomaa},
year = {2011},
issue_date = {July 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1988997.1989008},
doi = {10.1145/1988997.1989008},
journal = {SIGSOFT Softw. Eng. Notes},
month = aug,
pages = {35},
numpages = {1}
}

@inproceedings{10.1145/1370062.1370078,
author = {Espinoza, Huascar and Servat, David and G\'{e}rard, S\'{e}bastien},
title = {Leveraging analysis-aided design decision knowledge in UML-based development of embedded systems},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370078},
doi = {10.1145/1370062.1370078},
abstract = {Many important works have been carried out to provide modeling languages (e.g., UML, SDL) with expressiveness to support embedded system design, validation and verification. A fundamental shortcoming in current model-driven approaches is the inability to explicitly capture design decisions and trade-offs between different non-functional parameters, among which timeliness, memory usage, and power consumption are of primary interest. This paper highlights technical limitations in UML to specify complex non-functional evaluation scenarios of candidate architectures, and outlines our current work to provide straightforward solutions.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {55–62},
numpages = {8},
keywords = {UML, design space exploration, embedded systems, model-driven engineering, trade-off analysis},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@inproceedings{10.1145/2145204.2145402,
author = {Liu, Xiaoqing (Frank) and Barnes, Eric Christopher and Savolainen, Juha Erik},
title = {Conflict detection and resolution for product line design in a collaborative decision making environment},
year = {2012},
isbn = {9781450310864},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2145204.2145402},
doi = {10.1145/2145204.2145402},
abstract = {Ensuring that the non-functional requirements (NFRs), of a system are satisfied is an essential task in software development. However, this task is complicated by the fact that many NFRs conflict with each other from multiple perspectives. It is essential to resolve conflicts collectively in a collaborative decision making process since stakeholders often disagree on how conflicts should be resolved. In this paper, we describe a method for dividing high-level NFR conflicts within a product line into more manageable sub-problems. Stakeholders make use of an argumentation based collaborative decision support system to determine which design alternatives provide the best trade-offs between NFRs. Finally, we present an empirical study in which the aforementioned system was used to resolve a single instance of an NFR conflict across 3 members of a product line. It shows that the system is effective in resolving conflicts in a collaborative decision process.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
pages = {1327–1336},
numpages = {10},
keywords = {Collaboration architectures, collaborative software development, computer-mediated communication, participatory/cooperative design},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.1145/3243734.3243739,
author = {Ispoglou, Kyriakos K. and AlBassam, Bader and Jaeger, Trent and Payer, Mathias},
title = {Block Oriented Programming: Automating Data-Only Attacks},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243739},
doi = {10.1145/3243734.3243739},
abstract = {With the widespread deployment of Control-Flow Integrity (CFI), control-flow hijacking attacks, and consequently code reuse attacks, are significantly more difficult. CFI limits control flow to well-known locations, severely restricting arbitrary code execution. Assessing the remaining attack surface of an application under advanced control-flow hijack defenses such as CFI and shadow stacks remains an open problem. We introduce BOPC, a mechanism to automatically assess whether an attacker can execute arbitrary code on a binary hardened with CFI/shadow stack defenses. BOPC computes exploits for a target program from payload specifications written in a Turing-complete, high-level language called SPL that abstracts away architecture and program-specific details. SPL payloads are compiled into a program trace that executes the desired behavior on top of the target binary. The input for BOPC is an SPL payload, a starting point (e.g., from a fuzzer crash) and an arbitrary memory write primitive that allows application state corruption. To map SPL payloads to a program trace, BOPC introduces Block Oriented Programming (BOP), a new code reuse technique that utilizes entire basic blocks as gadgets along valid execution paths in the program, i.e., without violating CFI or shadow stack policies. We find that the problem of mapping payloads to program traces is NP-hard, so BOPC first reduces the search space by pruning infeasible paths and then uses heuristics to guide the search to probable paths. BOPC encodes the BOP payload as a set of memory writes. We execute 13 SPL payloads applied to 10 popular applications. BOPC successfully finds payloads and complex execution traces -- which would likely not have been found through manual analysis -- while following the target's Control-Flow Graph under an ideal CFI policy in 81% of the cases.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1868–1882},
numpages = {15},
keywords = {binary analysis, block oriented programming, data only attacks, exploitation, program synthesis},
location = {Toronto, Canada},
series = {CCS '18}
}

@article{10.1016/j.jss.2012.04.079,
author = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
title = {Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop},
year = {2012},
issue_date = {December, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {12},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.04.079},
doi = {10.1016/j.jss.2012.04.079},
abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.},
journal = {J. Syst. Softw.},
month = dec,
pages = {2707–2719},
numpages = {13},
keywords = {Earned business value, Feedback control theory, Goal-oriented reasoning, Preference, Self-tuning}
}

@inproceedings{10.1145/2658761.2658767,
author = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
title = {Automatic feature selection in large-scale system-software product lines},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658767},
doi = {10.1145/2658761.2658767},
abstract = {System software can typically be configured at compile time via a comfortable feature-based interface to tailor its functionality towards a specific use case. However, with the growing number of features, this tailoring process becomes increasingly difficult: As a prominent example, the Linux kernel in v3.14 provides nearly 14 000 configuration options to choose from. Even developers of embedded systems refrain from trying to build a minimized distinctive kernel configuration for their device – and thereby waste memory and money for unneeded functionality. In this paper, we present an approach for the automatic use-case specific tailoring of system software for special-purpose embedded systems. We evaluate the effectiveness of our approach on the example of Linux by generating tailored kernels for well-known applications of the Rasperry Pi and a Google Nexus 4 smartphone. Compared to the original configurations, our approach leads to memory savings of 15–70 percent and requires only very little manual intervention.},
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {39–48},
numpages = {10},
keywords = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@inproceedings{10.1145/2000259.2000274,
author = {Brosch, Franz and Buhnova, Barbora and Koziolek, Heiko and Reussner, Ralf},
title = {Reliability prediction for fault-tolerant software architectures},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000274},
doi = {10.1145/2000259.2000274},
abstract = {Software fault tolerance mechanisms aim at improving the reliability of software systems. Their effectiveness (i.e., reliability impact) is highly application-specific and depends on the overall system architecture and usage profile. When examining multiple architecture configurations, such as in software product lines, it is a complex and error-prone task to include fault tolerance mechanisms effectively. Existing approaches for reliability analysis of software architectures either do not support modelling fault tolerance mechanisms or are not designed for an efficient evaluation of multiple architecture variants. We present a novel approach to analyse the effect of software fault tolerance mechanisms in varying architecture configurations. We have validated the approach in multiple case studies, including a large-scale industrial system, demonstrating its ability to support architecture design, and its robustness against imprecise input data.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {75–84},
numpages = {10},
keywords = {component-based software architectures, fault tolerance, reliability prediction, software product lines},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining energy-aware commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2465478.2465490,
author = {Klein, John and van Vliet, Hans},
title = {A systematic review of system-of-systems architecture research},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465490},
doi = {10.1145/2465478.2465490},
abstract = {Context: A system of systems is an assemblage of components which individually may be regarded as systems, and which possesses the additional properties that the constituent systems are operationally independent, and are managerially independent. Much has been published about the field of systems of systems by researchers and practitioners, often with the assertion that the system-of-systems design context necessitates the use of architecture approaches that are somewhat different from system-level architecture. However, no systematic review has been conducted to provide an extensive overview of system of systems architecture research.Objective: This paper presents such a systematic review. The objective of this review is to classify and provide a thematic analysis of the reported results in system of systems architecture.Method: The primary studies for the systematic review were identified using a predefined search strategy followed by an extensive manual selection process.Results: We found the primary studies published in a large number of venues, mostly domain-oriented, with no obvious center of a research community of practice. The field seems to be maturing more slowly than other software technologies: Most reported results described individuals or teams working in apparent isolation to develop solutions to particular system-of-systems architecture problems, with no techniques gaining widespread adoption.Conclusions: A comprehensive research agenda for this field should be developed, and further studies should be performed to determine whether the information system-related problems of system of systems architecture are covered by existing software architecture knowledge, and if not, to develop general methods for system-of-systems architecture.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {13–22},
numpages = {10},
keywords = {architecture, system of systems, systematic review},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@article{10.1007/s10515-017-0215-4,
author = {Boussa\"{\i}d, Ilhem and Siarry, Patrick and Ahmed-Nacer, Mohamed},
title = {A survey on search-based model-driven engineering},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0215-4},
doi = {10.1007/s10515-017-0215-4},
abstract = {Model-driven engineering (MDE) and search-based software engineering (SBSE) are both relevant approaches to software engineering. MDE aims to raise the level of abstraction in order to cope with the complexity of software systems, while SBSE involves the application of metaheuristic search techniques to complex software engineering problems, reformulating engineering tasks as optimization problems. The purpose of this paper is to survey the relatively recent research activity lying at the interface between these two fields, an area that has come to be known as search-based model-driven engineering. We begin with an introduction to MDE, the concepts of models, of metamodels and of model transformations. We also give a brief introduction to SBSE and metaheuristics. Then, we survey the current research work centered around the combination of search-based techniques and MDE. The literature survey is accompanied by the presentation of references for further details.},
journal = {Automated Software Engg.},
month = jun,
pages = {233–294},
numpages = {62},
keywords = {Metaheuristic, Metaheuristics, Model-driven engineering (MDE), Search-based software engineering (SBSE)}
}

@article{10.1016/j.infsof.2016.01.019,
author = {Arcaini, Paolo and Gargantini, Angelo and Riccobene, Elvinia and Vavassori, Paolo},
title = {A novel use of equivalent mutants for static anomaly detection in software artifacts},
year = {2017},
issue_date = {January 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {81},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.01.019},
doi = {10.1016/j.infsof.2016.01.019},
abstract = {Equivalent mutants are usually seen as an inconvenience in mutation analysis.We claim that equivalent mutants can be useful to detect and remove static anomalies.A process for detecting static anomalies is proposed.The process is based on mutation, equivalence checking, and quality measurement.The process is applicable to different kinds of software artifacts. Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis.Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed.Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement.Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level.Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {52–64},
numpages = {13},
keywords = {Equivalent mutant, Quality measure, Static anomaly}
}

@inproceedings{10.1145/2380445.2380527,
author = {Herrera, Fernando and Posadas, Hector and Pe\~{n}il, Pablo and Villar, Eugenio and Ferrero, Francisco and Valencia, Ra\'{u}l},
title = {A MDD methodology for specification of embedded systems and automatic generation of fast configurable and executable performance models},
year = {2012},
isbn = {9781450314268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2380445.2380527},
doi = {10.1145/2380445.2380527},
abstract = {This paper presents the COMPLEX UML/MARTE modeling methodology and its related framework for automatic generation of executable performance models. The modeling methodology supports Model-Driven Development (MDD), required by industrial flows, and a novel set of modeling features specifically suitable for Design Space Exploration (DSE), a crucial design activity. The COMPLEX framework has other advantages for DSE. The COMPLEX tooling enables the automatic generation of an executable and configurable model for fast performance analysis without requiring engineering effort. The COMPLEX tooling automates the production of an easily portable text-based representation of the UML/MARTE model. This representation is read by the underlying simulation infrastructure, which automatically builds a fast performance model supporting the evaluation of different configurations of the system. An important aspect of this performance analysis framework is that it supports a system-level text-based front-end, which is produced from the COMPLEX UML/MARTE model, and which avoids the development of SW implementations, HW refinements, or the implementation of HW/SW interfaces. Moreover, neither code regeneration, nor recompilation is required for any DSE iterations, and thus, the time taken in the exploration is mostly due to model simulation.},
booktitle = {Proceedings of the Eighth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis},
pages = {529–538},
numpages = {10},
keywords = {design, languages, performance, standardization},
location = {Tampere, Finland},
series = {CODES+ISSS '12}
}

@inproceedings{10.1145/2593882.2593895,
author = {Hatcliff, John and Wassyng, Alan and Kelly, Tim and Comar, Cyrille and Jones, Paul},
title = {Certifiably safe software-dependent systems: challenges and directions},
year = {2014},
isbn = {9781450328654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593882.2593895},
doi = {10.1145/2593882.2593895},
abstract = {The amount and impact of software-dependence in critical systems impinging on daily life is increasing rapidly. In many of these systems, inadequate software and systems engineering can lead to economic disaster, injuries or death. Society generally does not recognize the potential of losses from deficiencies of systems due to software until after some mishap occurs. Then there is an outcry, reflecting societal expectations; however, few know what it takes to achieve the expected safety and, in general, loss-prevention.  On the one hand there are unprecedented, exponential increases in size, inter-dependencies, intricacies, numbers and variety in the systems and distribution of development processes across organizations and cultures. On the other hand, industry's capability to verify and validate these systems has not kept up. Mere compliance with existing standards, techniques, and regulations cannot guarantee the safety properties of these systems. The gap between practice and capability is increasing rapidly.  This paper considers the future of software engineering as needed to support development and certification of safety-critical software-dependent systems. We identify a collection of challenges and document their current state, the desired state, gaps and barriers to reaching the desired state, and potential directions in software engineering research and education that could address the gaps and barriers.},
booktitle = {Future of Software Engineering Proceedings},
pages = {182–200},
numpages = {19},
keywords = {Certification, assurance, hazard analysis, requirements, safety, standards, validation, verification},
location = {Hyderabad, India},
series = {FOSE 2014}
}

@article{10.1145/2579281.2579312,
author = {Ionita, Anca Daniela and Lewis, Grace A. and Litoiu, Marin},
title = {Report of the 2013 IEEE 7th international symposium on the maintenance and evolution of service-oriented and cloud-based systems (MESOCA 2013)},
year = {2014},
issue_date = {March 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/2579281.2579312},
doi = {10.1145/2579281.2579312},
abstract = {The 2013 IEEE 7th International Symposium on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2013) took place in Eindhoven, The Netherlands, on September 24, 2013, as a co-located event of the 29th IEEE International Conference on Software Maintenance (ICSM 2013). MESOCA 2013 covered a wide range of academic and industrial experiences, brought together through one keynote, two invited presentations and eleven paper presentations, which triggered lively discussions. They approached aspects related to the entire software maintenance process, from requirements to testing, with specific solutions for Service-Oriented Architecture and Cloud Computing environments. Technical and business perspectives were discussed, including issues about optimization techniques, pre-migration evaluation of legacy software, decision analysis, energy efficiency, multi-cloud architectures and adaptability. It thus confirmed MESOCA as an ongoing forum for researchers and practitioners to identify and address the increasing challenges related to the evolution of service-provisioning systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {34–37},
numpages = {4},
keywords = {SOA, cloud computing, cloudbased systems, service-oriented systems, serviceoriented architecture, services, software evolution, software maintenance}
}

@article{10.1504/IJWET.2016.081768,
author = {Cobaleda, Luz-Viviana and Mazo, Ra\'{u}l and Becerra, Jorge Luis Risco and Duitama, John-Freddy},
title = {Reference software architecture for improving modifiability of personalised web applications - a controlled experiment},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {11},
number = {4},
issn = {1476-1289},
url = {https://doi.org/10.1504/IJWET.2016.081768},
doi = {10.1504/IJWET.2016.081768},
abstract = {Although web personalisation has been studied for the last two decades, there remains a need to address current challenges: context-awareness and the inclusion in a business environment. The wide variety of mobile devices and their continuous technological evolution demands the permanent development of new personalisation strategies. Additionally, two factors complicate the inclusion of personalised web applications in a business environment: the frequent change of personalisation strategies for each business, and the technical complexity to integrate these strategies in a short time. We propose a reference architecture as a tool to favour their modifiability. Moreover, our proposal facilitates the opportunity for enterprises to adopt web-personalised systems into their business as a strategic tool. A controlled experiment validates our approach; we compare five change scenarios that are implemented under two architectures: experimental and control architecture. We used change scenarios derived from a real Brazilian e-commerce enterprise.},
journal = {Int. J. Web Eng. Technol.},
month = jan,
pages = {351–370},
numpages = {20},
keywords = {Brazil, business environment, context awareness, e-commerce, electronic commerce, modifiability, personalised apps, personalised web applications, reference software architecture, software components, web personalisation}
}

@article{10.5555/2773807.2774049,
author = {Ciobanu, R.I. and Reina, D.G. and Dobre, C. and Toral, S.L. and Johnson, P.},
title = {JDER},
year = {2014},
issue_date = {April 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {40},
number = {C},
issn = {1084-8045},
abstract = {Delay tolerant networks have arisen as a new paradigm of wireless communications in which nodes follow a store-carry-and-forward operation. Unlike other ad hoc networks, mobility of nodes is seen as an interesting feature to deliver information from a source node to a destination node. New forwarding schemes have been proposed to deal with the intermittent communications carried out by nodes in delay tolerant networks. Most forwarding schemes assume that nodes are divided into social communities and the communications are likely to be established between two nodes belonging to the same community. However, the social information is not always available, especially in large environments like cities so it has to be inferred from the history of encounters among nodes. Furthermore, there are cases in which the information has to be widely disseminated throughout the network such as alarm and emergency messages so it has to pass through different communities. In this paper, we propose JDER, a new probabilistic forwarding scheme which guarantees high reachability throughout the network by selecting cut-nodes. JDER is based on two metrics: the history encountered ration and the Jaccard distance, and it has been extensively validated through simulations using 8 different mobility models obtained from real life traces.},
journal = {J. Netw. Comput. Appl.},
month = apr,
pages = {279–291},
numpages = {13},
keywords = {Delay tolerant networks, Forwarding scheme, History encountered ration, Jaccard distance, Opportunistic networks}
}

@inproceedings{10.1007/978-3-319-05843-6_15,
author = {Alebrahim, Azadeh and Faβbender, Stephan and Heisel, Maritta and Meis, Rene},
title = {Problem-Based Requirements Interaction Analysis},
year = {2014},
isbn = {9783319058429},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-05843-6_15},
doi = {10.1007/978-3-319-05843-6_15},
abstract = {[Context] The ability to address the diverse interests of different stakeholders in a software project in a coherent way is one fundamental software quality. These diverse and maybe conflicting interests are reflected by the requirements of each stakeholder. [Problem] Thus, it is likely that aggregated requirements for a software system contain interactions. To avoid unwanted interactions and improve software quality, we propose a structured method consisting of three phases to find such interactions. [Principal ideas] For our method, we use problem diagrams, which describe requirements in a structured way. The information represented in the problem diagrams is translated into a formal Z model. Then we reduce the number of combinations of requirements, which might conflict. [Contribution] The reduction of requirements interaction candidates is crucial to lower the effort of the in depth interaction analysis. For validation of our method, we use a real-life example in the domain of smart grid.},
booktitle = {Proceedings of the 20th International Working Conference on Requirements Engineering: Foundation for Software Quality - Volume 8396},
pages = {200–215},
numpages = {16},
keywords = {Requirements interactions, Z notation, feature interaction, problem frames},
location = {Essen, Germany},
series = {REFSQ 2014}
}

@article{10.1007/s10846-021-01430-1,
author = {Sende, Micha and Schranz, Melanie and Prato, Gianluca and Brosse, Etienne and Morando, Omar and Umlauft, Martina},
title = {Engineering Swarms of Cyber-Physical Systems with the CPSwarm Workbench},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {4},
issn = {0921-0296},
url = {https://doi.org/10.1007/s10846-021-01430-1},
doi = {10.1007/s10846-021-01430-1},
abstract = {Engineering swarms of cyber-physical systems (CPSs) is a complex process. We present the CPSwarm workbench that creates an automated design workflow to ease this process. This formalized workflow guides the user from modeling, to code generation, to deployment, both in simulation and on CPS hardware platforms. The workbench combines existing and emerging tools to solve real-world CPS swarm problems. As a proof-of-concept, we use the workbench to design a swarm of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) for a search and rescue (SAR) use case. We evaluate the resulting swarm behaviors on three levels. First, abstract simulations for rapid prototyping. Second, detailed simulation to test the correctness of the results. Third, deployment on hardware to demonstrate the applicability. We measure the swarm performance in terms of area covered and victims rescued. The results show that the performance of the swarm is proportional to its size. Despite some manual steps, the proposed workbench shows to be well suited to ease the complicated task of deploying a swarm of CPSs.},
journal = {J. Intell. Robotics Syst.},
month = aug,
numpages = {18},
keywords = {Cyber-physical system (CPS), Behavior engineering, Swarm modeling, Code generation, Swarm intelligence, Search and rescue (SAR)}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A preliminary empirical assessment of similarity for combinatorial interaction testing of software product lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1007/978-3-662-45234-9_25,
author = {Beek, Maurice H. and Fantechi, Alessandro and Gnesi, Stefania},
title = {Challenges in Modelling and Analyzing Quantitative Aspects of Bike-Sharing Systems},
year = {2014},
isbn = {9783662452332},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-662-45234-9_25},
doi = {10.1007/978-3-662-45234-9_25},
abstract = {Bike-sharing systems are becoming popular not only as a sustainable means of transportation in the urban environment, but also as a challenging case study that presents interesting run-time optimization problems. As a side-study within a research project aimed at quantitative analysis that used such a case study, we have observed how the deployed systems enjoy a wide variety of different features. We have therefore applied variability analysis to define a family of bike-sharing systems, and we have sought support in available tools. We have so established a tool chain that includes academic tools that provide different functionalities regarding the analysis of software product lines, from feature modelling to product derivation and from quantitative evaluation of the attributes of products to model checking value-passing modal specifications. The tool chain is currently experimented inside the mentioned project as a complement to more sophisticated product-based analysis techniques.},
booktitle = {Part I of the Proceedings of the 6th International Symposium on Leveraging Applications of Formal Methods, Verification and Validation. Technologies for Mastering Change - Volume 8802},
pages = {351–367},
numpages = {17}
}

@inproceedings{10.5555/2337223.2337416,
author = {Perrouin, Gilles and Morin, Brice and Chauvel, Franck and Fleurey, Franck and Klein, Jacques and Le Traon, Yves and Barais, Olivier and J\'{e}z\'{e}quel, Jean-Marc},
title = {Towards flexible evolution of dynamically adaptive systems},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Modern software systems need to be continuously available under varying conditions. Their ability to dynamically adapt to their execution context is thus increasingly seen as a key to their success. Recently, many approaches were proposed to design and support the execution of Dynamically Adaptive Systems (DAS). However, the ability of a DAS to evolve is limited to the addition, update or removal of adaptation rules or reconfiguration scripts. These artifacts are very specific to the control loop managing such a DAS and runtime evolution of the DAS requirements may affect other parts of the DAS. In this paper, we argue to evolve all parts of the loop. We suggest leveraging recent advances in model-driven techniques to offer an approach that supports the evolution of both systems and their adaptation capabilities. The basic idea is to consider the control loop itself as an adaptive system.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1353–1356},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@article{10.1016/j.future.2019.02.069,
author = {Boukadi, Khouloud and Grati, Rima and Rekik, Molka and Ben-Abdallah, Han\^{e}ne},
title = {Business process outsourcing to cloud containers: How to find the optimal deployment?},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {97},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.02.069},
doi = {10.1016/j.future.2019.02.069},
journal = {Future Gener. Comput. Syst.},
month = aug,
pages = {397–408},
numpages = {12},
keywords = {Business process, Cloud, CaaS, Linear program, Optimal deployment}
}

@inproceedings{10.5555/2662572.2662581,
author = {Sayyad, Abdel Salam and Ingram, Joseph and Menzies, Tim and Ammar, Hany},
title = {Optimum feature selection in software product lines: let your model and values guide your search},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {In Search-Based Software Engineering, well-known metaheuristic search algorithms are utilized to find solutions to common software engineering problems. The algorithms are usually taken "off the shelf" and applied with trust, i.e. software engineers are not concerned with the inner workings of algorithms, only with the results. While this may be sufficient is some domains, we argue against this approach, particularly where the complexity of the models and the variety of user preferences pose greater challenges to the metaheuristic search algorithms. We build on our previous investigation which uncovered the power of Indicator-Based Evolutionary Algorithm (IBEA) over traditionally-used algorithms (such as NSGA-II), and in this work we scrutinize the time behavior of user objectives subject to optimization. This analysis brings out the business perspective, previously veiled under Pareto-collective gauges such as Hypervolume and Spread. In addition, we show how slowing down the rates of crossover and mutation can help IBEA converge faster, as opposed to following the higher rates used in many other studies as "rules of thumb".},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {22–27},
numpages = {6},
keywords = {feature models, indicator-based evolutionary algorithm, multiobjective optimization, optimal feature selection, search-based software engineering, software product lines},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.5555/1939864.1939916,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Dynamic resource reallocation between deployment components},
year = {2010},
isbn = {3642169007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Today's software systems are becoming increasingly configurable and designed for deployment on a plethora of architectures, ranging from sequential machines via multicore and distributed architectures to the cloud. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. To model and analyze systems without a fixed architecture, the models need to naturally capture and range over relevant deployment scenarios. For this purpose, it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. In this paper, the object-oriented modeling language Creol is extended with a notion of dynamic deployment components with parametric processing resources, such that processor resources may be explicitly reallocated. The approach is compositional in the sense that functional models and reallocation strategies are both expressed in Creol, and functional models can be run alone or in combination with different reallocation strategies. The formal semantics of deployment components is given in rewriting logic, extending the semantics of Creol, and executes on Maude, which allows simulations and test suites to be applied to models which vary in their available resources as well as in their resource reallocation strategies.},
booktitle = {Proceedings of the 12th International Conference on Formal Engineering Methods and Software Engineering},
pages = {646–661},
numpages = {16},
location = {Shanghai, China},
series = {ICFEM'10}
}

@inproceedings{10.1145/2493288.2493311,
author = {Kumar, Kiran and Prabhakar, T. V.},
title = {Pattern-oriented knowledge model for architecture design},
year = {2010},
isbn = {9781450301077},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493288.2493311},
doi = {10.1145/2493288.2493311},
abstract = {Software design patterns document the most recommended solutions to recurring design problems. Selection of the best design pattern in a given context involves analysis of available alternatives, which is a knowledge-intensive task. Pattern knowledge overload (due to the large number of design patterns) makes such analysis difficult. A knowledge base to generate available alternatives can alleviate the problem. In this paper, we propose a pattern-oriented knowledge model which considers four dimensions of the pattern knowledge space: Pattern to Tactic relationship, Pattern to Pattern relationship, Pattern to Quality-attribute relationship and Pattern to Application-type relationship. We perform analysis of these relationships for patterns in the two popular pattern catalogues viz GoF and POSA1.},
booktitle = {Proceedings of the 17th Conference on Pattern Languages of Programs},
articleno = {23},
numpages = {21},
keywords = {decision view, pattern to application type relationship, pattern to pattern relationship, pattern to quality attribute relationship, pattern to tactic relationship, patterns, tactics},
location = {Reno, Nevada, USA},
series = {PLOP '10}
}

@article{10.1016/j.scico.2019.07.003,
author = {Vidal, Santiago and Oizumi, Willian and Garcia, Alessandro and D\'{\i}az Pace, Andr\'{e}s and Marcos, Claudia},
title = {Ranking architecturally critical agglomerations of code smells},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2019.07.003},
doi = {10.1016/j.scico.2019.07.003},
journal = {Sci. Comput. Program.},
month = aug,
pages = {64–85},
numpages = {22},
keywords = {Code smells, Agglomerations, Software architecture}
}

@inproceedings{10.1007/11763864_6,
author = {Myll\"{a}rniemi, Varvana and Raatikainen, Mikko and M\"{a}nnist\"{o}, Tomi},
title = {Inter-organisational approach in rapid software product family development — a case study},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_6},
doi = {10.1007/11763864_6},
abstract = {Software product families provide an efficient means of reuse between a set of related products. However, software product families are often solely associated with intra-organisational reuse. This paper presents a case study of Fathammer, a small company developing games for different mobile devices. Reuse at Fathammer takes place at multiple levels. The game framework and engine of Fathammer is reused by partner companies that in turn produce game assets to be reused by Fathammer while developing games for various devices. Very rapid development of games is a necessity for Fathammer, whereas maintainability of games is not important. The above characteristics in particular distinguish Fathammer from other case studies and practices usually presented in the product family literature. The results show the applicability and challenges of software product family practices in the context of multiple collaborating companies and a fast-changing domain.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {73–86},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@inproceedings{10.5555/3124497.3124507,
author = {Guerra, Eduardo and Nakagawa, Elisa Yumi},
title = {Relating patterns and reference architectures},
year = {2015},
isbn = {9781941652039},
publisher = {The Hillside Group},
address = {USA},
abstract = {Both patterns and reference architectures aim to describe solutions to be reused for the software systems development. Despite that, they have a lot of differences and have been investigated separately. The objective of this paper is to discuss the relationship between them, how they can be complementary, while respecting their respective peculiarities. We also discuss how patterns can support the creation of reference architectures and how reference architectures can be a source for pattern mining.},
booktitle = {Proceedings of the 22nd Conference on Pattern Languages of Programs},
articleno = {8},
numpages = {9},
keywords = {patterns, reference architecture, software architecture},
location = {Pittsburgh, Pennsylvania},
series = {PLoP '15}
}

@article{10.1007/s10270-020-00795-5,
author = {Bucaioni, Alessio and Mubeen, Saad and Ciccozzi, Federico and Cicchetti, Antonio and Sj\"{o}din, Mikael},
title = {Modelling multi-criticality vehicular software systems: evolution of an industrial component model},
year = {2020},
issue_date = {Sep 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {5},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00795-5},
doi = {10.1007/s10270-020-00795-5},
abstract = {Software in modern vehicles consists of multi-criticality functions, where a function can be safety-critical with stringent real-time requirements, less critical from the vehicle operation perspective, but still with real-time requirements, or not critical at all. Next-generation autonomous vehicles will require higher computational power to run multi-criticality functions and such a power can only be provided by parallel computing platforms such as multi-core architectures. However, current model-based software development solutions and related modelling languages have not been designed to effectively deal with challenges specific of multi-core, such as core-interdependency and controlled allocation of software to hardware. In this paper, we report on the evolution of the Rubus Component Model for the modelling, analysis, and development of vehicular software systems with multi-criticality for deployment on multi-core platforms. Our goal is to provide a lightweight and technology-preserving transition from model-based software development for single-core to multi-core. This is achieved by evolving the Rubus Component Model to capture explicit concepts for multi-core and parallel hardware and for expressing variable criticality of software functions. The paper illustrates these contributions through an industrial application in the vehicular domain.},
journal = {Softw. Syst. Model.},
month = sep,
pages = {1283–1302},
numpages = {20},
keywords = {Model-based engineering, Metamodelling, Single-core, Multi-core, Multi-criticality, Vehicular embedded systems, Real-time systems}
}

@inproceedings{10.1145/2884781.2884861,
author = {Tan, Tian Huat and Chen, Manman and Sun, Jun and Liu, Yang and Andr\'{e}, \'{E}tienne and Xue, Yinxing and Dong, Jin Song},
title = {Optimizing selection of competing services with probabilistic hierarchical refinement},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884861},
doi = {10.1145/2884781.2884861},
abstract = {Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (ProHR). ProHR effectively reduces the search space by removing competing services that cannot be part of the selection. ProHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, ProHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. ProHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {85–95},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1016/j.csi.2019.103362,
author = {Barros-Justo, Jos\'{e} L. and Benitti, Fabiane B.V. and Tiwari, Saurabh},
title = {The impact of Use Cases in real-world software development projects: A systematic mapping study},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {66},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.103362},
doi = {10.1016/j.csi.2019.103362},
journal = {Comput. Stand. Interfaces},
month = oct,
numpages = {16},
keywords = {Evidence-based software engineering, Systematic mapping study, UML Use Cases, Impact in industry, Software engineering}
}

@inproceedings{10.5555/1949303.1949307,
author = {Johnsen, Einar Broch and Owe, Olaf and Schlatte, Rudolf and Tarifa, Silvia Lizeth Tapia},
title = {Validating timed models of deployment components with parametric concurrency},
year = {2010},
isbn = {3642180698},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Many software systems today are designed without assuming a fixed underlying architecture, and may be adapted for sequential, multicore, or distributed deployment. Examples of such systems are found in, e.g., software product lines, service-oriented computing, information systems, embedded systems, operating systems, and telephony. Models of such systems need to capture and range over relevant deployment scenarios, so it is interesting to lift aspects of low-level deployment concerns to the abstraction level of the modeling language. This paper proposes an abstract model of deployment components for concurrent objects, extending the Creol modeling language. The deployment components are parametric in the amount of concurrency they provide; i.e., they vary in processing resources. We give a formal semantics of deployment components and characterize equivalence between deployment components which differ in concurrent resources in terms of test suites. Our semantics is executable on Maude, which allows simulations and test suites to be applied to a deployment component with different concurrent resources.},
booktitle = {Proceedings of the 2010 International Conference on Formal Verification of Object-Oriented Software},
pages = {46–60},
numpages = {15},
location = {Paris, France},
series = {FoVeOOS'10}
}

@inproceedings{10.5555/3049877.3049879,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {23–37},
numpages = {15},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.5555/978-3-030-29983-5_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {i–xxii},
location = {Paris, France}
}

@inproceedings{10.5555/2486788.2486923,
author = {Bellomo, Stephany and Nord, Robert L. and Ozkaya, Ipek},
title = {A study of enabling factors for rapid fielding: combined practices to balance speed and stability},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Agile projects are showing greater promise in rapid fielding as compared to waterfall projects. However, there is a lack of clarity regarding what really constitutes and contributes to success. We interviewed project teams with incremental development lifecycles, from five government and commercial organizations, to gain a better understanding of success and failure factors for rapid fielding on their projects. A key area we explored involves how Agile projects deal with the pressure to rapidly deliver high-value capability, while maintaining project speed (delivering functionality to the users quickly) and product stability (providing reliable and flexible product architecture). For example, due to schedule pressure we often see a pattern of high initial velocity for weeks or months, followed by a slowing of velocity due to stability issues. Business stakeholders find this to be disruptive as the rate of capability delivery slows while the team addresses stability problems. We found that experienced practitioners, when faced with these challenges, do not apply Agile practices alone. Instead they combine practicesAgile, architecture, or otherin creative ways to respond quickly to unanticipated stability problems. In this paper, we summarize the practices practitioners we interviewed from Agile projects found most valuable and provide an overarching scenario that provides insight into how and why these practices emerge.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {982–991},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/1784860.1784876,
author = {Kuz, Ihor and Liu, Yan},
title = {Extending the capabilities of component models for embedded systems},
year = {2007},
isbn = {3540776176},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Component-based development helps to improve the modularity and reusability of embedded systems. Component models devised for embedded systems are typically restricted due to the limited computing, storage and power resources of the target systems. Most existing component models for embedded systems therefore only support a static component architecture and provide a simple and lightweight core. With the increasing demand for more feature-rich embedded systems these component architectures must be extended. In order to remain useful for the development of resource-restricted embedded systems, however, the extensions must be optional. Creating such extensions requires a cost-effective development process that can produce reusable, rather than application-specific, extensions. This necessitates a systematic approach to seamlessly integrate application specific requirements of the extension, the existing component model and the constraints of the computing environment. In this paper we propose a scenario-based architectural approach to extending the capabilities of the CAmkES component model. This approach is used to distil application specific requirements and computing constraints, summarise generic scenarios, drive the extension to the core CAmkES architecture. We illustrate our approach with a case study involving the addition of dynamic capabilities to CAmkES.},
booktitle = {Proceedings of the Quality of Software Architectures 3rd International Conference on Software Architectures, Components, and Applications},
pages = {182–196},
numpages = {15},
keywords = {architecture design, component, embedded system, extension, scenario},
location = {Medford, MA},
series = {QoSA'07}
}

@inproceedings{10.5555/2343576.2343600,
author = {Paraschos, Alexandros and Spanoudakis, Nikolaos I. and Lagoudakis, Michail G.},
title = {Model-driven behavior specification for robotic teams},
year = {2012},
isbn = {0981738117},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Modern model-driven engineering and Agent-Oriented Software Engineering (AOSE) methods are rarely utilized in developing robotic software. In this paper, we show how a Model-Driven AOSE methodology can be used for specifying the behavior of multi-robot teams. Specifically, the Agent Systems Engineering Methodology (ASEME) was used for developing the software that realizes the behavior of a physical robot team competing in the Standard Platform League of the RoboCup competition (the robot soccer world cup). The team consists of four humanoid robots, which play soccer autonomously in real time utilizing the on-board sensing, processing, and actuating capabilities, while communicating and coordinating with each other in order to achieve their common goal of winning the game. Our work focuses on the challenges of coordinating the base functionalities (object recognition, localization, motion skills) within each robot (intra-agent control) and coordinating the activities of the robots towards a desired team behavior (inter-agent control). We discuss the difficulties we faced and present the solutions we gave to a number of practical issues, which, in our view, are inherent in applying any AOSE methodology to robotics. We demonstrate the added value of using an AOSE methodology in the development of robotic systems, as ASEME allowed for a platform-independent team behavior specification, automated a large part of the code generation process, and reduced the total development time.},
booktitle = {Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems - Volume 1},
pages = {171–178},
numpages = {8},
keywords = {agent-oriented software engineering, intra-agent control, model-driven engineering, robotic software development},
location = {Valencia, Spain},
series = {AAMAS '12}
}

@inproceedings{10.1145/1808937.1808945,
author = {Ivanovi\'{c}, Ana and America, Pierre},
title = {Information needed for architecture decision making},
year = {2010},
isbn = {9781605589688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808937.1808945},
doi = {10.1145/1808937.1808945},
abstract = {This paper focuses on the business aspects of architecture decision making -- in particular information needed by managers and architects for making architecture investment decisions. We present the results of 19 interviews in an industrial organization aimed at identifying information used for architecture decision making in the context of product lines. We summarize the interview findings to investigate future possibilities in improving architecture decision making.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Product Line Approaches in Software Engineering},
pages = {54–57},
numpages = {4},
keywords = {architects, architecture investment, decision making, managers},
location = {Cape Town, South Africa},
series = {PLEASE '10}
}

@inproceedings{10.1145/2695664.2695875,
author = {Almeida, Andr\'{e} and Bencomo, Nelly and Batista, Thais and Cavalcante, Everton and Dantas, Francisco},
title = {Dynamic decision-making based on NFR for managing software variability and configuration selection},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695875},
doi = {10.1145/2695664.2695875},
abstract = {Due to dynamic variability, identifying the specific conditions under which non-functional requirements (NFRs) are satisfied may be only possible at runtime. Therefore, it is necessary to consider the dynamic treatment of relevant information during the requirements specifications. The associated data can be gathered by monitoring the execution of the application and its underlying environment to support reasoning about how the current application configuration is fulfilling the established requirements. This paper presents a dynamic decision-making infrastructure to support both NFRs representation and monitoring, and to reason about the degree of satisfaction of NFRs during runtime. The infrastructure is composed of: (i) an extended feature model aligned with a domain-specific language for representing NFRs to be monitored at runtime; (ii) a monitoring infrastructure to continuously assess NFRs at runtime; and (iii) a flexible decision-making process to select the best available configuration based on the satisfaction degree of the NRFs. The evaluation of the approach has shown that it is able to choose application configurations that well fit user NFRs based on runtime information. The evaluation also revealed that the proposed infrastructure provided consistent indicators regarding the best application configurations that fit user NFRs. Finally, a benefit of our approach is that it allows us to quantify the level of satisfaction with respect to NFRs specification.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {1376–1382},
numpages = {7},
keywords = {SPLs, monitoring, non-functional requirements, variability},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2578128.2578238,
author = {Guessi, Milena and Oquendo, Flavio and Nakagawa, Elisa Yumi},
title = {Variability viewpoint to describe reference architectures},
year = {2014},
isbn = {9781450325233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2578128.2578238},
doi = {10.1145/2578128.2578238},
abstract = {Reference architectures have emerged as a special type of software architecture that achieves well-recognized understanding of specific domains. Their purpose is therefore to be a guidance for the development, standardization, and evolution of systems of such domains or neighbor domains. Adequate representation of such architectures is essential to promote their effective use and dissemination, using, for instance, different architectural viewpoints. A comprehensive description of reference architectures should not only record common features and functionalities, but also variations that could be present in the instances of these architectures. In this scenario, the main contribution of our work is to propose an architecture viewpoint to represent variability in reference architectures. We also describe the steps for creating such viewpoint and present an example of a technique that could be used to represent it. A case study is also presented, demonstrating the feasibility of our approach. Based on initial results, we have observed that the variability viewpoint could contribute to a more adequate, complete description of reference architectures and, as a consequence, it could promote a more effective dissemination and use of such architectures.},
booktitle = {Proceedings of the WICSA 2014 Companion Volume},
articleno = {14},
numpages = {6},
keywords = {architecture description, reference architecture, variability},
location = {Sydney, Australia},
series = {WICSA '14 Companion}
}

@article{10.5555/2685119.2685127,
author = {Kazemi, Ali and Rostampour, Ali and Haghighi, Hassan and Abbasi, Sahel},
title = {A conceptual cohesion metric for service oriented systems},
year = {2014},
issue_date = {July 2014},
publisher = {Rinton Press, Incorporated},
address = {Paramus, NJ},
volume = {13},
number = {3–4},
issn = {1540-9589},
abstract = {Service conceptual cohesion has an incredible impact on the reusability and maintainability of service-oriented software systems. Conceptual cohesion indicates the degree of focus of services on a single business functionality. Current metrics for measuring service cohesion reflect the structural aspect of cohesion and therefore cannot be utilized to measure conceptual cohesion of services. Latent Semantic Indexing (LSI), on the other hand, is an information retrieval technique widely used to measure the degree of similarity between a set of text based documents. In our previous work, a metric, namely SCD (Service Cohesion Degree), has been proposed that measures conceptual cohesion of services based on the LSI technique. SCD provides a quantitative evaluation to measure how much a service concentrates on a single business functionality. In addition, SCD is applied in the service identification step, i.e., when services are not yet available, and the designer plans for developing services with high cohesion. This paper has two contributions in comparison to our previous work. At first, it resolves two anomalies occurring in our previous method when calculating conceptual relationship between service operations. Secondly, as the main contribution of the paper, it presents details of a theoretical validation and an empirical evaluation of SCD. By using a small-scale controlled study, the empirical evaluation demonstrates that SCD could measure conceptual cohesion of services acceptably.},
journal = {J. Web Eng.},
month = jul,
pages = {302–332},
numpages = {31},
keywords = {latent semantic indexing, service conceptual cohesion, service-oriented design principle, software metrics}
}

@inproceedings{10.1145/2465478.2465492,
author = {Schmid, Klaus},
title = {A formal approach to technical debt decision making},
year = {2013},
isbn = {9781450321266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465478.2465492},
doi = {10.1145/2465478.2465492},
abstract = {The notion of technical debt attracts significant attention, especially in the context of reconciling architecture and agile development. However, most work on technical debt is still largely informal and if it provides a formalization it is often ad-hoc. In this paper, we provide a detailed, formal analysis of decision making on technical debt in development. Using this formalization, we show that optimal decision making is not effectively computable in real-world situations and provide several well-defined approximations that allow to handle the problem nevertheless in practical situations. Combining these approximations in a single method leads to a light-weight approach that can be effectively applied in iterative software development, including agile approaches.},
booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
pages = {153–162},
numpages = {10},
keywords = {Cost Estimation, Decision Making, Formal Model, Incremental Development, Software Systems, Technical Debt},
location = {Vancouver, British Columbia, Canada},
series = {QoSA '13}
}

@article{10.1145/3377329,
author = {Guggenheim, Jacob W. and Parietti, Federico and Flash, Tamar and Asada, H. Harry},
title = {Laying the Groundwork for Intra-Robotic-Natural Limb Coordination: Is Fully Manual Control Viable?},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3377329},
doi = {10.1145/3377329},
abstract = {Supernumerary Robotic Limbs (SRLs) have been successfully applied in bracing and as an assistive technology for people with disabilities. These tasks only require perception internal to the SRL-human system. However, SRLs show promise in applications requiring external perception such as opening a door when one’s hands are full. One path toward developing SRLs that accomplish these tasks is to use human-in-the-loop control, thus leveraging the human’s superior perception system to help the SRLs. However, the effects on the user of controlling additional limbs are unclear. This article presents an experimental study where humans, wearing two single degree of freedom SRLs, were instructed to minimize the position error between the subject’s natural and robotic limbs and the corresponding targets, one for each limb. First, subjects performed worse with their natural limbs when asked to perform the task with two natural and two robotic limbs as opposed to with just their natural limbs, suggesting that shared control could help. Second, subjects moved their natural limbs together followed by moving their SRLs together. This informs both the choice of control scheme for the SRLs and the division of labor within a task. Third, subjects showed significant concurrent use of the natural and robotic limbs.},
journal = {J. Hum.-Robot Interact.},
month = may,
articleno = {18},
numpages = {12},
keywords = {Supernumerary robotic limbs}
}

@article{10.1016/j.infsof.2017.10.012,
author = {Budgen, David and Brereton, Pearl and Williams, Nikki and Drummond, Sarah},
title = {The contribution that empirical studies performed in industry make to the findings of systematic reviews},
year = {2018},
issue_date = {February 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {94},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.10.012},
doi = {10.1016/j.infsof.2017.10.012},
abstract = {ContextSystematic reviews can provide useful knowledge for software engineering practice, by aggregating and synthesising empirical studies related to a specific topic. ObjectiveWe sought to assess how far the findings of systematic reviews addressing practice-oriented topics have been derived from empirical studies that were performed in industry or that used industry data. MethodWe drew upon and augmented the data obtained from a tertiary study that performed a systematic review of systematic reviews published in the period up to the end of 2015, seeking to identify those with findings that are relevant for teaching and practice. For the supplementary analysis reported here, we then examined the profiles of the primary studies as reported in each systematic review. ResultsWe identified 48 systematic reviews as candidates for further analysis. The many differences that arise between systematic reviews, together with the incompleteness of reporting for these, mean that our counts should be treated as indicative rather than definitive. However, even when allowing for problems of classification, the findings from the majority of these systematic reviews were predominantly derived from using primary studies conducted in industry. There was also an emphasis upon the use of case studies, and a number of the systematic reviews also made some use of weaker experience or even opinion papers. ConclusionsPrimary studies from industry play an important role as inputs to systematic reviews. Using more rigorous industry-based primary studies can give greater authority to the findings of the systematic reviews, and should help with the creation of a corpus of sound empirical data to support evidence-informed decisions.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {234–244},
numpages = {11},
keywords = {Case study, Industry study, Primary study, Systematic review}
}

@inproceedings{10.5555/1759394.1759396,
author = {Bosch, Jan},
title = {Software product families: towards compositionality},
year = {2007},
isbn = {9783540712886},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software product families have become the most successful approach to intra-organizational reuse. Especially in the embedded systems industry, but also elsewhere, companies are building rich and diverse product portfolios based on software platforms that capture the commonality between products while allowing for their differences. Software product families, however, easily become victims of their own success in that, once successful, there is a tendency to increase the scope of the product family by incorporating a broader and more diverse product portfolio. This requires organizations to change their approach to product families from relying on a pre-integrated platform for product derivation to a compositional approach where platform components are composed in a product-specific configuration.},
booktitle = {Proceedings of the 10th International Conference on Fundamental Approaches to Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {compositionality, software product families},
location = {Braga, Portugal},
series = {FASE'07}
}

@inproceedings{10.1109/MISE.2009.5069896,
author = {Acher, Mathieu and Lahire, Philippe and Moisan, Sabine and Rigault, Jean-Paul},
title = {Tackling high variability in video surveillance systems through a model transformation approach},
year = {2009},
isbn = {9781424437221},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MISE.2009.5069896},
doi = {10.1109/MISE.2009.5069896},
abstract = {This work explores how model-driven engineering techniques can support the configuration of systems in domains presenting multiple variability factors. Video surveillance is a good candidate for which we have an extensive experience. Ultimately, we wish to automatically generate a software component assembly from an application specification, using model to model transformations. The challenge is to cope with variability both at the specification and at the implementation levels. Our approach advocates a clear separation of concerns. More precisely, we propose two feature models, one for task specification and the other for software components. The first model can be transformed into one or several valid component configurations through step-wise specialization. This paper outlines our approach, focusing on the two feature models and their relations. We particularly insist on variability and constraint modeling in order to achieve the mapping from domain variability to software variability through model transformations.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Modeling in Software Engineering},
pages = {44–49},
numpages = {6},
series = {MISE '09}
}

@article{10.1016/j.jcss.2014.11.008,
author = {Yu, Jian and Sheng, Quan Z. and Swee, Joshua K.Y. and Han, Jun and Liu, Chengfei and Noor, Talal H.},
title = {Model-driven development of adaptive web service processes with aspects and rules},
year = {2015},
issue_date = {May 2015},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {81},
number = {3},
issn = {0022-0000},
url = {https://doi.org/10.1016/j.jcss.2014.11.008},
doi = {10.1016/j.jcss.2014.11.008},
abstract = {Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach.},
journal = {J. Comput. Syst. Sci.},
month = may,
pages = {533–552},
numpages = {20},
keywords = {Adaptive systems, Aspect-oriented methodology, Design tools and techniques, Model-driven development, Web services}
}

@article{10.1007/s10270-017-0622-9,
author = {Bruneliere, Hugo and Burger, Erik and Cabot, Jordi and Wimmer, Manuel},
title = {A feature-based survey of model view approaches},
year = {2019},
issue_date = {June      2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0622-9},
doi = {10.1007/s10270-017-0622-9},
abstract = {When dealing with complex systems, information is very often fragmented across many different models expressed within a variety of (modeling) languages. To provide the relevant information in an appropriate way to different kinds of stakeholders, (parts of) such models have to be combined and potentially revamped by focusing on concerns of particular interest for them. Thus, mechanisms to define and compute views over models are highly needed. Several approaches have already been proposed to provide (semi)automated support for dealing with such model views. This paper provides a detailed overview of the current state of the art in this area. To achieve this, we relied on our own experiences of designing and applying such solutions in order to conduct a literature review on this topic. As a result, we discuss the main capabilities of existing approaches and propose a corresponding research agenda. We notably contribute a feature model describing what we believe to be the most important characteristics of the support for views on models. We expect this work to be helpful to both current and potential future users and developers of model view techniques, as well as to any person generally interested in model-based software and systems engineering.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {1931–1952},
numpages = {22},
keywords = {Model, Modeling, Survey, View, Viewpoint}
}

@article{10.1016/j.jss.2016.11.028,
author = {Kaur, Loveleen and Mishra, Ashutosh},
title = {Software component and the semantic Web},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {125},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.11.028},
doi = {10.1016/j.jss.2016.11.028},
abstract = {A detailed study on the use of Semantic Web technologies in CBSE.Advancement is being made from using ontological concepts to Linked Data in CBSE.Semantic web techniques help in addressing the various challenges that CBSE faces.Simple and freely accessible CBSE applications that employ Semantic Web are limited.Some open issues relevant to the topic in concern have been examined. With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.},
journal = {J. Syst. Softw.},
month = mar,
pages = {152–169},
numpages = {18},
keywords = {Component-based software engineering, Linked Data, Ontology, Reasoners, Semantic Web, Web services}
}

@article{10.1007/s10664-015-9399-z,
author = {Ram\'{\i}rez, Aurora and Romero, Jos\'{e} Ra\'{u}l and Ventura, Sebasti\'{a}n},
title = {A comparative study of many-objective evolutionary algorithms for the discovery of software architectures},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9399-z},
doi = {10.1007/s10664-015-9399-z},
abstract = {During the design of complex systems, software architects have to deal with a tangle of abstract artefacts, measures and ideas to discover the most fitting underlying architecture. A common way to structure such complex systems is in terms of their interacting software components, whose composition and connections need to be properly adjusted. Along with the expected functionality, non-functional requirements are key at this stage to guide the many design alternatives to be evaluated by software architects. The appearance of Search Based Software Engineering (SBSE) brings an approach that supports the software engineer along the design process. Evolutionary algorithms can be applied to deal with the abstract and highly combinatorial optimisation problem of architecture discovery from a multiple objective perspective. The definition and resolution of many-objective optimisation problems is currently becoming an emerging challenge in SBSE, where the application of sophisticated techniques within the evolutionary computation field needs to be considered. In this paper, diverse non-functional requirements are selected to guide the evolutionary search, leading to the definition of several optimisation problems with up to 9 metrics concerning the architectural maintainability. An empirical study of the behaviour of 8 multi- and many-objective evolutionary algorithms is presented, where the quality and type of the returned solutions are analysed and discussed from the perspective of both the evolutionary performance and those aspects of interest to the expert. Results show how some many-objective evolutionary algorithms provide useful mechanisms to effectively explore design alternatives on highly dimensional objective spaces.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2546–2600},
numpages = {55},
keywords = {Many-objective evolutionary algorithms, Multi-objective evolutionary algorithms, Search based software engineering, Software architecture discovery}
}

@article{10.14778/3229863.3229864,
author = {Li, Shen and Gerver, Paul and MacMillan, John and Debrunner, Daniel and Marshall, William and Wu, Kun-Lung},
title = {Challenges and experiences in building an efficient apache beam runner for IBM streams},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229864},
doi = {10.14778/3229863.3229864},
abstract = {This paper describes the challenges and experiences in the development of IBM Streams runner for Apache Beam. Apache Beam is emerging as a common stream programming interface for multiple computing engines. Each participating engine implements a runner to translate Beam applications into engine-specific programs. Hence, applications written with the Beam SDK can be executed on different underlying stream computing engines, with negligible migration penalty. IBM Streams is a widely-used enterprise streaming platform. It has a rich set of connectors and toolkits for easy integration of streaming applications with other enterprise applications. It also supports a broad range of programming language interfaces, including Java, C++, Python, Stream Processing Language (SPL) and Apache Beam. This paper focuses on our solutions to efficiently support the Beam programming abstractions in IBM Streams runner. Beam organizes data into discrete event time windows. This design, on the one hand, supports out-of-order data arrivals, but on the other hand, forces runners to maintain more states, which leads to higher space and computation overhead. IBM Streams runner mitigates this problem by efficiently indexing inter-dependent states, garbage-collecting stale keys, and enforcing bundle sizes. We also share performance concerns in Beam that could potentially impact applications. Evaluations show that IBM Streams runner outperforms Flink runner and Spark runner in most scenarios when running the Beam NEXMark benchmarks. IBM Streams runner is available for download from IBM Cloud Streaming Analytics service console.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1742–1754},
numpages = {13}
}

@inproceedings{10.5555/645882.672257,
author = {Thiel, Steffen and Hein, Andreas},
title = {Systematic Integration of Variability into Product Line Architecture Design},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Product lines consider related products, their commonalities and their differences. The differences between the single products are also referred to as variability. Consequently, variability is inherent in every product line and makes a key difference as compared to single systems. While, on the requirements level, the methods for analyzing product line variability are understood today, their transition to architecture remains vague. Bringing variability to architecture as an "add-on" is just a provisional solution and forebodes the risk of violating other intentions. This paper presents a systematic approach to integrate variability with product line architecture design. In particular, it promotes variability as an architectural driver, embeds variability requirements in the architecture design framework "Quality-Driven Software Architecting" (QUASAR), and gives guidelines and examples for documenting variability in architectural views.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {130–153},
numpages = {24},
series = {SPLC 2}
}

@inproceedings{10.1145/375212.375277,
author = {Savolainen, Juha and Kuusela, Juha},
title = {Violatility analysis framework for product lines},
year = {2001},
isbn = {1581133588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/375212.375277},
doi = {10.1145/375212.375277},
abstract = {Evolution of a software intensive system is unavoidable. In fact, evolution can be seen as a part of reuse process. During the evolution of the software asset, the major part of the system functionality is normally reused. So the key issue is to identify the volatile parts of the domain requirements. Additionally, there is promise that tailored tool support may help supporting evolution in software intensive systems. In this paper, we describe the volatility analysis method for product lines. This highly practical method has been used in multiple domains and is able to express and estimate common types of evolutional characteristics. The method is able to represent volatility in multiple levels and has capacity to tie the volatility estimation to one product line member specification. We  also briefly describe current tool support for the method. The main contribution of this paper is a volatility analysis framework that can be used to describe how requirements are estimated to evolve in the future. The method is based on the definition hierarchy framework.},
booktitle = {Proceedings of the 2001 Symposium on Software Reusability: Putting Software Reuse in Context},
pages = {133–141},
numpages = {9},
keywords = {commonality, domain analysis, evolution, product line, requirements engineering, variability, volatility analysis},
location = {Toronto, Ontario, Canada},
series = {SSR '01}
}

@inproceedings{10.1109/ICPC.2017.21,
author = {Tang, Yutian and Leung, Hareton},
title = {Constructing feature model by identifying variability-aware modules},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.21},
doi = {10.1109/ICPC.2017.21},
abstract = {Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {263–274},
numpages = {12},
keywords = {configuration, feature model recovery, feature modules, product line, variability-aware modularity},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@article{10.1007/s00607-014-0421-x,
author = {Paraiso, Fawaz and Merle, Philippe and Seinturier, Lionel},
title = {soCloud: a service-oriented component-based PaaS for managing portability, provisioning, elasticity, and high availability across multiple clouds},
year = {2016},
issue_date = {May       2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {98},
number = {5},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-014-0421-x},
doi = {10.1007/s00607-014-0421-x},
abstract = {Multi-cloud computing is a promising paradigm to support very large scale world wide distributed applications. Multi-cloud computing is the usage of multiple, independent cloud environments, which assumed no priori agreement between cloud providers or third party. However, multi-cloud computing has to face several key challenges such as portability, provisioning, elasticity, and high availability. Developers will not only have to deploy applications to a specific cloud, but will also have to consider application portability from one cloud to another, and to deploy distributed applications spanning multiple clouds. This article presents soCloud a service-oriented component-based Platform as a Service for managing portability, elasticity, provisioning, and high availability across multiple clouds. soCloud is based on the OASIS Service Component Architecture standard in order to address portability. soCloud provides services for managing provisioning, elasticity, and high availability across multiple clouds. soCloud has been deployed and evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE, Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an Eucalyptus private cloud.},
journal = {Computing},
month = may,
pages = {539–565},
numpages = {27},
keywords = {68N01, Elasticity, High availability, Multi-cloud computing, Platform as a Service, Portability, Provisioning, Service Component Architecture}
}

@article{10.1007/s10664-009-9121-0,
author = {Falessi, Davide and Babar, Muhammad Ali and Cantone, Giovanni and Kruchten, Philippe},
title = {Applying empirical software engineering to software architecture: challenges and lessons learned},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9121-0},
doi = {10.1007/s10664-009-9121-0},
abstract = {In the last 15 years, software architecture has emerged as an important software engineering field for managing the development and maintenance of large, software-intensive systems. Software architecture community has developed numerous methods, techniques, and tools to support the architecture process (analysis, design, and review). Historically, most advances in software architecture have been driven by talented people and industrial experience, but there is now a growing need to systematically gather empirical evidence about the advantages or otherwise of tools and methods rather than just rely on promotional anecdotes or rhetoric. The aim of this paper is to promote and facilitate the application of the empirical paradigm to software architecture. To this end, we describe the challenges and lessons learned when assessing software architecture research that used controlled experiments, replications, expert opinion, systematic literature reviews, observational studies, and surveys. Our research will support the emergence of a body of knowledge consisting of the more widely-accepted and well-formed software architecture theories.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {250–276},
numpages = {27},
keywords = {Empirical software engineering, Software architecture}
}

@article{10.1016/j.infsof.2012.04.001,
author = {Abramov, Jenny and Sturm, Arnon and Shoval, Peretz},
title = {Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment},
year = {2012},
issue_date = {September, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.04.001},
doi = {10.1016/j.infsof.2012.04.001},
abstract = {Context: Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective: We present an empirical study conducted to evaluate a Pattern-based method for Secure Development - PbSD - that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method: The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results: We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion: The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {1029–1043},
numpages = {15},
keywords = {Authorization, Controlled experiment, Database design, Model driven development, Secure software development, Security patterns}
}

@inproceedings{10.1145/2188286.2188345,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Keznikl, Jaroslav and Koubkov\'{a}, Alena and Podzimek, Andrej and T\r{u}ma, Petr},
title = {Capturing performance assumptions using stochastic performance logic},
year = {2012},
isbn = {9781450312028},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2188286.2188345},
doi = {10.1145/2188286.2188345},
abstract = {Compared to functional unit testing, automated performance testing is difficult, partially because correctness criteria are more difficult to express for performance than for functionality. Where existing approaches rely on absolute bounds on the execution time, we aim to express assertions on code performance in relative, hardware-independent terms. To this end, we introduce Stochastic Performance Logic (SPL), which allows making statements about relative method performance. Since SPL interpretation is based on statistical tests applied to performance measurements, it allows (for a special class of formulas) calculating the minimum probability at which a particular SPL formula holds. We prove basic properties of the logic and present an algorithm for SAT-solver-guided evaluation of SPL formulas, which allows optimizing the number of performance measurements that need to be made. Finally, we propose integration of SPL formulas with Java code using higher-level performance annotations, for performance testing and documentation purposes.},
booktitle = {Proceedings of the 3rd ACM/SPEC International Conference on Performance Engineering},
pages = {311–322},
numpages = {12},
keywords = {performance testing, regression benchmarking},
location = {Boston, Massachusetts, USA},
series = {ICPE '12}
}

@article{10.1007/s11704-016-6192-0,
author = {Karna, Anil Kumar and Chen, Yuting and Yu, Haibo and Zhong, Hao and Zhao, Jianjun},
title = {The role of model checking in software engineering},
year = {2018},
issue_date = {August    2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {4},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-016-6192-0},
doi = {10.1007/s11704-016-6192-0},
abstract = {Model checking is a formal verification technique. It takes an exhaustively strategy to check hardware circuits and network protocols against desired properties. Having been developed for more than three decades, model checking is now playing an important role in software engineering for verifying rather complicated software artifacts.This paper surveys the role of model checking in software engineering. In particular, we searched for the related literatures published at reputed conferences, symposiums, workshops, and journals, and took a survey of (1) various model checking techniques that can be adapted to software development and their implementations, and (2) the use of model checking at different stages of a software development life cycle. We observed that model checking is useful for software debugging, constraint solving, and malware detection, and it can help verify different types of software systems, such as object- and aspect-oriented systems, service-oriented applications, web-based applications, and GUI applications including safety- and mission-critical systems.The survey is expected to help human engineers understand the role of model checking in software engineering, and as well decide which model checking technique(s) and/or tool(s) are applicable for developing, analyzing and verifying a practical software system. For researchers, the survey also points out how model checking has been adapted to their research topics on software engineering and its challenges.},
journal = {Front. Comput. Sci.},
month = aug,
pages = {642–668},
numpages = {27},
keywords = {model checking, software engineering, state-explosion}
}

@inproceedings{10.1145/2088876.2088882,
author = {Huynh, Ngoc-Tho and Phung-Khac, An and Segarra, Maria-Teresa},
title = {Towards reliable distributed reconfiguration},
year = {2011},
isbn = {9781450310703},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2088876.2088882},
doi = {10.1145/2088876.2088882},
abstract = {In component-based software engineering, reconfiguration often refers to the activity of changing a running software system at the component level. Reconfiguration is widely used for evolving and adapting software systems that can not be shut down for update. However, in distributed systems, supporting reconfiguration is a challenging task since a reconfiguration consists of distributed reconfiguration actions that need to be coordinated. Particularly, this task becomes much more challenging in the context of unstable networks where nodes may disconnect frequently, even during reconfiguration. To address this challenge, we propose a platform supporting distributed reconfiguration that embodies a solution for managing system states at reconfiguration time. We define (1) different system states regarding reconfiguration and (2) ways that the system will act accordingly. When a disconnection is detected during a reconfiguration, the system may correct reconfiguration plans to continue the reconfiguration if possible, or recover if the reconfiguration fails.},
booktitle = {Adaptive and Reflective Middleware on Proceedings of the International Workshop},
pages = {36–41},
numpages = {6},
keywords = {component-based software engineering, distributed reconfiguration, reliability},
location = {Lisbon, Portugal},
series = {ARM '11}
}

@inproceedings{10.5555/2041790.2041806,
author = {D\'{\i}az, Jessica and P\'{e}rez, Jennifer and Garbajosa, Juan and Wolf, Alexander L.},
title = {Change impact analysis in product-line architectures},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Change impact analysis is fundamental in software evolution, since it allows one to determine potential effects upon a system resulting from changing requirements. While prior work has generically considered change impact analysis at architectural level, there is a distinct lack of support for the kinds of architectures used to realize software product lines, so-called product-line architectures (PLAs). In particular, prior approaches do not account for variability, a specific characteristic of software product lines. This paper presents a new technique for change impact analysis that targets product-line architectures. We propose to join a traceability-based algorithm and a rule-based inference engine to effectively traverse modeling artifacts that account for variability. In contrast to prior approaches, our technique supports the mechanisms for (i) specifying variability in PLAs, (ii) documenting PLA knowledge, and (iii) tracing variability between requirements and PLAs. We demonstrate our technique by applying it to the analysis of requirements changes in the product-line architecture of a banking system.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {114–129},
numpages = {16},
keywords = {change impact analysis, product-line architectures, product-line evolution},
location = {Essen, Germany},
series = {ECSA'11}
}

@inproceedings{10.1145/2000229.2000235,
author = {Tibermacine, Chouki and Sadou, Salah and Dony, Christophe and Fabresse, Luc},
title = {Component-based specification of software architecture constraints},
year = {2011},
isbn = {9781450307239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000229.2000235},
doi = {10.1145/2000229.2000235},
abstract = {Component-based software engineering provides for developers the ability to easily reuse and assemble software entities to build complex software. Component-based specification of software functional characteristics has been and is largely addressed, however this is not yet the case for what concerns software non-functional characteristics. In this paper, we propose a new way to express component-based software non-functional documentation, and we will focus more specifically on architecture constraints which formalize parts of architecture decisions, as executable, customizable, reusable and composable building blocks represented by components. Checking of architecture constraints is provided via service invocation through ports of a special kind of components, called constraint-components. The signatures of these checking services can be defined in required interfaces of business components, to document decisions taken while designing their architecture. They can also be part of other required interfaces of constraint components, making it possible to build higher-level or more complex constraints while reusing existing ones. We present an example of implementation of constraint components using, an ADL which is introduced in this paper. Architecture constraints can then be checked on the architecture of business components at design-time using the CLACS tool support, which has been implemented as an Eclipse plugin.},
booktitle = {Proceedings of the 14th International ACM Sigsoft Symposium on Component Based Software Engineering},
pages = {31–40},
numpages = {10},
keywords = {architecture constraint, architecture description language, constraint component, constraint reuse and composition},
location = {Boulder, Colorado, USA},
series = {CBSE '11}
}

@article{10.1007/s10664-017-9580-7,
author = {Przyby\l{}ek, Adam},
title = {An empirical study on the impact of AspectJ on software evolvability},
year = {2018},
issue_date = {August    2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9580-7},
doi = {10.1007/s10664-017-9580-7},
abstract = {Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2018–2050},
numpages = {33},
keywords = {AOP, Aspect-oriented programming, Controlled experiment, Maintainability, Separation of concerns, Understandability}
}

@article{10.1504/IJWMC.2014.063054,
author = {Siala, Fatma and Ghedira, Khaled},
title = {How to select dynamically a QoS-driven composite web service by a multi-agent system using CBR method},
year = {2014},
issue_date = {July 2014},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {4},
issn = {1741-1084},
url = {https://doi.org/10.1504/IJWMC.2014.063054},
doi = {10.1504/IJWMC.2014.063054},
abstract = {Service-oriented architecture permits the composition of web services provided with different Quality of Service QoS levels. In a given composition, finding the set of services that optimises some QoS attributes under its constraints is a problem that needs to be solved. Our aim is to propose an intelligent approach to the selection of a Composite Web Service CWS based on QoS. This paper reports the authors' recent research on addressing the issue. An overview on the previously proposed approaches is presented. These approaches correspond to several improvements of an existing multi-agent one, which is well-cited in the specialised literature. Each framework, implemented on JADE Java Agent Development framework, improves another one in terms of CPU time and/or QoS score, to reach a new agent-based and scalable framework. The last framework utilises the agents' ability of negotiation, interaction and cooperation in order to facilitate the selection of composite web services. By using CBR method, the agents can memorise QoS scores and availability. The improvements are related not only to the CPU time but also to the Composite QoS CQoS value, while operating in a dynamic environment and taking into account user preferences.},
journal = {Int. J. Wire. Mob. Comput.},
month = jul,
pages = {327–347},
numpages = {21}
}

@article{10.1016/j.infsof.2019.05.009,
author = {Nashaat, Mona and Ghosh, Aindrila and Miller, James and Quader, Shaikh and Marston, Chad},
title = {M-Lean: An end-to-end development framework for predictive models in B2B scenarios},
year = {2019},
issue_date = {Sep 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {113},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.05.009},
doi = {10.1016/j.infsof.2019.05.009},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {131–145},
numpages = {15},
keywords = {Big data, Machine learning, Business-to-business, User trust, Case study}
}

@inproceedings{10.1145/2994310.2994327,
author = {Mattila, Anna-Liisa and Ihantola, Petri and Kilamo, Terhi and Luoto, Antti and Nurminen, Mikko and V\"{a}\"{a}t\"{a}j\"{a}, Heli},
title = {Software visualization today: systematic literature review},
year = {2016},
isbn = {9781450343671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994310.2994327},
doi = {10.1145/2994310.2994327},
abstract = {Software visualization means visualizing various aspects and artifacts related to software. By this definition a wide range of different software engineering aspects from program comprehension to understanding software process and usage are covered. This paper presents the results of systematic literature review spanning six years of software visualization literature. The main result shows that the most studied topics in the past six years are related to software structure, behavior and evolution. Software process and usage are addressed only in few studies. In the future studying the adoption of software visualization tools in industry context would be beneficial.},
booktitle = {Proceedings of the 20th International Academic Mindtrek Conference},
pages = {262–271},
numpages = {10},
keywords = {human-centered computing, software visualization, systematic literature review},
location = {Tampere, Finland},
series = {AcademicMindtrek '16}
}

@inproceedings{10.1145/1774088.1774562,
author = {K\"{a}tev\"{a}, Janne and Laurinen, Perttu and Rautio, Taneli and Suutala, Jaakko and Tuovinen, Lauri and R\"{o}ning, Juha},
title = {SE-155 DBSA: a device-based software architecture for data mining},
year = {2010},
isbn = {9781605586397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1774088.1774562},
doi = {10.1145/1774088.1774562},
abstract = {In this paper a new architecture for a variety of data mining tasks is introduced. The Device-Based Software Architecture (DBSA) is a highly portable and generic data mining software framework where processing tasks are modeled as components linked together to form a data mining application. The name of the architecture comes from the analogy that each processing task in the framework can be thought of as a device. The framework handles all the devices in the same manner, regardless of whether they have a counterpart in the real world or whether they are just logical devices inside the framework. The DBSA offers many reusable devices, ready to be included in applications, and the application programmer can easily code new devices for the architecture. The framework is bundled with connections to several widely used external tools and languages, making prototyping new applications easy and fast. In the paper we compare DBSA to existing data mining frameworks, review its design and present a case study application implemented with the framework. The paper shows that the DBSA can act as a base for diverse data mining applications.},
booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
pages = {2273–2280},
numpages = {8},
keywords = {data mining, software frameworks},
location = {Sierre, Switzerland},
series = {SAC '10}
}

@inproceedings{10.1145/2600821.2600826,
author = {Lettner, Daniela and Angerer, Florian and Pr\"{a}hofer, Herbert and Gr\"{u}nbacher, Paul},
title = {A case study on software ecosystem characteristics in industrial automation software},
year = {2014},
isbn = {9781450327541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600821.2600826},
doi = {10.1145/2600821.2600826},
abstract = {In software ecosystems (SECOs) both internal and external developers build software solutions for specific market segments based on common technological platforms. Despite a significant body of research on SECOs there is still a need to empirically investigate the characteristics of SECOs in specific industrial environments to understand and improve development processes. In particular, when defining software processes understanding the roles of the participants in the SECO is crucial. This paper thus reports results of an exploratory case study in the industrial automation domain. We explore two research questions on SECO characteristics and discuss research issues we derived from our analyses. While our study confirms key SECO characteristics reported in the literature we also identify additional properties relevant for development processes in the domain of industrial automation.},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
pages = {40–49},
numpages = {10},
keywords = {Software ecosystems, case study, development processes, industrial automation},
location = {Nanjing, China},
series = {ICSSP '14}
}

@article{10.1016/j.compind.2015.03.008,
author = {Sadok, Djamel F.H. and Gomes, Lucas L. and Eisenhauer, Markus and Kelner, Judith},
title = {A middleware for industry},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2015.03.008},
doi = {10.1016/j.compind.2015.03.008},
abstract = {To specify and develop a framework for an industrial middleware with services such as dynamic spectrum management, distributed control logic, object virtualization, a SCADA gateway service, and data fusion transport capability.The middleware considers at the application level, new objects such as shop floor, manufacturing line, welding station, robots, and cells.We discuss the use of wireless technology in a manufacturing environment.We conduct a packet level traffic analysis showing the communication protocols used such environment. This paper describes an innovative distributed framework for monitoring and control of large-scale systems by integrating heterogeneous smart objects, the world of physical devices, sensors and actuators, legacy devices and sub-systems, cooperating to support holistic management 1]. Its featured Service Oriented Architecture (SOA) exposes objects' capabilities by means of web services, thus supporting syntactic and semantic interoperability among different technologies, including SCADA systems 23]. Wireless Sensor and Actuator Network (WSAN) devices and legacy subsystems cooperate while orchestrated by a manager in charge of enforcing a distributed logic. Particularly crafted for industrial networks are new middleware services such as dynamic spectrum management, distributed control logic, object virtualization, WSANs gateways, a SCADA gateway service, and data fusion transport capability. In addition, new application oriented objects such as shop floor, manufacturing line, welding station, robots, and cells have been introduced in the middleware. The combination of such objects and previous modules offers a new and flexible industry oriented middleware. A second contribution is in the form of traffic analysis conducted at the floor level. It shows the dominance of some end systems such as PLCs, the presence well behaved almost constant traffic made up of small packets.},
journal = {Comput. Ind.},
month = aug,
pages = {58–76},
numpages = {19},
keywords = {Computer networks, Industrial networks, Middleware, Wireless sensor networks}
}

@inproceedings{10.1145/1529282.1529701,
author = {Piveta, Eduardo and Pimenta, Marcelo and Ara\'{u}jo, Jo\~{a}o and Moreira, Ana and Guerreiro, Pedro and Price, R. Tom},
title = {Representing refactoring opportunities},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529701},
doi = {10.1145/1529282.1529701},
abstract = {Approaches for the representation of refactoring opportunities and their association with refactorings are usually described in an informal basis. This informality can hamper the creation of catalogues and tools to represent and search for refactoring opportunities. We propose an unified way to represent both the conditions in which the application of a refactoring can be advantageous and the mechanisms to associate these conditions with refactorings. The resulting representation mechanisms can be used to express search criteria based on software metrics, structural problems, heuristics or improvements on the software quality.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {1867–1872},
numpages = {6},
keywords = {refactoring, representation},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@article{10.1504/IJIPT.2016.079546,
author = {Balakrishnan, Senthil Murugan and Sangaiah, Arun Kumar},
title = {Aspect-oriented middleware framework for resolving service discovery issues in Internet of Things},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {2/3},
issn = {1743-8209},
url = {https://doi.org/10.1504/IJIPT.2016.079546},
doi = {10.1504/IJIPT.2016.079546},
abstract = {Internet of Things IoT is a model of future internet and pervasive computing which have its own challenges derived from the internet in terms of scalability, undefined topology and so on. The proposed work aims to resolve the challenges posed by IoT in service discovery functionality. Considering the pervasive and context dependent nature of IoT the planned work bases its development strategy using an aspect oriented software development methodology. The novelty lies in achieving high degree configuration and customisability by selecting subset of middleware functionality depending on the need. The performance is compared with MUSIC pervasive computing middleware and Android built-in configuration. The result reveals 6.5% decrease on average boot up and reconfiguration time for smart phones and 11.6% percentage decrease in Android tablets. In the context of boot up and reconfiguration time the middleware brings out 5% decrease for smart phones and 3% for Android tablets when compared with MUSIC middleware. The middleware shows 6.3% and 4% reduction in execution time of applications on smart phones and tablets when assessed with MUSIC middleware.},
journal = {Int. J. Internet Protoc. Technol.},
month = jan,
pages = {62–78},
numpages = {17},
keywords = {Android tablets, IoT, Spring AOP, aspect-oriented middleware, boot up time, internet of things, pervasive computing, reconfiguration time, service discovery, smartphones}
}

@article{10.1007/s10514-021-09966-9,
author = {Cruz, Nicol\'{a}s and Leiva, Francisco and Ruiz-del-Solar, Javier},
title = {Deep learning applied to humanoid soccer robotics: playing without using any color information},
year = {2021},
issue_date = {Mar 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {3},
issn = {0929-5593},
url = {https://doi.org/10.1007/s10514-021-09966-9},
doi = {10.1007/s10514-021-09966-9},
abstract = {The goal of this paper is to describe a vision system for humanoid robot soccer players that does not use any color information, and whose object detectors are based on the use of convolutional neural networks. The main features of this system are the following: (i) real-time operation in computationally constrained humanoid robots, and (ii) the ability to detect the ball, the pose of the robot players, as well as the goals, lines and other key field features robustly. The proposed vision system is validated in the RoboCup Standard Platform League, where humanoid NAO robots are used. Tests are carried out under realistic and highly demanding game conditions, where very high performance is obtained: a robot detection accuracy of 94.90%, a ball detection accuracy of 97.10%, and a correct determination of the robot orientation 99.88% of the times when the observed robot is static, and 95.52% when the robot is moving.},
journal = {Auton. Robots},
month = mar,
pages = {335–350},
numpages = {16},
keywords = {Soccer robotics, Deep learning, Convolutional neural networks, Robot detection, Ball detection, Robot orientation determination}
}

@inproceedings{10.1145/2509136.2509522,
author = {Bhattacharya, Suparna and Gopinath, Kanchi and Nanda, Mangala Gowri},
title = {Combining concern input with program analysis for bloat detection},
year = {2013},
isbn = {9781450323741},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2509136.2509522},
doi = {10.1145/2509136.2509522},
abstract = {Framework based software tends to get bloated by accumulating optional features (or concerns) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.},
booktitle = {Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &amp; Applications},
pages = {745–764},
numpages = {20},
keywords = {feature oriented programming, program concerns, software bloat},
location = {Indianapolis, Indiana, USA},
series = {OOPSLA '13}
}

@inproceedings{10.1145/3136014.3136027,
author = {Bari\v{s}i\'{c}, Ankica and Blouin, Dominique and Amaral, Vasco and Goul\~{a}o, Miguel},
title = {A requirements engineering approach for usability-driven DSL development},
year = {2017},
isbn = {9781450355254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136014.3136027},
doi = {10.1145/3136014.3136027},
abstract = {There is currently a lack of Requirements Engineering (RE) approaches applied to, or supporting, the development of a Domain-Specific Language (DSL) taking into account the environment in which it is to be used. We present a model-based RE approach to support DSL development with a focus on usability concerns. RDAL is a RE fragment language that can be complemented with other languages to support RE and design. USE-ME is a model driven approach for DSLs usability evaluation which is integrable with a DSL development approach. We combine RDAL and a new DSL, named DSSL, that we created for the specification of DSL-based systems. Integrated with this combination we add USE-ME to support usability evaluation. This combination of existing languages and tools provides a comprehensive RE approach for DSL development. We illustrate the approach with the development of the Gyro DSL for programming robots.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {115–128},
numpages = {14},
keywords = {Domain-Specific language, Requirements engineering, Usability evaluation},
location = {Vancouver, BC, Canada},
series = {SLE 2017}
}

@article{10.1016/j.sysarc.2013.10.003,
author = {Herrera, Fernando and Posadas, H\'{e}ctor and Pe\~{n}il, Pablo and Villar, Eugenio and Ferrero, Francisco and Valencia, Ra\'{u}l and Palermo, Gianluca},
title = {The COMPLEX methodology for UML/MARTE Modeling and design space exploration of embedded systems},
year = {2014},
issue_date = {January, 2014},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {60},
number = {1},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2013.10.003},
doi = {10.1016/j.sysarc.2013.10.003},
abstract = {The design of embedded systems is being challenged by their growing complexity and tight performance requirements. This paper presents the COMPLEX UML/MARTE Design Space Exploration methodology, an approach based on a novel combination of Model Driven Engineering (MDE), Electronic System Level (ESL) and design exploration technologies. The proposed framework enables capturing the set of possible design solutions, that is, the design space, in an abstract, standard and graphical way by relying on UML and the standard MARTE profile. From that UML/MARTE based model, the automated generation framework proposed produces an executable, configurable and fast performance model which includes functional code of the application components. This generated model integrates an XML-based interface for communication with the tool which steers the exploration. This way, the DSE loop iterations are efficiently performed, without user intervention, avoiding slow manual editions, or regeneration of the performance model. The novel DSE suited modelling features of the methodology are shown in detail. The paper also presents the performance model generation framework, including the enhancements with regard the previous simulation and estimation technology, and the exploration technology. The paper uses an EFR vocoder system example for showing the methodology and for demonstrative results.},
journal = {J. Syst. Archit.},
month = jan,
pages = {55–78},
numpages = {24},
keywords = {CBSE, DSE, ESL, Embedded systems, MARTE, MBD, MDE, UML}
}

@article{10.4018/jkm.2010040103,
author = {Kamthan, Pankaj},
title = {A Viewpoint-Based Approach for Understanding the Morphogenesis of Patterns},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {2},
issn = {1548-0666},
url = {https://doi.org/10.4018/jkm.2010040103},
doi = {10.4018/jkm.2010040103},
abstract = {An understanding of knowledge artifacts such as patterns is a necessary prerequisite for any subsequent action. In this article, as an initial step for formulating a theoretical basis for patterns, a conceptual model of primitive viewpoints is proposed and, by exploring one of the viewpoints, a conceptual model for stakeholders of a pattern is presented. This is followed by the description of a conceptual model of a process, namely P3, for the production of patterns. The workflows of P3 highlight, as appropriate, the interface of patterns to humans and/or machines. The implications of the Semantic Web and the Social Web towards P3 are briefly discussed.},
journal = {Int. J. Knowl. Manag.},
month = apr,
pages = {40–65},
numpages = {26},
keywords = {Conceptual Model, Conceptual Reuse, Experiential Knowledge, Knowledge Representation, Process, Web}
}

@article{10.1155/2020/4937049,
author = {An, Jaehyun and Huynh, Kim-Hung and Ha, Yuna and Jung, Heung Su and Kim, Hyung-Mo and Kim, Dong-Min and Kim, Jaehi and Pham, Xuan-Hung and Kim, Dong-Eun and Ho, Jin-Nyoung and Lee, Sangchul and Lee, Ho-Young and Jeong, Dae Hong and Jun, Bong-Hyun and Jin, Xiao},
title = {Surface Modification of a Stable CdSeZnS/ZnS Alloy Quantum Dot for Immunoassay},
year = {2020},
issue_date = {2020},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2020},
issn = {1687-4110},
url = {https://doi.org/10.1155/2020/4937049},
doi = {10.1155/2020/4937049},
abstract = {Quantum dots (QDs) are powerful materials in various bioapplications based on their excellent optical and electronic properties. For the application of various fields of QDs, surface modification of QDs is necessary. However, surface modification in QDs may result in a reduction in quantum yield (QY). This reduction of QY causes many weaknesses in the biological application of QDs. In this study, CdSeZnS/ZnS alloy QDs were used to prepare antibody-conjugated QDs for a sandwich immunoassay. The alloy QDs displayed a QY of 84.5% that was maintained at 83.0% (98.2% of QY was maintained) after surface modification with the anti-rabbit IgG as a model study. Surface-modified QDs successfully detected their corresponding target through antibody-antigen binding. The limit of detection was 1.1\texttimes{}102 ng mL-1 for rabbit IgG.},
journal = {J. Nanomaterials},
month = jan,
numpages = {9}
}

@article{10.1016/j.infsof.2015.04.002,
title = {How have we evaluated software pattern application? A systematic mapping study of research design practices},
year = {2015},
issue_date = {September 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {65},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.04.002},
doi = {10.1016/j.infsof.2015.04.002},
abstract = {ContextSoftware patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings. ObjectiveTo characterize the research design of empirical studies exploring software pattern application involving human participants. MethodWe conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution. ResultsUse of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for 'efficiency' and 'usability' are commonly used to evaluate the problem solving process. While measures for 'completeness', 'correctness' and 'quality' are commonly used to evaluate the final artifact. Overall, 'time to complete a task' is the most frequently used measure, employed in 15 studies to measure 'efficiency'. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity. ConclusionSubtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants' experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {14–38},
numpages = {25}
}

@article{10.1016/j.rcim.2019.101909,
author = {Helo, Petri and Shamsuzzoha, A.H.M.},
title = {Real-time supply chain—A blockchain architecture for project deliveries},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {63},
number = {C},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2019.101909},
doi = {10.1016/j.rcim.2019.101909},
journal = {Robot. Comput.-Integr. Manuf.},
month = jun,
numpages = {14},
keywords = {Supply chain management, Real-time, Blockchain, Key performance indicators}
}

@inproceedings{10.1145/3313151.3313166,
author = {Hartsell, Charles and Mahadevan, Nagabhushan and Ramakrishna, Shreyas and Dubey, Abhishek and Bapty, Theodore and Johnson, Taylor and Koutsoukos, Xenofon and Sztipanovits, Janos and Karsai, Gabor},
title = {Model-based design for CPS with learning-enabled components},
year = {2019},
isbn = {9781450366991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313151.3313166},
doi = {10.1145/3313151.3313166},
abstract = {Recent advances in machine learning led to the appearance of Learning-Enabled Components (LECs) in Cyber-Physical Systems. LECs are being evaluated and used for various, complex functions including perception and control. However, very little tool support is available for design automation in such systems. This paper introduces an integrated toolchain that supports the architectural modeling of CPS with LECs, but also has extensive support for the engineering and integration of LECs, including support for training data collection, LEC training, LEC evaluation and verification, and system software deployment. Additionally, the toolsuite supports the modeling and analysis of safety cases - a critical part of the engineering process for mission and safety critical systems.},
booktitle = {Proceedings of the Workshop on Design Automation for CPS and IoT},
pages = {1–9},
numpages = {9},
keywords = {cyber physical systems, machine learning, model based design},
location = {Montreal, Quebec, Canada},
series = {DESTION '19}
}

@inproceedings{10.5555/1885639.1885649,
author = {Gustavsson, H\r{a}kan and Eklund, Ulrik},
title = {Architecting automotive product lines: industrial practice},
year = {2010},
isbn = {3642155782},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures in the automotive industry. The study has been performed at two internationally well-known companies, one car manufacture and one commercial vehicle manufacture. The results are based on 12 interviews with architects performed at the two companies. The study shows what effect differences such as a strong line organization or a strong project organization has on the architecting process. It also shows what consequence technical choices and business strategy have on the architecting process. Despite the differences the results are surprisingly similar with respect to the process of managing architectural changes as well as the information the architects maintain and update, especially in the light that the companies have had no direct cooperation.},
booktitle = {Proceedings of the 14th International Conference on Software Product Lines: Going Beyond},
pages = {92–105},
numpages = {14},
keywords = {architecting, automotive industry, case study, process},
location = {Jeju Island, South Korea},
series = {SPLC'10}
}

@article{10.1016/j.patrec.2021.08.011,
author = {Mehta, Nancy and Murala, Subrahmanyam},
title = {MSAR-Net: Multi-scale attention based light-weight image super-resolution},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {151},
number = {C},
issn = {0167-8655},
url = {https://doi.org/10.1016/j.patrec.2021.08.011},
doi = {10.1016/j.patrec.2021.08.011},
journal = {Pattern Recogn. Lett.},
month = nov,
pages = {215–221},
numpages = {7},
keywords = {Multi-scale attention residual block, Up and down-sampling projection block, Image super-resolution, 41A05, 41A10, 65D05, 65D17}
}

@inproceedings{10.5555/381473.381599,
author = {Bosch, Jan},
title = {Software product lines and software architecture design},
year = {2001},
isbn = {0769510507},
publisher = {IEEE Computer Society},
address = {USA},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
pages = {717},
location = {Toronto, Ontario, Canada},
series = {ICSE '01}
}

@article{10.1007/s10664-016-9466-0,
author = {Behnamghader, Pooyan and Le, Duc Minh and Garcia, Joshua and Link, Daniel and Shahbazian, Arman and Medvidovic, Nenad},
title = {A large-scale study of architectural evolution in open-source software systems},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9466-0},
doi = {10.1007/s10664-016-9466-0},
abstract = {From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated, sometimes careless changes to a system during its lifespan. Despite decay's prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers' understanding of those changes. In this paper, we take a step toward addressing that scarcity by introducing an architecture recovery framework, ARCADE, for conducting large-scale replicable empirical studies of architectural change across different versions of a software system. ARCADE includes two novel architectural change metrics, which are the key to enabling large-scale empirical studies of architectural change. We utilize ARCADE to conduct an empirical study of changes found in software architectures spanning several hundred versions of 23 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system's architecture during the system's maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system's implementation-level structure as a proxy for its architecture.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1146–1193},
numpages = {48},
keywords = {Architectural change, Architecture recovery, Open-source software, Software architecture, Software evolution}
}

@inproceedings{10.1145/2642803.2642807,
author = {Torjusen, Arild B. and Abie, Habtamu and Paintsil, Ebenezer and Trcek, Denis and Skomedal, \r{A}smund},
title = {Towards Run-Time Verification of Adaptive Security for IoT in eHealth},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642807},
doi = {10.1145/2642803.2642807},
abstract = {This paper integrates run-time verification enablers in the feedback adaptation loop of the ASSET adaptive security framework for Internet of Things (IoT) in the eHealth settings and instantiates the resulting framework with Colored Petri Nets. The run-time enablers make machine-readable formal models of a system state and context available at run-time. In addition, they make requirements that define the objectives of verification available at run-time as formal specifications and enable dynamic context monitoring and adaptation. Run-time adaptive behavior that deviates from the normal mode of operation of the system represents a major threat to the sustainability of critical eHealth services. Therefore, the integration of run-time enablers into the ASSET adaptive framework could lead to a sustainable security framework for IoT in eHealth.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {4},
numpages = {8},
keywords = {Adaptive Security, Formal Run-time Verification, IoT, eHealth},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@article{10.1007/s11227-020-03268-0,
author = {Dehraj, Pooja and Sharma, Arun},
title = {A review on architecture and models for autonomic software systems},
year = {2021},
issue_date = {Jan 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {77},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-020-03268-0},
doi = {10.1007/s11227-020-03268-0},
abstract = {Autonomic computing was the term coined by IBM in 2001. The term autonomic computing was used to define the self-adaptable nature of the human body. According to IBM, the same self-adaptable feature was the need to be incorporated in the software systems. Autonomic computing is the combination of few self-capabilities such as self-configuration, self-healing, self-optimization, self-protection, self-awareness, etc. So, autonomic computing approach was then used to develop autonomic software systems. This approach makes the computing systems self-adaptable and self-decision-making support systems for various activities. It also helps to reduce the human intervention in the software management process. Though, the implementation of autonomic self-capabilities may increase the software complexity, which further requires human intervention for the software maintenance-related specific tasks. Still, IT industries are approaching to develop autonomic features in their existing architecture or developing new self-adaptable software systems. Autonomic computing has its importance for providing a bridge for handling and managing the run-time computation-based issues/exceptions of the software. So, the discussion of this solution has become a necessity for making the vision of autonomic decision making more clear and understandable for researchers and developers for the improvement in an autonomic area. The paper provides an insight vision of the autonomic decision-making concept and its importance for the various purposes such as intrusion detection, cloud-based data security, wireless sensor network, Internet of Things, Big Data and many other areas where management cannot be handled by a human in real time. To assess the degree of autonomic feature, there is another term used which is known as autonomicity. The paper also discusses some solutions suggested and implemented by different researchers during their studies for estimating the system’s autonomicity level. These solutions will help in comparing different autonomic applications based on the autonomic features implemented in each application. This paper is an attempt to provide better understandability in the autonomic computational field.},
journal = {J. Supercomput.},
month = jan,
pages = {388–417},
numpages = {30},
keywords = {Autonomic computing, Self-configuration, Self-healing, Self-optimization, Self-protection, System autonomicity level}
}

@article{10.1007/s11263-019-01236-7,
author = {Gupta, Saurabh and Tolani, Varun and Davidson, James and Levine, Sergey and Sukthankar, Rahul and Malik, Jitendra},
title = {Cognitive Mapping and Planning for Visual Navigation},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {128},
number = {5},
issn = {0920-5691},
url = {https://doi.org/10.1007/s11263-019-01236-7},
doi = {10.1007/s11263-019-01236-7},
abstract = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: (a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and (b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as “going to a chair”. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.},
journal = {Int. J. Comput. Vision},
month = may,
pages = {1311–1330},
numpages = {20},
keywords = {Visual navigation, Spatial representations, Learning for navigation}
}

@article{10.1007/s11219-013-9202-6,
author = {Nguyen, Tuong Huan and Vo, Bao Quoc and Lumpe, Markus and Grundy, John},
title = {KBRE: a framework for knowledge-based requirements engineering},
year = {2014},
issue_date = {March     2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9202-6},
doi = {10.1007/s11219-013-9202-6},
abstract = {Detecting inconsistencies is a critical part of requirements engineering (RE) and has been a topic of interest for several decades. Domain knowledge and semantics of requirements not only play important roles in elaborating requirements but are also a crucial way to detect conflicts among them. In this paper, we present a novel knowledge-based RE framework (KBRE) in which domain knowledge and semantics of requirements are central to elaboration, structuring, and management of captured requirements. Moreover, we also show how they facilitate the identification of requirements inconsistencies and other-related problems. In our KBRE model, description logic (DL) is used as the fundamental logical system for requirements analysis and reasoning. In addition, the application of DL in the form of Manchester OWL Syntax brings simplicity to the formalization of requirements while preserving sufficient expressive power. A tool has been developed and applied to an industrial use case to validate our approach.},
journal = {Software Quality Journal},
month = mar,
pages = {87–119},
numpages = {33},
keywords = {Description logics, Identification, Inconsistencies, Manchester OWL Syntax, Ontology, Requirements engineering}
}

@article{10.1007/s10270-012-0308-2,
author = {Farias, Kleinner and Garcia, Alessandro and Lucena, Carlos},
title = {Effects of stability on model composition effort: an exploratory study},
year = {2014},
issue_date = {October   2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0308-2},
doi = {10.1007/s10270-012-0308-2},
abstract = {Model composition plays a central role in many software engineering activities, e.g., evolving design models to add new features. To support these activities, developers usually rely on model composition heuristics. The problem is that the models to-be-composed usually conflict with each other in several ways and such composition heuristics might be unable to properly deal with all emerging conflicts. Hence, the composed model may bear some syntactic and semantic inconsistencies that should be resolved. As a result, the production of the intended model is an error-prone and effort-consuming task. It is often the case that developers end up examining all parts of the output composed model instead of prioritizing the most critical ones, i.e., those that are likely to be inconsistent with the intended model. Unfortunately, little is known about indicators that help developers (1) to identify which model is more likely to exhibit inconsistencies, and (2) to understand which composed models require more effort to be invested. It is often claimed that software systems remaining stable over time tends to have a lower number of defects and require less effort to be fixed than unstable systems. However, little is known about the effects of software stability in the context of model evolution when supported by composition heuristics. This paper, therefore, presents an exploratory study analyzing stability as an indicator of inconsistency rate and resolution effort on model composition activities. Our findings are derived from 180 compositions performed to evolve design models of three software product lines. Our initial results, supported by statistical tests, also indicate which types of changes led to lower inconsistency rate and lower resolution effort.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1473–1494},
numpages = {22},
keywords = {Design stability, Model composition, Software development effort}
}

@article{10.1016/j.scico.2009.05.003,
author = {Lung, Chung-Horng and Rajeswaran, Pragash and Sivadas, Sathyanarayanan and Sivabalasingam, Theleepan},
title = {Experience of building an architecture-based generator using GenVoca for distributed systems},
year = {2010},
issue_date = {August, 2010},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {75},
number = {8},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2009.05.003},
doi = {10.1016/j.scico.2009.05.003},
abstract = {Selecting the architecture that meets the requirements, both functional and non-functional, is a challenging task, especially at the early stage when more uncertainties exist. Architectural prototyping is a useful approach in supporting the evaluation of alternative architectures and balancing different architectural qualities. Generative programming has gained increasing attention, but it mostly deals with lower-level artifacts; hence, it usually supports lower degrees of software automation. This paper proposes an architecture-centric generative approach in facilitating architectural prototyping and evaluation. We also present our empirical experience in raising the level of abstraction to the architecture layer for distributed and concurrent systems using GenVoca. GenVoca is a generative programming approach that is used here to support the generation or instantiation of a particular architectural pattern in distributed computing based on user's selection. As a result, it can support rapid architectural prototyping and evaluation of both functional and non-functional requirements and encourage greater degrees of software automation and reuse. Lessons learned from the empirical study are also reported and could be applied to other areas.},
journal = {Sci. Comput. Program.},
month = aug,
pages = {672–688},
numpages = {17},
keywords = {Distributed systems, GenVoca, Generative programming, Prototyping, Software architecture, Software patterns}
}

@inproceedings{10.1145/3444950.3447285,
author = {Menard, Christian and Goens, Andr\'{e}s and Hempel, Gerald and Khasanov, Robert and Robledo, Julian and Teweleitt, Felix and Castrillon, Jeronimo},
title = {Mocasin—Rapid Prototyping of Rapid Prototyping Tools: A Framework for Exploring New Approaches in Mapping Software to Heterogeneous Multi-cores},
year = {2021},
isbn = {9781450389525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444950.3447285},
doi = {10.1145/3444950.3447285},
abstract = {We present Mocasin, an open-source rapid prototyping framework for researching, implementing and validating new algorithms and solutions in the field of mapping software to heterogeneous multi-cores. In contrast to the many existing tools that often specialize for a particular use-case, Mocasin is an open, flexible and generic research environment that abstracts over the approaches taken by other tools. Mocasin is designed to support a wide range of models of computation and input formats, implements manifold mapping strategies and provides an adjustable high-level simulator for performance estimation. This infrastructure serves as a flexible vehicle for exploring new approaches and as a blueprint for building customized tools. We highlight the key design aspects of Mocasin that enable its flexibility and illustrate its capabilities in a case-study showing how Mocasin can be used for building a customized tool for researching runtime mapping strategies in an LTE uplink receiver.},
booktitle = {Proceedings of the 2021 Drone Systems Engineering and Rapid Simulation and Performance Evaluation: Methods and Tools Proceedings},
pages = {66–73},
numpages = {8},
location = {Budapest, Hungary},
series = {DroneSE and RAPIDO '21}
}

@article{10.1145/1668862.1668863,
author = {Alvaro, Alexandre and Santana de Almeida, Eduardo and Romero de Lemos Meira, Silvio},
title = {A software component quality framework},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1668862.1668863},
doi = {10.1145/1668862.1668863},
abstract = {One of the major problems with Component-Based Software Engineering (CBSE) is the quality of the components used in a system. The reliability of a component-based software system depends on the reliability of the components that is made of. In CBSE, the proper search, selection and evaluation process of components is considered the cornerstone for the development of any effective component-based system. So far the software industry was concentrated on the functional aspects of components, leaving aside the difficult task of assessing their quality. In this way, we propose a software component quality framework to evaluate the quality of software components in an efficient way. Moreover, an experimental study was accomplished in order to evaluate the viability of the proposed framework.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–18},
numpages = {18}
}

@article{10.1016/j.future.2015.03.006,
author = {Garc\'{\i}a-Gal\'{a}n, Jes\'{u}s and Trinidad, Pablo and Rana, Omer F. and Ruiz-Cort\'{e}s, Antonio},
title = {Automated configuration support for infrastructure migration to the cloud},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {55},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2015.03.006},
doi = {10.1016/j.future.2015.03.006},
abstract = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration-in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. We support the decision making in migration planning to the cloud.We use Feature Models to describe the configuration space of an IaaS.We automate the search of the most suitable IaaS configuration.Our approach improves the results of commercial applications on Amazon EC2.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {200–212},
numpages = {13},
keywords = {Automated analysis, Cloud migration, EC2, Feature model, IaaS}
}

@inproceedings{10.1145/2108616.2108654,
author = {La, Hyun Jung and Oh, Sang Hun and Kim, Soo Dong},
title = {Methods to utilizing cloud computing in developing mobile internet device (MID) applications},
year = {2010},
isbn = {9781605588933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2108616.2108654},
doi = {10.1145/2108616.2108654},
abstract = {Cloud Computing (CC) is emerged as an effective reuse paradigm, where software functionality, hardware computing power, and other computing resources are delivered in the form of service. Mobile Internet Device (MID), as a portable handheld device, becomes a strong candidate for client-side computing. MID has a limited resource and computing power, hence, it is not deploy the applications which require complex computation and large amount of resources. The MID feature of limited resource forbids deploying and running complex software application on MID, and a key concept of CC is to deploy all the computing resources are placed on the provider side. Hence, by applying CC concepts to the MID environment, disadvantages derived from limited resource can be overcome. Therefore, we first identify key features of MID applications. To overcome a drawback of MID, limited resource, we present our justification of applying CC to MID environment and key methods which are raised in applying CC to MID environment.},
booktitle = {Proceedings of the 4th International Conference on Uniquitous Information Management and Communication},
articleno = {31},
numpages = {9},
keywords = {cloud computing, mobile internet device},
location = {Suwon, Republic of Korea},
series = {ICUIMC '10}
}

@article{10.1016/j.cl.2018.01.002,
author = {Braz, Larissa and Gheyi, Rohit and Mongiovi, Melina and Ribeiro, M\'{a}rcio and Medeiros, Fl\'{a}vio and Teixeira, Leopoldo and Souto, Sabrina},
title = {A change-aware per-file analysis to compile configurable systems with #ifdefs      },
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {54},
number = {C},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2018.01.002},
doi = {10.1016/j.cl.2018.01.002},
journal = {Comput. Lang. Syst. Struct.},
month = dec,
pages = {427–450},
numpages = {24},
keywords = {Compilation, #ifdef, Configurable systems, Impact analysis}
}

@inproceedings{10.1145/2480362.2480567,
author = {Durelli, Rafael S. and Santib\'{a}\~{n}ez, Daniel S. M. and Anquetil, Nicolas and Delamaro, M\'{a}rcio E. and de Camargo, Valter Vieira},
title = {A systematic review on mining techniques for crosscutting concerns},
year = {2013},
isbn = {9781450316569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2480362.2480567},
doi = {10.1145/2480362.2480567},
abstract = {&lt;u&gt;Background:&lt;/u&gt; The several maintenance tasks a system is submitted during its life usually cause its architecture deviates from the original conceivable design, ending up with scattered and tangled concerns across the software. The research area named concern mining attempts to identify such scattered and tangled concerns to support maintenance and reverse-engineering. &lt;u&gt;Objectives:&lt;/u&gt; The aim of this paper is threefold: (i) identifying techniques employed in this research area, (ii) extending a taxonomy available on the literature and (iii) recommending an initial combination of some techniques. &lt;u&gt;Results:&lt;/u&gt; We selected 62 papers by their mining technique. Among these papers, we identified 18 mining techniques for crosscutting concern. Based on these techniques, we have extended a taxonomy available in the literature, which can be used to position each new technique, and to compare it with the existing ones along relevant dimensions. As consequence, we present some combinations of these techniques taking into account high values of precision and recall that could improve the identification of both Persistence and Observer concerns. The combination that we recommend may serve as a roadmap to potential users of mining techniques for crosscutting concerns.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on Applied Computing},
pages = {1080–1087},
numpages = {8},
keywords = {aspect mining, concern mining, cross-cutting concerns, systematic review},
location = {Coimbra, Portugal},
series = {SAC '13}
}

@inproceedings{10.1007/978-3-642-33666-9_6,
author = {Alf\'{e}rez, Germ\'{a}n H. and Pelechano, Vicente},
title = {Dynamic evolution of context-aware systems with models at runtime},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_6},
doi = {10.1007/978-3-642-33666-9_6},
abstract = {Model-driven techniques have proven to yield significant benefits for context-aware systems. Specifically, semantically-rich models are used at runtime to monitor the system context and guide necessary changes. Under the closed-world assumption, adaptations are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations that may arise in uncertain and complex contexts. In this paper, we present a model-based framework to support the dynamic evolution of context-aware systems to deal with unexpected context events in the open world. If model adaptations are not enough to solve uncertainty, our model-based evolution planner guides the evolution of the supporting models to preserve high-level requirements. A case study about a context-aware Web service composition, which is executed in a distributed computing infrastructure, illustrates the applicability of our framework. A realization methodology and a prototype system support our approach.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {70–86},
numpages = {17},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@article{10.1016/j.jss.2006.05.024,
author = {Hofmeister, Christine and Kruchten, Philippe and Nord, Robert L. and Obbink, Henk and Ran, Alexander and America, Pierre},
title = {A general model of software architecture design derived from five industrial approaches},
year = {2007},
issue_date = {January, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.05.024},
doi = {10.1016/j.jss.2006.05.024},
abstract = {We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the ''ideal'' pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons.},
journal = {J. Syst. Softw.},
month = jan,
pages = {106–126},
numpages = {21},
keywords = {Architectural method, Software architecture, Software architecture analysis, Software architecture design}
}

@article{10.1016/j.infsof.2017.03.011,
author = {Martnez-Fernndez, Silverio and Ayala, Claudia P. and Franch, Xavier and Marques, Helena Martins},
title = {Benefits and drawbacks of software reference architectures},
year = {2017},
issue_date = {August 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {88},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.03.011},
doi = {10.1016/j.infsof.2017.03.011},
abstract = {ContextSoftware Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with SRAs in industrial practice. ObjectiveTo investigate the current industrial practice of SRAs in a single company from the perspective of different stakeholders. MethodAn exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine SRAs designed by a multinational software consulting company. ResultsThe study shows the perceptions of different stakeholders regarding the benefits and drawbacks of SRAs (e.g., both SRA designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the SRA benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of SRAs (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the SRA are managed inappropriately. ConclusionsThis study aims to help practitioners and researchers to better understand real SRAs projects and the contexts where these benefits and drawbacks appeared, as well as some SRA improvement strategies. This would contribute to strengthening the evidence regarding SRAs and support practitioners in making better informed decisions about the expected SRA benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature SRA research and practice.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {37–52},
numpages = {16},
keywords = {Benefits, Case study, Drawbacks, Empirical software engineering, Reference architecture, Software architecture}
}

@article{10.1145/3104028,
author = {Pahl, Claus and Jamshidi, Pooyan and Zimmermann, Olaf},
title = {Architectural Principles for Cloud Software},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3104028},
doi = {10.1145/3104028},
abstract = {A cloud is a distributed Internet-based software system providing resources as tiered services. Through service-orientation and virtualization for resource provisioning, cloud applications can be deployed and managed dynamically. We discuss the building blocks of an architectural style for cloud-based software systems. We capture style-defining architectural principles and patterns for control-theoretic, model-based architectures for cloud software. While service orientation is agreed on in the form of service-oriented architecture and microservices, challenges resulting from multi-tiered, distributed and heterogeneous cloud architectures cause uncertainty that has not been sufficiently addressed. We define principles and patterns needed for effective development and operation of adaptive cloud-native systems.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {17},
numpages = {23},
keywords = {Cloud computing, adaptive system, architectural style, cloud-native, control theory, devops, microservice, model-based controller, software architecture, uncertainty}
}

@article{10.1016/j.procs.2020.03.423,
author = {Challagidad, Praveen S. and Birje, Mahantesh N.},
title = {Efficient Multi-authority Access Control using Attribute-based Encryption in Cloud Storage},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.423},
doi = {10.1016/j.procs.2020.03.423},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {840–849},
numpages = {10},
keywords = {Attribute-based Encryption, Access Structure, Access Control, Privileged User, Multi-authority, Cloud Storage}
}

@article{10.1016/j.infsof.2015.07.005,
author = {Martini, Antonio and Bosch, Jan and Chaudron, Michel},
title = {Investigating Architectural Technical Debt accumulation and refactoring over time},
year = {2015},
issue_date = {November 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.07.005},
doi = {10.1016/j.infsof.2015.07.005},
abstract = {Display Omitted We provide a taxonomy of the causes for Architectural Technical Debt accumulation.Crisis model: shows the increasing accumulation of Architectural Technical Debt.Phases model: shows when Architectural Technical Debt is accumulated and refactored.Refactoring strategies: best and worst case scenarios with respect to crises.We conduct an empirical evaluation of the factors and models. ContextA known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility. ObjectiveThis paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time. MethodWe conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models. ResultsWe compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research. ConclusionSeveral factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {237–253},
numpages = {17},
keywords = {Agile software development, Architectural Technical Debt, Qualitative model, Software architecture, Software life-cycle, Software management}
}

@inproceedings{10.1145/2973839.2973848,
author = {Cedrim, Diego and Sousa, Leonardo and Garcia, Alessandro and Gheyi, Rohit},
title = {Does refactoring improve software structural quality? A longitudinal study of 25 projects},
year = {2016},
isbn = {9781450342018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2973839.2973848},
doi = {10.1145/2973839.2973848},
abstract = {Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. Refactoring is widely practiced by developers, and considerable development effort has been invested in refactoring tooling support. There is an explicit assumption that software refactoring improves the structural quality of a program by reducing its density of code smells. However, little has been reported about whether and to what extent developers successfully remove code smells through refactoring. This paper reports a first longitudinal study intended to address this gap. We analyze how often the commonly-used refactoring types affect the density of 5 types of code smells along the version histories of 25 projects. Our findings are based on the analysis of 2,635 refactorings distributed in 11 different types. Surprisingly, 2,506 refactorings (95.1%) did not reduce or introduce code smells. Thus, these findings suggest that refactorings lead to smell reduction less often than what has been reported. According to our data, only 2.24% of refactoring changes removed code smells and 2.66% introduced new ones. Moreover, several smells induced by refactoring tended to live long, i.e., 146 days on average. These smells were only eventually removed when smelly elements started to exhibit poor structural quality and, as a consequence, started to be more costly to get rid of.},
booktitle = {Proceedings of the XXX Brazilian Symposium on Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Code Smells, Refactoring, Structural Quality},
location = {Maring\'{a}, Brazil},
series = {SBES '16}
}

@inbook{10.5555/1985596.1985604,
author = {Brennan, Shane and Fritsch, Serena and Liu, Yu and Sterritt, Ashley and Fox, Jorge and Linehan, \'{E}amonn and Driver, Cormac and Meier, Ren\'{e} and Cahill, Vinny and Harrison, William and Clarke, Siobh\'{a}n},
title = {A framework for flexible and dependable service-oriented embedded systems},
year = {2010},
isbn = {364217244X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The continued development and deployment of distributed, real-time embedded systems technologies in recent years has resulted in a multitude of ecosystems in which service-oriented embedded systems can now be realised. Such ecosystems are often exposed to dynamic changes in user requirements, environmental conditions and network topologies that require service-oriented embedded systems to evolve at runtime. This paper presents a framework for service-oriented embedded systems that can dynamically adapt to changing conditions at runtime. Supported by model-driven development techniques, the framework facilitates lightweight dynamic service composition in embedded systems while predicting the temporal nature of unforeseen service assemblies and coping with adverse feature interactions following dynamic service composition. This minimises the complexity of evolving software where services are deployed dynamically and ultimately, enables flexible and dependable service-oriented embedded systems.},
booktitle = {Architecting Dependable Systems VII},
pages = {123–145},
numpages = {23}
}

@article{10.1007/s10270-011-0219-7,
author = {Mohagheghi, Parastoo and Gilani, Wasif and Stefanescu, Alin and Fernandez, Miguel A. and Nordmoen, Bj\o{}rn and Fritzsche, Mathias},
title = {Where does model-driven engineering help? Experiences from three industrial cases},
year = {2013},
issue_date = {July      2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-011-0219-7},
doi = {10.1007/s10270-011-0219-7},
abstract = {There have been few experience reports from industry on how Model-Driven Engineering (MDE) is applied and what the benefits are. This paper summarizes the experiences of three large industrial participants in a European research project with the objective of developing techniques and tools for applying MDE on the development of large and complex software systems. The participants had varying degrees of previous experience with MDE. They found MDE to be particularly useful for providing abstractions of complex systems at multiple levels or from different viewpoints, for the development of domain-specific models that facilitate communication with non-technical experts, for the purposes of simulation and testing, and for the consumption of models for analysis, such as performance-related decision support and system design improvements. From the industrial perspective, a methodology is considered to be useful and cost-efficient if it is possible to reuse solutions in multiple projects or products. However, developing reusable solutions required extra effort and sometimes had a negative impact on the performance of tools. While the companies identified several benefits of MDE, merging different tools with one another in a seamless development environment required several transformations, which increased the required implementation effort and complexity. Additionally, user-friendliness of tools and the provision of features for managing models of complex systems were identified as crucial for a wider industrial adoption of MDE.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {619–639},
numpages = {21},
keywords = {Complex systems, Domain-specific language, Eclipse, Experience report, Model-driven engineering, Simulation}
}

@article{10.1016/j.infsof.2007.12.002,
author = {Koning, Michiel and Sun, Chang-ai and Sinnema, Marco and Avgeriou, Paris},
title = {VxBPEL: Supporting variability for Web services in BPEL},
year = {2009},
issue_date = {February, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.12.002},
doi = {10.1016/j.infsof.2007.12.002},
abstract = {Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {258–269},
numpages = {12},
keywords = {Business Process Execution Language, Service-based system, Variability, Web service}
}

@inproceedings{10.5555/1789757.1789774,
author = {Li, Juan and Hou, Lishan and Qin, Zhongsen and Wang, Qing and Chen, Guisheng},
title = {An empirically-based process to improve the practice of requirement review},
year = {2008},
isbn = {3540795871},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Requirement quality serves as the basis of the whole software development.How to improve and assure the quality of requirements is one of themost difficult issues. Aiming to improve requirement review in a softwarecompany, we propose a role-based requirement review process based on theidea that requirement quality should meet needs of all roles involved to thelargest extent, and not only determined by the number of defects found in requirements.This process helps reviewers focus on their own concerns and findmore defects related to their tasks, and provides a quantitative method to analyzeand evaluate the quality of the requirement document. We also provide acase study to illustrate the new process and report some preliminary results.},
booktitle = {Proceedings of the Software Process, 2008 International Conference on Making Globally Distributed Software Development a Success Story},
pages = {135–146},
numpages = {12},
keywords = {quantitative, requirement quality characteristic, requirement review, role-based},
location = {Leipzig, Germany},
series = {ICSP'08}
}

@inproceedings{10.5555/1939345.1939377,
author = {Autili, Marco and Chilton, Chris and Inverardi, Paola and Kwiatkowska, Marta and Tivoli, Massimo},
title = {Towards a connector algebra},
year = {2010},
isbn = {3642165605},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Interoperability of heterogeneous networked systems has yet to reach the maturity required by ubiquitous computing due to the technology-dependent nature of solutions. The CONNECT Integrated Project attempts to develop a novel network infrastructure to allow heterogeneous networked systems to freely communicate with one another by synthesising the required connectors on-the-fly. A key objective of CONNECT is to build a comprehensive theory of composable connectors, by devising an algebra for rigorously characterising complex interaction protocols in order to support automated reasoning. With this aim in mind, we formalise a high-level algebra for reasoning about protocol mismatches. Basic mismatches can be solved by suitably defined primitives, while complex mismatches can be settled by composition operators that build connectors out of simpler ones. The semantics of the algebra is given in terms of Interface Automata, and an example in the domain of instant messaging is used to illustrate how the algebra can characterise the interaction behaviour of a connector for mediating protocols.},
booktitle = {Proceedings of the 4th International Conference on Leveraging Applications of Formal Methods, Verification, and Validation - Volume Part II},
pages = {278–292},
numpages = {15},
location = {Heraklion, Crete, Greece},
series = {ISoLA'10}
}

@inproceedings{10.1109/MODELS-C.2019.00090,
author = {Mendoza, Camilo and Garc\'{e}s, Kelly and Casallas, Rubby and Bocanegra, Jos\'{e}},
title = {Detecting architectural issues during the continuous integration pipeline},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00090},
doi = {10.1109/MODELS-C.2019.00090},
abstract = {The use of a software reference architecture limits possible deviations and errors in the implementation of software projects, as the code must follow predefined rules that developers must respect to guarantee quality. However, when introducing new code to projects these rules can be violated. As a result, architectural erosion, bad smells, or even bugs that can be difficult to find are introduced to the projects. This paper proposes an approach for reviewing compliance to predefined rules that map architectural decisions to code. During the continuous integration process, the automatic analysis raises an issue for each rule violation. Developers can analyze and correct issues, and trace/visualize improvements, or lack thereof, through time. We present a validation experiment carried out in the context of a Software Development course, and we show how the approach helps developers to write better code1.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {589–597},
numpages = {9},
keywords = {architectural rules, continuous integration, issue identification, issue visualization, rule violation},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@article{10.1007/s10270-013-0358-0,
author = {Ab. Rahim, Lukman and Whittle, Jon},
title = {A survey of approaches for verifying model transformations},
year = {2015},
issue_date = {May       2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0358-0},
doi = {10.1007/s10270-013-0358-0},
abstract = {As with other software development artifacts, model transformations are not bug-free and so must be systematically verified. Their nature, however, means that transformations require specialist verification techniques. This paper brings together current research on model transformation verification by classifying existing approaches along two dimensions. Firstly, we present a coarse-grained classification based on the technical details of the approach (e.g., testing, theorem proving, model checking). Secondly, we present a finer-grained classification which categorizes approaches according to criteria such as level of formality, transformation language, properties verified. The purpose of the survey is to bring together research in model transformation verification to act as a resource for the community. Furthermore, based on the survey, we identify a number of trends in current and past research on model transformation verification.},
journal = {Softw. Syst. Model.},
month = may,
pages = {1003–1028},
numpages = {26},
keywords = {Model transformations, Survey, Verification}
}

@article{10.1145/979743.979745,
author = {ACM SIGSOFT Software Engineering Notes staff},
title = {Back matter (abstracts and calendar)},
year = {2004},
issue_date = {March 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/979743.979745},
doi = {10.1145/979743.979745},
journal = {SIGSOFT Softw. Eng. Notes},
month = mar,
pages = {27–62},
numpages = {36}
}

@inproceedings{10.1109/SHARK.2009.5069114,
author = {Unphon, Hataichanok},
title = {Making use of architecture throughout the software life cycle - How the build hierarchy can facilitate product line development},
year = {2009},
isbn = {9781424437269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SHARK.2009.5069114},
doi = {10.1109/SHARK.2009.5069114},
abstract = {This paper presents an empirical study of how the application of genuine architecture can be employed beyond the design phase of product line development. The study is based on a co-operative research project with a company developing product line architecture for hydraulic modelling software. By concretising the architecture as a build hierarchy the architecture mediates the evolution of the design throughout the whole software life cycle. The empirical evidence has confirmed the improvements of (1) the software quality and flexibility, (2) the communication and cooperation with new developers, (3) the distribution of work and parallel implementation, and (4) the foreseen usage by hydraulic and environmental consultants who tailor the software. Our research further indicates requirements for the architectural analysis tools that are deliberately embedded in the daily development practices.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Sharing and Reusing Architectural Knowledge},
pages = {41–48},
numpages = {8},
series = {SHARK '09}
}

@article{10.1016/j.infsof.2011.06.001,
author = {La, Hyun Jung and Kim, Soo Dong},
title = {Static and dynamic adaptations for service-based systems},
year = {2011},
issue_date = {December, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.06.001},
doi = {10.1016/j.infsof.2011.06.001},
abstract = {Context: In service-oriented computing (SOC), service providers publish reusable services, and service consumers subscribe them. However, there exist potential problems in reusing services. Mismatch is a problem that occurs when a candidate service does not fully match to the feature expected. Fault is a problem that occurs when an invocation of services results in some abnormality at runtime. Without remedying mismatch problems, services would not be reusable. Without remedying fault problems, service invocations at runtime would result in failures. Static and dynamic adaptations are practical approaches to remedying the problems. Objective: Our objective is to define a comprehensive framework which includes a design of service adaptation framework (SAF), and design of static and dynamic adapters. Method: We design the SAF which governs dynamic adaptations, and define a service life-cycle with adaptation-related activities. Based on causal-effect relationships among mismatch, fault, cause, and adapter, we derive mismatches and faults, from which their relevant causes are identified. For the causes, we define six static adapters and five dynamic adapters. We specify instructions for designing static adapters, and provide step-wise algorithms for designing dynamic adapters based on enterprise service bus (ESB). And, we show a proof-of-concept (POC) of implementation to show applicability of the methods. Results: The paper presents service life-cycle with adaptation-related activities, SAF design, and design of static and dynamic adapters. Conclusion: Mismatch and fault problems in utilizing services present threats to high reusability of services. Static adaptations can remedy mismatch problems, and dynamic adaptations can remedy fault problems. In this paper, we presented technical insights of service adaption, SAF design, and definitions of static and dynamic adapters. By utilizing the proposed SAF and service adapters, reusability of services can be greatly enhanced.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1275–1296},
numpages = {22},
keywords = {Dynamic adaptation, Fault, Mismatch problem, Service adaptation, Static adaptation}
}

@article{10.1016/j.jss.2016.03.038,
author = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
title = {Extracting finite state models from i* models},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.03.038},
doi = {10.1016/j.jss.2016.03.038},
abstract = {The Naive Algorithm (NA) extracts all possible finite state models from i* models.We observe an explosion in the finite state model space.The Semantic Implosion Algorithm (SIA) is a solution to this explosion problem.NA and SIA are extensively simulated on different categories of i* models.SIA drastically reduces the model space growth from O(1020) (for NA) to O(103). i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
journal = {J. Syst. Softw.},
month = nov,
pages = {265–280},
numpages = {16},
keywords = {Model checking, Model transformation, i* model}
}

@article{10.1016/j.datak.2014.07.003,
author = {Bre\ss{}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
title = {Load-aware inter-co-processor parallelism in database query processing},
year = {2014},
issue_date = {September 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {93},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2014.07.003},
doi = {10.1016/j.datak.2014.07.003},
abstract = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. Contribute heuristics to enhance performance by exploiting inter-device parallelismHeuristics consider load and speed on (co-)processors.Extensive evaluation on four use cases: aggregation, selection, sort, and joinAssess the performance of best heuristic for systems with multiple co-processorsDiscuss how operator-stream-based scheduling can be used in a query processor},
journal = {Data Knowl. Eng.},
month = sep,
pages = {60–79},
numpages = {20},
keywords = {Co-processing, Query optimization, Query processing}
}

@article{10.1007/s13748-020-00205-3,
author = {Ram\'{\i}rez, Aurora and Delgado-P\'{e}rez, Pedro and Ferrer, Javier and Romero, Jos\'{e} Ra\'{u}l and Medina-Bulo, Inmaculada and Chicano, Francisco},
title = {A systematic literature review of the SBSE research community in Spain},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {9},
number = {2},
url = {https://doi.org/10.1007/s13748-020-00205-3},
doi = {10.1007/s13748-020-00205-3},
abstract = {Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation.},
journal = {Prog. in Artif. Intell.},
month = jun,
pages = {113–128},
numpages = {16},
keywords = {Search-based software engineering, Systematic review, Research trends, Spanish community}
}

@inproceedings{10.1145/1083063.1083075,
author = {Lapouchnian, Alexei and Liaskos, Sotirios and Mylopoulos, John and Yu, Yijun},
title = {Towards requirements-driven autonomic systems design},
year = {2005},
isbn = {1595930396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083063.1083075},
doi = {10.1145/1083063.1083075},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying and analyzing alternative ways of how the main objectives of the system can be achieved and designing a system that supports all of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and sketches a possible architecture for autonomic systems that can be built using the this approach.},
booktitle = {Proceedings of the 2005 Workshop on Design and Evolution of Autonomic Application Software},
pages = {1–7},
numpages = {7},
keywords = {autonomic computing software customization, goal-oriented requirements engineering, self-management, software variability},
location = {St. Louis, Missouri},
series = {DEAS '05}
}

@inproceedings{10.1007/11754305_34,
author = {Kiebusch, Sebastian and Franczyk, Bogdan and Speck, Andreas},
title = {Process-Family-Points},
year = {2006},
isbn = {3540341994},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11754305_34},
doi = {10.1007/11754305_34},
abstract = {Software system families are characterized through a structured reuse of components and a high degree of automation based on a common infrastructure. It is possible to increase the efficiency of software system families by an explicit consideration of process flows in application domains which are driven by processes. Based on that fact this article briefly describes the approach of process family engineering. Afterwards the metrics of Process-Family-Points are explained in detail. These are the only framework to measure the size and estimate the effort of process families. Subsequently this paper shows the first results from a validation of the Process-Family-Points in the application domains of eBusiness and Automotive. After an evaluation of these empirical data this paper concludes with an outlook on future activities.},
booktitle = {Proceedings of the 2006 International Conference on Software Process Simulation and Modeling},
pages = {314–321},
numpages = {8},
location = {Shanghai, China},
series = {SPW/ProSim'06}
}

@inproceedings{10.1145/1449814.1449874,
author = {Danovaro, Emanuele and Janes, Andrea and Succi, Giancarlo},
title = {Jidoka in software development},
year = {2008},
isbn = {9781605582207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1449814.1449874},
doi = {10.1145/1449814.1449874},
abstract = {Lean management is based on two concepts: the elimination of Muda, the waste, from the production process, and Jidoka, the introduction of quality inside the production process and product. In software production, the elimination of Muda received significant attention, while Jidoka has not yet been fully exploited. In this work we want to propose a holistic approach to insert Jidoka in software production. We depict the architecture of a tool to support Jidoka and describe the components that are part of it.},
booktitle = {Companion to the 23rd ACM SIGPLAN Conference on Object-Oriented Programming Systems Languages and Applications},
pages = {827–830},
numpages = {4},
keywords = {Jidoka, quality assurance},
location = {Nashville, TN, USA},
series = {OOPSLA Companion '08}
}

@inproceedings{10.5555/1768904.1768924,
author = {Regnell, Bj\"{o}rn and H\"{o}st, Martin and Svensson, Richard Berntsson},
title = {A quality performance model for cost-benefit analysis of non-functional requirements applied to the mobile handset domain},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In market-driven requirements engineering for platform-based development of embedded systems such as mobile phones, it is crucial to market success to find the right balance among competing quality aspects (aka nonfunctional requirements). This paper presents a conceptual model that incorporates quality as a dimension in addition to the cost and value dimensions used in prioritisation approaches for functional requirements. The model aims at supporting discussion and decision-making in early requirements engineering related to activities such as roadmapping, release planning and platform scoping. The feasibility and relevance of the model is initially validated through interviews with requirements experts in six cases that represent important areas in the mobile handset domain. The validation suggests that the model is relevant and feasible for this particular domain.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {277–291},
numpages = {15},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.1007/11527800_25,
author = {Czarnecki, Krzysztof},
title = {Overview of generative software development},
year = {2004},
isbn = {3540278842},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11527800_25},
doi = {10.1007/11527800_25},
abstract = {System family engineering seeks to exploit the commonalities among systems from a given problem domain while managing the variabilities among them in a systematic way. In system family engineering, new system variants can be rapidly created based on a set of reusable assets (such as a common architecture, components, models, etc.). Generative software development aims at modeling and implementing system families in such a way that a given system can be automatically generated from a specification written in one or more textual or graphical domain-specific languages. This paper gives an overview of the basic concepts and ideas of generative software development including DSLs, domain and application engineering, generative domain models, networks of domains, and technology projections. The paper also discusses the relationship of generative software development to other emerging areas such as Model Driven Development and Aspect-Oriented Software Development.},
booktitle = {Proceedings of the 2004 International Conference on Unconventional Programming Paradigms},
pages = {326–341},
numpages = {16},
location = {Le Mont Saint Michel, France},
series = {UPP'04}
}

@article{10.1145/1754405.1754407,
author = {Ahmed, Waseem and Myers, Douglas},
title = {Concept-based partitioning for large multidomain multifunctional embedded systems},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1084-4309},
url = {https://doi.org/10.1145/1754405.1754407},
doi = {10.1145/1754405.1754407},
abstract = {Hardware-software partitioning is an important phase in embedded systems. Decisions made during this phase impact the quality, cost, performance, and the delivery date of the final product. Over the past decade or more, various partitioning approaches have been proposed. A majority operate at a relatively fine granularity and use a low-level executable specification as the starting point. This presents problems if the context is families of industrial products with frequent release of upgraded or new members. Managing complexity using a low-level specification is extremely challenging and impacts developer productivity. Designing using a high-level specification and component-based development, although a better option, imposes component integration and replacement problems during system evolution and new product release. A new approach termed Concept-Based Partitioning is presented that focuses on system evolution, product lines, and large-scale reuse when partitioning. Beginning with information from UML 2.0 sequence diagrams and a concept repository concepts are identified and used as the unit of partitioning within a specification. A methodology for the refinement of interpart communication in the system specification using sequence diagrams is also presented. Change localization during system evolution, composability during large-scale reuse, and provision for configurable feature variations for a product line are facilitated by a Generic Adaptive Layer (GAL) around selected concepts. The methodology was applied on a subsystem of an Unmanned Aerial Vehicle (UAV) using various concepts which improved the composability of concepts while keeping performance and size overhead within the 2% range.},
journal = {ACM Trans. Des. Autom. Electron. Syst.},
month = jun,
articleno = {22},
numpages = {41},
keywords = {Codesign, UML, embedded system design, product families, system evolution, system partitioning}
}

@inproceedings{10.1145/2513534.2513541,
author = {Guill\'{e}n, Joaqu\'{\i}n and Miranda, Javier and Murillo, Juan Manuel and Canal, Carlos},
title = {Developing migratable multicloud applications based on MDE and adaptation techniques},
year = {2013},
isbn = {9781450323079},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2513534.2513541},
doi = {10.1145/2513534.2513541},
abstract = {Developing software for the cloud usually implies using the tools and libraries supplied by cloud vendors for each of their platforms. This strongly couples the software to specific platforms and penalizes its migration or interoperability with external cloud services, in what is known as vendor lock-in. Under these circumstances multicloud applications become difficult to build and maintain since they require multidisciplinary teams with expertise on multiple platforms, and the redevelopment of some components if the cloud deployment scenario is altered. The MULTICLAPP framework described in this paper tackles these issues by presenting a three-stage development process that allows multicloud applications to be developed without being coupled to any concrete vendor. MDE and adaptation techniques are used throughout the software development stages in order to abstract the software from each vendor's service specifications. As a result of this, multicloud applications or their subcomponents can be reassigned to different cloud platforms without having to undergo a partial or complete redevelopment process.},
booktitle = {Proceedings of the Second Nordic Symposium on Cloud Computing &amp; Internet Technologies},
pages = {30–37},
numpages = {8},
keywords = {UML profile, cloud, framework, multicloud, vendor lock-in},
location = {Oslo, Norway},
series = {NordiCloud '13}
}

@inproceedings{10.1007/978-3-642-35623-0_2,
author = {Toffetti, Giovanni},
title = {Web engineering for cloud computing (web engineering forecast: cloudy with a chance of opportunities)},
year = {2012},
isbn = {9783642356223},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-35623-0_2},
doi = {10.1007/978-3-642-35623-0_2},
abstract = {Web Engineering has always been concerned with modelling the functional aspects of Web applications. Non-functional (e.g., performance, availability) properties of Web applications have traditionally been a minor concern in the Web engineering community and have been seen as technology- or system-related issues. The advent of Cloud computing, with substantial delegation of "system concerns" to infrastructure or platform providers, seems at a first sight to confirm the validity of this choice. But is this really true?We will argue that, in order to be able to actually profit from the Cloud computing paradigm, Web Engineering methodologies need several interventions transcending the platform-specific concerns of adapting to Cloud technologies.In this position paper, we call for a long-due revamp of Web engineering methodologies to become more sound engineering practices with respect to both functional and non-functional aspects of Web applications. To this end, we propose a methodological framework that preserves the advantages of model-driven development, but also takes into account performance and cost considerations for Cloud-based applications.},
booktitle = {Proceedings of the 12th International Conference on Current Trends in Web Engineering},
pages = {5–19},
numpages = {15},
location = {Berlin, Germany},
series = {ICWE'12}
}

@inproceedings{10.1145/2602928.2603080,
author = {Lytra, Ioanna and Sobernig, Stefan and Tran, Huy and Zdun, Uwe},
title = {A pattern language for service-based platform integration and adaptation},
year = {2012},
isbn = {9781450329439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2602928.2603080},
doi = {10.1145/2602928.2603080},
abstract = {Often software systems accommodate one or more software platforms on top of which various applications are developed and executed. Different application areas, such as enterprise resource planning, mobile devices, telecommunications, and so on, require different and specialized platforms. Many of them offer their services using standardized interface technologies to support integration with the applications built on top of them and with other platforms. The diversity of platform technologies and interfaces, however, renders the integration of multiple platforms challenging. In this paper, we discuss design alternatives for tailoring heterogeneous service platforms by studying high-level and low-level architectural design decisions for integrating and for adapting platforms. We survey and organize existing patterns and design decisions in the literature as a pattern language. With this pattern language, we address the various decision categories and interconnections for the service-based integration and the adaptation of applications developed based on software platforms. We apply this pattern language in an industry case study.},
booktitle = {Proceedings of the 17th European Conference on Pattern Languages of Programs},
articleno = {4},
numpages = {27},
keywords = {design patterns, pattern language, service-based platform integration},
location = {Irsee, Germany},
series = {EuroPLoP '12}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Multi-objective optimization, Search-based software engineering, Software library, Software reuse}
}

@article{10.1007/s10270-015-0498-5,
author = {Rodrigues, Taniro and Delicato, Fl\'{a}via C. and Batista, Thais and Pires, Paulo F. and Pirmez, Luci},
title = {An approach based on the domain perspective to develop WSAN applications},
year = {2017},
issue_date = {October   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-015-0498-5},
doi = {10.1007/s10270-015-0498-5},
abstract = {As wireless sensor and actuator networks (WSANs) can be used in many different domains, WSAN applications have to be built from two viewpoints: domain and network. These different viewpoints create a gap between the abstractions handled by the application developers, namely the domain and network experts. Furthermore, there is a coupling between the application logic and the underlying sensor platform, which results in platform-dependent projects and source codes difficult to maintain, modify, and reuse. Consequently, the process of developing an application becomes cumbersome. In this paper, we propose a model-driven architecture (MDA) approach for WSAN application development. Our approach aims to facilitate the task of the developers by: (1) enabling application design through high abstraction level models; (2) providing a specific methodology for developing WSAN applications; and (3) offering an MDA infrastructure composed of PIM, PSM, and transformation programs to support this process. Our approach allows the direct contribution of domain experts in the development of WSAN applications, without requiring specific knowledge of programming WSAN platforms. In addition, it allows network experts to focus on the specific characteristics of their area of expertise without the need of knowing each specific application domain.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {949–977},
numpages = {29},
keywords = {Abstraction, Architecture, Code generation, Domain-specific language, Model-driven architecture, UML profile, WSAN applications}
}

@inproceedings{10.1145/1810295.1810404,
author = {Farias, Kleinner},
title = {Empirical evaluation of effort on composing design models},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810404},
doi = {10.1145/1810295.1810404},
abstract = {The importance of model composition in model-centric software development is recognized by researchers and practitioners. However, the lack of empirical evidence about the impact of model composition techniques on developers' effort is a key impairment for their adoption in real-world design settings. Software engineers are left without any guidance on how to properly use certain model techniques in a way that effectively reduces their development effort. This work aims to address this problem by: (1) providing empirical evidence on model composition effort through a family of experimental studies; (2) defining quantitative indicators to objectively assess key attributes of model composition effort; (3) deriving a method to support the systematic application of composition techniques; and (4) conceiving a new model composition technique to overcome the problems identified throughout the experimental evaluations.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {405–408},
numpages = {4},
keywords = {UML, empirical studies, model composition},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1007/978-3-540-30587-3_36,
author = {Morris, Edwin and Anderson, Wm B. and Ward, Mary Catherine and Smith, Dennis},
title = {Ten signs of a good reuse management plan},
year = {2005},
isbn = {3540245480},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30587-3_36},
doi = {10.1007/978-3-540-30587-3_36},
abstract = {A Reuse Management Plan defines the strategy for selecting, approving and upgrading common reusable software components The SEI, in conjunction with the U.S. Army, the Boeing Company, and the Fraunhofer USA Center for Experimental Software Engineering, is developing a Reuse Management Plan for a large Army program. Ten critical features of quality Reuse Management Plans have been identified and are presented.},
booktitle = {Proceedings of the 4th International Conference on COTS-Based Software Systems},
pages = {268–277},
numpages = {10},
location = {Bilbao, Spain},
series = {ICCBSS'05}
}

@inproceedings{10.5555/648114.748900,
author = {Knauber, Peter and Thiel, Steffen},
title = {Session Report on Product Issues in Product Family Engineering},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This report gives an overview of the session on product issues of the 4th International Workshop on Product Family Engineering. It briefly sketches the issues presented in the technical session and summarizes the results and open issues of the subsequent discussion session.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {3–12},
numpages = {10},
series = {PFE '01}
}

@inproceedings{10.1145/1185448.1185468,
author = {Hunt, John M. and McGregor, John D.},
title = {A series of choices variability in the development process},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185468},
doi = {10.1145/1185448.1185468},
abstract = {Software variability is "the ability of a software artifact to vary its behavior at some point in its life cycle" [12]. Almost every software artifact requires some type of variability. While variability is endemic to the creation of software it is rarely the direct focus of study. In addition, software systems have shown an increasing amount of variability in recent years. This work provides an analysis of the decisions involved in providing variability at a specific point in a product. A classification scheme and related choice model is provided that describes the decisions related to variability, making them more explicit and quantifiable.},
booktitle = {Proceedings of the 44th Annual ACM Southeast Conference},
pages = {85–90},
numpages = {6},
keywords = {modeling},
location = {Melbourne, Florida},
series = {ACMSE '06}
}

@inproceedings{10.1145/2668930.2688051,
author = {Hork\'{y}, Vojt\v{e}ch and Libi\v{c}, Peter and Marek, Luk\'{a}\v{s} and Steinhauser, Antonin and T\r{u}ma, Petr},
title = {Utilizing Performance Unit Tests To Increase Performance Awareness},
year = {2015},
isbn = {9781450332484},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668930.2688051},
doi = {10.1145/2668930.2688051},
abstract = {Many decisions taken during software development impact the resulting application performance. The key decisions whose potential impact is large are usually carefully weighed. In contrast, the same care is not used for many decisions whose individual impact is likely to be small -- simply because the costs would outweigh the benefits. Developer opinion is the common deciding factor for these cases, and our goal is to provide the developer with information that would help form such opinion, thus preventing performance loss due to the accumulated effect of many poor decisions.Our method turns performance unit tests into recipes for generating performance documentation. When the developer selects an interface and workload of interest, relevant performance documentation is generated interactively. This increases performance awareness -- with performance information available alongside standard interface documentation, developers should find it easier to take informed decisions even in situations where expensive performance evaluation is not practical. We demonstrate the method on multiple examples, which show how equipping code with performance unit tests works.},
booktitle = {Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering},
pages = {289–300},
numpages = {12},
keywords = {java, javadoc, performance awareness, performance documentation, performance testing},
location = {Austin, Texas, USA},
series = {ICPE '15}
}

@article{10.1016/j.jss.2012.03.071,
author = {Poort, Eltjo R. and Van Vliet, Hans},
title = {RCDA: Architecting as a risk- and cost management discipline},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {9},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.03.071},
doi = {10.1016/j.jss.2012.03.071},
abstract = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work.},
journal = {J. Syst. Softw.},
month = sep,
pages = {1995–2013},
numpages = {19},
keywords = {Cost management, Risk Management, Software architecture}
}

@inproceedings{10.1145/2576768.2598366,
author = {Mkaouer, Mohamed Wiem and Kessentini, Marouane and Bechikh, Slim and Deb, Kalyanmoy and \'{O} Cinn\'{e}ide, Mel},
title = {High dimensional search-based software engineering: finding tradeoffs among 15 objectives for automating software refactoring using NSGA-III},
year = {2014},
isbn = {9781450326629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2576768.2598366},
doi = {10.1145/2576768.2598366},
abstract = {There is a growing need for scalable search-based software engineering approaches that address software engineering problems where a large number of objectives are to be optimized. Software refactoring is one of these problems where a refactoring sequence is sought that optimizes several software metrics. Most of the existing refactoring work uses a large set of quality metrics to evaluate the software design after applying refactoring operations, but current search-based software engineering approaches are limited to using a maximum of five metrics. We propose for the first time a scalable search-based software engineering approach based on a newly proposed evolutionary optimization method NSGA-III where there are 15 different objectives to be optimized. In our approach, automated refactoring solutions are evaluated using a set of 15 distinct quality metrics. We evaluated this approach on seven large open source systems and found that, on average, more than 92% of code smells were corrected. Statistical analysis of our experiments over 31 runs shows that NSGA-III performed significantly better than two other many-objective techniques (IBEA and MOEA/D), a multi-objective algorithm (NSGA-II) and two mono-objective approaches, hence demonstrating that our NSGA-III approach represents the new state of the art in fully-automated refactoring.},
booktitle = {Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation},
pages = {1263–1270},
numpages = {8},
keywords = {code-smells, refactroing, search-based software engineering},
location = {Vancouver, BC, Canada},
series = {GECCO '14}
}

@inproceedings{10.1145/1289927.1289930,
author = {Kirsch, Christoph M. and Wilhelm, Reinhard},
title = {Grand challenges in embedded software},
year = {2007},
isbn = {9781595938251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1289927.1289930},
doi = {10.1145/1289927.1289930},
abstract = {This is an introduction to the EMSOFT 2007 Panel on Grand Challenges in Embedded Software.},
booktitle = {Proceedings of the 7th ACM &amp; IEEE International Conference on Embedded Software},
pages = {2–6},
numpages = {5},
keywords = {analysis, challenges, design, embedded software, performance, scheduling, verification},
location = {Salzburg, Austria},
series = {EMSOFT '07}
}

@inproceedings{10.1145/2000259.2000285,
author = {Galster, Matthias and Avgeriou, Paris},
title = {Empirically-grounded reference architectures: a proposal},
year = {2011},
isbn = {9781450307246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000259.2000285},
doi = {10.1145/2000259.2000285},
abstract = {A reference architecture describes core elements of the software architecture for systems that stem from the same domain. A reference architecture ensures interoperability of systems through standardization. It also facilitates the instantiation of new concrete architectures. However, we currently lack procedures for systematically designing reference architectures that are empirically-grounded. Being empirically-grounded would increase the validity and reusability of a reference architecture. We therefore present an approach which helps systematically design reference architectures. Our approach consists of six steps performed by the software architect and domain experts. It helps design reference architectures either from scratch, or based on existing architecture artifacts. We also illustrate how our approach could be applied to the design of two existing reference architectures found in literature.},
booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
pages = {153–158},
numpages = {6},
keywords = {design process, empirically-grounded, reference architecture, software architecture},
location = {Boulder, Colorado, USA},
series = {QoSA-ISARCS '11}
}

@inproceedings{10.1007/978-3-642-34059-8_20,
author = {Henzinger, Thomas A. and Ni\v{c}kovi\'{c}, Dejan},
title = {Independent implementability of viewpoints},
year = {2012},
isbn = {9783642340581},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-34059-8_20},
doi = {10.1007/978-3-642-34059-8_20},
abstract = {Interface theories provide a formal framework for component-based development of software and hardware which supports the incremental design of systems and the independent implementability of components. These capabilities are ensured through mathematical properties of the parallel composition operator and the refinement relation for components. More recently, a conjunction operation was added to interface theories in order to provide support for handling multiple viewpoints, requirements engineering, and component reuse. Unfortunately, the conjunction operator does not allow independent implementability in general.In this paper, we study conditions that need to be imposed on interface models in order to enforce independent implementability with respect to conjunction. We focus on multiple viewpoint specifications and propose a new compatibility criterion between two interfaces, which we call orthogonality. We show that orthogonal interfaces can be refined separately, while preserving both orthogonality and composability with other interfaces. We illustrate the independent implementability of different viewpoints with a FIFO buffer example.},
booktitle = {Proceedings of the 17th Monterey Conference on Large-Scale Complex IT Systems: Development, Operation and Management},
pages = {380–395},
numpages = {16},
location = {Oxford, UK}
}

@inproceedings{10.1145/3396743.3396784,
author = {Suebsook, Sasiwimon and Chaveesuk, Singha and Chaiyasoonthorn, Wornchanok},
title = {Thailand Automotive Industry: Road to Smart Manufacturing},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396784},
doi = {10.1145/3396743.3396784},
abstract = {Rapid ICT growth has resulted in many technologies such as cloud computing, the Internet of Things (IoT), and big data. These innovations now reach almost all sectors including automotive sector which is one of the main industries in Thailand, In order to advance research and implementation of smart manufacturing, authors present a conceptual framework of smart manufacturing. Core innovations such as IoT, Cyber Physical Systems and Visual Analytics for Smart Manufacturing Systems will be captured on the basis of demonstrative scenarios. Also highlighted are today's challenges and future prospects.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {130–134},
numpages = {5},
keywords = {Automotive Industry, Smart Manufacturing},
location = {Osaka, Japan},
series = {MSIE '20}
}

@inproceedings{10.5555/648114.748901,
author = {Northrop, Linda M. and Bachmann, Felix and Due\~{n}as, Juan C.},
title = {Report on Discussion Sessions "Diversity Solutions" and "Light-Weight Processes"},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This document reports about the papers presented in the sessions "Diversity solutions" and "Light-weight processes", as well as the discussion session. The main results, conclusions and open points in the sessions are included.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {258–263},
numpages = {6},
series = {PFE '01}
}

@article{10.1007/s10009-016-0432-3,
author = {Parizi, Reza Meimandi and Ghani, Abdul Azim and Lee, Sai Peck and Khan, Saif Ur},
title = {RAMBUTANS: automatic AOP-specific test generation tool},
year = {2017},
issue_date = {November  2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {19},
number = {6},
issn = {1433-2779},
url = {https://doi.org/10.1007/s10009-016-0432-3},
doi = {10.1007/s10009-016-0432-3},
abstract = {Aspect-oriented programming (AOP) is a programmatic methodology to handle better modularized code by separating crosscutting concerns from the traditional abstraction boundaries. Automated testing, as one of the most demanding needs of the software development to reduce both human effort and costs, is a delicate issue in testing aspect-oriented programs. Prior studies in the automated test generation for aspect-oriented programs have been very limited with respect to the need for both adequate tool support and capability concerning effectiveness and efficiency. This paper describes a new AOP-specific tool for testing aspect-oriented programs, called RAMBUTANS. The RAMBUTANS tool uses a directed random testing technique that is especially well suited for generating tests for aspectual features in AspectJ. The directed random aspect of the tool is parameterized by associating weights to aspects, advice, methods, and classes by controlling object and joint point creations during the test generation process. We present a comprehensive empirical evaluation of our tool against the current AOP test generation approaches on three industrial aspect-oriented projects. The results of the experimental and statistical tests showed that RAMBUTANS tool produces test suites that have higher fault-detection capability and efficiency for AspectJ-like programs.},
journal = {Int. J. Softw. Tools Technol. Transf.},
month = nov,
pages = {743–761},
numpages = {19},
keywords = {Aspect-oriented programming, AspectJ, Automated test generation, Object-oriented programming, Software testing, Testing tool}
}

@article{10.1016/j.websem.2006.11.006,
author = {Wang, Hai H. and Li, Yuan Fang and Sun, Jing and Zhang, Hongyu and Pan, Jeff},
title = {Verifying feature models using OWL},
year = {2007},
issue_date = {June, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {5},
number = {2},
issn = {1570-8268},
url = {https://doi.org/10.1016/j.websem.2006.11.006},
doi = {10.1016/j.websem.2006.11.006},
abstract = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web OWL ontologies. We use OWL DL ontologies to precisely capture the inter-relationships among the features in a feature diagram. OWL reasoning engines such as FaCT++ are deployed to check for the inconsistencies of feature configurations fully automatically. Furthermore, a general OWL debugger has been developed to tackle the disadvantage of lacking debugging aids for the current OWL reasoner and to complement our verification approach. We also developed a CASE tool to facilitate visual development, interchange and reasoning of feature diagrams in the Semantic Web environment.},
journal = {Web Semant.},
month = jun,
pages = {117–129},
numpages = {13},
keywords = {Feature modeling, OWL, Ontologies, Semantic Web}
}

@inproceedings{10.5555/648114.748905,
author = {Clements, Paul C.},
title = {On the Importance of Product Line Scope},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {70–78},
numpages = {9},
series = {PFE '01}
}

@article{10.1155/2021/5597337,
author = {Salma and Saeed, Maham and ur Rahim, Rauf and Gufran Khan, Muhammad and Zulfiqar, Adil and Bhatti, Muhammad Tahir and Bueno, Atila},
title = {Development of ANPR Framework for Pakistani Vehicle Number Plates Using Object Detection and OCR},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1076-2787},
url = {https://doi.org/10.1155/2021/5597337},
doi = {10.1155/2021/5597337},
abstract = {The metropolis of the future demands an efficient Automatic Number Plate Recognition (ANPR) system. Since every region has a distinct number plate format and style, an unconstrained ANPR system is still not available. There is not much work done on Pakistani number plates because of the unavailability of the data and heterogeneous plate formations. Addressing this issue, we have collected a Pakistani vehicle dataset having various plate configurations and developed a novel ANPR framework using the dataset. The proposed framework localizes the number plate region using the YOLO (You Only Look Once) object detection model, applies robust preprocessing techniques on the extracted plate region, and finally recognizes the plate label using OCR (optical character recognition) Tesseract. The obtained mAP score of the YOLOv3 is 94.3% and the YOLOv4 model is 99.5% on the 0.50 threshold, whereas the average accuracy score of our framework is found to be 73%. For comparison and validation, we implemented a LeNet Convolutional Neural Network (CNN) architecture which uses the segmented image as an input. The comparative analysis shows that the proposed ANPR framework comprising the YOLOv4 and OCR Tesseract has good accuracy and inference time for a wide variation of illumination and style of Pakistani number plates and can be used to develop a real-time system. The proposed ANPR framework will be helpful for researchers developing ANPR for countries having similar challenging vehicle number plate formats and styles.},
journal = {Complex.},
month = jan,
numpages = {14}
}

@article{10.1016/j.future.2018.09.006,
author = {Munoz, Daniel-Jesus and Montenegro, Jos\'{e} A. and Pinto, M\'{o}nica and Fuentes, Lidia},
title = {Energy-aware environments for the development of green applications for cyber–physical systems},
year = {2019},
issue_date = {Feb 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2018.09.006},
doi = {10.1016/j.future.2018.09.006},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {536–554},
numpages = {19},
keywords = {Energy consumption, Cyber–physical systems, Green plugin, HADAS eco-assistant}
}

@inproceedings{10.1007/978-3-030-32239-7_52,
author = {Xie, Yutong and Lu, Hao and Zhang, Jianpeng and Shen, Chunhua and Xia, Yong},
title = {Deep Segmentation-Emendation Model for Gland Instance Segmentation},
year = {2019},
isbn = {978-3-030-32238-0},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-32239-7_52},
doi = {10.1007/978-3-030-32239-7_52},
abstract = {Accurate and automated gland instance segmentation on histology microscopy images can assist pathologists to diagnose the malignancy degree of colorectal adenocarcinoma. To address this problem, many deep convolutional neural network (DCNN) based methods have been proposed, most of which aim to generate better segmentation by improving the model structure and loss function. Few of them, however, focus on further emendating the inferred predictions, thus missing a chance to refine the obtained segmentation results. In this paper, we propose the deep segmentation-emendation (DSE) model for gland instance segmentation. This model consists of a segmentation network (Seg-Net) and an emendation network (Eme-Net). The Seg-Net is dedicated to generating segmentation results, and the Eme-Net learns to predict the inconsistency between the ground truth and the segmentation results generated by Seg-Net. The predictions made by Eme-Net can in turn be used to refine the segmentation result. We evaluated our DSE model against five recent deep learning models on the 2015 MICCAI Gland Segmentation challenge (GlaS) dataset and against two deep learning models on the colorectal adenocarcinoma (CRAG) dataset. Our results indicate that using Eme-Net results in significant improvement in segmentation accuracy, and the proposed DSE model is able to substantially outperform all the rest models in gland instance segmentation on both datasets.},
booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13–17, 2019, Proceedings, Part I},
pages = {469–477},
numpages = {9},
location = {Shenzhen, China}
}

@article{10.1007/s00766-013-0193-4,
author = {Antonelli, Leandro and Rossi, Gustavo and Leite, Julio Cesar and Ara\'{u}jo, Jo\~{a}o},
title = {Early identification of crosscutting concerns with the Language Extended Lexicon},
year = {2015},
issue_date = {June      2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {2},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-013-0193-4},
doi = {10.1007/s00766-013-0193-4},
abstract = {Large-scale software applications are complex systems that involve a myriad of different concerns. Ideally, these concerns should be organized into separated and different modules, but often some of these concerns overlap and crosscut each other. Such a situation is problematic, as concerns are tangled and scattered into different modules; thus, design and source code become difficult to produce and maintain. The Modularity community has been addressing crosscutting concerns by developing techniques based on separation of concerns. This separation must be done as early as possible during software construction to obtain a more modular and consequently better maintainable software, where evolution is performed with less effort and the possibility of introducing unforeseen mistakes is minimal. In this paper, we propose a strategy to identify crosscutting concerns at requirements level, i.e., at early stages in the software development process, by using the Language Extended Lexicon.},
journal = {Requir. Eng.},
month = jun,
pages = {139–161},
numpages = {23},
keywords = {Crosscutting concerns, Language Extended Lexicon, Modularity, Requirements engineering}
}

@inproceedings{10.1145/2627788.2627802,
author = {Wang, Peng and Ansari, Junaid and Petrova, Marina and M\"{a}h\"{o}nen, Petri},
title = {Demo: CogMAC+ - a decentralized multichannel MAC protocol for cognitive wireless networks},
year = {2014},
isbn = {9781450329958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627788.2627802},
doi = {10.1145/2627788.2627802},
abstract = {Cognitive MAC schemes are emerging as a prospective solution to efficiently utilize the wireless medium. In order to enable opportunistic access to unused licensed band, a node has to monitor the frequency spectrum and carry out its transmission without causing harmful interference to the primary user. In this work, we demonstrate a decentralized multichannel MAC protocol CogMAC+ which uses a multichannel preamble reservation scheme to achieve parallel transmissions for multiple secondary users. Moreover, CogMAC+ uses an adaptive energy detection scheme to dynamically set the frame detection threshold based on the false positive detection ratio. Our table-top demonstration shows that CogMAC+ enables spectral coexistence and allows nodes to utilize spectrum opportunities efficiently in a dynamic fashion.},
booktitle = {Proceedings of the 2014 ACM Workshop on Software Radio Implementation Forum},
pages = {23–26},
numpages = {4},
keywords = {cognitive radios, mac, sdr platform, spectrum agile},
location = {Chicago, Illinois, USA},
series = {SRIF '14}
}

@inproceedings{10.5555/1862739.1862748,
author = {Thalheim, Bernhard and Schewe, Klaus-Dieter and Ma, Hui},
title = {Conceptual application domain modelling},
year = {2009},
isbn = {9781920682774},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Application domain description precedes requirements engineering, and is the basis for the development of a software or information system that satisfies all expectations of its users. The greatest challenge in this area is the evolution of the application domain itself. In this paper we address this problem by explicit consideration of application cases that are defined by user profiles and intentions and the system environment, i.e. scope and context. User profiles and intentions are captured through the concept of persona. We show how the application domain description can be mapped to requirements and discuss engineering of application domain descriptions.},
booktitle = {Proceedings of the Sixth Asia-Pacific Conference on Conceptual Modeling - Volume 96},
pages = {49–58},
numpages = {10},
location = {Wellington, New Zealand},
series = {APCCM '09}
}

@inproceedings{10.5555/2025896.2025909,
author = {Przyby\l{}ek, Adam},
title = {Systems evolution and software reuse in object-oriented programming and aspect-oriented programming},
year = {2011},
isbn = {9783642219511},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Every new programming technique makes claims that software engineers want to hear. Such is the case with aspect-oriented programming (AOP). This paper describes a quasi-controlled experiment which compares the evolution of two functionally equivalent programs, developed in two different paradigms. The aim of the study is to explore the claims that software developed with aspect-oriented languages is easier to maintain and reuse than this developed with object-oriented languages. We have found no evidence to support these claims.},
booktitle = {Proceedings of the 49th International Conference on Objects, Models, Components, Patterns},
pages = {163–178},
numpages = {16},
keywords = {AOP, maintainability, reusability, separation of concerns},
location = {Zurich, Switzerland},
series = {TOOLS'11}
}

@inproceedings{10.5555/2666795.2666811,
author = {Weyns, Danny and Iftikhar, M. Usman and Malek, Sam and Andersson, Jesper},
title = {Claims and supporting evidence for self-adaptive systems: a literature study},
year = {2012},
isbn = {9781467317870},
publisher = {IEEE Press},
abstract = {Despite the vast body of work on self-adaption, no systematic study has been performed on the claims associated with self-adaptation and the evidence that exists for these claims. As such an insight is crucial for researchers and engineers, we performed a literature study of the research results from SEAMS since 2006 and the associated Dagstuhl seminar in 2008. The study shows that the primary claims of self-adaptation are improved flexibility, reliability, and performance of the system. On the other hand, the tradeoffs implied by self-adaptation have not received much attention. Evidence is obtained from basic examples, or simply lacking. Few systematic empirical studies have been performed, and no industrial evidence is reported. From the study, we offer the following recommendations to move the field forward: to improve evaluation, researchers should make their assessment methods, tools and data publicly available; to deal with poor discussion of limitations, conferences/workshops should require an explicit section on limitations in engineering papers; to improve poor treatment of tradeoffs, this aspect should be an explicit subject of reviews; and finally, to enhance industrial validation, the best academy-industry efforts could be formally recognized by the community.},
booktitle = {Proceedings of the 7th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {89–98},
numpages = {10},
location = {Zurich, Switzerland},
series = {SEAMS '12}
}

@inproceedings{10.1007/978-3-540-30554-5_3,
author = {Calero, Coral and Abreu, Fernando Brito e and Poels, Geert and Sahraoui, Houari A.},
title = {8th workshop on quantitative approaches in object-oriented software engineering (QAOOSE 2004)},
year = {2004},
isbn = {354023988X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-30554-5_3},
doi = {10.1007/978-3-540-30554-5_3},
abstract = {The workshop was a direct continuation of seven successful workshops, held at previous editions of ECOOP in Darmstadt (2003), Malaga (2002), Budapest (2001), Cannes (2000), Lisbon (1999), Brussels (1998) and Aarhus (1995). This time, as in previous editions, the workshop attracted participants from both academia and industry that are involved / interested in the application of quantitative methods in object oriented software engineering research and practice.},
booktitle = {Proceedings of the 2004 International Conference on Object-Oriented Technology},
pages = {23–35},
numpages = {13},
location = {Oslo, Norway},
series = {ECOOP'04}
}

@article{10.1007/s10270-021-00896-9,
author = {Kugele, Stefan and Obergfell, Philipp and Sax, Eric},
title = {Model-based resource analysis and synthesis of service-oriented automotive software architectures},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {6},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-021-00896-9},
doi = {10.1007/s10270-021-00896-9},
journal = {Softw. Syst. Model.},
month = dec,
pages = {1945–1975},
numpages = {31},
keywords = {Service-oriented architecture, Real-time behaviour, Model-based design, Automotive architectures}
}

@inproceedings{10.1145/1643823.1643830,
author = {Briscoe, Gerard and De Wilde, Philippe},
title = {Computing of applied digital ecosystems},
year = {2009},
isbn = {9781605588292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1643823.1643830},
doi = {10.1145/1643823.1643830},
abstract = {A primary motivation for our research in digital ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex, dynamic problems. However, the computing technologies that contribute to these properties have not been made explicit in digital ecosystems research. Here, we discuss how different computing technologies can contribute to providing the necessary self-organising features, including Multi-Agent Systems (MASs), Service-Oriented Architectures (SOAs), and distributed evolutionary computing (DEC). The potential for exploiting these properties in digital ecosystems is considered, suggesting how several key features of biological ecosystems can be exploited in Digital Ecosystems, and discussing how mimicking these features may assist in developing robust, scalable self-organising architectures. An example architecture, the Digital Ecosystem, is considered in detail. The Digital Ecosystem is then measured experimentally through simulations, considering the self-organised diversity of its evolving agent populations relative to the user request behaviour.},
booktitle = {Proceedings of the International Conference on Management of Emergent Digital EcoSystems},
articleno = {5},
numpages = {8},
keywords = {distributed evolutionary computing, ecosystem-oriented architectures, multi-agent systems, service-oriented architectures},
location = {France},
series = {MEDES '09}
}

@inproceedings{10.5555/648114.746416,
author = {Stoermer, Christoph and Roeddiger, Markus},
title = {Introducing Product Lines in Small Embedded Systems},
year = {2001},
isbn = {3540436596},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {How do you introduce product lines into a hardware dominated organization that has increasing software architecture awareness and products with extremely limited memory resources__ __ This experience paper describes the transition steps from a conventional development to a first product, conformant to a product line design. Further steps towards a full product line are outlined in this on-going project. Key aspects like investigation of requirements, design, set of tools, speed of change, skills, and organization commitment are addressed. The investigation phase involved an architecture reconstruction of existing products and a requirements elicitation. The architectural design phase used the Attribute Driven Design method (ADD) of the Software Engineering Institute (SEI). The generated architecture had to be mapped onto the business unit's design tool, which generated the component code. Instead of reaching a full product line approach with the first product this experience report emphasizes the right speed of change by firstly reaching a high commitment level at the organization in software architecture techniques. This builds the necessary foundation to survive higher investments for the first few products until the cost benefit of product lines pay back later on. Essential in the introduction phase are personal skills, like integrity in order to support a successful change at the organization. Those skills form a foundation to achieve a committed organization.},
booktitle = {Revised Papers from the 4th International Workshop on Software Product-Family Engineering},
pages = {101–112},
numpages = {12},
series = {PFE '01}
}

@article{10.1007/s00450-012-0234-0,
author = {Baresi, Luciano and Ghezzi, Carlo},
title = {A journey through SMScom: self-managing situational computing},
year = {2013},
issue_date = {November  2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {4},
issn = {1865-2034},
url = {https://doi.org/10.1007/s00450-012-0234-0},
doi = {10.1007/s00450-012-0234-0},
abstract = {This article provides an overall view of the research that has been done in the context of self-managing software within the SMScom project. We start by the motivations that inspired the research, and then we focus on a reference framework that explains its conceptual underpinnings and on the paradigm shift it calls for in the way we currently engineer software. Next we focus on some specific research results achieved at the architecture and verification support level.},
journal = {Comput. Sci.},
month = nov,
pages = {267–277},
numpages = {11},
keywords = {Cyber-physical systems, Internet of things, Ubiquitous, pervasive systems}
}

@inproceedings{10.1007/978-3-642-33666-9_41,
author = {Iqbal, Muhammad Zohaib and Ali, Shaukat and Yue, Tao and Briand, Lionel},
title = {Experiences of applying UML/MARTE on three industrial projects},
year = {2012},
isbn = {9783642336652},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33666-9_41},
doi = {10.1007/978-3-642-33666-9_41},
abstract = {MARTE (Modeling and Analysis of Real-Time and Embedded Systems) is a UML profile, which has been developed to model concepts specific to Real-Time and Embedded Systems (RTES). In previous years, we have applied UML/MARTE to three distinct industrial problems in various industry sectors: architecture modeling and configuration of large-scale and highly configurable integrated control systems, model-based robustness testing of communication-intensive systems, and model-based environment simulator generation of large-scale RTES for testing. In this paper, we report on our experiences of solving these problems by applying UML/MARTE on four industrial case studies. Based on our common experiences, we derive a framework to help practitioners for future applications of UML/MARTE. The framework provides a set of detailed guidelines on how to apply MARTE in industrial contexts and will help reduce the gap between the modeling standards and industrial needs.},
booktitle = {Proceedings of the 15th International Conference on Model Driven Engineering Languages and Systems},
pages = {642–658},
numpages = {17},
keywords = {MARTE, UML, architecture modeling, model-based testing, real-time embedded systems},
location = {Innsbruck, Austria},
series = {MODELS'12}
}

@inproceedings{10.1145/1960314.1960321,
author = {Silva Filho, Roberto Silveira and Bronsard, Fran\c{c}ois and Hasling, William M.},
title = {Experiences documenting and preserving software constraints using aspects},
year = {2011},
isbn = {9781450306065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960314.1960321},
doi = {10.1145/1960314.1960321},
abstract = {Software systems are increasingly being built as compositions of reusable artifacts (components, frameworks, toolkits, plug-ins, APIs, etc) that have non-trivial usage constraints in the form of interface contracts, underlying assumptions and design composition rules. Satisfying these constraints is challenging: they are often not well documented; or they are difficult to integrate into the software development process in ways that allow their identification by developers; or they may not be enforced by existing tools and development environments. Aspect-Oriented Programming has been advocated as an approach to represent and enforce software constraints in code artifacts. Aspects can be used to detect constraint violations, or more pro-actively, to ensure that the constraints are satisfied without requiring the developer's attention. This paper discusses our experience using aspects to document and enforce software constraints in an industrial application, specifically TDE/UML, a model-driven software testing tool developed at SIEMENS. We present an analysis of common constraints found in our case study, a set of primitive aspects developed to help the enforcement of software constraints, and show how AOP has been incorporated into existing software development and governance approaches in the TDE/UML project. We conclude with a discussion of strengths and limitations of AspectJ in supporting these constraints.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development Companion},
pages = {7–18},
numpages = {12},
keywords = {architectural constraints, aspect-oriented programming, design documentation, software architecture},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@article{10.4018/joci.2010040101,
author = {Ghedira, Chirine and Maamar, Zakaria and Vincent, Lucien and Boukadi, Khouloud},
title = {CSMA: Context-Based, Service-Oriented Modeling and Analysis Method for Modern Enterprise Applications},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {2},
issn = {1947-9344},
url = {https://doi.org/10.4018/joci.2010040101},
doi = {10.4018/joci.2010040101},
abstract = {Since the beginning of the Service-Oriented Architecture SOA paradigm, with its various implementation technologies such as Web services, the focus of industrial communities has been on providing tools that would allow seamless and flexible application integration within and across enterprises' boundaries. In this paper, the authors present a Context-based, Service-oriented Modeling and Analysis CSMA method that guides service engineers in their choices of identifying, defining, and analyzing adaptable business services. The proposed method is business centric and comprises a set of structured steps grouped in two phases. Besides, the CSMA embraces Model-Driven Architecture MDA principles to model and refine adaptable business services models in the PIM level. The results from a pilot validation of CSMA for SOA enablement of a realistic enterprise training solutions are also presented.},
journal = {Int. J. Organ. Collect. Intell.},
month = apr,
pages = {1–28},
numpages = {28},
keywords = {Adaptable Business Services, Model-Driven Architecture, Service engineers, Service-Oriented Architecture, Service-oriented Modeling and Analysis}
}

@inproceedings{10.1145/2642937.2642990,
author = {Abal, Iago and Brabrand, Claus and Wasowski, Andrzej},
title = {42 variability bugs in the linux kernel: a qualitative analysis},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642990},
doi = {10.1145/2642937.2642990},
abstract = {Feature-sensitive verification pursues effective analysis of the exponentially many variants of a program family. However, researchers lack examples of concrete bugs induced by variability, occurring in real large-scale systems. Such a collection of bugs is a requirement for goal-oriented research, serving to evaluate tool implementations of feature-sensitive analyses by testing them on real bugs. We present a qualitative study of 42 variability bugs collected from bug-fixing commits to the Linux kernel repository. We analyze each of the bugs, and record the results in a database. In addition, we provide self-contained simplified C99 versions of the bugs, facilitating understanding and tool evaluation. Our study provides insights into the nature and occurrence of variability bugs in a large C software system, and shows in what ways variability affects and increases the complexity of software bugs.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {421–432},
numpages = {12},
keywords = {bugs, feature interactions, linux, software variability},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.5555/1927882.1927894,
author = {Baumgart, Andreas and Reinkemeier, Philipp and Rettberg, Achim and Stierand, Ingo and Thaden, Eike and Weber, Raphael},
title = {A model-based design methodology with contracts to enhance the development process of safety-critical systems},
year = {2010},
isbn = {364216255X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper a new methodology to support the development process of safety-critical systems with contracts is described. The meta-model of Heterogeneous Rich Component (HRC) is extended to a Common System Meta-Model (CSM) that benefits from the semantic foundation of HRC and provides analysis techniques such as compatibility checks or refinement analyses. The idea of viewpoints, perspectives, and abstraction levels is discussed in detail to point out how the CSM supports separation of concerns. An example is presented to detail the transition concepts between models. From the example we conclude that our approach proves valuable and supports the development process.},
booktitle = {Proceedings of the 8th IFIP WG 10.2 International Conference on Software Technologies for Embedded and Ubiquitous Systems},
pages = {59–70},
numpages = {12},
location = {Waidhofen/Ybbs, Austria},
series = {SEUS'10}
}

@inproceedings{10.1109/SHARK-ADI.2007.9,
author = {Capilla, Rafael and Nava, Francisco and Duenas, Juan C.},
title = {Modeling and Documenting the Evolution of Architectural Design Decisions},
year = {2007},
isbn = {0769529518},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SHARK-ADI.2007.9},
doi = {10.1109/SHARK-ADI.2007.9},
abstract = {All software systems are built as a result of a set of design decisions that are made during the architecting phase. At present, there is still a lack of appropriate notations, methods and tools for recording and exploiting these architectural design decisions. In addition, the need for maintaining and evolving the decisions made in the past turns critical for the success of the evolution of the system. In this research paper we extend a previous work to detail those issues related to the evolution of architectural design decisions.},
booktitle = {Proceedings of the Second Workshop on SHAring and Reusing Architectural Knowledge Architecture, Rationale, and Design Intent},
pages = {9},
series = {SHARK-ADI '07}
}

@techreport{10.1145/2965631,
author = {The Joint Task Force on Computing Curricula},
title = {Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering},
year = {2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The primary purpose of this volume is to provide guidance to academic institutions and accreditation agencies about what should constitute an undergraduate software engineering education. These recommendations have been developed by a broad, internationally based group of volunteer participants. This group has taken into account much of the work that has been done in software engineering education over the last quarter of a century. Software engineering curriculum recommendations are of particular relevance, since there is currently a surge in the creation of software engineering degree programs and accreditation processes for such programs have been established in a number of countries.}
}

@article{10.5555/1349897.1350159,
author = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
title = {Software architecture reliability analysis using failure scenarios},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
journal = {J. Syst. Softw.},
month = apr,
pages = {558–575},
numpages = {18},
keywords = {FMEA, Fault trees, Reliability analysis, Scenario-based architectural evaluation}
}

@article{10.1016/j.scico.2012.07.021,
author = {Ruiz-L\'{o}Pez, Tom\'{a}S and Noguera, Manuel and Rodr\'{\i}Guez, Mar\'{\i}A Jos\'{e} and Garrido, Jos\'{e} Luis and Chung, Lawrence},
title = {REUBI: A Requirements Engineering method for ubiquitous systems},
year = {2013},
issue_date = {October, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {78},
number = {10},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2012.07.021},
doi = {10.1016/j.scico.2012.07.021},
abstract = {Recent technological advances are increasing the spread of Ubiquitous Computing, leading to the appearance of numerous software systems, which benefit from the features of this new paradigm. Nevertheless, there are a lack of methodologies to properly support the development process of these systems. An important part of the Software Engineering lifecycle is the Requirements Engineering stage, as it grounds the bases for system design for their success. In particular, systematically addressing Non-Functional Requirements such as dynamicity and adaptation, that are important features of ubiquitous systems, eventually leads to higher quality designs. In this paper, a Requirements Engineering Method for the analysis of Ubiquitous Systems, called REUBI, is introduced. It is a goal-based method that represents the influence of context and adverse situations, providing an evaluation procedure to help in the decision making about objectives satisfaction. The proposal is illustrated through the analysis of a Positioning Service of a real system. Additionally, the application of the method has been evaluated by a team of software engineers for the analysis of an Ambient Assisted Living (AAL) health care system.},
journal = {Sci. Comput. Program.},
month = oct,
pages = {1895–1911},
numpages = {17},
keywords = {Context-awareness, Non-functional requirements, Positioning systems, Requirements engineering, Software design, Ubiquitous systems}
}

@article{10.1007/s10664-014-9313-0,
author = {Bavota, Gabriele and Qusef, Abdallah and Oliveto, Rocco and Lucia, Andrea and Binkley, Dave},
title = {Are test smells really harmful? An empirical study},
year = {2015},
issue_date = {August    2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9313-0},
doi = {10.1007/s10664-014-9313-0},
abstract = {Bad code smells have been defined as indicators of potential problems in source code. Techniques to identify and mitigate bad code smells have been proposed and studied. Recently bad test code smells (test smells for short) have been put forward as a kind of bad code smell specific to tests such a unit tests. What has been missing is empirical investigation into the prevalence and impact of bad test code smells. Two studies aimed at providing this missing empirical data are presented. The first study finds that there is a high diffusion of test smells in both open source and industrial software systems with 86 % of JUnit tests exhibiting at least one test smell and six tests having six distinct test smells. The second study provides evidence that test smells have a strong negative impact on program comprehension and maintenance. Highlights from this second study include the finding that comprehension is 30 % better in the absence of test smells.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1052–1094},
numpages = {43},
keywords = {Controlled experiments, Mining software repositories, Test smells, Unit testing}
}

@inproceedings{10.1145/1982185.1982333,
author = {Varela, Patr\'{\i}cia and Ara\'{u}jo, Jo\~{a}o and Brito, Isabel and Moreira, Ana},
title = {Aspect-oriented analysis for software product lines requirements engineering},
year = {2011},
isbn = {9781450301138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1982185.1982333},
doi = {10.1145/1982185.1982333},
abstract = {Requirements analysis and modeling for Software Product Lines demands the use of feature models, but also requires additional models to help identifying, describing, and specifying features. Traditional approaches usually perform this manually and, in general, the identification and modularization of crosscutting features is ignored, or not handled systematically. This hinders requirements change. We propose an aspect-oriented approach for SPL enriched to automatically derive feature models where crosscutting features are identified and modularized using aspect-oriented concepts and techniques. This is achieved by adapting and extending the AORA (Aspect-Oriented Requirements Analysis) approach. AORA provides templates to specify and organize requirements based on concerns and responsibilities. A set of heuristics is defined to help identifying features and their dependencies in a product line. A tool was developed to automatically generate the feature model from AORA templates.},
booktitle = {Proceedings of the 2011 ACM Symposium on Applied Computing},
pages = {667–674},
numpages = {8},
keywords = {aspect-oriented requirements analysis, software product lines},
location = {TaiChung, Taiwan},
series = {SAC '11}
}

@inproceedings{10.1007/11678779_7,
author = {Pesonen, Jani and Katara, Mika and Mikkonen, Tommi},
title = {Production-Testing of embedded systems with aspects},
year = {2005},
isbn = {3540326049},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11678779_7},
doi = {10.1007/11678779_7},
abstract = {A test harness plays an important role in the development of any embedded system. Although the harness can be excluded from final products, its architecture should support maintenance and reuse, especially in the context of testing product families. Aspect-orientation is a new technique for software architecture that should enable scattered and tangled code to be addressed in a modular fashion, thus facilitating maintenance and reuse. However, the design of interworking between object-oriented baseline architecture and aspects attached on top of it is an issue, which has not been solved conclusively. For industrial-scale use, guidelines on what to implement with objects and what with aspects should be derived. In this paper, we introduce a way to reflect the use of aspect-orientation to production testing software of embedded systems. Such piece of a test harness is used to smoke test the proper functionality of a manufactured device. The selection of suitable implementation technique is based on variance of devices to be tested, with aspects used as means for increased flexibility. Towards the end of the paper, we also present the results of our experiments in the Symbian OS context that show some obstacles in the current tool support that should be addressed before further case studies can be conducted.},
booktitle = {Proceedings of the First Haifa International Conference on Hardware and Software Verification and Testing},
pages = {90–102},
numpages = {13},
location = {Haifa, Israel},
series = {HVC'05}
}

@article{10.1016/j.jss.2013.06.064,
author = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
title = {A systematic review on the functional testing of semantic web services},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.06.064},
doi = {10.1016/j.jss.2013.06.064},
abstract = {Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.},
journal = {J. Syst. Softw.},
month = nov,
pages = {2877–2889},
numpages = {13},
keywords = {Functional testing, Semantic web services, Systematic literature review, Testing approach}
}

@article{10.1016/j.jnca.2014.07.019,
author = {Sun, Le and Dong, Hai and Hussain, Farookh Khadeer and Hussain, Omar Khadeer and Chang, Elizabeth},
title = {Cloud service selection},
year = {2014},
issue_date = {October 2014},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {45},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2014.07.019},
doi = {10.1016/j.jnca.2014.07.019},
abstract = {Cloud technology connects a network of virtualized computers that are dynamically provisioned as computing resources, based on negotiated agreements between service providers and users. It delivers information technology resources in diverse forms of service, and the explosion of Cloud services on the Internet brings new challenges in Cloud service discovery and selection. To address these challenges, a range of studies has been carried out to develop advanced techniques that will assist service users to choose appropriate services. In this paper, we survey state-of-the-art Cloud service selection approaches, which are analyzed from the following five perspectives: decision-making techniques; data representation models; parameters and characteristics of Cloud services; contexts, purposes. After comparing and summarizing the reviewed approaches from these five perspectives, we identify the primary research issues in contemporary Cloud service selection. This survey is expected to bring benefits to both researchers and business agents.},
journal = {J. Netw. Comput. Appl.},
month = oct,
pages = {134–150},
numpages = {17},
keywords = {Cloud computing, Cloud service selection, Decision-making}
}

@inproceedings{10.1145/1806799.1806813,
author = {Ferrari, Fabiano and Burrows, Rachel and Lemos, Ot\'{a}vio and Garcia, Alessandro and Figueiredo, Eduardo and Cacho, Nelio and Lopes, Frederico and Temudo, Nathalia and Silva, Liana and Soares, Sergio and Rashid, Awais and Masiero, Paulo and Batista, Thais and Maldonado, Jos\'{e}},
title = {An exploratory study of fault-proneness in evolving aspect-oriented programs},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1806799.1806813},
doi = {10.1145/1806799.1806813},
abstract = {This paper presents the results of an exploratory study on the fault-proneness of aspect-oriented programs. We analysed the faults collected from three evolving aspect-oriented systems, all from different application domains. The analysis develops from two different angles. Firstly, we measured the impact of the obliviousness property on the fault-proneness of the evaluated systems. The results show that 40% of reported faults were due to the lack of awareness among base code and aspects. The second analysis regarded the fault-proneness of the main aspect-oriented programming (AOP) mechanisms, namely pointcuts, advices and intertype declarations. The results indicate that these mechanisms present similar fault-proneness when we consider both the overall system and concern-specific implementations. Our findings are reinforced by means of statistical tests. In general, this result contradicts the common intuition stating that the use of pointcut languages is the main source of faults in AOP.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 1},
pages = {65–74},
numpages = {10},
keywords = {aspect-oriented programming, fault-proneness, software testing},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@article{10.1016/j.infsof.2007.01.001,
author = {Kim, Chul Jin and Chung, Hyun Sook and Cho, Eun Sook},
title = {Micro and macro workflow variability design techniques of component},
year = {2008},
issue_date = {March, 2008},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {50},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2007.01.001},
doi = {10.1016/j.infsof.2007.01.001},
abstract = {Components should provide variability in satisfying a variety of domains [C. Szyperski, Component Software: Beyond Object-Oriented Programming, Addison-Wesley, 2002.], but it is not easy to develop components which can be applied to all domains. Although components are developed by analyzing many different requirements, developing components that satisfy all requirements is difficult since unexpected requirements occur during the utilization of components. Hence, providing the variability of components becomes an important prerequisite for a successful component-based application development. In this paper, we propose a variability design technique that can satisfy the business workflow requirements of many different kinds of domains. The technique addresses a method for designing the variability of the workflow in a more detailed method and uses an object-oriented mechanism and design patterns. One of the most important goals of this technique is to provide a practical process can be effectively applied in component-based application development.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {259–279},
numpages = {21},
keywords = {Component variability, Micro/macro workflow, Reusability}
}

@inproceedings{10.5555/1787553.1787572,
author = {Canal, Carlos and Murillo, Juan Manuel and Poizat, Pascal},
title = {Practical approaches for software adaptation: report on the 4th workshop WCAT at ECOOP 2007},
year = {2007},
isbn = {3540781943},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Coordination and Adaptation are two key issues when developing complex distributed systems. Coordination focuses on the interaction among software entities. Adaptation focuses on solving the problems that arise when the interacting entities do not match properly. This is the report of the fourth edition of the WCAT workshop, that took place in Berlin jointly with ECOOP 2007. Previous editions the workshop dealt with general issues which mainly served for a better characterization of Software Adaptation as an emerging discipline within the field of Software Engineering. For this edition, we wanted to put the focus on practical approaches for software adaptation, in order to show how this discipline helps in the construction of current software systems.},
booktitle = {Proceedings of the 2007 Conference on Object-Oriented Technology},
pages = {154–165},
numpages = {12},
location = {Berlin, Germany},
series = {ECOOP'07}
}

@article{10.1016/j.infsof.2009.04.004,
author = {Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
title = {Definitions and approaches to model quality in model-based software development - A review of literature},
year = {2009},
issue_date = {December, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {12},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2009.04.004},
doi = {10.1016/j.infsof.2009.04.004},
abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.},
journal = {Inf. Softw. Technol.},
month = dec,
pages = {1646–1669},
numpages = {24},
keywords = {Model quality, Model-driven development, Modelling, Systematic review, UML}
}

@inproceedings{10.1109/ESEM.2009.5316001,
author = {Barney, Sebastian and Wohlin, Claes and Aurum, Aybuke},
title = {Balancing software product investments},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2009.5316001},
doi = {10.1109/ESEM.2009.5316001},
abstract = {The long-term sustainability of a software product depends on more than developing features. Priorities are placed on aspects that support the development of software, like software product quality (eg. ISO 9126), project constraints — time and cost, and even the development of intellectual capital (IC). A greater focus on any one aspect takes priority from another, but as each aspects delivers a different type of value managers have trouble comparing and balancing these aspects. This paper presents a method to help determine the balance between key priorities in the software development process. The method is applied to a new case study, that also combines with results from previous studies. The results show it is possible to compare features, quality, time, cost and IC in a comprehensive way, with the case study showing that participants perceive a change from a shorter-term product perspective to a longer-term organisation beneficial to the business.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {257–268},
numpages = {12},
series = {ESEM '09}
}

@article{10.1016/j.infsof.2012.11.005,
author = {Li, Zengyang and Liang, Peng and Avgeriou, Paris},
title = {Application of knowledge-based approaches in software architecture: A systematic mapping study},
year = {2013},
issue_date = {May, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {5},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.005},
doi = {10.1016/j.infsof.2012.11.005},
abstract = {Context: Knowledge management technologies have been employed across software engineering activities for more than two decades. Knowledge-based approaches can be used to facilitate software architecting activities (e.g., architectural evaluation). However, there is no comprehensive understanding on how various knowledge-based approaches (e.g., knowledge reuse) are employed in software architecture. Objective: This work aims to collect studies on the application of knowledge-based approaches in software architecture and make a classification and thematic analysis on these studies, in order to identify the gaps in the existing application of knowledge-based approaches to various architecting activities, and promising research directions. Method: A systematic mapping study is conducted for identifying and analyzing the application of knowledge-based approaches in software architecture, covering the papers from major databases, journals, conferences, and workshops, published between January 2000 and March 2011. Results: Fifty-five studies were selected and classified according to the architecting activities they contribute to and the knowledge-based approaches employed. Knowledge capture and representation (e.g., using an ontology to describe architectural elements and their relationships) is the most popular approach employed in architecting activities. Knowledge recovery (e.g., documenting past architectural design decisions) is an ignored approach that is seldom used in software architecture. Knowledge-based approaches are mostly used in architectural evaluation, while receive the least attention in architecture impact analysis and architectural implementation. Conclusions: The study results show an increased interest in the application of knowledge-based approaches in software architecture in recent years. A number of knowledge-based approaches, including knowledge capture and representation, reuse, sharing, recovery, and reasoning, have been employed in a spectrum of architecting activities. Knowledge-based approaches have been applied to a wide range of application domains, among which ''Embedded software'' has received the most attention.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {777–794},
numpages = {18},
keywords = {Architecting activity, Knowledge-based approach, Software architecture, Systematic mapping study}
}

@article{10.1016/j.eswa.2020.113808,
author = {Mohsin, Hufsa and Shi, Chongyang},
title = {SPBC: A self-paced learning model for bug classification from historical repositories of open-source software},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.113808},
doi = {10.1016/j.eswa.2020.113808},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {15},
keywords = {Bug triaging, Defect localization, Self-paced learning, Bug report analysis, Bug classification}
}

@article{10.1016/j.datak.2010.07.009,
author = {Parreiras, Fernando Silva and Staab, Steffen},
title = {Editorial: Using ontologies with UML class-based modeling: The TwoUse approach},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {11},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2010.07.009},
doi = {10.1016/j.datak.2010.07.009},
abstract = {UML class-based models and OWL ontologies constitute modeling approaches with different strengths and weaknesses that make them appropriate for specifying distinct aspects of software systems. We propose an integrated use of both modeling approaches in a coherent framework - TwoUse. We present a framework involving different concrete syntaxes for developing integrated models and use a SPARQL-like approach for writing query operations. We illustrate TwoUse's applicability with a case study and conclude that TwoUse achieves enhancements of non-functional software requirements like maintainability, reusability and extensibility.},
journal = {Data Knowl. Eng.},
month = nov,
pages = {1194–1207},
numpages = {14},
keywords = {CASE tools+UML, Language definition, Manipulation, Ontologies}
}

@inproceedings{10.1007/11560333_12,
author = {Shin, Insik and Lee, Insup},
title = {A compositional framework for real-time embedded systems},
year = {2005},
isbn = {3540291032},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11560333_12},
doi = {10.1007/11560333_12},
abstract = {While component technology has been widely accepted as a methodology for designing complex systems, there are few component technologies that have been developed to accommodate the characteristics of embedded systems. Embedded systems are often subject to resource constraints as well as timing constraints. Typical scarce resources include memory for cost-sensitive systems. Many techniques, developed for reducing code size, often yield code size vs. execution time tradeoffs. Our goal is to develop a framework for supporting the compositionality of resource and timing properties. The proposed framework allows component-level resource and timing properties, which include the resource/time tradeoffs, to be independently analyzed, abstracted, and composed into the system-level resource and timing properties. In this paper, we focus on the problem of composing the collective task-level code size vs. execution time tradeoffs into a component-level code size vs. execution time tradeoff.},
booktitle = {Proceedings of the Second International Conference on Service Availability},
pages = {137–148},
numpages = {12},
location = {Berlin, Germany},
series = {ISAS'05}
}

@article{10.1016/j.cogsys.2019.11.001,
author = {Osuna, Enrique and Rodr\'{\i}guez, Luis-Felipe and Gutierrez-Garcia, J. Octavio and Castro, Luis A.},
title = {Development of computational models of emotions: A software engineering perspective},
year = {2020},
issue_date = {May 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {60},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2019.11.001},
doi = {10.1016/j.cogsys.2019.11.001},
journal = {Cogn. Syst. Res.},
month = may,
pages = {1–19},
numpages = {19},
keywords = {Computational model of emotion, Software engineering, Formal development process, Software methodology}
}

@article{10.1016/j.jss.2011.06.071,
author = {Kilamo, Terhi and Hammouda, Imed and Mikkonen, Tommi and Aaltonen, Timo},
title = {From proprietary to open source-Growing an open source ecosystem},
year = {2012},
issue_date = {July, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {7},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.06.071},
doi = {10.1016/j.jss.2011.06.071},
abstract = {In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem.},
journal = {J. Syst. Softw.},
month = jul,
pages = {1467–1478},
numpages = {12},
keywords = {Open source, Open source engineering, Opening proprietary software, Software ecosystem}
}

@article{10.1016/j.adhoc.2012.06.002,
author = {Khalil, Issa M. and Khreishah, Abdallah and Ahmed, Faheem and Shuaib, Khaled},
title = {Dependable wireless sensor networks for reliable and secure humanitarian relief applications},
year = {2014},
issue_date = {February, 2014},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {13},
issn = {1570-8705},
url = {https://doi.org/10.1016/j.adhoc.2012.06.002},
doi = {10.1016/j.adhoc.2012.06.002},
abstract = {Disasters such as flooding, earthquake, famine and terrorist attacks might occur any time anywhere without prior warnings. In most cases it is difficult to predict when a disaster might occur however, well-planned disaster recovery procedures will reduce the intensity of expected consequences. When a disaster occurs, infrastructure based communications are most likely to be crippled, worsening the critical situation on hand. Wireless ad hoc and sensor network (WASN) technologies are proven to be valuable in coordinating and managing rescue operations during disasters. However, the increasing reliance on WASNs make them attractive to malicious attackers, especially terrorist groups, in a bid to hamper rescue operations amplifying the damage and increasing the number of casualties. Therefore, it is necessary to ensure the fidelity of data traffic through WASN against malicious traffic disruption attacks. In this paper, we first demonstrate how WASN can be used in a well-planned disaster recovery effort. Then, we introduce and analyze one of the most severe traffic disruption attacks against WASNs, called Identity Delegation, and its countermeasures. Its severity lies in its capability to evade detection by even state-of-the-art intrusion detection techniques such as the neighbor monitoring based mechanisms. Through identity delegation, an adversary can drop packets, evade detection, and frame innocent nodes for dropping the traffic. We introduce a technique to mitigate identity delegation attack, dubbed Sadec, and compare it with the state-of-the-art mitigation technique namely Basic Local Monitoring (BLM) under a wide range of network scenarios. Our analysis which is validated by extensive ns-2 simulation scenarios show that BLM fails to efficiently mitigate packet drop through identity delegation attacks while Sadec successfully mitigates them. The results also show that Sadec achieves higher delivery ratios of data packets compared to BLM. On the other hand, the results show similar behavior in framing probabilities between Sadec and BLM. However, the desirable features of Sadec come at the expense of higher false isolation probabilities in networks with heavy traffic load and poor communication links.},
journal = {Ad Hoc Netw.},
month = feb,
pages = {94–106},
numpages = {13},
keywords = {Identity delegation, Local monitoring, Multi-hop wireless networks, Packet dropping, Security attacks}
}

@inproceedings{10.5555/2041790.2041832,
author = {Capilla, Rafael and Zimmermann, Olaf and Zdun, Uwe and Avgeriou, Paris and K\"{u}ster, Jochen M.},
title = {An enhanced architectural knowledge metamodel linking architectural design decisions to other artifacts in the software engineering lifecycle},
year = {2011},
isbn = {9783642237973},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software architects create and consume many interrelated artifacts during the architecting process. These artifacts may represent functional and nonfunctional requirements, architectural patterns, infrastructure topology units, code, and deployment descriptors as well as architecturally significant design decisions. Design decisions have to be linked to chunks of architecture description in order to achieve a fine-grained control when a design is modified. Moreover, it is imperative to identify quickly the key decisions affected by a runtime change that are critical for a system's mission. This paper extends previous work on architectural knowledge with a metamodel for architectural decision capturing and sharing to: (i) create and maintain fine-grained dependency links between the entities during decision identification, making, and enforcement, (ii) keep track of the evolution of the decisions, and (iii) support runtime decisions.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture},
pages = {303–318},
numpages = {16},
keywords = {architectural design decisions, architectural knowledge, evolution, metamodel, runtime decisions, traceability},
location = {Essen, Germany},
series = {ECSA'11}
}

@inproceedings{10.1007/978-3-642-31762-0_13,
author = {Johnsen, Einar Broch and Schlatte, Rudolf and Tapia Tarifa, S. Lizeth},
title = {A formal model of user-defined resources in resource-restricted deployment scenarios},
year = {2011},
isbn = {9783642317613},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-31762-0_13},
doi = {10.1007/978-3-642-31762-0_13},
abstract = {Software today is often developed for deployment on varying architectures. In order to model and analyze the consequences of such deployment choices at an early stage in software development, it seems desirable to capture aspects of low-level deployment concerns in high-level models. In this paper, we propose an integration of a generic cost model for resource consumption with deployment components in Timed ABS, an abstract behavioral specification language for executable object-oriented models. The actual cost model may be user-defined and specified by means of annotations in the executable Timed ABS model, and can be used to capture specific resource requirements such as processing capacity or memory usage. Architectural variations are specified by resource-restricted deployment scenarios with different capacities. For this purpose, the models have deployment components which are parametric in their assigned resources. The approach is demonstrated on an example of multimedia processing servers with a user-defined cost model for memory usage. We use our simulation tool to analyze deadline misses for given usage and deployment scenarios.},
booktitle = {Proceedings of the 2011 International Conference on Formal Verification of Object-Oriented Software},
pages = {196–213},
numpages = {18},
location = {Turin, Italy},
series = {FoVeOOS'11}
}

@article{10.1016/j.jss.2009.08.032,
author = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Ali Babar, Muhammad},
title = {A comparative study of architecture knowledge management tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.08.032},
doi = {10.1016/j.jss.2009.08.032},
abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
journal = {J. Syst. Softw.},
month = mar,
pages = {352–370},
numpages = {19},
keywords = {Architectural design, Architectural knowledge management tool, Design rationale}
}

@article{10.4018/jaras.2010100105,
author = {Pfeffer, Heiko and Baude, Fran\c{c}oise and Legrand, Virginie and Henrio, Ludovic and Naoumenko, Paul and Bassbouss, Louay and Linner, David},
title = {Mixing Workflows and Components to Support Evolving Services},
year = {2010},
issue_date = {October 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {4},
issn = {1947-9220},
url = {https://doi.org/10.4018/jaras.2010100105},
doi = {10.4018/jaras.2010100105},
abstract = {Composite distributed services involve local and remote services that get orchestrated according to specific business logic. This logic can be programmed by applying a traditional general-purpose programming language, but is generally described using a workflow language that coordinates a set of given services. The services involved in the composition, or the composition may need to evolve both at the business logic level (workflow level) and the global architecture level. This paper presents a solution to ease such evolution for compound distributed services and the authors' proposal enables the evolution of both the business logic and the underlying architecture. This paper suggests relying on a distributed software component model to represent and easily manage the set of local or remote software entities (services) involved in the composition. Composite services are represented in a model that combines the use of a distributed and hierarchical software component model and new timed-automata based workflow language. This combination makes explicit the separation between functional and non-functional concerns, and as a consequence this approach helps in defining the required and various evolution procedures in context to compound services.},
journal = {Int. J. Adapt. Resilient Auton. Syst.},
month = oct,
pages = {60–84},
numpages = {25},
keywords = {Components, GCM, Service Composition, Service Evolution, Workflows}
}

@article{10.4018/joci.2010100101,
author = {Briscoe, Gerard and De Wilde, Philippe},
title = {The Computing of Digital Ecosystems},
year = {2010},
issue_date = {October 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {4},
issn = {1947-9344},
url = {https://doi.org/10.4018/joci.2010100101},
doi = {10.4018/joci.2010100101},
abstract = {A primary motivation this research in digital ecosystems is the desire to exploit the self-organising properties of biological ecosystems. Ecosystems are thought to be robust, scalable architectures that can automatically solve complex and dynamic problems. However, the computing technologies that contribute to these properties have not been made explicit in digital ecosystems research. In this paper, the authors discuss how different computing technologies can contribute to providing the necessary self-organising features, including Multi-Agent Systems MASs, Service-Oriented Architectures SOAs, and distributed evolutionary computing DEC. The potential for exploiting these properties in digital ecosystems is considered, suggesting how several key features of biological ecosystems can be exploited in Digital Ecosystems, and discussing how mimicking these features may assist in developing robust, scalable self-organising architectures. An example architecture, the Digital Ecosystem, is considered in detail. The Digital Ecosystem is then measured experimentally through simulations, which consider the self-organised diversity of its evolving agent populations relative to the user request behaviour.},
journal = {Int. J. Organ. Collect. Intell.},
month = oct,
pages = {1–17},
numpages = {17},
keywords = {Agents, Business, Ecosystems, Evolution, Services}
}

@inproceedings{10.1109/ASEW.2008.4686323,
author = {Brcina, Robert and Riebisch, Matthias},
title = {Architecting for evolvability by means of traceability and features},
year = {2008},
isbn = {9781424427765},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASEW.2008.4686323},
doi = {10.1109/ASEW.2008.4686323},
abstract = {The frequent changes during the development and usage of large software systems often lead to a loss of architectural quality which hampers the implementation of further changes and thus the systems' evolution. To maintain the evolvability of such software systems, their architecture has to fulfil particular quality criteria. Available metrics and rigour approaches do not provide sufficient means to evaluate architectures regarding these criteria, and reviews require a high effort. This paper presents an approach for an evaluation of architectural models during design decisions, for early feedback and as part of architectural assessments. As the quality criteria for evolvability, model relations in terms of traceability links between feature model, design and implementation are evaluated. Indicators are introduced to assess these model relations, similar to metrics, but accompanied by problem resolution actions. The indicators are defined formally to enable a tool-based evaluation. The approach has been developed within a large software project for an IT infrastructure.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {III–72–III–81},
location = {L'Aquila, Italy},
series = {ASE'08}
}

@inproceedings{10.1145/1985793.1985882,
author = {Hutchinson, John and Rouncefield, Mark and Whittle, Jon},
title = {Model-driven engineering practices in industry},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985882},
doi = {10.1145/1985793.1985882},
abstract = {In this paper, we attempt to address the relative absence of empirical studies of model driven engineering through describing the practices of three commercial organizations as they adopted a model driven engineering approach to their software development. Using in-depth semi-structured interviewing we invited practitioners to reflect on their experiences and selected three to use as exemplars or case studies. In documenting some details of attempts to deploy model driven practices, we identify some 'lessons learned', in particular the importance of complex organizational, managerial and social factors - as opposed to simple technical factors - in the relative success, or failure, of the endeavour. As an example of organizational change management the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {633–642},
numpages = {10},
keywords = {empirical evaluation, model driven engineering, software engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.5555/646781.705927,
author = {Pulvermueller, Elke and Speck, Andreas and Coplien, James and D'Hondt, Maja and Meuter, Wolfgang De},
title = {Feature Interaction in Composed Systems},
year = {2001},
isbn = {3540436758},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The history of computer science has shown that decomposing software applications helps managing their complexity and facilitates reuse, but also bears challenging problems still unsolved, such as the assembly of the decomposed features when non-trivial feature interactions are involved. Examples of features include concerns or aspects, black box or white box components, and functional and non-functional requirements. Approaches such as object-oriented and component-based software development, as well as relatively new directions such as aspect-oriented programming, multi-dimensional separation of concerns and generative programming, all provide technical support for the definition and syntactical assembly of features, but fall short on the semantic level, for example in spotting meaningless or even faulty combinations. At previous ECOOPs, OOPSLAsand GCSEs dedicated events have been organised around the aforementioned technologies, where we experienced a growing awareness of this feature interaction problem. However, feature interaction is often merely dismissed as a secondary problem, percolating as an afterthought while other issues are being addressed. The intention of this workshop was to be the first co-ordinated effort to address the general problem of feature interaction in composed systems separately from other issues.},
booktitle = {Proceedings of the Workshops on Object-Oriented Technology},
pages = {86–97},
numpages = {12},
series = {ECOOP '01}
}

@article{10.1016/j.robot.2016.10.013,
author = {St-Onge, David and Brches, Pierre-Yves and Sharf, Inna and Reeves, Nicolas and Rekleitis, Ioannis and Abouzakhm, Patrick and Girdhar, Yogesh and Harmat, Adam and Dudek, Gregory and Gigure, Philippe},
title = {Control, localization and human interaction with an autonomous lighter-than-air performer},
year = {2017},
issue_date = {February 2017},
publisher = {North-Holland Publishing Co.},
address = {NLD},
volume = {88},
number = {C},
issn = {0921-8890},
url = {https://doi.org/10.1016/j.robot.2016.10.013},
doi = {10.1016/j.robot.2016.10.013},
abstract = {Due to the recent technological progress, HumanRobotInteraction (HRI) has become a major field of research in both engineering and artistic realms, particularly so in the last decade. The mainstream interests are, however, extremely diverse: challenges are continuously shifting, the evolution of robot skills, as well as the advances in methods for understanding their environment radically impact the design and implementation of research prototypes. When directly deployed in a public installation or artistic performances, robots help foster the next level of understanding in HRI. To this effect, this paper presents a successful interdisciplinary art-science-technology project, the Aerostabiles, leading to a new way of conducting HRI research. The project consists of developing a mechatronic, intelligent platform embodied in multiple geometric blimps cubes that hover and move in the air. The artistic context of this project required a number of advances in engineering on the aspects of localization and control systems, flight dynamics, as well as interaction strategies, and their evolution through periods of collective activities called researchcreation residencies. These events involve artists, engineers, and performers working in close collaboration, sometimes, over several weeks at a time. They generate fruitful exchanges between all researchers, but most of all, they present a unique and creative way to direct and focus the robotics development. This paper represents an overview of the technical contributions from a range of expertise through the artistic drive of the Aerostabiles project. Presents an art-science-technology project for HRI research in the artistic realm.Details the dynamics, with effects of its added mass and vision-based localization.Experiments with controlled trajectories of the blimp for evoking emotions.Denes a sonar-based sphere of intimacy and a vision-based perplexing features scanner.Uses of electromyography sensors to interpret performers moods.},
journal = {Robot. Auton. Syst.},
month = feb,
pages = {165–186},
numpages = {22},
keywords = {Airship, Dynamic modeling, Humanrobot interaction, Mobile robotics, Robotic art, Robotic blimp, Theater}
}

@article{10.1007/s10515-015-0188-0,
author = {Bulej, Lubom\'{\i}r and Bure\v{s}, Tom\'{a}\v{s} and Hork\'{y}, Vojtundefinedch and Kotr\u{a}\'{z}, Jaroslav and Marek, Luk\'{a}\v{s} and Troj\'{a}nek, Tom\'{a}\v{s} and T\'{z}Ma, Petr},
title = {Unit testing performance with Stochastic Performance Logic},
year = {2017},
issue_date = {March     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-015-0188-0},
doi = {10.1007/s10515-015-0188-0},
abstract = {Unit testing is an attractive quality management tool in the software development process, however, practical obstacles make it difficult to use unit tests for performance testing. We present Stochastic Performance Logic, a formalism for expressing performance requirements, together with interpretations that facilitate performance evaluation in the unit test context. The formalism and the interpretations are implemented in a performance testing framework and evaluated in multiple experiments, demonstrating the ability to identify performance differences in realistic unit test scenarios.},
journal = {Automated Software Engg.},
month = mar,
pages = {139–187},
numpages = {49},
keywords = {Java, Performance evaluation, Unit testing}
}

@inproceedings{10.5555/3242181.3242210,
author = {Ahmed, Kishwar and Liu, Jason and Badawy, Abdel-Hameed and Eidenbenz, Stephan},
title = {A brief history of HPC simulation and future challenges},
year = {2017},
isbn = {9781538634271},
publisher = {IEEE Press},
abstract = {High-performance Computing (HPC) systems have gone through many changes during the past two decades in their architectural design to satisfy the increasingly large-scale scientific computing demand. Accurate, fast, and scalable performance models and simulation tools are essential for evaluating alternative architecture design decisions for the massive-scale computing systems. This paper recounts some of the influential work in modeling and simulation for HPC systems and applications, identifies some of the major challenges, and outlines future research directions which we believe are critical to the HPC modeling and simulation community.},
booktitle = {Proceedings of the 2017 Winter Simulation Conference},
articleno = {27},
numpages = {12},
location = {Las Vegas, Nevada},
series = {WSC '17}
}

@article{10.1016/j.jss.2010.06.043,
author = {Unphon, Hataichanok and Dittrich, Yvonne},
title = {Software architecture awareness in long-term software product evolution},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {11},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.06.043},
doi = {10.1016/j.jss.2010.06.043},
abstract = {Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a 'walking architecture' devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the 'architecturing' practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the 'walking architecture'.},
journal = {J. Syst. Softw.},
month = nov,
pages = {2211–2226},
numpages = {16},
keywords = {Architecture knowledge management, Cooperative and human aspects, Long-term evolution, Qualitative empirical studies, Software architecture, Software products}
}

@article{10.1016/j.eswa.2020.114161,
author = {Houssein, Essam H. and Emam, Marwa M. and Ali, Abdelmgeid A. and Suganthan, Ponnuthurai Nagaratnam},
title = {Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114161},
doi = {10.1016/j.eswa.2020.114161},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {20},
keywords = {Breast cancer classification, Convolutional neural network, Computer-aided diagnosis system (CAD), Deep learning, Histological images, Machine learning, Magnetic resonance imaging (MRI), Medical imaging modalities, Mammogram images, Ultrasound images, Thermography images}
}

@inproceedings{10.1145/1988676.1988682,
author = {Nowak, Marcin and Pautasso, Cesare},
title = {Goals, questions and metrics for architectural decision models},
year = {2011},
isbn = {9781450305969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1988676.1988682},
doi = {10.1145/1988676.1988682},
abstract = {Architectural decisions are the key element behind the design process leading to a software architecture. Making software architects aware of the implications of their decisions is only the beginning of what can be achieved by capturing the rationale and the constraints influencing the decision making process in a reusable body of architectural knowledge. In this paper we propose a metric-based approach to the analysis of architectural decision models. Using a hierarchically-structured approach we identify a number of useful goals and stakeholders involved in the architectural design process. Next, we sketch a set of metrics to provide data for the evaluation of the aforementioned goals. Our aim is to stimulate a discussion on how to find indicators relevant for software architects by measuring the intrinsic properties of architectural knowledge.},
booktitle = {Proceedings of the 6th International Workshop on SHAring and Reusing Architectural Knowledge},
pages = {21–28},
numpages = {8},
keywords = {architectural decision modeling, software architecture, visualization},
location = {Waikiki, Honolulu, HI, USA},
series = {SHARK '11}
}

@article{10.1007/s10664-009-9120-1,
author = {Hackbarth, Randy L. and Mockus, Audris and Palframan, John Douglas and Weiss, David M.},
title = {Assessing the state of software in a large enterprise},
year = {2010},
issue_date = {June      2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9120-1},
doi = {10.1007/s10664-009-9120-1},
abstract = {To be relevant to the goals of an enterprise, an industrial software engineering research organization must identify problems of interest to, and find solutions that have an impact on, the software development organizations within the company. Using a systematic measurement program both to identify the problems and assess the impact of solutions is key to satisfying this need. Avaya has had such a program in place for about seven years. Every year we produce an annual report known as the State of Software in Avaya that describes software development trends throughout the company and that contains prioritized recommendations for improving Avaya's software development capabilities. We start by identifying the goals of the enterprise and use the goal-question-metric approach to identify the measures to compute. The result is insight into the enterprise's problems in software development, recommendations for improving the development process, and problems that require research to solve. We will illustrate the process with examples from the Software Technology Research Department in Avaya Labs whose purpose is to improve the state of software development and know it. "Know it" means that improvement should be subjectively evident and objectively quantifiable. "Know it" also means that one must be skilled at identifying the data sources, performing the appropriate analyses to answer the questions of interest, and validating that the data are accurate and appropriate for the purpose. Examples will include how and why we developed a measure of software quality that appeals to customers, how and why we are studying the effectiveness of distributed software development, and how and why we are helping development organizations to adopt iterative development methods. We will also discuss how we keep the company and the department apprised of the current strengths and weaknesses of software development in Avaya through the publication of the annual State of Software in Avaya Report. Our purpose is both to provide a model for assessment that others may emulate, based on seven years of experience, and to spotlight analyses and conclusions that we feel are common to software development today.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {219–249},
numpages = {31},
keywords = {Agile Development, Code growth, Distributed and offshore development, Iterative development, R&amp;D assessment, R&amp;D domain expertise, R&amp;D productivity, Software assessments, Software development, Software engineering, Software metrics, Software practices, Software process improvement, Software quality}
}

@article{10.1016/j.jpdc.2019.05.002,
author = {Hafaiedh, Imene Ben},
title = {A generic formal model for the comparison and analysis of distributed job-scheduling algorithms in grid environment},
year = {2019},
issue_date = {Oct 2019},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2019.05.002},
doi = {10.1016/j.jpdc.2019.05.002},
journal = {J. Parallel Distrib. Comput.},
month = oct,
pages = {331–343},
numpages = {13},
keywords = {Formal verification, Distributed scheduling, Grid computing environment, Load balancing algorithms}
}

@article{10.1109/TASLP.2018.2860786,
author = {Salehi, Haniyeh and Suelzle, David and Folkeard, Paula and Parsa, Vijay},
title = {Learning-Based Reference-Free Speech Quality Measures for Hearing Aid Applications},
year = {2018},
issue_date = {December 2018},
publisher = {IEEE Press},
volume = {26},
number = {12},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2018.2860786},
doi = {10.1109/TASLP.2018.2860786},
abstract = {Objective measures of speech quality are highly desirable in benchmarking and monitoring the performance of hearing aids HAs. Existing HA speech quality indices such as the hearing aid speech quality index HASQI are intrusive in that they require a properly time-aligned and frequency-shaped reference signal to predict the quality of HA output. Two new reference-free HA speech quality indices are proposed in this paper, based on a model that amalgamates perceptual linear prediction PLP, hearing loss HL modeling, and machine learning concepts. For the first index, HL-modified PLP coefficients and their statistics were used as the feature set, which was subsequently mapped to the predicted quality scores using support vector regression SVR. For the second index, HL-impacted gammatone auditory filterbank energies and their second-order statistics constituted the feature set, which was again mapped using SVR. Two databases involving HA recordings were collected and utilized for the evaluation of the robustness and generalizability of the two indices. Experimental results showed that the index based on the gammatone filterbank energies not only correlated well with HA quality ratings by hearing impaired listeners, but also exhibited robust performance across different test conditions and was comparable to the full-reference HASQI performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {2277–2288},
numpages = {12}
}

@inproceedings{10.5555/2394450.2394490,
author = {Her, Jin Sun and Choi, Si Won and Cheun, Du Wan and Bae, Jeong Seop and Kim, Soo Dong},
title = {A component-based process for developing automotive ECU software},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software plays a vital role in operating modern automobiles, and it is a key element in providing innovative features such as Collision Prevention System. There are two essential issues to be resolved; managing software complexity, and reducing software cost and time-to-market. A key solution to these two issues is to maximize reusing components in building various Electronic Control Units (ECUs). Component-based development (CBD) is regarded as an effective reuse technology. However, current CBD methodologies do not effectively support developing reusable automotive components and ECUs. Hence, in this paper, we first define variability types and variation points for ECUs. Based on the variability types, we propose a component-based development process for developing ECUs. To assess the applicability of the proposed CBD process, we present the case study of developing an innovative automotive ECU for Automatic Parking System (APS).},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {358–373},
numpages = {16},
location = {Riga, Latvia},
series = {PROFES'07}
}

@inbook{10.5555/1980562.1980563,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@inbook{10.5555/1986548.1986549,
author = {Kienzle, J\"{o}rg and Guelfi, Nicolas and Mustafiz, Sadaf},
title = {Crisis management systems: a case study for aspect-oriented modeling},
year = {2010},
isbn = {3642160859},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The intent of this document is to define a common case study for the aspect-oriented modeling research community. The domain of the case study is crisis management systems, i.e., systems that help in identifying, assessing, and handling a crisis situation by orchestrating the communication between all parties involved in handling the crisis, by allocating and managing resources, and by providing access to relevant crisis-related information to authorized users. This document contains informal requirements of crisis management systems (CMSs) in general, a feature model for a CMS product line, use case models for a car crash CMS (CCCMS), a domain model for the CCCMS, an informal physical architecture description of the CCCMS, as well as some design models of a possible object-oriented implementation of parts of the CCCMS backend. AOM researchers who want to demonstrate the power of their AOM approach or technique can hence apply the approach at the most appropriate level of abstraction.},
booktitle = {Transactions on Aspect-Oriented Software Development VII: A Common Case Study for Aspect-Oriented Modeling},
pages = {1–22},
numpages = {22}
}

@article{10.1016/j.advengsoft.2009.10.005,
author = {Contreras, Felipe and Hitschfeld-Kahler, Nancy and Bastarrica, Mar\i{}a Cecilia and Lillo, Carlos},
title = {Balancing flexibility and performance in three dimensional meshing tools},
year = {2010},
issue_date = {March, 2010},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {41},
number = {3},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2009.10.005},
doi = {10.1016/j.advengsoft.2009.10.005},
abstract = {It is generally thought within the meshing tool community that object-orientation and other decoupling techniques penalize performance when they are used for building concrete meshing tools. In this paper we show that building a meshing tool with good object-oriented design metrics could not only improve maintainability and all other derived attributes such as portability and extensibility, but also its performance is comparable to a standard meshing tool that implements the same algorithms.},
journal = {Adv. Eng. Softw.},
month = mar,
pages = {471–479},
numpages = {9},
keywords = {Meshing tools, Object-oriented design, Software architecture}
}

@inproceedings{10.1007/978-3-030-27544-0_12,
author = {Felbinger, Georg Christian and G\"{o}ttsch, Patrick and Loth, Pascal and Peters, Lasse and Wege, Felix},
title = {Designing Convolutional Neural Networks Using a Genetic Approach for Ball Detection},
year = {2018},
isbn = {978-3-030-27543-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27544-0_12},
doi = {10.1007/978-3-030-27544-0_12},
abstract = {At RoboCup 2017, the HULKs reached the Standard Platform League’s quarter finals and won the mixed team competition together with our fellow team B-Human. This paper describes the design of a convolutional neural network used for the detection of the black and white ball - one of the key contributions that led to the team’s success. We present a genetic design approach that optimizes network hyperparameters for a cost effective inference on the NAO, with limited amount of training data. Experimental results demonstrate that the genetic algorithm is able to optimize the hyperparameters of convolutional neural networks. We show that the resulting network is able to run in real-time on the robot with a very precise classification in generalization test.},
booktitle = {RoboCup 2018: Robot World Cup XXII},
pages = {150–161},
numpages = {12},
location = {Montr\'{e}al, QC, Canada}
}

@article{10.1016/j.jss.2007.08.025,
author = {Jansen, Anton and Bosch, Jan and Avgeriou, Paris},
title = {Documenting after the fact: Recovering architectural design decisions},
year = {2008},
issue_date = {April, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.08.025},
doi = {10.1016/j.jss.2007.08.025},
abstract = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. ADDRA uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system.},
journal = {J. Syst. Softw.},
month = apr,
pages = {536–557},
numpages = {22},
keywords = {Architectural design decisions, Software architecture recovery}
}

@inproceedings{10.1007/11763864_27,
author = {Kolb, Ronny and Ganesan, Dharmalingam and Muthig, Dirk and Kagino, Masanori and Teranishi, Hideharu},
title = {Goal-Oriented performance analysis of reusable software components},
year = {2006},
isbn = {3540346066},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11763864_27},
doi = {10.1007/11763864_27},
abstract = {To establish software reuse successfully in the long run, it is crucial for providers of reusable components to continuously react on problems or future trends arising around their component. In practice, however, many providers of reusable components are not able to do so due to insufficient feedback and details from reusers. Additionally, they often have too little knowledge on system context and constraints that may lead to major deficits of the reusable component especially with respect to non-functional aspects. This paper presents an approach for systematically engineering performance of reusable components that has been validated in an industrial context.},
booktitle = {Proceedings of the 9th International Conference on Reuse of Off-the-Shelf Components},
pages = {368–381},
numpages = {14},
location = {Turin, Italy},
series = {ICSR'06}
}

@inproceedings{10.1145/2897053.2897062,
author = {McGee, Ethan T. and McGregor, John D.},
title = {Using dynamic adaptive systems in safety-critical domains},
year = {2016},
isbn = {9781450341875},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897053.2897062},
doi = {10.1145/2897053.2897062},
abstract = {The development of safety-critical Cyber-Physical Systems (CPS) is expanding due to the Internet of Things' promise to make high-integrity applications and services part of everyday life. This expansion is seen in the dependencies some connected vehicles have on cloud services that provide guidance and accident avoidance / detection features. Such systems are safety-critical since failure could result in serious injury or death. Due to the severe consequences of failure, fault-tolerance, reliability and dependability should be primary driving qualities in the design and development of these systems. However, the cost of the analysis, evaluation and certification activities needed to ensure that the possibility of failure has been sufficiently mitigated is significantly higher than the cost of developing traditional software.Our group is exploring the addition of dynamic adaptive capabilities to safety-critical systems. We postulate that dynamic adaptivity could provide several enhancements to safety-critical systems. It would allow systems to reason about the environment within which they are sited and about their internal operation enabling decision making that is context-specific and appropriately prioritized. However, the addition of adaptivity with the associated overhead of reasoning is not without drawbacks particularly when hard real-time safety-critical systems are involved. In this brief position paper, we explore some of the questions and concerns that are raised when dynamic adaptive behavior is introduced into safety-critical systems as well as ways that the Architecture Analysis &amp; Design Language (AADL) can be used to model / analyze such systems.},
booktitle = {Proceedings of the 11th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {115–121},
numpages = {7},
keywords = {dynamic adaptive systems, dynamic software product lines, safety critical systems, software product lines},
location = {Austin, Texas},
series = {SEAMS '16}
}

@article{10.1007/s10664-013-9263-y,
author = {Bjarnason, Elizabeth and Runeson, Per and Borg, Markus and Unterkalmsteiner, Michael and Engstr\"{o}m, Emelie and Regnell, Bj\"{o}rn and Sabaliauskaite, Giedre and Loconsole, Annabella and Gorschek, Tony and Feldt, Robert},
title = {Challenges and practices in aligning requirements with verification and validation: a case study of six companies},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9263-y},
doi = {10.1007/s10664-013-9263-y},
abstract = {Weak alignment of requirements engineering (RE) with verification and validation (VV) may lead to problems in delivering the required products in time with the right quality. For example, weak communication of requirements changes to testers may result in lack of verification of new requirements and incorrect verification of old invalid requirements, leading to software quality problems, wasted effort and delays. However, despite the serious implications of weak alignment research and practice both tend to focus on one or the other of RE or VV rather than on the alignment of the two. We have performed a multi-unit case study to gain insight into issues around aligning RE and VV by interviewing 30 practitioners from 6 software developing companies, involving 10 researchers in a flexible research process for case studies. The results describe current industry challenges and practices in aligning RE with VV, ranging from quality of the individual RE and VV activities, through tracing and tools, to change control and sharing a common understanding at strategy, goal and design level. The study identified that human aspects are central, i.e. cooperation and communication, and that requirements engineering practices are a critical basis for alignment. Further, the size of an organisation and its motivation for applying alignment practices, e.g. external enforcement of traceability, are variation factors that play a key role in achieving alignment. Our results provide a strategic roadmap for practitioners improvement work to address alignment challenges. Furthermore, the study provides a foundation for continued research to improve the alignment of RE with VV.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1809–1855},
numpages = {47},
keywords = {Alignment, Case study, Requirements engineering, Testing, Validation, Verification}
}

@article{10.1016/j.future.2014.11.010,
author = {Lytra, Ioanna and Tran, Huy and Zdun, Uwe},
title = {Harmonizing architectural decisions with component view models using reusable architectural knowledge transformations and constraints},
year = {2015},
issue_date = {June 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2014.11.010},
doi = {10.1016/j.future.2014.11.010},
abstract = {Architectural design decisions (ADDs) have been used in recent years for capturing design rationale and documenting architectural knowledge (AK). However, various architectural design views still provide the most common means for describing and communicating architectural design. The evolution of software systems requires that both ADDs and architectural design views are documented and maintained, which is a tedious and time-consuming task in the long run. Also, in lack of a systematic and automated support for bridging between ADDs and architectural design views, decisions and designs tend to become inconsistent over time. In our proposal, we introduce a reusable AK transformation language for supporting the automated transformation of reusable AK knowledge to component-and-connector models, the architectural design view used most commonly today. In addition, reusable consistency checking rules verify the consistency between decisions and designs. We evaluate our approach in an industrial case study and show that it offers high reusability, provides automation, and can, in principle, deal with large numbers of recurring decisions. Reusable AK transformation language for automated transformation of ADDs into designs.Reusable constraints for consistency checking between decisions and designs.Tool support based on integration of two existing tools-ADvISE and VbMF.},
journal = {Future Gener. Comput. Syst.},
month = jun,
pages = {80–96},
numpages = {17},
keywords = {AK transformation language, Architectural decisions, Architectural design, Architectural knowledge, Consistency checking}
}

@article{10.1007/s00354-021-00126-2,
author = {Li, Peipei and Wu, Man and He, Junhong and Hu, Xuegang},
title = {Recurring Drift Detection and Model Selection-Based Ensemble Classification for Data Streams with Unlabeled Data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Ohmsha},
address = {JPN},
volume = {39},
number = {2},
issn = {0288-3635},
url = {https://doi.org/10.1007/s00354-021-00126-2},
doi = {10.1007/s00354-021-00126-2},
abstract = {Data stream classification is widely popular in the field of network monitoring, sensor network and electronic commerce, etc. However, in the real-world applications, recurring concept drifting and label missing in data streams seriously aggravate the difficulty on the classification solutions. And this challenge has received little attention from the research community. Motivated by this, we propose a new ensemble classification approach based on the recurring concept drifting detection and model selection for data streams with unlabeled data. First, we build an ensemble model based on the classifiers and clusters. To improve the classification accuracy, we use the ensemble model to predict each data chunk and partition clusters according to the distribution of predicted class labels. Second, we adopt a new concept drifting detection method based on the divergence of concept distributions between adjoining data chunks to distinguish recurring concept drifts. All historical new concepts will be maintained. Meanwhile, we introduce the time-stamp-based weights for base models in the ensemble model. In the selection of the base model, we consider the time-stamp-based weight and the divergence between concept distributions simultaneously. Finally, extensive experiments conducted on four benchmark data sets show that our approach can quickly adapt to data streams with recurring concept drifts, and improve the classification accuracy compared to several state-of-the-art classification algorithms for data streams with concept drifts and unlabeled data.},
journal = {New Gen. Comput.},
month = aug,
pages = {341–376},
numpages = {36},
keywords = {Data stream classification, Ensemble learning, Recurring concept drift, Unlabeled data}
}

@inproceedings{10.1145/3395351.3399421,
author = {Acar, Abbas and Fereidooni, Hossein and Abera, Tigist and Sikder, Amit Kumar and Miettinen, Markus and Aksu, Hidayet and Conti, Mauro and Sadeghi, Ahmad-Reza and Uluagac, Selcuk},
title = {Peek-a-boo: i see your smart home activities, even encrypted!},
year = {2020},
isbn = {9781450380065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395351.3399421},
doi = {10.1145/3395351.3399421},
abstract = {A myriad of IoT devices such as bulbs, switches, speakers in a smart home environment allow users to easily control the physical world around them and facilitate their living styles through the sensors already embedded in these devices. Sensor data contains a lot of sensitive information about the user and devices. However, an attacker inside or near a smart home environment can potentially exploit the innate wireless medium used by these devices to exfiltrate sensitive information from the encrypted payload (i.e., sensor data) about the users and their activities, invading user privacy. With this in mind, in this work, we introduce a novel multi-stage privacy attack against user privacy in a smart environment. It is realized utilizing state-of-the-art machine-learning approaches for detecting and identifying the types of IoT devices, their states, and ongoing user activities in a cascading style by only passively sniffing the network traffic from smart home devices and sensors. The attack effectively works on both encrypted and unencrypted communications. We evaluate the efficiency of the attack with real measurements from an extensive set of popular off-the-shelf smart home IoT devices utilizing a set of diverse network protocols like WiFi, ZigBee, and BLE. Our results show that an adversary passively sniffing the traffic can achieve very high accuracy (above 90%) in identifying the state and actions of targeted smart home devices and their users. To protect against this privacy leakage, we also propose a countermeasure based on generating spoofed traffic to hide the device states and demonstrate that it provides better protection than existing solutions.},
booktitle = {Proceedings of the 13th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {207–218},
numpages = {12},
keywords = {BLE, ZigBee, network traffic, privacy, smart-home, wifi},
location = {Linz, Austria},
series = {WiSec '20}
}

@article{10.1016/j.infsof.2010.05.003,
author = {Ali, Muhammad Sarmad and Ali Babar, Muhammad and Chen, Lianping and Stol, Klaas-Jan},
title = {A systematic review of comparative evidence of aspect-oriented programming},
year = {2010},
issue_date = {September, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {9},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.003},
doi = {10.1016/j.infsof.2010.05.003},
abstract = {Context: Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Objective: This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches. Method: We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings. Results: Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported. Conclusion: AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.},
journal = {Inf. Softw. Technol.},
month = sep,
pages = {871–887},
numpages = {17},
keywords = {Aspect-oriented programming, Evidence-based software engineering, Systematic literature review}
}

@article{10.1007/s10270-010-0147-y,
author = {Choi, Yunja and Bunse, Christian},
title = {Design verification in model-based μ-controller development using an abstract component},
year = {2011},
issue_date = {February  2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-010-0147-y},
doi = {10.1007/s10270-010-0147-y},
abstract = {Component-based software development is a promising approach for controlling the complexity and quality of software systems. Nevertheless, recent advances in quality control techniques do not seem to keep up with the growing complexity of embedded software; embedded systems often consist of dozens to hundreds of software/hardware components that exhibit complex interaction behavior. Unanticipated quality defects in a component can be a major source of system failure. To address this issue, this paper suggests a design verification approach integrated into the model-driven, component-based development methodology Marmot. The notion of abstract components--the basic building blocks of Marmot--helps to lift the level of abstraction, facilitates high-level reuse, and reduces verification complexity by localizing verification problems between abstract components before refinement and after refinement. This enables the identification of unanticipated design errors in the early stages of development. This work introduces the Marmot methodology, presents a design verification approach in Marmot, and demonstrates its application on the development of a μ-controller-based abstraction of a car mirror control system. An application on TinyOS shows that the approach helps to reuse models as well as their verification results in the development process.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {91–115},
numpages = {25},
keywords = {Abstract component, Design verification, Embedded systems, Model-driven development}
}

@article{10.1007/s10664-013-9255-y,
author = {Borg, Markus and Runeson, Per and Ard\"{o}, Anders},
title = {Recovering from a decade: a systematic mapping  of information retrieval approaches  to software traceability},
year = {2014},
issue_date = {December  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9255-y},
doi = {10.1007/s10664-013-9255-y},
abstract = {Engineers in large-scale software development have to manage large amounts of information, spread across many artifacts. Several researchers have proposed expressing retrieval of trace links among artifacts, i.e. trace recovery, as an Information Retrieval (IR) problem. The objective of this study is to produce a map of work on IR-based trace recovery, with a particular focus on previous evaluations and strength of evidence. We conducted a systematic mapping of IR-based trace recovery. Of the 79 publications classified, a majority applied algebraic IR models. While a set of studies on students indicate that IR-based trace recovery tools support certain work tasks, most previous studies do not go beyond reporting precision and recall of candidate trace links from evaluations using datasets containing less than 500 artifacts. Our review identified a need of industrial case studies. Furthermore, we conclude that the overall quality of reporting should be improved regarding both context and tool details, measures reported, and use of IR terminology. Finally, based on our empirical findings, we present suggestions on how to advance research on IR-based  trace recovery.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1565–1616},
numpages = {52},
keywords = {Information retrieval, Software artifacts, Systematic mapping study, Traceability}
}

@inproceedings{10.5555/3304222.3304408,
author = {Zhang, Yuanxing and Zhang, Yangbin and Bian, Kaigui and Li, Xiaoming},
title = {Towards reading comprehension for long documents},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Machine reading comprehension has gained attention from both industry and academia. It is a very challenging task that involves various domains such as language comprehension, knowledge inference, summarization, etc. Previous studies mainly focus on reading comprehension on short paragraphs, and these approaches fail to perform well on the documents. In this paper, we propose a hierarchical match attention model to instruct the machine to extract answers from a specific short span of passages for the long document reading comprehension (LDRC) task. The model takes advantages from hierarchical-LSTM to learn the paragraph-level representation, and implements the match mechanism (i.e., quantifying the relationship between two contexts) to find the most appropriate paragraph that includes the hint of answers. Then the task can be decoupled into reading comprehension task for short paragraph, such that the answer can be produced. Experiments on the modified SQuAD dataset show that our proposed model outperforms existing reading comprehension models by at least 20% regarding exact match (EM), F1 and the proportion of identified paragraphs which are exactly the short paragraphs where the original answers locate.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4588–4594},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@article{10.1145/3280988,
author = {Razzaq, Abdul and Wasala, Asanka and Exton, Chris and Buckley, Jim},
title = {The State of Empirical Evaluation in Static Feature Location},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280988},
doi = {10.1145/3280988},
abstract = {Feature location (FL) is the task of finding the source code that implements a specific, user-observable functionality in a software system. It plays a key role in many software maintenance tasks and a wide variety of Feature Location Techniques (FLTs), which rely on source code structure or textual analysis, have been proposed by researchers. As FLTs evolve and more novel FLTs are introduced, it is important to perform comparison studies to investigate “Which are the best FLTs?” However, an initial reading of the literature suggests that performing such comparisons would be an arduous process, based on the large number of techniques to be compared, the heterogeneous nature of the empirical designs, and the lack of transparency in the literature. This article presents a systematic review of 170 FLT articles, published between the years 2000 and 2015. Results of the systematic review indicate that 95% of the articles studied are directed towards novelty, in that they propose a novel FLT. Sixty-nine percent of these novel FLTs are evaluated through standard empirical methods but, of those, only 9% use baseline technique(s) in their evaluations to allow cross comparison with other techniques. The heterogeneity of empirical evaluation is also clearly apparent: altogether, over 60 different FLT evaluation metrics are used across the 170 articles, 272 subject systems have been used, and 235 different benchmarks employed. The review also identifies numerous user input formats as contributing to the heterogeneity. Analysis of the existing research also suggests that only 27% of the FLTs presented might be reproduced from the published material. These findings suggest that comparison across the existing body of FLT evaluations is very difficult. We conclude by providing guidelines for empirical evaluation of FLTs that may ultimately help to standardise empirical research in the field, cognisant of FLTs with different goals, leveraging common practices in existing empirical evaluations and allied with rationalisations. This is seen as a step towards standardising evaluation in the field, thus facilitating comparison across FLTs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {2},
numpages = {58},
keywords = {Feature location, bug location, concept location, requirement traceability}
}

@inproceedings{10.1145/1404520.1404523,
author = {Bennedssen, Jens and Caspersen, Michael E.},
title = {Abstraction ability as an indicator of success for learning computing science?},
year = {2008},
isbn = {9781605582160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1404520.1404523},
doi = {10.1145/1404520.1404523},
abstract = {Computing scientists generally agree that abstract thinking is a crucial component for practicing computer science.We report on a three-year longitudinal study to confirm the hypothesis that general abstraction ability has a positive impact on performance in computing science.Abstraction ability is operationalized as stages of cognitive development for which validated tests exist. Performance in computing science is operationalized as grade in the final assessment of ten courses of a bachelor's degree programme in computing science. The validity of the operationalizations is discussed.We have investigated the positive impact overall, for two groupings of courses (a content-based grouping and a grouping based on SOLO levels of the courses' intended learning outcome), and for each individual course.Surprisingly, our study shows that there is hardly any correlation between stage of cognitive development (abstraction ability) and final grades in standard CS courses, neither for the various group-ings, nor for the individual courses. Possible explanations are discussed.},
booktitle = {Proceedings of the Fourth International Workshop on Computing Education Research},
pages = {15–26},
numpages = {12},
keywords = {CS, abstraction, computer science, indicator, learning, success},
location = {Sydney, Australia},
series = {ICER '08}
}

@inproceedings{10.1145/974044.974052,
author = {Grassi, Vincenzo and Mirandola, Raffaela},
title = {Towards automatic compositional performance analysis of component-based systems},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974052},
doi = {10.1145/974044.974052},
abstract = {To make predictive analysis an effective tool for component-based software development (CBSD), it should be, as much as possible: compositional, to allow the re-use of known information about the properties of existing components, and automatic, to keep the pace with the timeliness and cost-effectiveness promises of CBSD. Towards this end, focusing on the predictive analysis of performance properties, we define a simple language, based on an abstract component model, to describe a component assembly, outlining which information should be included in it to support compositional performance analysis. Moreover, we outline a mapping of the constructs of the proposed language to elements of the RT-UML Profile, to give them a precisely defined "performance semantics", and to get a starting point for the exploitation of proposed UML-based methodologies and algorithms for performance analysis.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {59–63},
numpages = {5},
keywords = {component specification, performance, predictive analysis, software component},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@inproceedings{10.1007/978-3-642-30598-6_7,
author = {L\'{o}pez Mart\'{\i}nez, Patricia and Vardanega, Tullio},
title = {Handling synchronization requirements under separation of concerns in model-driven component-based development},
year = {2012},
isbn = {9783642305979},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30598-6_7},
doi = {10.1007/978-3-642-30598-6_7},
abstract = {In this paper we discuss how the concept of separation of concerns could be conveniently applied to improve the model-driven component-based development of real-time high-integrity systems. Interpreting Dijkstra's view in this regard, we seek separation of concerns between the specification of needs (expressed declaratively by the user as requirements and assumptions) and the conception of a demonstrable solution for them (which we want to implement automatically, in the spirit of model-driven development). We aim to enable software designers to specify the assumptions needed on the expected behavior of the system solely by attaching declarative attributes to the affected elements of the system model. We then want the underlying design environment to produce a solution that provably achieves that behavior at run time. We find this vision to fit very well in a component-based development as it naturally allows the declarative space to be confined to interfaces (for the outside view of components) and operations (for the inside view of them). To prove the viability of our vision we apply it to the handling of synchronization requirements as seen from the perspective of the calling component, which is acutely more challenging than from the standpoint of the provider component.},
booktitle = {Proceedings of the 17th Ada-Europe International Conference on Reliable Software Technologies},
pages = {89–104},
numpages = {16},
keywords = {component-based development, high-level data races, separation of concerns, synchronization},
location = {Stockholm, Sweden},
series = {Ada-Europe'12}
}

@article{10.1016/j.compind.2007.02.001,
author = {Chien, Shih-Wen and Tsaur, Shu-Ming},
title = {Investigating the success of ERP systems: Case studies in three Taiwanese high-tech industries},
year = {2007},
issue_date = {December, 2007},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {58},
number = {8–9},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2007.02.001},
doi = {10.1016/j.compind.2007.02.001},
abstract = {The measurement of enterprise resource planning (ERP) systems success or effectiveness is critical to our understanding of the value and efficacy of ERP investment and managerial actions. Whether traditional information systems success models can be extended to investigating ERP systems success is yet to be investigated. This paper proposes a partial extension and respecification of the DeLone and MacLean model of IS success to ERP systems. The purpose of the present research is to re-examine the updated DeLone and McLean model [W. DeLone, E. McLean, The DeLone McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 3-9] of ERP systems success. The updated DeLone and McLean model was applied to collect data from the questionnaires answered by 204 users of ERP systems at three high-tech firms in Taiwan. Finally, this study suggests that system quality, service quality, and information quality are most important successful factors.},
journal = {Comput. Ind.},
month = dec,
pages = {783–793},
numpages = {11},
keywords = {DeLone and McLean model, ERP success model, High-tech firms}
}

@article{10.1007/s11761-011-0080-0,
author = {Gu, Qing and Lago, Patricia},
title = {Guiding the selection of service-oriented software engineering methodologies},
year = {2011},
issue_date = {December  2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {5},
number = {4},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-011-0080-0},
doi = {10.1007/s11761-011-0080-0},
abstract = {Service-oriented computing is a paradigm for effectively delivering software services in a dynamic environment. Accordingly, many service-oriented software engineering (SOSE) methodologies have been proposed and practiced in both academia and industry. Some of these methodologies share common features (e.g. cover similar life-cycle phases) but are presented for different purposes, ranging from project management to system modernization, and from business analysis to technical solutions development. Given this diversity in the methodologies available in the literature, it is very hard for a company to decide which methodology would fit best for its specific needs. With this aim, we took a feature analysis approach and devised a framework for comparing the existing SOA methodologies. Different from existing comparison frameworks, ours specifically highlights aspects that are specific to SOA and aims to differentiate the methodologies that are truly service-oriented from those that deal little with service aspects. As such, the criteria defined in the framework can be used as a checklist for selecting a SOSE methodology.},
journal = {Serv. Oriented Comput. Appl.},
month = dec,
pages = {203–223},
numpages = {21},
keywords = {Evaluation framework, Service-Oriented software engineering methodology, Service-oriented software engineering}
}

@inproceedings{10.1145/1403375.1403508,
author = {Heinecke, H. and Damm, W. and Josko, B. and Metzner, A. and Kopetz, H. and Sangiovanni-Vincentelli, A. and Di Natale, M.},
title = {Software components for reliable automotive systems},
year = {2008},
isbn = {9783981080131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1403375.1403508},
doi = {10.1145/1403375.1403508},
abstract = {System-level integration requires an overall understanding of the interplay of the sub-systems to enable component-based development with portability, reconfigurability and extensibility, together with guaranteed reliability and performance levels. Integration by simple interfaces and plug-and-play of sub-systems, which is the main objective of AUTOSAR, requires solving essential technical problems. We discuss to what degree the existing AUTOSAR standard can support the development of safety- and time-critical software and what is required to move toward the desirable goal of timing isolation when integrating multiple applications into the same execution platform.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {549–554},
numpages = {6},
location = {Munich, Germany},
series = {DATE '08}
}

@article{10.1155/2014/536362,
author = {Said, Mouna Ben and Kacem, Yessine Hadj and Kerboeuf, Micka\"{e}l and Amor, Nader Ben and Abid, Mohamed},
title = {Design patterns for self-adaptive RTE systems specification},
year = {2014},
issue_date = {January 2014},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2014},
issn = {1687-7195},
url = {https://doi.org/10.1155/2014/536362},
doi = {10.1155/2014/536362},
abstract = {The development of self-adaptive real-time embedded (RTE) systems is an increasingly hard task due to the growing complexity of both hardware and software and the high variability of the execution environment. Different approaches, platforms, and middleware have been proposed in the field, from low to high abstraction level. However, there is still a lack of generic and reusable designs for self-adaptive RTE systems that fit different system domains, lighten designers' task, and decrease development cost. In this paper, we propose five design patterns for self-adaptive RTE systems modeling resulting from the generalization of relevant existing adaptation-related works. Combined together, the patterns form the design of an adaptation loop composed of five adaptation modules. The proposed solution offers a modular, reusable, and flexible specification of these modules and enables the separation of concerns. It also permits dealing with concurrency, real-time features, and adaptation cost relative to the adaptation activities. To validate our solution, we applied it to a complex case study, a cross-layer self-adaptive object tracking system, to show patterns utilization and prove the solution benefits.},
journal = {Int. J. Reconfig. Comput.},
month = jan,
articleno = {8},
numpages = {1}
}

@article{10.1007/s10270-016-0575-4,
author = {Voelter, Markus and Kolb, Bernd and Szab\'{o}, Tam\'{a}s and Ratiu, Daniel and Deursen, Arie},
title = {Lessons learned from developing mbeddr: a case study in language engineering with MPS},
year = {2019},
issue_date = {February  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-016-0575-4},
doi = {10.1007/s10270-016-0575-4},
abstract = {Language workbenches are touted as a promising technology to engineer languages for use in a wide range of domains, from programming to science to business. However, not many real-world case studies exist that evaluate the suitability of language workbench technology for this task. This paper contains such a case study. In particular, we evaluate the development of mbeddr, a collection of integrated languages and language extensions built with the Jetbrains MPS language workbench. mbeddr consists of 81 languages, with their IDE support, 34 of them C extensions. The mbeddr languages use a wide variety of notations--textual, tabular, symbolic and graphical--and the C extensions are modular; new extensions can be added without changing the existing implementation of C. mbeddr's development has spanned 10 person-years so far, and the tool is used in practice and continues to be developed. This makes mbeddr a meaningful case study of non-trivial size and complexity. The evaluation is centered around five research questions: language modularity, notational freedom and projectional editing, mechanisms for managing complexity, performance and scalability issues and the consequences for the development process. We draw generally positive conclusions; language engineering with MPS is ready for real-world use. However, we also identify a number of areas for improvement in the state of the art in language engineering in general, and in MPS in particular.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {585–630},
numpages = {46},
keywords = {Case study, Domain-specific language, Experimentation, Language engineering, Language extension, Language workbenches, Languages}
}

@inproceedings{10.5555/645882.672392,
author = {Ommering, Rob C. van and Bosch, Jan},
title = {Widening the Scope of Software Product Lines - From Variation to Composition},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Architecture, components and reuse form the key elements to build a large variety of complex, high-quality products with a short lead-time. But the balance between an architecture-driven and a component-driven approach is influenced by the scope of the product line and the characteristics of the development organization. This paper discusses that balance and claims that a paradigm shift from variation to composition is necessary to cope with an increasing diversity of products created by an ever-larger part of an organization. We illustrate our claim with various examples.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {328–347},
numpages = {20},
keywords = {composition, product population, software components, variation},
series = {SPLC 2}
}

@article{10.1016/j.asoc.2018.07.021,
author = {Li, Hao and Gong, Maoguo and Wang, Congcong and Miao, Qiguang},
title = {Self-paced stacked denoising autoencoders based on differential evolution for change detection},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {71},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2018.07.021},
doi = {10.1016/j.asoc.2018.07.021},
journal = {Appl. Soft Comput.},
month = oct,
pages = {698–714},
numpages = {17},
keywords = {Image change detection, Synthetic aperture radar, Self-paced learning, Stacked denoising autoencoders, Differential evolution}
}

@article{10.1177/0037549709105240,
author = {Hardebolle, C\'{e}cile and Boulanger, Fr\'{e}d\'{e}ric},
title = {Exploring Multi-Paradigm Modeling Techniques},
year = {2009},
issue_date = {November  2009},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
volume = {85},
number = {11–12},
issn = {0037-5497},
url = {https://doi.org/10.1177/0037549709105240},
doi = {10.1177/0037549709105240},
abstract = {Multi-Paradigm Modeling (MPM) addresses the necessity of using multiple modeling paradigms when designing complex systems. Because of its multidisciplinary nature, the MPM field involves research teams with technical backgrounds as different as control science, model checking, modeling language engineering or system-on-chip development. In this paper, we propose to explore the MPM domain through a survey of existing techniques from different horizons. Since the heterogeneity of models is at the heart of MPM, we first identify the sources of this heterogeneity and introduce the problems it raises. Then we show how the different existing techniques address these problems.},
journal = {Simulation},
month = nov,
pages = {688–708},
numpages = {21},
keywords = {component composition, composition of models, cosimulation, heterogeneous interactions, heterogeneous models, metamodeling, models of computation, multi-paradigm modeling, transformation of models}
}

@article{10.1007/s10994-016-5570-z,
author = {Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
title = {A topological insight into restricted Boltzmann machines},
year = {2016},
issue_date = {September 2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {104},
number = {2–3},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-016-5570-z},
doi = {10.1007/s10994-016-5570-z},
abstract = {Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.},
journal = {Mach. Learn.},
month = sep,
pages = {243–270},
numpages = {28},
keywords = {Complex networks, Deep learning, Scale-free networks, Small-world networks, Sparse restricted Boltzmann machines}
}

@article{10.1287/msom.2020.0869,
author = {Hu, Ming and Liu, Jingchen and Zhai, Xin},
title = {Intertemporal Segmentation via Flexible-Duration Group Buying},
year = {2021},
issue_date = {September–October 2021},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {23},
number = {5},
issn = {1526-5498},
url = {https://doi.org/10.1287/msom.2020.0869},
doi = {10.1287/msom.2020.0869},
abstract = {Problem definition
: We study a special form of group buying: the group buying succeeds only if the number of sign-ups reaches a preset threshold, with no duration constraint. Customers with heterogeneous valuations arrive sequentially and decide between signing up for the group buying or purchasing a regular product. To decide whether to join the group buying, customers need to estimate their expected waiting time, which varies depending on the cumulative sign-ups by the time of their arrival. The firm decides on the prices for the group-buying product and regular product, with the product quality levels and group-buying size exogenously determined. 
Academic/practical relevance
: This type of group buying is often adopted for a special edition of the product and offered alongside a constantly available regular product. 
Methodology
: We study the product line design with the group-buying sign-up behavior of customers characterized by the rational expectations equilibrium in a random pledging process. 
Results
: We show that group buying with flexible duration can result in intertemporal customer segmentation, as different segments might be admitted at different times in the dynamic sign-up process. Such intertemporal segmentation is a natural discrimination scheme and has nontrivial implications. First, the efficiency loss due to waiting for enough sign-ups may decrease when a larger batch size is required for economic production. Second, as valuation heterogeneity in the market increases, the firm may not always benefit from offering group buying along with the regular product. Third, group buying can achieve a win-win-win situation for both high-end and low-end customers as well as the firm. 
Managerial implications
: In addition to demonstrating the profitability of flexible-duration group buying, we show that the firm can strengthen its profitability by contingently setting prices or concealing sign-up information in group buying. We also confirm the robustness of our main insights by considering customers’ heterogeneous patience levels and horizontally differentiated products, among other factors.},
journal = {Manufacturing &amp; Service Operations Management},
month = sep,
pages = {1157–1174},
numpages = {18},
keywords = {group buying, flexible duration, intertemporal segmentation, price discrimination}
}

@article{10.1007/s10270-012-0293-5,
author = {Ali, Shaukat and Yue, Tao and Briand, Lionel C.},
title = {Does aspect-oriented modeling help improve the readability of UML state machines?},
year = {2014},
issue_date = {July      2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0293-5},
doi = {10.1007/s10270-012-0293-5},
abstract = {Aspect-oriented modeling (AOM) is a relatively recent and very active field of research, whose application has, however, been limited in practice. AOM is assumed to yield several potential benefits such as enhanced modularization, easier evolution, increased reusability, and improved readability of models, as well as reduced modeling effort. However, credible, solid empirical evidence of such benefits is lacking. We evaluate the "readability" of state machines when modeling crosscutting behavior using AOM and more specifically AspectSM, a recently published UML profile. This profile extends the UML state machine notation with mechanisms to define aspects using state machines. Readability is indirectly measured through defect identification and fixing rates in state machines, and the scores obtained when answering a comprehension questionnaire about the system behavior. With AspectSM, crosscutting behavior is modeled using so-called "aspect state machines". Their readability is compared with that of system state machines directly modeling crosscutting and standard behavior together. An initial controlled experiment and a much larger replication were conducted with trained graduate students, in two different institutions and countries, to achieve the above objective. We use two baselines of comparisons--standard UML state machines without hierarchical features (flat state machines) and standard state machines with hierarchical/concurrent features (hierarchical state machines). The results showed that defect identification and fixing rates are significantly better with AspectSM than with both flat and hierarchical state machines. However, in terms of comprehension scores and inspection effort, no significant difference was observed between any of the approaches. Results of the experiments suggest that one should use, when possible, aspect state machines along with hierarchical and/or concurrent features of UML state machines to model crosscutting behaviors.},
journal = {Softw. Syst. Model.},
month = jul,
pages = {1189–1221},
numpages = {33},
keywords = {Aspect-oriented modeling, Comprehension, Controlled experiment, Defect identification and fixing, UML state machines}
}

@article{10.1016/j.neucom.2016.07.040,
author = {Maurya, Chandresh Kumar and Toshniwal, Durga and Vijendran Venkoparao, Gopalan},
title = {Online sparse class imbalance learning on big data},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {216},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2016.07.040},
doi = {10.1016/j.neucom.2016.07.040},
abstract = {Class imbalance learning is the study of problems in which some classes appear more frequently than the others. Most existing works that study this problem assume data set to be dense and do not exploit the rich structure of the data. One such structure is the sparsity. In the present work, we focus on solving the class imbalance problem under the sparsity assumption. More specifically, a well-known Gmean metric for class imbalance learning problem in binary classification setting has been maximized, which results in a non-convex loss function. Convex relaxation techniques are used to convert the non-convex problem to the convex problem. The problem formulation in the present work uses L1 regularized proximal learning framework and is solved via accelerated-stochastic-proximal gradient descent algorithm. Our aim in the paper is to show: (i) the application of proximal algorithms to solve real world problems (class imbalance); (ii) how it scales to Big data; and (iii) how it outperforms some recently proposed algorithms in terms of Gmean, F-measure and Mistake rate on several benchmark data sets.},
journal = {Neurocomput.},
month = dec,
pages = {250–260},
numpages = {11},
keywords = {Big data, Class imbalance learning, Online learning, Proximal algorithm}
}

@inproceedings{10.5555/2447556.2447559,
author = {Valtazanos, Aris and Ramamoorthy, Subramanian},
title = {Evaluating the effects of limited perception on interactive decisions in mixed robotic domains},
year = {2013},
isbn = {9781467330558},
publisher = {IEEE Press},
abstract = {Many robotic applications feature a mixture of interacting teleoperated and autonomous robots. In several such domains, human operators must make decisions using very limited perceptual information, e.g. by viewing only the noisy camera feed of their robot. There are many interaction scenarios where such restricted visibility impacts teleoperation performance, and where the role of autonomous robots needs to be reinforced. In this paper, we report on an experimental study assessing the effects of limited perception on human decision making, in interactions between autonomous and teleoperated NAO robots, where subjects do not have prior knowledge of how other agents will respond to their decisions. We evaluate the performance of several subjects under varying perceptual constraints in two scenarios; a simple cooperative task requiring collaboration with an autonomous robot, and a more demanding adversarial task, where an autonomous robot is actively trying to outperform the human. Our results indicate that limited perception has minimal impact on user performance when the task is simple. By contrast, when the other agent becomes more strategic, restricted visibility has an adverse effect on most subjects, with the performance level even falling below that achieved by an autonomous robot with identical restrictions. Our results could inform decisions about the division of control between humans and robots in mixed-initiative systems, and in determining when autonomous robots should intervene to assist operators.},
booktitle = {Proceedings of the 8th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {9–16},
numpages = {8},
keywords = {interactive teleoperation, limited perception},
location = {Tokyo, Japan},
series = {HRI '13}
}

@article{10.1145/3478094,
author = {Lian, Jie and Yuan, Xu and Li, Ming and Tzeng, Nian-Feng},
title = {Fall Detection via Inaudible Acoustic Sensing},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478094},
doi = {10.1145/3478094},
abstract = {The fall detection system is of critical importance in protecting elders through promptly discovering fall accidents to provide immediate medical assistance, potentially saving elders' lives. This paper aims to develop a novel and lightweight fall detection system by relying solely on a home audio device via inaudible acoustic sensing, to recognize fall occurrences for wide home deployment. In particular, we program the audio device to let its speaker emit 20kHz continuous wave, while utilizing a microphone to record reflected signals for capturing the Doppler shift caused by the fall. Considering interferences from different factors, we first develop a set of solutions for their removal to get clean spectrograms and then apply the power burst curve to locate the time points at which human motions happen. A set of effective features is then extracted from the spectrograms for representing the fall patterns, distinguishable from normal activities. We further apply the Singular Value Decomposition (SVD) and K-mean algorithms to reduce the data feature dimensions and to cluster the data, respectively, before input them to a Hidden Markov Model for training and classification. In the end, our system is implemented and deployed in various environments for evaluation. The experimental results demonstrate that our system can achieve superior performance for detecting fall accidents and is robust to environment changes, i.e., transferable to other environments after training in one environment.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {114},
numpages = {21},
keywords = {Device-free, Fall Detection, Hidden Markov Model, Ultrasonic}
}

@article{10.3233/SW-150186,
author = {Polleres, Axel and Saleem, Muhammad and Khan, Yasar and Hasnain, Ali and Ermilov, Ivan and Ngonga Ngomo, Axel-Cyrille},
title = {A fine-grained evaluation of SPARQL endpoint federation systems},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {7},
number = {5},
issn = {1570-0844},
url = {https://doi.org/10.3233/SW-150186},
doi = {10.3233/SW-150186},
abstract = {The Web of Data has grown enormously over the last years. Currently, it comprises a large compendium of interlinked and distributed datasets from multiple domains. Running complex queries on this compendium often requires accessing data from different endpoints within one query. The abundance of datasets and the need for running complex query has thus motivated a considerable body of work on SPARQL query federation systems, the dedicated means to access data distributed over the Web of Data. However, the granularity of previous evaluations of such systems has not allowed deriving of insights concerning their behavior in different steps involved during federated query processing. In this work, we perform extensive experiments to compare state-of-the-art SPARQL endpoint federation systems using the comprehensive performance evaluation framework FedBench. In addition to considering the tradition query runtime as an evaluation criterion, we extend the scope of our performance evaluation by considering criteria, which have not been paid much attention to in previous studies. In particular, we consider the number of sources selected, the total number of SPARQL ASK requests used, the completeness of answers as well as the source selection time. Yet, we show that they have a significant impact on the overall query runtime of existing systems. Moreover, we extend FedBench to mirror a highly distributed data environment and assess the behavior of existing systems by using the same performance criteria. As the result we provide a detailed analysis of the experimental outcomes that reveal novel insights for improving current and future SPARQL federation systems.},
journal = {Semant. Web},
month = jan,
pages = {493–518},
numpages = {26},
keywords = {SPARQL federation, Web of Data, RDF}
}

@article{10.1016/j.jss.2016.07.031,
author = {Kuhrmann, Marco and Ternit\'{e}, Thomas and Friedrich, Jan and Rausch, Andreas and Broy, Manfred},
title = {Flexible software process lines in practice},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.031},
doi = {10.1016/j.jss.2016.07.031},
abstract = {An extension for software process metamodels to support software process lines.We present two concepts partitioned software process and variability operation.We present insights into the practical application and evidence of applicability.We present the practical implementation in the German V-Modell XT standard. Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-Modell\'{z}XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-Modell\'{z}XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches.},
journal = {J. Syst. Softw.},
month = nov,
pages = {49–71},
numpages = {23},
keywords = {Process design, Process realisation, Software process, Software process lines, Software process metamodel, V-Modell XT metamodel}
}

@inproceedings{10.5555/2429759.2430074,
author = {Steiniger, Alexander and Kr\"{u}ger, Frank and Uhrmacher, Adelinde M.},
title = {Modeling agents and their environment in multi-level-DEVS},
year = {2012},
publisher = {Winter Simulation Conference},
abstract = {Environments play an important role in multi-agent systems. They present the context agents operate in. When testing multi-agent systems by simulation, the environment and partly agents have to be modeled. We explore the potential of Multi-Level-DEVS to serve as a modeling formalism for agents, their environment, and the interaction between them. Multi-Level-DEVS combines a modular, hierarchical modeling with variable structures, dynamic interfaces, and explicit means for describing up- and downward causation between different levels of the compositional hierarchy. The modeling in Multi-Level-DEVS emphasizes the role of the environment to provide information for and enforce constrains on the situated agents. A smart meeting room scenario is modeled, and an approach aimed at recognizing user activities in smart environments is tested and evaluated in a simulation study.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {233},
numpages = {12},
location = {Berlin, Germany},
series = {WSC '12}
}

@inproceedings{10.5555/1775321.1775343,
author = {Siradjev, Djakhongir and Ke, Qiao and Park, JeongKi and Kim, Young-Tak},
title = {Highspeed and flexible source-end DDoS protection system using IXP2400 network processor},
year = {2007},
isbn = {3540758526},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper proposes an architecture of source-end DDoS protection system on IXP2400 network processor, which monitors traffic from the source network and polices traffic at the source without affecting the traffic from other network. The proposed architecture includes usual IPv4 forwarder with additional modules for source filtering, packet classification and flow control, and uses modified non-parametric CUSUM algorithm. We analyze the major shortcomings of previous approaches, and present basic performance analysis. The proposed system can handle 65,000 aggregated flows, and can operate at OC-48 line rate.},
booktitle = {Proceedings of the 7th IEEE International Conference on IP Operations and Management},
pages = {180–183},
numpages = {4},
keywords = {DDoS, classification, flow monitoring, network processors, network security},
location = {San Jos\'{e}, USA},
series = {IPOM'07}
}

@article{10.1016/j.comcom.2019.05.017,
author = {Mazouzi, Houssemeddine and Boussetta, Khaled and Achir, Nadjib},
title = {Maximizing mobiles energy saving through tasks optimal offloading placement in two-tier cloud: A theoretical and an experimental study},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {144},
number = {C},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2019.05.017},
doi = {10.1016/j.comcom.2019.05.017},
journal = {Comput. Commun.},
month = aug,
pages = {132–148},
numpages = {17},
keywords = {Computation offloading, Mobile cloud computing, Mobile edge computing, Cloudlet, Lagrangian decomposition, Offloading middleware}
}

@inproceedings{10.1145/1173706.1173740,
author = {Leavens, Gary T. and Abrial, Jean-Raymond and Batory, Don and Butler, Michael and Coglio, Alessandro and Fisler, Kathi and Hehner, Eric and Jones, Cliff and Miller, Dale and Peyton-Jones, Simon and Sitaraman, Murali and Smith, Douglas R. and Stump, Aaron},
title = {Roadmap for enhanced languages and methods to aid verification},
year = {2006},
isbn = {1595932372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1173706.1173740},
doi = {10.1145/1173706.1173740},
abstract = {This roadmap describes ways that researchers in four areas---specification languages, program generation, correctness by construction, and programming languages---might help further the goal of verified software. It also describes what advances the "verified software" grand challenge might anticipate or demand from work in these areas. That is, the roadmap is intended to help foster collaboration between the grand challenge and these research areas.A common goal for research in these areas is to establish language designs and tool architectures that would allow multiple annotations and tools to be used on a single program. In the long term, researchers could try to unify these annotations and integrate such tools.},
booktitle = {Proceedings of the 5th International Conference on Generative Programming and Component Engineering},
pages = {221–236},
numpages = {16},
keywords = {annotations, correctness by construction, program generation, programming languages, specification languages, tools, verification, verified software grand challenge},
location = {Portland, Oregon, USA},
series = {GPCE '06}
}

@inproceedings{10.1145/1101480.1101492,
author = {Marin, Cristina and Desertot, Mikael},
title = {Sensor bean: a component platform for sensor-based services},
year = {2005},
isbn = {1595932682},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101480.1101492},
doi = {10.1145/1101480.1101492},
abstract = {Sensor-based services propose to gather, manage, analyze, access and react to sensor data. These services are distributed over heterogeneous platforms. The complexity of the implementation of such services requires software engineering tools to relieve the architect and the developer who are often business experts and not technologies experts. Our proposal relates to the definition of a component model dedicated to the development of SBS, called Sensor Bean. Sensor Bean differs from the usual components models by its service orientation and dynamic architecture and the introduction of data-centric connectors well adapted to measurement flows. This proposal is validated by a prototype coupling OSGi and J2EE and by a first component container for OSGi gateways.},
booktitle = {Proceedings of the 3rd International Workshop on Middleware for Pervasive and Ad-Hoc Computing},
pages = {1–8},
numpages = {8},
keywords = {component models, data gathering, sensor-based applications, sensor-based services, sensors, services},
location = {Grenoble, France},
series = {MPAC '05}
}

@article{10.1007/s10270-012-0227-2,
author = {Kraft, Stephan and Casale, Giuliano and Krishnamurthy, Diwakar and Greer, Des and Kilpatrick, Peter},
title = {Performance models of storage contention in cloud environments},
year = {2013},
issue_date = {October   2013},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0227-2},
doi = {10.1007/s10270-012-0227-2},
abstract = {We propose simple models to predict the performance degradation of disk requests due to storage device contention in consolidated virtualized environments. Model parameters can be deduced from measurements obtained inside Virtual Machines (VMs) from a system where a single VM accesses a remote storage server. The parameterized model can then be used to predict the effect of storage contention when multiple VMs are consolidated on the same server. We first propose a trace-driven approach that evaluates a queueing network with fair share scheduling using simulation. The model parameters consider Virtual Machine Monitor level disk access optimizations and rely on a calibration technique. We further present a measurement-based approach that allows a distinct characterization of read/write performance attributes. In particular, we define simple linear prediction models for I/O request mean response times, throughputs and read/write mixes, as well as a simulation model for predicting response time distributions. We found our models to be effective in predicting such quantities across a range of synthetic and emulated application workloads.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {681–704},
numpages = {24},
keywords = {Performance modeling, Storage, Virtualization}
}

@article{10.1007/s10515-018-0247-4,
author = {Gonzalez-Fernandez, Yasser and Hamidi, Saeideh and Chen, Stephen and Liaskos, Sotirios},
title = {Efficient elicitation of software configurations using crowd preferences and domain knowledge},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-018-0247-4},
doi = {10.1007/s10515-018-0247-4},
abstract = {As software systems grow in size and complexity, the process of configuring them to meet individual needs becomes more and more challenging. Users, especially those that are new to a system, are faced with an ever increasing number of configuration possibilities, making the task of choosing the right one more and more daunting. However, users are rarely alone in using a software system. Crowds of other users or the designers themselves can provide with examples and rules as to what constitutes a meaningful configuration. We introduce a technique for designing optimal interactive configuration elicitation dialogs, aimed at utilizing crowd and expert information to reduce the amount of manual configuration effort. A repository of existing user configurations supplies us with information about popular ways to complete an existing partial configuration. Designers augment this information with their own constraints. A Markov decision process (MDP) model is then created to encode configuration elicitation dialogs that maximize the automatic configuration decisions based on the crowd and the designers' information. A genetic algorithm is employed to solve the MDP when problem sizes prevent use of common exact techniques. In our evaluation with various configuration models we show that the technique is feasible, saves configuration effort and scales for real problem sizes of a few hundreds of features.},
journal = {Automated Software Engg.},
month = mar,
pages = {87–123},
numpages = {37},
keywords = {Genetic algorithms, Markov decision processes, Software configuration, Software customization}
}

@inproceedings{10.1145/1188966.1188976,
author = {Lapouchnian, Alexei and Yu, Yijun and Liaskos, Sotirios and Mylopoulos, John},
title = {Requirements-driven design of autonomic application software},
year = {2006},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1188966.1188976},
doi = {10.1145/1188966.1188976},
abstract = {Autonomic computing systems reduce software maintenance costs and management complexity by taking on the responsibility for their configuration, optimization, healing, and protection. These tasks are accomplished by switching at runtime to a different system behaviour - the one that is more efficient, more secure, more stable, etc. - while still fulfilling the main purpose of the system. Thus, identifying the objectives of the system, analyzing alternative ways of how these objectives can be met, and designing a system that supports all or some of these alternative behaviours is a promising way to develop autonomic systems. This paper proposes the use of requirements goal models as a foundation for such software development process and demonstrates this on an example.},
booktitle = {Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {7–es},
location = {Toronto, Ontario, Canada},
series = {CASCON '06}
}

@article{10.4018/jossp.2011010101,
author = {Raza, Arif and Capretz, Luiz Fernando and Ahmed, Faheem},
title = {An Empirical Study of Open Source Software Usability: The Industrial Perspective},
year = {2011},
issue_date = {January 2011},
publisher = {IGI Global},
address = {USA},
volume = {3},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/jossp.2011010101},
doi = {10.4018/jossp.2011010101},
abstract = {Recent years have seen a sharp increase in the use of open source projects by common novice users; Open Source Software OSS is thus no longer a reserved arena for software developers and computer gurus. Although user-centered designs are gaining popularity in OSS, usability is still not considered one of the prime objectives in many design scenarios. This paper analyzes industry users' perception of usability factors, including understandability, learnability, operability, and attractiveness on OSS usability. The research model of this empirical study establishes the relationship between the key usability factors and OSS usability from industrial perspective. In order to conduct the study, a data set of 105 industry users is included. The results of the empirical investigation indicate the significance of the key factors for OSS usability.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–16},
numpages = {16},
keywords = {Empirical Study, Industry, Open Source Software OSS, Usability, Users}
}

@article{10.1016/j.jnca.2017.12.001,
author = {Dias de Assuno, Marcos and da Silva Veith, Alexandre and Buyya, Rajkumar},
title = {Distributed data stream processing and edge computing},
year = {2018},
issue_date = {February 2018},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {103},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2017.12.001},
doi = {10.1016/j.jnca.2017.12.001},
abstract = {Under several emerging application scenarios, such as in smart cities, operational monitoring of large infrastructure, wearable assistance, and Internet of Things, continuous data streams must be processed under very short delays. Several solutions, including multiple software engines, have been developed for processing unbounded data streams in a scalable and efficient manner. More recently, architecture has been proposed to use edge computing for data stream processing. This paper surveys state of the art on stream processing engines and mechanisms for exploiting resource elasticity features of cloud computing in stream processing. Resource elasticity allows for an application or service to scale out/in according to fluctuating demands. Although such features have been extensively investigated for enterprise applications, stream processing poses challenges on achieving elastic systems that can make efficient resource management decisions based on current load. Elasticity becomes even more challenging in highly distributed environments comprising edge and cloud computing resources. This work examines some of these challenges and discusses solutions proposed in the literature to address them. HighlightsThe paper surveys state of the art on stream processing engines and mechanisms.The work describes how existing solutions exploit resource elasticity features of cloud computing in stream processing.It presents a gap analysis and future directions on stream processing on heterogeneous environments.},
journal = {J. Netw. Comput. Appl.},
month = feb,
pages = {1–17},
numpages = {17},
keywords = {Big Data, Cloud computing, Resource elasticity, Stream processing}
}

@article{10.1007/s10515-020-00273-8,
author = {Velez, Miguel and Jamshidi, Pooyan and Sattler, Florian and Siegmund, Norbert and Apel, Sven and K\"{a}stner, Christian},
title = {ConfigCrusher: towards white-box performance analysis for configurable systems},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00273-8},
doi = {10.1007/s10515-020-00273-8},
abstract = {Stakeholders of configurable systems are often interested in knowing how configuration options influence the performance of a system to facilitate, for example, the debugging and optimization processes of these systems. Several black-box approaches can be used to obtain this information, but they either sample a large number of configurations to make accurate predictions or miss important performance-influencing interactions when sampling few configurations. Furthermore, black-box approaches cannot pinpoint the parts of a system that are responsible for performance differences among configurations. This article proposes ConfigCrusher, a white-box performance analysis that inspects the implementation of a system to guide the performance analysis, exploiting several insights of configurable systems in the process. ConfigCrusher employs a static data-flow analysis to identify how configuration options may influence control-flow statements and instruments code regions, corresponding to these statements, to dynamically analyze the influence of configuration options on the regions’ performance. Our evaluation on 10 configurable systems shows the feasibility of our white-box approach to more efficiently build performance-influence models that are similar to or more accurate than current state of the art approaches. Overall, we showcase the benefits of white-box performance analyses and their potential to outperform black-box approaches and provide additional information for analyzing configurable systems.},
journal = {Automated Software Engg.},
month = dec,
pages = {265–300},
numpages = {36},
keywords = {Configurable systems, Performance analysis, Static analysis, Dynamic analysis}
}

@article{10.1007/s10515-012-0117-4,
author = {N\"{o}hrer, Alexander and Egyed, Alexander},
title = {C2O configurator: a tool for guided decision-making},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-012-0117-4},
doi = {10.1007/s10515-012-0117-4},
abstract = {Decision models are widely used in software engineering to describe and restrict decision-making (e.g., deriving a product from a product-line). Since decisions are typically interdependent, it is often neither obvious which decisions have the most significant impact nor which decisions might ultimately conflict. Unfortunately, the current state-of-the-art provides little support for dealing with such situations. On the one hand, some conflicts can be avoided by providing more freedom in which order decisions are made (i.e., most important decisions first). On the other hand, conflicts are unavoidable at times, and living with conflicts may be preferable over forcing the user to fix them right away--particularly because fixing conflicts becomes easier as more is known about a user's intentions. This paper introduces the C2O (Configurator 2.0) tool for guided decision-making. The tool allows the user to answer questions in an arbitrary order--with and without the presence of inconsistencies. While giving users those freedoms, it still supports and guides them by (i) rearranging the order of questions according to their potential to minimize user input, (ii) providing guidance to avoid follow-on conflicts, and (iii) supporting users in fixing conflicts at a later time.},
journal = {Automated Software Engg.},
month = jun,
pages = {265–296},
numpages = {32}
}

@article{10.1016/j.cmpb.2019.105100,
author = {Romao, Matheus and Tierra-Criollo, Carlos Julio},
title = {A bayesian approach to the spectral F-Test: Application to auditory steady-state responses},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {183},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2019.105100},
doi = {10.1016/j.cmpb.2019.105100},
journal = {Comput. Methods Prog. Biomed.},
month = jan,
numpages = {9},
keywords = {Bayesian detection, Spectral F-test, Objective Response Detection, Auditory Steady-state Responses, Evoked Potentials}
}

@article{10.1016/j.jss.2013.02.061,
author = {Anand, Saswat and Burke, Edmund K. and Chen, Tsong Yueh and Clark, John and Cohen, Myra B. and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and Mcminn, Phil},
title = {An orchestrated survey of methodologies for automated software test case generation},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2013.02.061},
doi = {10.1016/j.jss.2013.02.061},
abstract = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1978–2001},
numpages = {24},
keywords = {Adaptive random testing, Combinatorial testing, Model-based testing, Orchestrated survey, Search-based software testing, Software testing, Symbolic execution, Test automation, Test case generation}
}

@article{10.1016/j.jss.2006.08.016,
author = {kerholm, Mikael and Carlson, Jan and Fredriksson, Johan and Hansson, Hans and H\r{a}kansson, John and M\"{o}ller, Anders and Pettersson, Paul and Tivoli, Massimo},
title = {The SAVE approach to component-based development of vehicular systems},
year = {2007},
issue_date = {May, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.08.016},
doi = {10.1016/j.jss.2006.08.016},
abstract = {The component-based strategy aims at managing complexity, shortening time-to-market, and reducing maintenance requirements by building systems with existing components. The full potential of this strategy has not yet been demonstrated for embedded software, mainly because of specific requirements in the domain, e.g., those related to timing, dependability, and resource consumption. We present SaveCCT - a component technology intended for vehicular systems, show the applicability of SaveCCT in the engineering process, and demonstrate its suitability for vehicular systems in an industrial case-study. Our experiments indicate that SaveCCT provides appropriate expressiveness, resource efficiency, analysis and verification support for component-based development of vehicular software.},
journal = {J. Syst. Softw.},
month = may,
pages = {655–667},
numpages = {13},
keywords = {Component based software engineering, Component technology, Embedded systems, Vehicular systems}
}

@article{10.1016/j.jss.2009.02.011,
author = {White, Jules and Dougherty, Brian and Schmidt, Douglas C.},
title = {Selecting highly optimal architectural feature sets with Filtered Cartesian Flattening},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.02.011},
doi = {10.1016/j.jss.2009.02.011},
abstract = {Feature modeling is a common method used to capture the variability in a configurable application. A key challenge developers face when using a feature model is determining how to select a set of features for a variant that simultaneously satisfy a series of resource constraints. This paper presents an approximation technique for selecting highly optimal feature sets while adhering to resource limits. The paper provides the following contributions to configuring application variants from feature models: (1) we provide a polynomial time approximation algorithm for selecting a highly optimal set of features that adheres to a set of resource constraints, (2) we show how this algorithm can incorporate complex configuration constraints; and (3) we present empirical results showing that the approximation algorithm can be used to derive feature sets that are more than 90%+ optimal.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1268–1284},
numpages = {17},
keywords = {Approximation algorithm, Feature modeling, Optimization, Resource constraints}
}

@article{10.1016/j.infsof.2018.02.010,
author = {Schermann, Gerald and Cito, J\"{u}rgen and Leitner, Philipp and Zdun, Uwe and Gall, Harald C.},
title = {We’re doing it live: A multi-method empirical study on continuous experimentation},
year = {2018},
issue_date = {Jul 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {99},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.010},
doi = {10.1016/j.infsof.2018.02.010},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {41–57},
numpages = {17},
keywords = {Release engineering, Continuous deployment, Continuous experimentation, Empirical study}
}

@inproceedings{10.1007/978-3-642-13464-7_4,
author = {Basu, Ananda and Bensalem, Saddek and Bozga, Marius and Caillaud, Beno\^{\i}t and Delahaye, Beno\^{\i}t and Legay, Axel},
title = {Statistical abstraction and model-checking of large heterogeneous systems},
year = {2010},
isbn = {3642134637},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-13464-7_4},
doi = {10.1007/978-3-642-13464-7_4},
abstract = {We propose a new simulation-based technique for verifying applications running within a large heterogeneous system. Our technique starts by performing simulations of the system in order to learn the context in which the application is used. Then, it creates a stochastic abstraction for the application, which takes the context information into account. This smaller model can be verified using efficient techniques such as statistical model checking. We have applied our technique to an industrial case study: the cabin communication system of an airplane. We use the BIP toolset to model and simulate the system. We have conducted experiments to verify the clock synchronization protocol i.e., the application used to synchronize the clocks of all computing devices within the system.},
booktitle = {Proceedings of the 12th IFIP WG 6.1 International Conference and 30th IFIP WG 6.1 International Conference on Formal Techniques for Distributed Systems},
pages = {32–46},
numpages = {15},
location = {Amsterdam, The Netherlands},
series = {FMOODS'10/FORTE'10}
}

@article{10.1016/j.rcim.2009.11.011,
author = {Li, Di and Li, Fang and Huang, Xin and Lai, Yizong and Zheng, Shixiong},
title = {A model based integration framework for computer numerical control system development},
year = {2010},
issue_date = {August, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {26},
number = {4},
issn = {0736-5845},
url = {https://doi.org/10.1016/j.rcim.2009.11.011},
doi = {10.1016/j.rcim.2009.11.011},
abstract = {As a typical complex embedded computer control system, Computer Numerical Control (CNC) system development is confronted with a great challenge because of its specific requirements as well as some recent development trends such as ever more complex products while at lower prices and shorter develop cycle. In this paper, a model based integration framework (CNCMIF) for CNC system design and development is presented, which integrates modeling, simulation, verification and implementation in a uniform environment. The CNCML (CNC modeling language) with well defined syntax and unambiguous semantics is developed to describe the CNC system in an accurate and explicit way. Model transformation strategy for formal verification and code automatic generation for implementation in the framework are also presented. The approach is an attempt to create an infrastructure to support the CNC system design in an efficient way, while at the same time guarantees the function and performance requirements with advanced capability of the system such as modularity, flexibility, reusability, etc.},
journal = {Robot. Comput.-Integr. Manuf.},
month = aug,
pages = {333–343},
numpages = {11},
keywords = {CNC, CNCMIF, CNCML, Model based design}
}

@inproceedings{10.1007/978-3-319-71501-8_12,
author = {Macwan, Kamalkumar R. and Patel, Sankita J.},
title = {Mutual Friend Attack Prevention in Social Network Data Publishing},
year = {2017},
isbn = {978-3-319-71500-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-71501-8_12},
doi = {10.1007/978-3-319-71501-8_12},
abstract = {Due to increasing demand of publishing social network data, privacy has raised more concern for data publisher. There are different risks and attacks still exist that can breach user privacy. Online social network such as Facebook, Google Plus and LinkedIn provide a feature that allows finding out number of mutual friends (NMF) between two users. Adversary can use such information to identify individual user and his/her connections. As published dataset itself reveals mutual friends information for each connection, it becomes very easy for an adversary to re-identify the individual user.Existing anonymization techniques for mutual friends attack are based on edge anonymization. It performs edge anonymization operation without considering the NMF-requirement of other edges that results into more edge insertion operations. Due to that, the data utility of anonymized dataset is very low. In this paper, we propose the anonymization approach that works on the mutual friend sequence. It ensures that, there exist at least k elements in mutual friend sequence that holds same value. The vertex selection process to increase the number of mutual friend (NMF) for one edge reduces the mutual friend anonymization requirement for other edges too. The experimental results demonstrate that the proposed anonymization approach preserve the privacy and the utility of the published dataset against mutual friend attack.},
booktitle = {Security, Privacy, and Applied Cryptography Engineering: 7th International Conference, SPACE 2017, Goa, India, December 13-17, 2017, Proceedings},
pages = {210–225},
numpages = {16},
keywords = {Social network data publishing, Mutual friend attack, k-NMF, Data utility},
location = {Goa, India}
}

@article{10.1007/s10664-008-9094-4,
author = {Lee, Jihyun and Kang, Sungwon and Kim, Chang-Ki},
title = {Software architecture evaluation methods based on cost benefit analysis and quantitative decision making},
year = {2009},
issue_date = {August    2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9094-4},
doi = {10.1007/s10664-008-9094-4},
abstract = {Since many parts of the architecture evaluation steps of the Cost Benefit Analysis Method (CBAM) depend on the stakeholders' empirical knowledge and intuition, it is very important that such an architecture evaluation method be able to faithfully reflect the knowledge of the experts in determining Architectural Strategy (AS). However, because CBAM requires the stakeholders to make a consensus or vote for collecting data for decision making, it is difficult to accurately reflect the stakeholders' knowledge in the process. In order to overcome this limitation of CBAM, we propose the two new CBAM-based methods for software architecture evaluation, which respectively adopt the Analytic Hierarchy Process (AHP) and the Analytic Network Process (ANP). Since AHP and ANP use pair-wise comparison they are suitable for a cost and benefit analysis technique since its purpose is not to calculate correct values of benefit and cost but to decide AS with highest return on investment. For that, we first define a generic process of CBAM and develop variations from the generic process by applying AHP and ANP to obtain what we call the CBAM+AHP and CBAM+ANP methods. These new methods not only reflect the knowledge of experts more accurately but also reduce misjudgments. A case study comparison of CBAM and the two new methods is conducted using an industry software project. Because the cost benefit analysis process that we present is generic, new cost benefit analysis techniques with capabilities and characteristics different from the three methods we examine here can be derived by adopting various different constituent techniques.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {453–475},
numpages = {23},
keywords = {AHP, ANP, CBAM, Software architecture evaluation}
}

@article{10.1016/j.compeleceng.2017.11.002,
author = {AbuZeina, Dia and Al-Anzi, Fawaz S.},
title = {Employing fisher discriminant analysis for Arabic text classification},
year = {2018},
issue_date = {February 2018},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {66},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2017.11.002},
doi = {10.1016/j.compeleceng.2017.11.002},
abstract = {Linear discriminant analysis (LDA) is proposed for Arabic text classification.LDA employs less dimensions, which is helpful for sizable textual feature vectors.Despite that LDA is semantic loss feature reduction method, it shows useful results. Fisher's discriminant analysis; also called linear discriminant analysis (LDA), is a popular dimensionality reduction technique that is widely used for features extraction. LDA aims at finding an optimal linear transformation based on maximizing a class separability. Even though LDA shows useful results in various pattern recognition problems, such as face recognition, less attention has been devoted to employing this technique in Arabic information retrieval tasks. In particular, the sizable feature vectors in textual data enforces to implement dimensionality reduction techniques such as LDA. In this paper, we empirically investigated an LDA based method for Arabic text classification. We used a corpus that contains 2,000 documents belonging to five categories. The experimental results showed that the performance of semantic loss LDA based method was almost the same as the semantic rich singular value decomposition (SVD), and that is indication that LDA is a promising method for text mining applications. Display Omitted},
journal = {Comput. Electr. Eng.},
month = feb,
pages = {474–486},
numpages = {13},
keywords = {Arabic, Classification, Eigenvectors, Fisher, Linear discriminant analysis, Text}
}

@inproceedings{10.1145/1370018.1370036,
author = {Irmert, Florian and Fischer, Thomas and Meyer-Wegener, Klaus},
title = {Runtime adaptation in a service-oriented component model},
year = {2008},
isbn = {9781605580371},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370018.1370036},
doi = {10.1145/1370018.1370036},
abstract = {Developing software applications which manage, optimize or adapt themselves at runtime requires an architecture which provides adaptation of software components at runtime. An architecture model that has gained a lot of attention in recent years is SOA (service-oriented architecture). In a SOA environment services as well as applications build up complex dependencies. Therefore it is crucial for self-managing SOA applications to adapt services at runtime without interference of the application execution and the service availability. In this paper, we discuss the problems arising from the requirement of runtime adaptation and present our solution by replacing service implementations at execution time in a service-oriented component model. For a seamless integration we strive for a transparent and atomic replacement of a service implementation in respect to the other services/applications.},
booktitle = {Proceedings of the 2008 International Workshop on Software Engineering for Adaptive and Self-Managing Systems},
pages = {97–104},
numpages = {8},
keywords = {adaptation, component replacement, migration, modularity, service-oriented architecture},
location = {Leipzig, Germany},
series = {SEAMS '08}
}

@inproceedings{10.1145/1081706.1081757,
author = {Estublier, Jacky and Vega, German},
title = {Reuse and variability in large software applications},
year = {2005},
isbn = {1595930140},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1081706.1081757},
doi = {10.1145/1081706.1081757},
abstract = {Reuse has always been a major goal in software engineering, since it promises large gains in productivity, quality and time to market reduction. Practical experience has shown that substantial reuse has only successfully happened in two cases: libraries, where many generic and small components can be found; and product lines, where domains-specific components can be assembled in different ways to produce variations of a given product.In this paper we examine how product lines have successfully achieved reuse of coarse-grained components, and the underlying factors limiting this approach to narrowly scoped domains. We then build on this insight to present an approach, called software federation, which proposes a mechanism to overcome the identified limitations, and therefore makes reuse of coarse-grained components possible over a larger range of applications. Our approach extends and generalizes the product line approach, extending the concepts and mechanisms available to manage variability. The system is in use in different companies, validating the claims made in this paper.},
booktitle = {Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {316–325},
numpages = {10},
keywords = {AOP, COTS, EAI, MDA, interoperability, model driven software engineering, process driven application, product families, product line, reuse, variability, workflow},
location = {Lisbon, Portugal},
series = {ESEC/FSE-13}
}

@article{10.1145/1363102.1363104,
author = {Mohagheghi, Parastoo and Conradi, Reidar},
title = {An empirical investigation of software reuse benefits in a large telecom product},
year = {2008},
issue_date = {June 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1363102.1363104},
doi = {10.1145/1363102.1363104},
abstract = {Background. This article describes a case study on the benefits of software reuse in a large telecom product. The reused components were developed in-house and shared in a product-family approach. Methods. Quantitative data mined from company repositories are combined with other quantitative data and qualitative observations. Results. We observed significantly lower fault density and less modified code between successive releases of the reused components. Reuse and standardization of software architecture and processes allowed easier transfer of development when organizational changes happened. Conclusions. The study adds to the evidence of quality benefits of large-scale reuse programs and explores organizational motivations and outcomes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {13},
numpages = {31},
keywords = {Software reuse, fault density, product family, risks, standardization}
}

@inproceedings{10.5555/1927661.1927724,
author = {Si, Xiaojie and Zhang, Xuyun and Dou, Wanchun},
title = {A novel local optimization method for QoS-aware web service composition},
year = {2010},
isbn = {3642165141},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {QoS-aware web service selection has become a hot-spot research topic in the domain of web service composition. In previous works, the multiple tasks recruited in a composite schema are usually considered of equal importance. However, it is unreasonable for each task to have the absolutely same weight in certain circumstances. Hence, it is a great challenge to mine the weights among different tasks to reflect customers' partial preferences. In view of this challenge, a novel local optimization method is presented in this paper, which is based on a two-hierarchy weight, i.e., weight of task's criteria and weight of tasks. Finally, a case study is demonstrated to validate the feasibility of our proposal.},
booktitle = {Proceedings of the 2010 International Conference on Web Information Systems and Mining},
pages = {402–409},
numpages = {8},
keywords = {QoS, local optimization, service composition, weight},
location = {Sanya, China},
series = {WISM'10}
}

@inproceedings{10.1145/2328909.2328935,
author = {Sobernig, Stefan and Zdun, Uwe},
title = {Inversion-of-control layer},
year = {2010},
isbn = {9781450302593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2328909.2328935},
doi = {10.1145/2328909.2328935},
abstract = {Inversion of control is a common design practise that has been used in various application areas. It gained popularity in the context of object-oriented application frameworks and designs based on abstract classes and interfaces. Recently, dependency injection techniques, especially in the context of lightweight containers such as Spring, have raised the attention for inversion of control again. However, inversion of control has not yet been described in its architectural dimension with a focus on layering architectures, and the pros and cons of the design decision for control inversion. In this paper, we present the inversion-of-control layer pattern which describes the design practise from an architectural point of view, rather than focusing on particular implementation techniques.},
booktitle = {Proceedings of the 15th European Conference on Pattern Languages of Programs},
articleno = {21},
numpages = {22},
keywords = {application framework, architectural patterns, inversion of control, layers},
location = {Irsee, Germany},
series = {EuroPLoP '10}
}

@article{10.1145/3351239,
author = {Gao, Yang and Wang, Wei and Phoha, Vir V. and Sun, Wei and Jin, Zhanpeng},
title = {EarEcho: Using Ear Canal Echo for Wearable Authentication},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
url = {https://doi.org/10.1145/3351239},
doi = {10.1145/3351239},
abstract = {Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {81},
numpages = {24},
keywords = {Acoustic, authentication, biometric, ear canal, echo, wearable devices}
}

@article{10.1145/3300148,
author = {Li, Miqing and Yao, Xin},
title = {Quality Evaluation of Solution Sets in Multiobjective Optimisation: A Survey},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3300148},
doi = {10.1145/3300148},
abstract = {Complexity and variety of modern multiobjective optimisation problems result in the emergence of numerous search techniques, from traditional mathematical programming to various randomised heuristics. A key issue raised consequently is how to evaluate and compare solution sets generated by these multiobjective search techniques. In this article, we provide a comprehensive review of solution set quality evaluation. Starting with an introduction of basic principles and concepts of set quality evaluation, this article summarises and categorises 100 state-of-the-art quality indicators, with the focus on what quality aspects these indicators reflect. This is accompanied in each category by detailed descriptions of several representative indicators and in-depth analyses of their strengths and weaknesses. Furthermore, issues regarding attributes that indicators possess and properties that indicators are desirable to have are discussed, in the hope of motivating researchers to look into these important issues when designing quality indicators and of encouraging practitioners to bear these issues in mind when selecting/using quality indicators. Finally, future trends and potential research directions in the area are suggested, together with some guidelines on these directions.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {26},
numpages = {38},
keywords = {Quality evaluation, evolutionary algorithms, exact method, heuristic, indicator, measure, metaheuristic, metric, multi-criteria optimisation, multobjective optimisation, performance assessment}
}

@article{10.1023/A:1016589208824,
author = {Atkinson, Colin and Bunse, Christian and Gro\ss{}, Hans-Gerhard and K\"{u}hne, Thomas},
title = {Towards a General Component Model for Web-Based Applications},
year = {2002},
issue_date = {June 2002},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {13},
number = {1–4},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1016589208824},
doi = {10.1023/A:1016589208824},
abstract = {The cost effective development of web applications is perhaps one of the most challenging areas of software engineering today. Not only are the problems to be solved, and the solution technologies to be used, in web application development among the most rapidly changing in the software industry, but the business pressures of cost, quality and time-to-market are among the most extreme. Web application development therefore has potentially the most to gain from software reuse approaches that can offer a greater return on development time than traditional approaches. However, simply combining ideas from these reuse paradigms and traditional web development technologies in ad-hoc ways will not result in sustainable improvements. In this paper we describe a systematic way of combining the benefits of component-based development and model driven architectures, two important reuse approaches, to support the cost effective development and maintenance of web applications. After first defining a suitably abstract component-model, the paper explains how component architectures can be systematically and rigorously modeled using UML. It then describes a powerful technique, known as stratification, for separating the various cross cutting aspects of a web application such that a suitable platform specific architecture can be traceably generated. Finally, the paper introduces a technique for increasing the trustworthiness of components by giving them the capability to check their deployment environment at run-time.},
journal = {Ann. Softw. Eng.},
month = jun,
pages = {35–69},
numpages = {35}
}

@article{10.1016/j.sysarc.2012.04.001,
author = {Mart\'{\i}Nez, Patricia L\'{o}Pez and Cuevas, Cesar and Drake, Jos\'{e} M.},
title = {Compositional real-time models},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {58},
number = {6–7},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2012.04.001},
doi = {10.1016/j.sysarc.2012.04.001},
abstract = {This paper proposes a methodology for modelling the timing behaviour of hard real-time systems oriented to compositionality and reusability. When a system is built according to a modular structure, the methodology provides the system designer with capacity to build the real-time model of the system as a composition of the reusable timing models of the modules that make up the system. The modularization is applied at all levels: software, hardware and middleware. The methodology relies on a reactive modelling approach, i.e. the timing behaviour of a system is modelled by identifying and describing the timing behaviour of the activities executed in the system in response to events, coming either from the environment or from the timer. The methodology is based on the complementary concepts of model descriptor and model instance. The reusable timing model of a software or hardware module is formulated as a parameterized descriptor, which contains all the information about the internal elements of the module that is required to evaluate the behaviour of any application in which the module may be used. The analysable real-time model of a system is built by composing the model instances of the modules that form it, which are generated from their corresponding descriptors by assigning concrete values to all their parameters according to the specific configuration of the system.},
journal = {J. Syst. Archit.},
month = jun,
pages = {257–276},
numpages = {20},
keywords = {Model composition, Modelling, Real-time, Reusability, Schedulability analysis}
}

@inproceedings{10.5555/645882.672256,
author = {Smith, Dennis B. and Brien, Liam O' and Bergey, John},
title = {Using the Options Analysis for Reengineering (OAR) Method for Mining Components for a Product Line},
year = {2002},
isbn = {3540439854},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The Options Analysis for Reengineering (OAR) method is a systematic, architecture-centric means for mining existing components for a product line or new software architecture. The method incorporates a set of scalable techniques and exercises to collaboratively analyze existing components, determine viable mining options, and evaluate the most promising options. The OAR method has 5 activities that are followed in a systematic manner to identify components for mining and estimate the cost and risk of changes required to each legacy component to enable its reuse within a new software architecture. The OAR method provides visibility into this highly complex analysis activity. It also provides insights into implicit stakeholder assumptions, constraints, and other major drivers that impact the mining of components. Results from a pilot application of the OAR method are presented in this paper.},
booktitle = {Proceedings of the Second International Conference on Software Product Lines},
pages = {316–327},
numpages = {12},
series = {SPLC 2}
}

@inproceedings{10.1145/568760.568881,
author = {Fresa, A. and Nucera, G. and Peciola, E. and Santucci, G.},
title = {Assessment of software architectures: a case study},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568881},
doi = {10.1145/568760.568881},
abstract = {Producing high quality software is a very hard task. In the last years a big effort has been spent in devising techniques for estimating and/or measuring software properties. This ranges from forecasting, in a very early stage, the cost of software production to measuring several subcharacterics in order to assess the internal and external software quality. The role of predicting vs. measuring is gaining an increas ing relevance. As an example, the recently revised ISO 9126 standard [1] introduces the concept of Estimated (Predicted) Product Quality. It is clear that the sooner estimated figures are available, the better is possible to modify some design choices. Among all the aspects involved in software developing, a central role is played by the chosen software architecture. Estimating the quality characteristics of such architecture is a strategic activity that can drive several following design decision. In this paper we report the experience of an architectural assessment performed in Ericsson Lab Italy. The assessment was performed according to the framework presented by Jan Bosch in [2].},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {699–706},
numpages = {8},
location = {Ischia, Italy},
series = {SEKE '02}
}

@article{10.1016/j.jss.2009.11.709,
author = {Zhang, Pengcheng and Muccini, Henry and Li, Bixin},
title = {A classification and comparison of model checking software architecture techniques},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.11.709},
doi = {10.1016/j.jss.2009.11.709},
abstract = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it.},
journal = {J. Syst. Softw.},
month = may,
pages = {723–744},
numpages = {22},
keywords = {Model checking, Software architecture}
}

@article{10.1504/IJCSE.2015.067054,
author = {Modica, Giuseppe Di and Tomarchio, Orazio},
title = {A semantic framework to support resource discovery in future cloud markets},
year = {2015},
issue_date = {January 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {10},
number = {1/2},
issn = {1742-7185},
url = {https://doi.org/10.1504/IJCSE.2015.067054},
doi = {10.1504/IJCSE.2015.067054},
abstract = {The market of cloud resources is currently dominated by proprietary solutions for what concerns resource delivering, pricing models and service level agreements. In the future cloud markets, when cloud standards will get mature and full interoperability among cloud systems will be a reality, the competition challenge among providers will be played on the capability of supplying high and differentiated QoS levels. In this new scenario advanced and flexible mechanisms to support the matchmaking between what providers offer and what customers demand must be devised. Along with an analysis of the current cloud offering in terms of pricing model, SLA negotiation capabilities, service performance levels and cloud application requirements, this work proposes the definition of a semantic model to support the supply-demand matchmaking process in future cloud markets. Leveraging on a semantic description of the cloud resources' features, customers will be able to discover cloud offers that best suit their own business needs. Tests conducted on an implementation prototype proved the viability of the approach.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {1–14},
numpages = {14}
}

@article{10.1145/1082983.1085124,
title = {Frontmatter (TOC, Letters, Election results, Software Reliability Resources!, Computing Curricula 2004 and the Software Engineering Volume SE2004, Software Reuse Research, ICSE 2005 Forward)},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1085124},
doi = {10.1145/1082983.1085124},
journal = {SIGSOFT Softw. Eng. Notes},
month = jul,
pages = {0},
numpages = {63}
}

@inbook{10.5555/2554542.2554582,
author = {Dodds, Ricardo and Iocchi, Luca and Guerrero, Pablo and Ruiz-Del-Solar, Javier},
title = {Benchmarks for robotic soccer vision},
year = {2012},
isbn = {9783642320590},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Robotic soccer vision has been a major research problem in RoboCup and, even though many progresses have been made so that, for example, games now can run without many constraints on the lighting conditions, the problem has not been completely solved and on-site camera calibration is always a major activity for RoboCup soccer teams. While different robotic soccer vision and object perception techniques continue to appear in the RoboCup Soccer League, there is a lack of quantitative evaluation of existing methods.Since we believe that a quantitative evaluation of soccer vision algorithms will led to significant advances in the performance on perception and on the entire soccer task, in this paper we propose a benchmarking methodology for evaluating robotic soccer vision systems. We discuss the main issues of a successful benchmarking methodology: (i) a large and complete data base or data sets with ground truth; (ii) a public repository with data sets, algorithms and implementations that can be dynamically updated and (iii) evaluation metrics, error functions and comparison results.},
booktitle = {Robot Soccer World Cup XV},
pages = {427–439},
numpages = {13}
}

@article{10.1007/s10270-013-0393-x,
author = {Fan, Zhiqiang and Yue, Tao and Zhang, Li},
title = {SAMM: an architecture modeling methodology for ship command and control systems},
year = {2016},
issue_date = {February  2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0393-x},
doi = {10.1007/s10270-013-0393-x},
abstract = {Ship command and control systems (SCCSs) are composed of large-scale, complex, real-time and software-intensive systems that complete tasks collaboratively. Open architecture has been introduced to design the architecture of SCCSs and has been refined into functional architecture (FA) and technical architecture (TA) to meet architectural requirements such as adapting fast-speed functional and technical changes. Thereby, specifying the architecture of SCCSs, based on FA and TA, becomes a key issue for stakeholders of the domain. In this paper, we propose an architecture modeling methodology (named as SAMM) for describing the architecture of SCCSs. SAMM is derived by following a systematic and generic framework--modeling Goal, domain-specific Conceptual model, architecture Viewpoint, and architecture description Language (GCVL), which guides domain experts to devise domain-specific architecture modeling methodologies of large-scale software-intensive systems. SAMM contains three viewpoints and 22 models, and a UML/SysML-based architecture description language. An industrial application of SAMM, along with the subsequent application of the derived SAMM architecture model (i.e., a deployed SCCS prototype) was conducted to evaluate SAMM. A questionnaire-based survey was also conducted to subjectively evaluate whether SAMM meets the modeling goals and its applicability. Results show that SAMM meets all modeling goals and is easy to apply.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {71–118},
numpages = {48},
keywords = {Architecture modeling, Ship command and control systems, SysML, UML, Viewpoint}
}

@article{10.4018/jsse.2010040103,
author = {Khan, Khaled Md and Han, Jun},
title = {A Tool Support for Secure Software Integration},
year = {2010},
issue_date = {April 2010},
publisher = {IGI Global},
address = {USA},
volume = {1},
number = {2},
issn = {1947-3036},
url = {https://doi.org/10.4018/jsse.2010040103},
doi = {10.4018/jsse.2010040103},
abstract = {This paper presents a tool for the integration of security-aware services based applications that is constructed on the principles of security characterization of individual software services. The tool uses the technique of reasoning between the ensured security properties of the services and the security requirements of the user's system. Rather than reporting the research outcomes, in this paper the authors describe the architecture and capabilities of the tool for secure software integration. The main objective of this paper is to show that an automatic tool support could assist the process of security-aware service based software integration.},
journal = {Int. J. Secur. Softw. Eng.},
month = apr,
pages = {35–56},
numpages = {22},
keywords = {Characterization, Compositional Contracts, Security Properties, Software Components, Software Services}
}

@article{10.1145/2729974,
author = {Mkaouer, Wiem and Kessentini, Marouane and Shaout, Adnan and Koligheu, Patrice and Bechikh, Slim and Deb, Kalyanmoy and Ouni, Ali},
title = {Many-Objective Software Remodularization Using NSGA-III},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2729974},
doi = {10.1145/2729974},
abstract = {Software systems nowadays are complex and difficult to maintain due to continuous changes and bad design choices. To handle the complexity of systems, software products are, in general, decomposed in terms of packages/modules containing classes that are dependent. However, it is challenging to automatically remodularize systems to improve their maintainability. The majority of existing remodularization work mainly satisfy one objective which is improving the structure of packages by optimizing coupling and cohesion. In addition, most of existing studies are limited to only few operation types such as move class and split packages. Many other objectives, such as the design semantics, reducing the number of changes and maximizing the consistency with development change history, are important to improve the quality of the software by remodularizing it. In this article, we propose a novel many-objective search-based approach using NSGA-III. The process aims at finding the optimal remodularization solutions that improve the structure of packages, minimize the number of changes, preserve semantics coherence, and reuse the history of changes. We evaluate the efficiency of our approach using four different open-source systems and one automotive industry project, provided by our industrial partner, through a quantitative and qualitative study conducted with software engineers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {17},
numpages = {45},
keywords = {Search-based software engineering, remodularization, software maintenance, software quality}
}

@article{10.1145/3170432,
author = {Dayarathna, Miyuru and Perera, Srinath},
title = {Recent Advancements in Event Processing},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3170432},
doi = {10.1145/3170432},
abstract = {Event processing (EP) is a data processing technology that conducts online processing of event information. In this survey, we summarize the latest cutting-edge work done on EP from both industrial and academic research community viewpoints. We divide the entire field of EP into three subareas: EP system architectures, EP use cases, and EP open research topics. Then we deep dive into the details of each subsection. We investigate the system architecture characteristics of novel EP platforms, such as Apache Storm, Apache Spark, and Apache Flink. We found significant advancements made on novel application areas, such as the Internet of Things; streaming machine learning (ML); and processing of complex data types such as text, video data streams, and graphs. Furthermore, there has been significant body of contributions made on event ordering, system scalability, development of EP languages and exploration of use of heterogeneous devices for EP, which we investigate in the latter half of this article. Through our study, we found key areas that require significant attention from the EP community, such as Streaming ML, EP system benchmarking, and graph stream processing.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {33},
numpages = {36},
keywords = {Event processing, complex event processing, data stream processing}
}

@inproceedings{10.1109/ASE.2015.45,
author = {Sarkar, Atri and Guo, Jianmei and Siegmund, Norbert and Apel, Sven and Czarnecki, Krzysztof},
title = {Cost-efficient sampling for performance prediction of configurable systems},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.45},
doi = {10.1109/ASE.2015.45},
abstract = {A key challenge of the development and maintenance of configurable systems is to predict the performance of individual system variants based on the features selected. It is usually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predict performance based on small samples of measured variants, but it is still open how to dynamically determine an ideal sample that balances prediction accuracy and measurement effort. In this paper, we adapt two widely-used sampling strategies for performance prediction to the domain of configurable systems and evaluate them in terms of sampling cost, which considers prediction accuracy and measurement effort simultaneously. To generate an initial sample, we introduce a new heuristic based on feature frequencies and compare it to a traditional method based on t-way feature coverage. We conduct experiments on six real-world systems and provide guidelines for stakeholders to predict performance by sampling.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {342–352},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2095536.2095554,
author = {Siala, Fatma and Lajmi, Soufiene and Ghedira, Khaled},
title = {Multi-agent selection of multiple composite web services based on CBR method and driven by QoS},
year = {2011},
isbn = {9781450307840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095536.2095554},
doi = {10.1145/2095536.2095554},
abstract = {Many companies aim to use Web services to integrate heterogeneous or remote applications in SOA (Service Oriented Architecture) contexts. Indeed, one of the main assets of service-orientation is a composition to develop higher level services, so-called composite services, by re-using existing services. Since many available Web services provide overlapping or identical functionality, with different Quality of Service (QoS), a choice needs to be made to determine which services are to participate in a given composite service. However, for a composition, we can have different combinations and execution paths. Particularly, a composite service can generate different schemes that give various QoS scores.This paper presents a framework which deals with the selection of composite Web services on the base of Multi-Agents negotiation. The objective of these agents is to find out the best Composite QoS (CQoS) based on Web services availability. This scalable framework supports different combinations and execution paths using CBR technique. The proposed Multi-Agents framework is compared to an existing approach in terms of execution time. Experiments have demonstrated that our framework provide reliable results in comparison with the existing approach.},
booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-Based Applications and Services},
pages = {90–97},
numpages = {8},
keywords = {CBR technique, QoS, composition, contract-net protocol, execution paths, multi-agent system, web service},
location = {Ho Chi Minh City, Vietnam},
series = {iiWAS '11}
}

@inbook{10.5555/2167810.2167821,
author = {Lu, Shourong and Halang, Wolfgang A.},
title = {Platform-independent specification of component architectures for embedded real-time systems based on an extended UML},
year = {2005},
isbn = {3540306447},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {A way to specify component-based software architectures for embedded real-time systems is introduced. Component models are specified taking the Model Driven Architecture (MDA) approach, and employing UML notations. First, the principle of the developing process based on the MDA approach and the new concepts of UML-specified component architectures are addressed. Then, a conceptual framework architecture for the design of embedded real-time systems is presented, in which platform-independent component models are built. Taking specific platform features into regard, specific component models result from transformations mapping the platform-independent component model to either the Process and Experiment Automation Real-Time Language (PEARL) or to Function Blocks according to IEC 61131-3 or IEC 61499.},
booktitle = {Component-Based Software Development for Embedded Systems: An Overview of Current Research Trends},
pages = {123–142},
numpages = {20}
}

@article{10.1145/3432195,
author = {Mao, Wenguang and Sun, Wei and Wang, Mei and Qiu, Lili},
title = {DeepRange: Acoustic Ranging via Deep Learning},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432195},
doi = {10.1145/3432195},
abstract = {Acoustic ranging is a technique for estimating the distance between two objects using acoustic signals, which plays a critical role in many applications, such as motion tracking, gesture/activity recognition, and indoor localization. Although many ranging algorithms have been developed, their performance still degrades significantly under strong noise, interference and hardware limitations. To improve the robustness of the ranging system, in this paper we develop a Deep learning based Ranging system, called DeepRange. We first develop an effective mechanism to generate synthetic training data that captures noise, speaker/mic distortion, and interference in the signals and remove the need of collecting a large volume of training data. We then design a deep range neural network (DRNet) to estimate distance. Our design is inspired by signal processing that ultra-long convolution kernel sizes help to combat the noise and interference. We further apply an ensemble method to enhance the performance. Moreover, we analyze and visualize the network neurons and filters, and identify a few important findings that can be useful for improving the design of signal processing algorithms. Finally, we implement and evaluate DeepRangeusing 11 smartphones with different brands and models, 4 environments (i.e., a lab, a conference room, a corridor, and a cubic area), and 10 users. Our results show that DRNet significantly outperforms existing ranging algorithms.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = dec,
articleno = {143},
numpages = {23},
keywords = {Acoustic Sensing, Convolutional Neural Network, Motion Tracking, Ranging}
}

