@article{10.1016/j.jss.2021.111026,
author = {Zhu, Kun and Ying, Shi and Zhang, Nana and Zhu, Dandan},
title = {Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111026},
doi = {10.1016/j.jss.2021.111026},
journal = {J. Syst. Softw.},
month = oct,
numpages = {25},
keywords = {Software defect prediction, Metaheuristic feature selection, Whale optimization algorithm, Convolutional neural network, Kernel extreme learning machine}
}

@article{10.1016/j.neucom.2019.11.067,
author = {Qiao, Lei and Li, Xuesong and Umer, Qasim and Guo, Ping},
title = {Deep learning based software defect prediction},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {385},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2019.11.067},
doi = {10.1016/j.neucom.2019.11.067},
journal = {Neurocomput.},
month = apr,
pages = {100–110},
numpages = {11},
keywords = {Software defect prediction, Deep learning, Software quality, Software metrics, Robustness evaluation}
}

@inproceedings{10.1007/978-3-030-90785-3_16,
author = {Wang, Baoping and Wang, Wennan and Zhu, Linkai and Liu, Wenjian},
title = {Research on Cross-Project Software Defect Prediction Based on Machine Learning},
year = {2021},
isbn = {978-3-030-90784-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-90785-3_16},
doi = {10.1007/978-3-030-90785-3_16},
abstract = {In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution.},
booktitle = {Advances in Web-Based Learning – ICWL 2021: 20th International Conference, ICWL 2021, Macau, China, November 13–14, 2021, Proceedings},
pages = {160–165},
numpages = {6},
keywords = {Machine learning, Software defect prediction model, Metric},
location = {Macau, China}
}

@inproceedings{10.1007/978-3-030-86472-9_28,
author = {Shakhovska, Natalya and Yakovyna, Vitaliy},
title = {Feature Selection and Software Defect Prediction by Different Ensemble Classifiers},
year = {2021},
isbn = {978-3-030-86471-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86472-9_28},
doi = {10.1007/978-3-030-86472-9_28},
abstract = {Software defect prediction can improve its quality and is actively studied during the last decade. This paper focuses on the improvement of software defect prediction accuracy by proper feature selection techniques and using ensemble classifier. The software code metrics were used to predict the defective modules. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. Boruta, ACE, regsubsets and simple correlation are used for feature selection. The results of selection are formed based on hard voting of all features selectors. A new stacking classifier for software defects prediction is presented in this paper. The stacking classifier for defects prediction algorithm is based on combination of 5 weak classifiers. Random forest algorithm is used to combine the predictions. The obtained prediction accuracy was up to 96.26%.},
booktitle = {Database and Expert Systems Applications: 32nd International Conference, DEXA 2021, Virtual Event, September 27–30, 2021, Proceedings, Part I},
pages = {307–313},
numpages = {7},
keywords = {Ensemble of classifiers, Feature selection, Software defect analysis}
}

@article{10.1007/s00521-020-04960-1,
author = {Wang, Kechao and Liu, Lin and Yuan, Chengjun and Wang, Zhifei},
title = {Software defect prediction model based on LASSO–SVM},
year = {2021},
issue_date = {Jul 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {14},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-020-04960-1},
doi = {10.1007/s00521-020-04960-1},
abstract = {A software defect report is a bug in the software system that developers and users submit to the software defect library during software development and maintenance. Managing a software defect report that is overwhelming is a challenging task. The traditional method is manual identification, which is time-consuming and laborious and delays the repair of important software defects. Based on the above background, the purpose of this paper is to study the software defect prediction (SDP) model based on LASSO–SVM. In this paper, the problem of poor prediction accuracy of most SDP models is proposed. A SDP model combining minimum absolute value compression and selection method and support vector machine algorithm is proposed. Firstly, the feature selection ability of the minimum absolute value compression and selection method is used to reduce the dimension of the original data set, and the data set not related to SDP is removed. Then, the optimal value of SVM is obtained by using the parameter optimization ability of cross-validation algorithm. Finally, the SDP is completed by the nonlinear computing ability of SVM. The accuracy of simulation results is 93.25% and 66.67%, recall rate is 78.04%, and f-metric is 72.72%. The results show that the proposed defect prediction model has higher prediction accuracy than the traditional defect prediction model, and the prediction speed is faster.},
journal = {Neural Comput. Appl.},
month = jul,
pages = {8249–8259},
numpages = {11},
keywords = {Software defect prediction, Feature selection, Support vector machine, Cross-validation}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the 10th International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Community Structure Detection, Machine Learning, Software Defect Prediction, Software Engineering, Unsupervised Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT '19}
}

@article{10.1155/2021/2323100,
author = {Liu, Wenjian and Wang, Baoping and Wang, Wennan and Ni, Tongguang},
title = {Deep Learning Software Defect Prediction Methods for Cloud Environments Research},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/2323100},
doi = {10.1155/2021/2323100},
abstract = {This paper provides an in-depth study and analysis of software defect prediction methods in a cloud environment and uses a deep learning approach to justify software prediction. A cost penalty term is added to the supervised part of the deep ladder network; that is, the misclassification cost of different classes is added to the model. A cost-sensitive deep ladder network-based software defect prediction model is proposed, which effectively mitigates the negative impact of the class imbalance problem on defect prediction. To address the problem of lack or insufficiency of historical data from the same project, a flow learning-based geodesic cross-project software defect prediction method is proposed. Drawing on data information from other projects, a migration learning approach was used to embed the source and target datasets into a Gaussian manifold. The kernel encapsulates the incremental changes between the differences and commonalities between the two domains. To this point, the subspace is the space of two distributional approximations formed by the source and target data transformations, with traditional in-project software defect classifiers used to predict labels. It is found that real-time defect prediction is more practical because it has a smaller amount of code to review; only individual changes need to be reviewed rather than entire files or packages while making it easier for developers to assign fixes to defects. More importantly, this paper combines deep belief network techniques with real-time defect prediction at a fine-grained level and TCA techniques to deal with data imbalance and proposes an improved deep belief network approach for real-time defect prediction, while trying to change the machine learning classifier underlying DBN for different experimental studies, and the results not only validate the effectiveness of using TCA techniques to solve the data imbalance problem but also show that the defect prediction model learned by the improved method in this paper has better prediction performance.},
journal = {Sci. Program.},
month = jan,
numpages = {11}
}

@inproceedings{10.1145/3416508.3417114,
author = {Aljamaan, Hamoud and Alazba, Amal},
title = {Software defect prediction using tree-based ensembles},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417114},
doi = {10.1145/3416508.3417114},
abstract = {Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Bagging, Boosting, Classification, Ensemble Learning, Machine Learning, Prediction, Software Defect},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@article{10.1007/s10586-018-1730-1,
author = {Jayanthi, R. and Florence, Lilly},
title = {Software defect prediction techniques using metrics based on neural network classifier},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1730-1},
doi = {10.1007/s10586-018-1730-1},
abstract = {Software industries strive for software quality improvement by consistent bug prediction, bug removal and prediction of fault-prone module. This area has attracted researchers due to its significant involvement in software industries. Various techniques have been presented for software defect prediction. Recent researches have recommended data-mining using machine learning as an important paradigm for software bug prediction. state-of-art software defect prediction task suffer from various issues such as classification accuracy. However, software defect datasets are imbalanced in nature and known fault prone due to its huge dimension. To address this issue, here we present a combined approach for software defect prediction and prediction of software bugs. Proposed approach delivers a concept of feature reduction and artificial intelligence where feature reduction is carried out by well-known principle component analysis (PCA) scheme which is further improved by incorporating maximum-likelihood estimation for error reduction in PCA data reconstruction. Finally, neural network based classification technique is applied which shows prediction results. A framework is formulated and implemented on NASA software dataset where four datasets i.e., KC1, PC3, PC4 and JM1 are considered for performance analysis using MATLAB simulation tool. An extensive experimental study is performed where confusion, precision, recall, classification accuracy etc. parameters are computed and compared with existing software defect prediction techniques. Experimental study shows that proposed approach can provide better performance for software defect prediction.},
journal = {Cluster Computing},
month = jan,
pages = {77–88},
numpages = {12},
keywords = {Defect prediction models, Machine learning techniques, Software defect prediction, Software metrics}
}

@inproceedings{10.1145/3474198.3478215,
author = {Du, Xiaozhi and Yue, Hehe and Dong, Honglei},
title = {Software Defect Prediction Method based on Hybrid Sampling},
year = {2022},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478215},
doi = {10.1145/3474198.3478215},
abstract = {Software defect prediction is an essential technology to provide guidance and assistance for software testers and developers. However, the problem of imbalanced data sets limits the effect and application of the software defect prediction. To address this issue, this paper proposes a software defect prediction method based on hybrid sampling, which combines the strategies of over-sampling with under-sampling. For minority class, over-sampling uses k-means to cluster samples, then adopts SMOTE to generate artificial data based on safe areas of the clustering outcome. For majority class, under-sampling uses logistic regression classifier to get the misclassification probability of each sample and its instance hardness value. Then the samples, whose instance hardness values are lower than the threshold, are removed from the datasets. The experimental results show that our method is superior to the previous methods. Compared with SMOTE-kNN, SMOTE-Tomek, SMOTE and DBSMOTE, the accuracy of our method is improved by 17.60%, 6.99%, 8.66% and 26.18% on average respectively.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {93},
numpages = {9},
keywords = {Data imbalance, Hybrid sampling, Software defect prediction},
location = {Changsha, China},
series = {ICFEICT 2021}
}

@article{10.1007/s10664-021-09984-2,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Weighted software metrics aggregation and its application to defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09984-2},
doi = {10.1007/s10664-021-09984-2},
abstract = {It is a well-known practice in software engineering to aggregate software metrics to assess software artifacts for various purposes, such as their maintainability or their proneness to contain bugs. For different purposes, different metrics might be relevant. However, weighting these software metrics according to their contribution to the respective purpose is a challenging task. Manual approaches based on experts do not scale with the number of metrics. Also, experts get confused if the metrics are not independent, which is rarely the case. Automated approaches based on supervised learning require reliable and generalizable training data, a ground truth, which is rarely available. We propose an automated approach to weighted metrics aggregation that is based on unsupervised learning. It sets metrics scores and their weights based on probability theory and aggregates them. To evaluate the effectiveness, we conducted two empirical studies on defect prediction, one on ca. 200 000 code changes, and another ca. 5 000 software classes. The results show that our approach can be used as an agnostic unsupervised predictor in the absence of a ground truth.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {34},
keywords = {Software assessment, Quantitative methods, Defect prediction, Software metrics, Aggregation, Weighting}
}

@article{10.1007/s11334-021-00399-2,
author = {Suresh Kumar, P. and Behera, H. S. and Nayak, Janmenjoy and Naik, Bighnaraj},
title = {Bootstrap aggregation ensemble learning-based reliable approach for software defect prediction by using characterized code feature},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-021-00399-2},
doi = {10.1007/s11334-021-00399-2},
abstract = {To ensure software quality, software defect prediction plays a prominent role for the software developers and practitioners. Software defect prediction can assist us with distinguishing software defect modules and enhance the software quality. In present days, many supervised machine learning algorithms have proved their efficacy to identify defective modules. However, those are limited to prove their major significance due to the limitations such as the adaptation of parameters with the environment and complexity. So, it is important to develop a key methodology to improve the efficiency of the prediction module. In this paper, an ensemble learning technique called&nbsp;Bootstrap&nbsp;aggregating has been proposed for software defect prediction object-oriented modules. The proposed method's accuracy, recall, precision, F-measure, and AUC-ROC efficiency were compared to those of many qualified machine learning algorithms. Simulation results and performance comparison are evident that the proposed method outperformed well compared to other approaches.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {355–379},
numpages = {25},
keywords = {Ensemble learning, Software defect prediction, Software reliability, Machine learning}
}

@article{10.1016/j.infsof.2021.106664,
author = {Yao, Jingxiu and Shepperd, Martin},
title = {The impact of using biased performance metrics on software defect prediction research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106664},
doi = {10.1016/j.infsof.2021.106664},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Software engineering, Machine learning, Software defect prediction, Computational experiment, Classification metrics}
}

@article{10.1016/j.asoc.2021.107870,
author = {Kabir, Md Alamgir and Keung, Jacky and Turhan, Burak and Bennin, Kwabena Ebo},
title = {Inter-release defect prediction with feature selection using temporal chunk-based learning: An empirical study},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {113},
number = {PA},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107870},
doi = {10.1016/j.asoc.2021.107870},
journal = {Appl. Soft Comput.},
month = dec,
numpages = {17},
keywords = {Software defect prediction, Inter-release defect prediction, Feature selection}
}

@article{10.1504/ijcat.2019.100297,
author = {Jayanthi, R. and Florence, M. Lilly},
title = {Improved Bayesian regularisation using neural networks based on feature selection for software defect prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {60},
number = {3},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2019.100297},
doi = {10.1504/ijcat.2019.100297},
abstract = {Demand for software-based applications has grown drastically in various real-time applications. However, software testing schemes have been developed which include manual and automatic testing. Manual testing requires human effort and chances of error may still affect the quality of software. To overcome this issue, automatic software testing techniques based on machine learning techniques have been developed. In this work, we focus on the machine learning scheme for early prediction of software defects using Levenberg-Marquardt algorithm (LM), Back Propagation (BP) and Bayesian Regularisation (BR) techniques. Bayesian regularisation achieves better performance in terms of bug prediction. However, this performance can be enhanced further. Hence, we developed a novel approach for attribute selection-based feature selection technique to improve the performance of BR classification. An extensive study is carried out with the PROMISE repository where we considered KC1 and JM1 datasets. Experimental study shows that the proposed approach achieves better performance in predicting the defects in software.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {225–241},
numpages = {16},
keywords = {defect prediction model, machine learning techniques, software defect prediction, software metrics, gradient descent optimisation, gradient-based approach, feature subset selection, cross entropy error function, adaptive computation process}
}

@article{10.1007/s11063-020-10355-z,
author = {Niu, Liang and Wan, Jianwu and Wang, Hongyuan and Zhou, Kaiwei},
title = {Cost-sensitive Dictionary Learning for Software Defect Prediction},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {52},
number = {3},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-020-10355-z},
doi = {10.1007/s11063-020-10355-z},
abstract = {In recent years, software defect prediction has been recognized as a cost-sensitive learning problem. To deal with the unequal misclassification losses resulted by different classification errors, some cost-sensitive dictionary learning methods have been proposed recently. Generally speaking, these methods usually define the misclassification costs to measure the unequal losses and then propose to minimize the cost-sensitive reconstruction loss by embedding the cost information into the reconstruction function of dictionary learning. Although promising performance has been achieved, their cost-sensitive reconstruction functions are not well-designed. In addition, no sufficient attentions are paid to the coding coefficients which can also be helpful to reduce the reconstruction loss. To address these issues, this paper proposes a new cost-sensitive reconstruction loss function and introduces an additional cost-sensitive discrimination regularization for the coding coefficients. Both the two terms are jointly optimized in a unified cost-sensitive dictionary learning framework. By doing so, we can achieve the minimum reconstruction loss and thus obtain a more cost-sensitive dictionary for feature encoding of test data. In the experimental part, we have conducted extensive experiments on twenty-five software projects from four benchmark datasets of NASA, AEEEM, ReLink and Jureczko. The results, in comparison with ten state-of-the-art software defect prediction methods, demonstrate the effectiveness of learned cost-sensitive dictionary for software defect prediction.},
journal = {Neural Process. Lett.},
month = dec,
pages = {2415–2449},
numpages = {35},
keywords = {Software defect prediction, Cost-sensitive, Dictionary learning, Discrimination}
}

@inproceedings{10.1109/ICSE43902.2021.00050,
author = {Shrikanth, N. C. and Majumder, Suvodeep and Menzies, Tim},
title = {Early Life Cycle Software Defect Prediction: Why? How?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00050},
doi = {10.1109/ICSE43902.2021.00050},
abstract = {Many researchers assume that, for software analytics, "more data is better." We write to show that, at least for learning defect predictors, this may not be true.To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models.We hope these results inspire other researchers to adopt a "simplicity-first" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for "short cuts" that can simplify the analysis.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {448–459},
numpages = {12},
keywords = {analytics, defect prediction, early, sampling},
location = {Madrid, Spain},
series = {ICSE '21}
}

@article{10.1155/2021/4997459,
author = {Li, Zhen and Li, Tong and Wu, YuMei and Yang, Liu and Miao, Hong and Wang, DongSheng and Precup, Radu-Emil},
title = {Software Defect Prediction Based on Hybrid Swarm Intelligence and Deep Learning},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/4997459},
doi = {10.1155/2021/4997459},
abstract = {In order to improve software quality and testing efficiency, this paper implements the prediction of software defects based on deep learning. According to the respective advantages and disadvantages of the particle swarm algorithm and the wolf swarm algorithm, the two algorithms are mixed to realize the complementary advantages of the algorithms. At the same time, the hybrid algorithm is used in the search of model hyperparameter optimization, the loss function of the model is used as the fitness function, and the collaborative search ability of the swarm intelligence population is used to find the global optimal solution in multiple local solution spaces. Through the analysis of the experimental results of six data sets, compared with the traditional hyperparameter optimization method and a single swarm intelligence algorithm, the model using the hybrid algorithm has higher and better indicators. And, under the processing of the autoencoder, the performance of the model has been further improved.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {17}
}

@article{10.1155/2019/6230953,
author = {Fan, Guisheng and Diao, Xuyang and Yu, Huiqun and Yang, Kang and Chen, Liqiong and Vitiello, Autilia},
title = {Software Defect Prediction via Attention-Based Recurrent Neural Network},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/6230953},
doi = {10.1155/2019/6230953},
abstract = {In order to improve software reliability, software defect prediction is applied to the process of software maintenance to identify potential bugs. Traditional methods of software defect prediction mainly focus on designing static code metrics, which are input into machine learning classifiers to predict defect probabilities of the code. However, the characteristics of these artificial metrics do not contain the syntactic structures and semantic information of programs. Such information is more significant than manual metrics and can provide a more accurate predictive model. In this paper, we propose a framework called defect prediction via attention-based recurrent neural network (DP-ARNN). More specifically, DP-ARNN first parses abstract syntax trees (ASTs) of programs and extracts them as vectors. Then it encodes vectors which are used as inputs of DP-ARNN by dictionary mapping and word embedding. After that, it can automatically learn syntactic and semantic features. Furthermore, it employs the attention mechanism to further generate significant features for accurate defect prediction. To validate our method, we choose seven open-source Java projects in Apache, using F1-measure and area under the curve (AUC) as evaluation criteria. The experimental results show that, in average, DP-ARNN improves the F1-measure by 14% and AUC by 7% compared with the state-of-the-art methods, respectively.},
journal = {Sci. Program.},
month = jan,
numpages = {14}
}

@article{10.1007/s11219-016-9353-3,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Software defect prediction: do different classifiers find the same defects?},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9353-3},
doi = {10.1007/s11219-016-9353-3},
abstract = {During the last 10 years, hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall. We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers. We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in NASA, open source and commercial datasets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty of each classifier is compared. Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others. Our results confirm that a unique subset of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Given our results, we conclude that classifier ensembles with decision-making strategies not based on majority voting are likely to perform best in defect prediction.},
journal = {Software Quality Journal},
month = jun,
pages = {525–552},
numpages = {28},
keywords = {Machine learning, Prediction modelling, Software defect prediction}
}

@inproceedings{10.1007/978-3-030-58817-5_45,
author = {Balogun, Abdullateef O. and Lafenwa-Balogun, Fatimah B. and Mojeed, Hammed A. and Adeyemo, Victor E. and Akande, Oluwatobi N. and Akintola, Abimbola G. and Bajeh, Amos O. and Usman-Hamza, Fatimah E.},
title = {SMOTE-Based Homogeneous Ensemble Methods for Software Defect Prediction},
year = {2020},
isbn = {978-3-030-58816-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58817-5_45},
doi = {10.1007/978-3-030-58817-5_45},
abstract = {Class imbalance is a prevalent problem in machine learning which affects the prediction performance of classification algorithms. Software Defect Prediction (SDP) is no exception to this latent problem. Solutions such as data sampling and ensemble methods have been proposed to address the class imbalance problem in SDP. This study proposes a combination of Synthetic Minority Oversampling Technique (SMOTE) and homogeneous ensemble (Bagging and Boosting) methods for predicting software defects. The proposed approach was implemented using Decision Tree (DT) and Bayesian Network (BN) as base classifiers on defects datasets acquired from NASA software corpus. The experimental results showed that the proposed approach outperformed other experimental methods. High accuracy of 86.8% and area under operating receiver characteristics curve value of 0.93% achieved by the proposed technique affirmed its ability to differentiate between the defective and non-defective labels without bias.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part VI},
pages = {615–631},
numpages = {17},
keywords = {Software Defect Prediction, Class imbalance, Data sampling, Ensemble methods},
location = {Cagliari, Italy}
}

@article{10.1016/j.neucom.2021.05.043,
author = {Harzevili, Nima Shiri and Alizadeh, Sasan H.},
title = {Analysis and modeling conditional mutual dependency of metrics in software defect prediction using latent variables},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {460},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.05.043},
doi = {10.1016/j.neucom.2021.05.043},
journal = {Neurocomput.},
month = oct,
pages = {309–330},
numpages = {22},
keywords = {Software defect prediction Software metrics, Naive Bayes classifier, Latent variable, 00–01, 99–00}
}

@article{10.1007/s11277-017-5117-z,
author = {Zhou, Lijuan and Li, Ran and Zhang, Shudong and Wang, Hua},
title = {Imbalanced Data Processing Model for Software Defect Prediction},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {102},
number = {2},
issn = {0929-6212},
url = {https://doi.org/10.1007/s11277-017-5117-z},
doi = {10.1007/s11277-017-5117-z},
abstract = {In the field of software engineering, software defect prediction is the hotspot of the researches which can effectively guarantee the quality during software development. However, the problem of class imbalanced datasets will affect the accuracy of overall classification of software defect prediction, which is the key issue to be solved urgently today. In order to better solve this problem, this paper proposes a model named ASRA which combines attribute selection, sampling technologies and ensemble algorithm. The model adopts the Chi square test of attribute selection and then utilizes the combined sampling technique which includes SMOTE over-sampling and under-sampling to remove the redundant attributes and make the datasets balance. Afterwards, the model ASRA is eventually established by ensemble algorithm named Adaboost with basic classifier J48 decision tree. The data used in the experiments comes from UCI datasets. It can draw the conclusion that the effect of software defect prediction classification which using this model is improved and better than before by comparing the precision P, F-measure and AUC values from the results of the experiments.},
journal = {Wirel. Pers. Commun.},
month = sep,
pages = {937–950},
numpages = {14},
keywords = {Attribute selection, Class imbalance, Ensemble algorithm, Sampling, Software defect prediction}
}

@article{10.1016/j.jss.2019.03.012,
author = {Ni, Chao and Chen, Xiang and Wu, Fangfang and Shen, Yuxiang and Gu, Qing},
title = {An empirical study on pareto based multi-objective feature selection for software defect prediction},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.03.012},
doi = {10.1016/j.jss.2019.03.012},
journal = {J. Syst. Softw.},
month = jun,
pages = {215–238},
numpages = {24},
keywords = {xx-xx, xx-xx, Software defect prediction, Search based software engineering, Feature selection, Multi-Objective optimization, Empirical study}
}

@article{10.1155/2021/5069016,
author = {Balogun, Abdullateef O. and Basri, Shuib and Mahamad, Saipunidzam and Capretz, Luiz Fernando and Imam, Abdullahi Abubakar and Almomani, Malek A. and Adeyemo, Victor E. and Kumar, Ganesh and Dourado, Ant\'{o}nio},
title = {A Novel Rank Aggregation-Based Hybrid Multifilter Wrapper Feature Selection Method in Software Defect Prediction},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1687-5265},
url = {https://doi.org/10.1155/2021/5069016},
doi = {10.1155/2021/5069016},
abstract = {The high dimensionality of software metric features has long been noted as a data quality problem that affects the performance of software defect prediction (SDP) models. This drawback makes it necessary to apply feature selection (FS) algorithm(s) in SDP processes. FS approaches can be categorized into three types, namely, filter FS (FFS), wrapper FS (WFS), and hybrid FS (HFS). HFS has been established as superior because it combines the strength of both FFS and WFS methods. However, selecting the most appropriate FFS (filter rank selection problem) for HFS is a challenge because the performance of FFS methods depends on the choice of datasets and classifiers. In addition, the local optima stagnation and high computational costs of WFS due to large search spaces are inherited by the HFS method. Therefore, as a solution, this study proposes a novel rank aggregation-based hybrid multifilter wrapper feature selection (RAHMFWFS) method for the selection of relevant and irredundant features from software defect datasets. The proposed RAHMFWFS is divided into two stepwise stages. The first stage involves a rank aggregation-based multifilter feature selection (RMFFS) method that addresses the filter rank selection problem by aggregating individual rank lists from multiple filter methods, using a novel rank aggregation method to generate a single, robust, and non-disjoint rank list. In the second stage, the aggregated ranked features are further preprocessed by an enhanced wrapper feature selection (EWFS) method based on a dynamic reranking strategy that is used to guide the feature subset selection process of the HFS method. This, in turn, reduces the number of evaluation cycles while amplifying or maintaining its prediction performance. The feasibility of the proposed RAHMFWFS was demonstrated on benchmarked software defect datasets with Na\"{\i}ve Bayes and Decision Tree classifiers, based on accuracy, the area under the curve (AUC), and F-measure values. The experimental results showed the effectiveness of RAHMFWFS in addressing filter rank selection and local optima stagnation problems in HFS, as well as the ability to select optimal features from SDP datasets while maintaining or enhancing the performance of SDP models. To conclude, the proposed RAHMFWFS achieved good performance by improving the prediction performances of SDP models across the selected datasets, compared to existing state-of-the-arts HFS methods.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {19}
}

@article{10.1504/ijcat.2020.110428,
author = {Bai, Xue and Zhou, Hua and Yang, Hongji and Wang, Dong},
title = {Connecting historical changes for cross-version software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {63},
number = {4},
issn = {0952-8091},
url = {https://doi.org/10.1504/ijcat.2020.110428},
doi = {10.1504/ijcat.2020.110428},
abstract = {In the whole software life cycle, software defects are inevitable and increase the cost of software development and evolution. Cross-Version Software Defect Prediction (CVSDP) aims at learning the defect patterns from the historical data of previous software versions to distinguish buggy software modules from clean ones. In CVSDP, metrics are intrinsic properties associated with the external manifestation of defects. However, traditional software defect measures ignore the sequential information of changes during software evolution process which may play a crucial role in CVSDP. Therefore, researchers tried to connect traditional metrics across versions as a new kind of evolution metrics. This study proposes a new way to connect historical sequence of metrics based on change sequence named HCSM and designs a novel deep learning algorithm GDNN as a classifier to process it. Compared to the traditional metrics approaches and other relevant approaches, the proposed approach fits in projects with stable and orderly defect control trend.},
journal = {Int. J. Comput. Appl. Technol.},
month = jan,
pages = {371–383},
numpages = {12},
keywords = {software testing, cross-version defect prediction, software metrics, historical change sequences, deep learning, DNN, deep neural networks, gate recurrent unit}
}

@article{10.1007/s10515-021-00289-8,
author = {Ali, Aftab and Khan, Naveed and Abu-Tair, Mamun and Noppen, Joost and McClean, Sally and McChesney, Ian},
title = {Discriminating features-based cost-sensitive approach for software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00289-8},
doi = {10.1007/s10515-021-00289-8},
abstract = {Correlated quality metrics extracted from a source code repository can be utilized to design a model to automatically predict defects in a software system. It is obvious that the extracted metrics will result in a highly unbalanced data, since the number of defects in a good quality software system should be far less than the number of normal instances. It is also a fact that the selection of the best discriminating features significantly improves the robustness and accuracy of a prediction model. Therefore, the contribution of this paper is twofold, first it selects the best discriminating features that help in accurately predicting a defect in a software component. Secondly, a cost-sensitive logistic regression and decision tree ensemble-based prediction models are applied to the best discriminating features for precisely predicting a defect in a software component. The proposed models are compared with the most recent schemes in the literature in terms of accuracy, area under the curve, and recall. The models are evaluated using 11 datasets and it is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature.},
journal = {Automated Software Engg.},
month = nov,
numpages = {18},
keywords = {Software bugs/defects, Machine learning models, Discriminating features, Cost-sensitivity, AUC, Recall}
}

@article{10.1016/j.asoc.2015.04.045,
author = {Arar, \"{O}mer Faruk and Ayan, K\"{u}r\c{s}at},
title = {Software defect prediction using cost-sensitive neural network},
year = {2015},
issue_date = {August 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {33},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.04.045},
doi = {10.1016/j.asoc.2015.04.045},
abstract = {Software defect prediction model was built by Artificial Neural Network (ANN).ANN connection weights were optimized by Artificial Bee Colony (ABC).Parametric cost-sensitivity feature was added to ANN by using a new error function.Model was applied to five publicly available datasets from the NASA repository.Results were compared with other cost-sensitive and non-cost-sensitive studies. The software development life cycle generally includes analysis, design, implementation, test and release phases. The testing phase should be operated effectively in order to release bug-free software to end users. In the last two decades, academicians have taken an increasing interest in the software defect prediction problem, several machine learning techniques have been applied for more robust prediction. A different classification approach for this problem is proposed in this paper. A combination of traditional Artificial Neural Network (ANN) and the novel Artificial Bee Colony (ABC) algorithm are used in this study. Training the neural network is performed by ABC algorithm in order to find optimal weights. The False Positive Rate (FPR) and False Negative Rate (FNR) multiplied by parametric cost coefficients are the optimization task of the ABC algorithm. Software defect data in nature have a class imbalance because of the skewed distribution of defective and non-defective modules, so that conventional error functions of the neural network produce unbalanced FPR and FNR results. The proposed approach was applied to five publicly available datasets from the NASA Metrics Data Program repository. Accuracy, probability of detection, probability of false alarm, balance, Area Under Curve (AUC), and Normalized Expected Cost of Misclassification (NECM) are the main performance indicators of our classification approach. In order to prevent random results, the dataset was shuffled and the algorithm was executed 10 times with the use of n-fold cross-validation in each iteration. Our experimental results showed that a cost-sensitive neural network can be created successfully by using the ABC optimization algorithm for the purpose of software defect prediction.},
journal = {Appl. Soft Comput.},
month = aug,
pages = {263–277},
numpages = {15},
keywords = {Artificial Bee Colony, Artificial Neural Network, Cost-sensitive classification, Machine learning, Software defect prediction, Software quality}
}

@article{10.1016/j.jss.2017.03.044,
author = {Nuez-Varela, Alberto S. and Prez-Gonzalez, Hctor G. and Martnez-Perez, Francisco E. and Soubervielle-Montalvo, Carlos},
title = {Source code metrics},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.03.044},
doi = {10.1016/j.jss.2017.03.044},
abstract = {Three major programming paradigms measured by source code metrics were identified.The CK metrics and the object oriented paradigm are the most studied subjects.Java benchmark systems are the most commonly measured systems in research.Technology on metrics extraction mechanisms are not up to research advances.Empirical studies have a major impact on the code metrics community. ContextSource code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. ObjectivesThis paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. MethodA systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. ResultsAlmost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. ConclusionsObject oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
journal = {J. Syst. Softw.},
month = jun,
pages = {164–197},
numpages = {34},
keywords = {Aspect-oriented metrics, Feature-oriented metrics, Object-oriented metrics, Software metrics, Source code metrics, Systematic mapping study}
}

@article{10.1504/ijcse.2020.106871,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {A benchmarking framework using nonlinear manifold detection techniques for software defect prediction},
year = {2020},
issue_date = {2020},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {4},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2020.106871},
doi = {10.1504/ijcse.2020.106871},
abstract = {Prediction of software defects in time improves quality and helps in locating the defect-prone areas accurately. Although earlier considerable methods were applied, actually none of those measures was found to be fool-proof and accurate. Hence, a newer framework includes nonlinear manifold detection model, and its algorithm originated for defect prediction using different techniques of nonlinear manifold detection (nonlinear MDs) along with 14 different machine learning techniques (MLTs) on eight defective software datasets. A critical analysis cum exhaustive comparative estimation revealed that nonlinear manifold detection model has a more accurate and effective impact on defect prediction as compared to feature selection techniques. The outcome of the experiment was statistically tested by Friedman and post hoc analysis using Nemenyi test, which validates that hidden Markov model (HMM) along with nonlinear manifold detection model outperforms and is significantly different from MLTs.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {593–614},
numpages = {21},
keywords = {dimensionality reduction, feature selection, Friedman test, machine learning, Nemenyi test, nonlinear manifold detection, software defect prediction, post hoc analysis}
}

@inproceedings{10.1145/3239576.3239607,
author = {Du, Yuntao and Zhang, Lu and Shi, Jiahao and Tang, Jingjuan and Yin, Ying},
title = {Feature-Grouping-Based Two Steps Feature Selection Algorithm in Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239607},
doi = {10.1145/3239576.3239607},
abstract = {In order to improve the effect of software defect prediction, many algorithms including feature selection, have been proposed. Based on Wrapper and Filter hybrid framework, a feature-grouping-based feature selection algorithm is proposed in this paper. The algorithm is composed of two steps. In the first step, in order to remove the redundant features, we group the features according to the redundancy between the features. The symmetry uncertainty is used as the constant indicator of the correlation and the FCBF-based grouping algorithm is used to group the features. In the second step, a subset of the features are selected from each group to form the final subset of features. Many classical methods select the representative feature from each group. We consider that when the number of intra-group features is large, the representative features are not enough to reflect the information in this group. Therefore, we require that at least one feature be selected within each group, in this step, the PSO algorithm is used for Searching Randomly from each group. We tested on the open source NASA and PROMISE data sets. Using three kinds of classifier. Compared to the other methods tested in this article, our method resulted in 90% improvement in the predictive performance of 30 sets of results on 10 data sets. Compared with the algorithms without feature selection, the AUC values of this method in the Logistic regression, Naive Bayesian, and K-neighbor classifiers are improved by 5.94% and 4.69% And 8.05%. The FCBF algorithm can also be regarded as a kind of first performing feature grouping. Compared with the FCBF algorithm, the AUC values of this method are improved by 4.78%, 6.41% and 4.4% on the basis of Logistic regression, Naive Bayes and K-neighbor. We can also see that for the FCBF-based grouping algorithm, it could be better to choose a characteristic cloud from each group than to choose a representative one.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {173–178},
numpages = {6},
keywords = {FCBF-based grouping algorithm, Feature grouping, Intra-group feature selection, PSO, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1007/s00500-021-06096-3,
author = {Pandey, Sushant Kumar and Tripathi, Anil Kumar},
title = {An empirical study toward dealing with noise and class imbalance issues in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {21},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06096-3},
doi = {10.1007/s00500-021-06096-3},
abstract = {The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models.},
journal = {Soft Comput.},
month = nov,
pages = {13465–13492},
numpages = {28},
keywords = {Software testing, Software fault prediction, Class imbalance, Noisy instance, Machine learning, Software metrics, Fault proneness}
}

@inproceedings{10.1145/3342999.3343010,
author = {Cui, Mengtian and Sun, Yue and Lu, Yang and Jiang, Yue},
title = {Study on the Influence of the Number of Features on the Performance of Software Defect Prediction Model},
year = {2019},
isbn = {9781450371605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342999.3343010},
doi = {10.1145/3342999.3343010},
abstract = {The software defect prediction model based on machine learning technology is the key to improve the reliability of software. The influence of the number of features on the performance of different software defect prediction models was proposed in this paper. First, a new data sets was built, which is increasing by the number of features based on the NASA public data sets. Then, the eight predictive models are experimented based on these data sets. Next, the influence of the number of features on the performance of different prediction models was analyzed based on the experimental results. Next, the AUC values obtained from the experiment were used to evaluate the performance of different prediction models, and the coefficient of variation C·V values was used to evaluate the performance stability of different prediction models while the number of features changed. In the end, the experiments show that the performance of the predictive model C4.5 is highly susceptible to changes in the number of features, while the performance of the predictive model SMO is relatively stable.},
booktitle = {Proceedings of the 2019 3rd International Conference on Deep Learning Technologies},
pages = {32–37},
numpages = {6},
keywords = {feature selection, machine learning, number of features, software defect prediction},
location = {Xiamen, China},
series = {ICDLT '19}
}

@article{10.1016/j.infsof.2021.106662,
author = {Feng, Shuo and Keung, Jacky and Yu, Xiao and Xiao, Yan and Zhang, Miao},
title = {Investigation on the stability of SMOTE-based oversampling techniques in software defect prediction},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106662},
doi = {10.1016/j.infsof.2021.106662},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {14},
keywords = {Software defect prediction, Class imbalance, Oversampling, SMOTE, Empirical Software Engineering}
}

@article{10.1007/s11227-019-03051-w,
author = {NezhadShokouhi, Mohammad Mahdi and Majidi, Mohammad Ali and Rasoolzadegan, Abbas},
title = {Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {1},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-019-03051-w},
doi = {10.1007/s11227-019-03051-w},
abstract = {As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods.},
journal = {J. Supercomput.},
month = jan,
pages = {602–635},
numpages = {34},
keywords = {Software defect prediction, Software metrics, Mahalanobis distance, Over-sampling, Feature extraction}
}

@article{10.1016/j.eswa.2018.12.033,
author = {Turabieh, Hamza and Mafarja, Majdi and Li, Xiaodong},
title = {Iterated feature selection algorithms with layered recurrent neural network for software fault prediction},
year = {2019},
issue_date = {May 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2018.12.033},
doi = {10.1016/j.eswa.2018.12.033},
journal = {Expert Syst. Appl.},
month = may,
pages = {27–42},
numpages = {16},
keywords = {Software fault prediction, Feature selection, Layered recurrent neural network}
}

@article{10.1007/s11219-016-9342-6,
author = {Chen, Lin and Fang, Bin and Shang, Zhaowei and Tang, Yuanyan},
title = {Tackling class overlap and imbalance problems in software defect prediction},
year = {2018},
issue_date = {March     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-016-9342-6},
doi = {10.1007/s11219-016-9342-6},
abstract = {Software defect prediction (SDP) is a promising solution to save time and cost in the software testing phase for improving software quality. Numerous machine learning approaches have proven effective in SDP. However, the unbalanced class distribution in SDP datasets could be a problem for some conventional learning methods. In addition, class overlap increases the difficulty for the predictors to learn the defective class accurately. In this study, we propose a new SDP model which combines class overlap reduction and ensemble imbalance learning to improve defect prediction. First, the neighbor cleaning method is applied to remove the overlapping non-defective samples. The whole dataset is then randomly under-sampled several times to generate balanced subsets so that multiple classifiers can be trained on these data. Finally, these individual classifiers are assembled with the AdaBoost mechanism to build the final prediction model. In the experiments, we investigated nine highly unbalanced datasets selected from a public software repository and confirmed that the high rate of overlap between classes existed in SDP data. We assessed the performance of our proposed model by comparing it with other state-of-the-art methods including conventional SDP models, imbalance learning and data cleaning methods. Test results and statistical analysis show that the proposed model provides more reasonable defect prediction results and performs best in terms of G-mean and AUC among all tested models.},
journal = {Software Quality Journal},
month = mar,
pages = {97–125},
numpages = {29},
keywords = {Class imbalance, Class overlap, Machine learning, Software defect prediction}
}

@article{10.1016/j.infsof.2018.02.003,
author = {Mahmood, Zaheed and Bowes, David and Hall, Tracy and Lane, Peter C.R. and Petri\'{c}, Jean},
title = {Reproducibility and replicability of software defect prediction studies},
year = {2018},
issue_date = {Jul 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {99},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.003},
doi = {10.1016/j.infsof.2018.02.003},
journal = {Inf. Softw. Technol.},
month = jul,
pages = {148–163},
numpages = {16},
keywords = {Replication, Reproducibility, Software defect prediction}
}

@inproceedings{10.1145/3172871.3172872,
author = {Kumar, Lov and Sureka, Ashish},
title = {Feature Selection Techniques to Counter Class Imbalance Problem for Aging Related Bug Prediction: Aging Related Bug Prediction},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172872},
doi = {10.1145/3172871.3172872},
abstract = {Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs.We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {2},
numpages = {11},
keywords = {Aging Related Bugs, Empirical Software Engineering, Feature Selection Techniques, Imbalance Learning, Machine Learning, Predictive Modeling, Software Maintenance, Source Code Metrics},
location = {Hyderabad, India},
series = {ISEC '18}
}

@article{10.1007/s11219-018-9436-4,
author = {Ji, Haijin and Huang, Song and Wu, Yaning and Hui, Zhanwei and Zheng, Changyou},
title = {A new weighted naive Bayes method based on information diffusion for software defect prediction},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9436-4},
doi = {10.1007/s11219-018-9436-4},
abstract = {Software defect prediction (SDP) plays a significant part in identifying the most defect-prone modules before software testing and allocating limited testing resources. One of the most commonly used classifiers in SDP is naive Bayes (NB). Despite the simplicity of the NB classifier, it can often perform better than more complicated classification models. In NB, the features are assumed to be equally important, and the numeric features are assumed to have a normal distribution. However, the features often do not contribute equivalently to the classification, and they usually do not have a normal distribution after performing a Kolmogorov-Smirnov test; this may harm the performance of the NB classifier. Therefore, this paper proposes a new weighted naive Bayes method based on information diffusion (WNB-ID) for SDP. More specifically, for the equal importance assumption, we investigate six weight assignment methods for setting the feature weights and then choose the most suitable one based on the F-measure. For the normal distribution assumption, we apply the information diffusion model (IDM) to compute the probability density of each feature instead of the acquiescent probability density function of the normal distribution. We carry out experiments on 10 software defect data sets of three types of projects in three different programming languages provided by the PROMISE repository. Several well-known classifiers and ensemble methods are included for comparison. The final experimental results demonstrate the effectiveness and practicability of the proposed method.},
journal = {Software Quality Journal},
month = sep,
pages = {923–968},
numpages = {46},
keywords = {Software defect prediction, Naive Bayes, Feature weighting, Information diffusion}
}

@inproceedings{10.1007/978-3-030-33709-4_5,
author = {Tasnim Cynthia, Shamse and Rasul, Md. Golam and Ripon, Shamim},
title = {Effect of Feature Selection in Software Fault Detection},
year = {2019},
isbn = {978-3-030-33708-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-33709-4_5},
doi = {10.1007/978-3-030-33709-4_5},
abstract = {The quality of software is enormously affected by the faults associated with it. Detection of faults at a proper stage in software development is a challenging task and plays a vital role in the quality of the software. Machine learning is, now a days, a commonly used technique for fault detection and prediction. However, the effectiveness of the fault detection mechanism is impacted by the number of attributes in the publicly available datasets. Feature selection is the process of selecting a subset of all the features that are most influential to the classification and it is a challenging task. This paper thoroughly investigates the effect of various feature selection techniques on software fault classification by using NASA’s some benchmark publicly available datasets. Various metrics are used to analyze the performance of the feature selection techniques. The experiment discovers that the most important and relevant features can be selected by the adopted feature selection techniques without sacrificing the performance of fault detection.},
booktitle = {Multi-Disciplinary Trends in Artificial Intelligence: 13th International Conference, MIWAI 2019, Kuala Lumpur, Malaysia, November 17–19, 2019, Proceedings},
pages = {52–63},
numpages = {12},
keywords = {Fault detection, Feature selection, Feature classification},
location = {Kuala Lumpur, Malaysia}
}

@inproceedings{10.1145/3416506.3423577,
author = {Yang, Xingguang and Yu, Huiqun and Fan, Guisheng and Yang, Kang},
title = {A differential evolution-based approach for effort-aware just-in-time software defect prediction},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423577},
doi = {10.1145/3416506.3423577},
abstract = {Software defect prediction technology is an effective method to improve software quality. Effort-aware just-in-time software defect prediction (JIT-SDP) aims to identify more defective changes in limited effort. Although many methods have been proposed for JIT-SDP, the prediction performance of existing prediction models still needs to be improved. To improve the effort-aware prediction performance, we propose a new method called DEJIT based on differential evolution algorithm. First, we propose a metric called density-percentile-average (DPA), which is used as the optimization objective of models on the training set. Then, we use logistic regression to build models and use the differential evolution algorithm to determine coefficients of logistic regression. We conduct empirical research on six open source projects. Empirical results demonstrate that the proposed method significantly outperforms the state-of-the-art 4 supervised models and 4 unsupervised models.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {13–16},
numpages = {4},
keywords = {defect prediction, differential evolution, just-in-time},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@article{10.3233/KES-210061,
author = {Shatnawi, Raed},
title = {Software fault prediction using machine learning techniques with metric thresholds},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {25},
number = {2},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-210061},
doi = {10.3233/KES-210061},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {159–172},
numpages = {14},
keywords = {Fault prediction, software metrics, threshold values, machine learning}
}

@article{10.1016/j.infsof.2017.11.008,
author = {Tong, Haonan and Liu, Bin and Wang, Shihai},
title = {Software defect prediction using stacked denoising autoencoders and two-stage ensemble learning},
year = {2018},
issue_date = {Apr 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {96},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.11.008},
doi = {10.1016/j.infsof.2017.11.008},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {94–111},
numpages = {18},
keywords = {Software defect prediction, Stacked denoising autoencoders, Ensemble learning, Software metrics, Deep learning}
}

@article{10.1504/ijista.2019.102667,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {Statistical assessment of nonlinear manifold detection-based software defect prediction techniques},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {6},
issn = {1740-8865},
url = {https://doi.org/10.1504/ijista.2019.102667},
doi = {10.1504/ijista.2019.102667},
abstract = {Prediction of software defects has immense importance for obtaining desired outcome at minimised cost and so attracted researchers working on this topic applying various techniques, which were not found fully effective. Software datasets comprise of redundant features that hinder effective application of techniques resulting inappropriate defect prediction. Hence, it requires newer application of nonlinear manifold detection techniques (nonlinear MDTs) that has been examined for accurate prediction of defects at lesser time and cost using different classification techniques. In this work, we analysed and tested the effect of nonlinear MDTs to find out accurate and best classification technique for all datasets. Comparison has been made between the results of without or with nonlinear MDTs and paired two-tailed T-test has been performed for statistical testing and verifying the performance of classifiers using nonlinear MDTs on all datasets. Outcome revealed that among all nonlinear MDTs, FastMVU makes most accurate prediction of software defects.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {579–605},
numpages = {26},
keywords = {dimensionality reduction, fast maximum variance unfolding, FastMVU, machine learning, manifold detection, nonlinear, promise repository, software defect prediction}
}

@inproceedings{10.1145/3449365.3449384,
author = {Malhotra, Ruchika and Budhiraja, Anmol and Kumar Singh, Abhinav and Ghoshal, Ishani},
title = {A Novel Feature Selection Approach based on Binary Particle Swarm Optimization and Ensemble Learning for Heterogeneous Defect Prediction},
year = {2021},
isbn = {9781450388108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449365.3449384},
doi = {10.1145/3449365.3449384},
abstract = {Software defect prediction is an integral part of the software development process. Defect prediction helps focus on the grey areas beforehand, thus saving the considerable amount of money that is otherwise wasted in finding and fixing the faults once the software is already in production. One of the popular areas of defect prediction in recent years is Heterogeneous Defect Prediction, which predicts defects in a target project using a source project with different metrics. Through our paper, we provide a novel feature selection based approach, En-BPSO, based on binary particle swarm optimization, coupled with majority voting ensemble classifier based fitness function for heterogeneous defect prediction. The datasets we are using are MORPH and SOFTLAB. The results show that the En-BPSO method provides the highest Friedman mean rank amongst all the feature selection methods used for comparison. En-BPSO technique also helps us dynamically determine the optimal number of features to build an accurate heterogeneous defect prediction model.},
booktitle = {Proceedings of the 2021 3rd Asia Pacific Information Technology Conference},
pages = {115–121},
numpages = {7},
keywords = {Binary Particle Swarm Optimization, Defect Prediction, Ensemble Learning, Feature Selection, Heterogeneous Metrics},
location = {Bangkok, Thailand},
series = {APIT '21}
}

@article{10.1007/s10515-020-00277-4,
author = {Esteves, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Understanding machine learning software defect predictions},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3–4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00277-4},
doi = {10.1007/s10515-020-00277-4},
abstract = {Software defects are well-known in software development and might cause several problems for users and developers aside. As a result, researches employed distinct techniques to mitigate the impacts of these defects in the source code. One of the most notable techniques focuses on defect prediction using machine learning methods, which could support developers in handling these defects before they are introduced in the production environment. These studies provide alternative approaches to predict the likelihood of defects. However, most of these works concentrate on predicting defects from a vast set of software features. Another key issue with the current literature is the lack of a satisfactory explanation of the reasons that drive the software to a defective state. Specifically, we use a tree boosting algorithm (XGBoost) that receives as input a training set comprising records of easy-to-compute characteristics of each module and outputs whether the corresponding module is defect-prone. To exploit the link between predictive power and model explainability, we propose a simple model sampling approach that finds accurate models with the minimum set of features. Our principal idea is that features not contributing to increasing the predictive power should not be included in the model. Interestingly, the reduced set of features helps to increase model explainability, which is important to provide information to developers on features related to each module of the code which is more defect-prone. We evaluate our models on diverse projects within Jureczko datasets, and we show that (i) features that contribute most for finding best models may vary depending on the project and (ii) it is possible to find effective models that use few features leading to better understandability. We believe our results are useful to developers as we provide the specific software features that influence the defectiveness of selected projects.},
journal = {Automated Software Engg.},
month = dec,
pages = {369–392},
numpages = {24},
keywords = {Software defects, Explainable models, Jureczko datasets, SHAP values}
}

@inproceedings{10.1109/ISISE.2012.114,
author = {Wang, Pei and Jin, Cong and Jin, Shu-Wei},
title = {Software Defect Prediction Scheme Based on Feature Selection},
year = {2012},
isbn = {9780769549514},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISISE.2012.114},
doi = {10.1109/ISISE.2012.114},
abstract = {Predicting defect-prone software modules accurately and effectively are important ways to control the quality of a software system during software development. Feature selection can highly improve the accuracy and efficiency of the software defect prediction model. The main purpose of this paper is to discuss the best size of feature subset for building a prediction model and prove that feature selection method is useful for establishing software defect prediction model. Mutual information is an outstanding indicator of relevance between variables, and it has been used as a measurement in our feature selection algorithm. We also introduce a nonlinear factor to our evaluation function for feature selection to improve its performance. The results of our feature selection algorithm are validated by different machine learning methods. The experiment results show that all the classifiers achieve higher accuracy by using the feature subset provided by our algorithm.},
booktitle = {Proceedings of the 2012 Fourth International Symposium on Information Science and Engineering},
pages = {477–480},
numpages = {4},
keywords = {feature selection, mutual information, software defect prediction},
series = {ISISE '12}
}

@inproceedings{10.1145/2896387.2900324,
author = {Rahman, Md. Habibur and Sharmin, Sadia and Sarwar, Sheikh Muhammad and Shoyaib, Mohammad},
title = {Software Defect Prediction Using Feature Space Transformation},
year = {2016},
isbn = {9781450340632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896387.2900324},
doi = {10.1145/2896387.2900324},
abstract = {In software quality estimation research, software defect prediction is a key topic. A defect prediction model is generally constructed using a variety of software attributes and each attribute may have positive, negative or neutral effect on a specific model. Selection of an optimal set of attributes for model development remains a vital yet unexplored issue. In this paper, we have introduced a new feature space transformation process with a normalization technique to improve the defect prediction accuracy. We proposed a feature space transformation technique and classify the instances using Support Vector Machine (SVM) with its histogram intersection kernel. The proposed method is evaluated using the data sets from NASA metric data repository and its application demonstrates acceptable accuracy.},
booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
articleno = {72},
numpages = {6},
keywords = {Attribute selection, Feature space transformation, Software defect prediction},
location = {Cambridge, United Kingdom},
series = {ICC '16}
}

@article{10.1016/j.neucom.2018.04.090,
author = {Malhotra, Ruchika and Kamal, Shine},
title = {An empirical study to investigate oversampling methods for improving software defect prediction using imbalanced data},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {343},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2018.04.090},
doi = {10.1016/j.neucom.2018.04.090},
journal = {Neurocomput.},
month = may,
pages = {120–140},
numpages = {21},
keywords = {Defect prediction, Imbalanced data, Oversampling methods, MetaCost learners, Machine learning techniques, Procedural metrics}
}

@article{10.5555/2684939.2684969,
author = {Ma, Ying and Pan, Weiwei and Zhu, Shunzhi and Yin, Huayi and Luo, Jian},
title = {An improved semi-supervised learning method for software defect prediction},
year = {2014},
issue_date = {September 2014},
publisher = {IOS Press},
address = {NLD},
volume = {27},
number = {5},
issn = {1064-1246},
abstract = {This paper presents an improved semi-supervised learning approach for defect prediction involving class imbalanced and limited labeled data problem. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. It makes the defect predictor more practical for real applications, by combating these problems. In comparison with conventional machine learning approaches, our method has significant superior performance. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning.},
journal = {J. Intell. Fuzzy Syst.},
month = sep,
pages = {2473–2480},
numpages = {8},
keywords = {Class Imbalance, Co-Train, Defect Prediction, Random Sampling, Semi-Supervised Learning}
}

@inproceedings{10.1145/3239576.3239622,
author = {Yang, Zhao and Qian, Hongbing},
title = {Automated Parameter Tuning of Artificial Neural Networks for Software Defect Prediction},
year = {2018},
isbn = {9781450364607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239576.3239622},
doi = {10.1145/3239576.3239622},
abstract = {Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies.},
booktitle = {Proceedings of the 2nd International Conference on Advances in Image Processing},
pages = {203–209},
numpages = {7},
keywords = {Artificial Neural Networks, Automated Parameter Tuning, Metrics, Software defect prediction},
location = {Chengdu, China},
series = {ICAIP '18}
}

@article{10.1016/j.procs.2018.05.012,
author = {Ghosh, Soumi and Rana, Ajay and Kansal, Vineet},
title = {A Nonlinear Manifold Detection based Model for Software Defect Prediction},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.012},
doi = {10.1016/j.procs.2018.05.012},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {581–594},
numpages = {14},
keywords = {Feature Selection techniques, Friedman test, Nonlinear Manifold Detection techniques, Paired two-tailed T-test, Software Defect Prediction}
}

@inproceedings{10.1007/978-3-030-87007-2_14,
author = {Peng\H{o}, Edit},
title = {Examining the Bug Prediction Capabilities of Primitive Obsession Metrics},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_14},
doi = {10.1007/978-3-030-87007-2_14},
abstract = {Bug prediction is an approach that helps make bug detection more automated during software development. Based on a bug dataset a prediction model is built to locate future bugs. Bug datasets contain information about previous defects in the code, process metrics, or source code metrics, etc. As code smells can indicate potential flaws in the source code, they can be used for bug prediction as well.In our previous work, we introduced several source code metrics to detect and describe the occurrence of Primitive Obsession in Java. This paper is a further study on three of the Primitive Obsession metrics. We integrated them into an existing, source code metrics-based bug dataset, and studied the effectiveness of the prediction built upon it. We performed a 10 fold cross-validation on the whole dataset and a cross-project validation as well. We compared the new models with the results of the original dataset. While the cross-validation showed no significant change, in the case of the cross-project validation, we have found that the amount of improvement exceeded the amount of deterioration by 5%. Furthermore, the variance added to the dataset was confirmed by correlation and PCA calculations.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {185–200},
numpages = {16},
keywords = {Bug prediction, Code smells, Primitive obsession, Static analysis, Refactoring},
location = {Cagliari, Italy}
}

@article{10.1007/s42979-020-0119-4,
author = {Khuat, Thanh Tung and Le, My Hanh},
title = {Evaluation of Sampling-Based Ensembles of Classifiers on Imbalanced Data for Software Defect Prediction Problems},
year = {2020},
issue_date = {Mar 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {2},
url = {https://doi.org/10.1007/s42979-020-0119-4},
doi = {10.1007/s42979-020-0119-4},
abstract = {Defect prediction in software projects plays a crucial role to reduce quality-based risk and increase the capability of detecting faulty program modules. Hence, classification approaches to anticipate software defect proneness based on static code characteristics have become a hot topic with a great deal of attention in recent years. While several novel studies show that the use of a single classifier causes the performance bottleneck, ensembles of classifiers might effectively enhance classification performance compared to a single classifier. However, the class imbalance property of software defect data severely hinders the classification efficiency of ensemble learning. To cope with this problem, resampling methods are usually combined into ensemble models.
This paper empirically assesses the importance of sampling with regard to ensembles of various classifiers on imbalanced data in software defect prediction problems. Extensive experiments with the combination of seven different kinds of classification algorithms, three sampling methods, and two balanced data learning schemata were conducted over ten datasets. Empirical results indicated the positive effects of combining sampling techniques and the ensemble learning model on the performance of defect prediction regarding datasets with imbalanced class distributions.},
journal = {SN Comput. Sci.},
month = mar,
numpages = {16},
keywords = {Software defect prediction, Random undersampling, Random oversampling, SMOTE, Data balancing, Ensemble learning, Imbalanced data}
}

@inproceedings{10.5555/3432601.3432619,
author = {Jahanshahi, Hadi and Cevik, Mucahit and Ba\c{s}ar, Ay\c{s}e},
title = {Moving from cross-project defect prediction to heterogeneous defect prediction: a partial replication study},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Software defect prediction heavily relies on the metrics collected from software projects. Earlier studies often used machine learning techniques to build, validate, and improve bug prediction models using either a set of metrics collected within a project or across different projects. However, techniques applied and conclusions derived by those models are restricted by how identical those metrics are. Knowledge coming from those models will not be extensible to a target project if no sufficient overlapping metrics have been collected in the source projects. To explore the feasibility of transferring knowledge across projects without common labeled metrics, we systematically integrated Heterogeneous Defect Prediction (HDP) by replicating and validating the obtained results. Our main goal is to extend prior research and explore the feasibility of HDP and finally to compare its performance with that of its predecessor, Cross-Project Defect Prediction. We construct an HDP model on different publicly available datasets. Moreover, we propose a new ensemble voting approach in the HDP context to utilize the predictive power of multiple available datasets. The result of our experiment is comparable to that of the original study. However, we also explored the feasibility of HDP in real cases. Our results shed light on the infeasibility of many cases for the HDP algorithm due to its sensitivity to the parameter selection. In general, our analysis gives a deep insight into why and how to perform transfer learning from one domain to another, and in particular, provides a set of guidelines to help researchers and practitioners to disseminate knowledge to the defect prediction domain.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {133–142},
numpages = {10},
keywords = {defect prediction, heterogeneous metrics, software quality, transfer learning},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@article{10.1016/j.ins.2018.02.027,
author = {Miholca, Diana-Lucia and Czibula, Gabriela and Czibula, Istvan Gergely},
title = {A novel approach for software defect prediction through hybridizing gradual relational association rules with artificial neural networks},
year = {2018},
issue_date = {May 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {441},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2018.02.027},
doi = {10.1016/j.ins.2018.02.027},
abstract = {The growing complexity of software projects requires increasing consideration of their analysis and testing. Identifying defective software entities is essential for software quality assurance and it also improves activities related to software testing. In this study, we developed a novel supervised classification method called HyGRAR for software defect prediction. HyGRAR is a non-linear hybrid model that combines gradual relational association rule mining and artificial neural networks to discriminate between defective and non-defective software entities. Experiments performed based on 10 open-source data sets demonstrated the excellent performance of the HYGRAR classifier. HyGRAR performed better than most of the previously proposed approaches for software defect prediction in performance evaluations using the same data sets.},
journal = {Inf. Sci.},
month = may,
pages = {152–170},
numpages = {19},
keywords = {Artificial neural network, Gradual relational association rule, Machine learning, Software defect prediction}
}

@article{10.1016/j.procs.2015.02.161,
author = {Arora, Ishani and Tetarwal, Vivek and Saha, Anju},
title = {Open Issues in Software Defect Prediction},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2015.02.161},
doi = {10.1016/j.procs.2015.02.161},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {906–912},
numpages = {7},
keywords = {data mining, defect prediction, machine learning, software quality, software testing}
}

@article{10.1007/s11227-018-2326-5,
author = {Kalsoom, Anum and Maqsood, Muazzam and Ghazanfar, Mustansar Ali and Aadil, Farhan and Rho, Seungmin},
title = {A dimensionality reduction-based efficient software fault prediction using Fisher linear discriminant analysis (FLDA)},
year = {2018},
issue_date = {Sep 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {74},
number = {9},
issn = {0920-8542},
url = {https://doi.org/10.1007/s11227-018-2326-5},
doi = {10.1007/s11227-018-2326-5},
abstract = {Software quality is an important factor in the success of software companies. Traditional software quality assurance techniques face some serious limitations especially in terms of time and budget. This leads to increase in the use of machine learning classification techniques to predict software faults. Software fault prediction can help developers to uncover software problems in early stages of software life cycle. The extent to which these techniques can be generalized to different sizes of software, class imbalance problem, and identification of discriminative software metrics are the most critical challenges. In this paper, we have analyzed the performance of nine widely used machine learning classifiers--Bayes Net, NB, artificial neural network, support vector machines, K nearest neighbors, AdaBoost, Bagging, Zero R, and Random Forest for software fault prediction. Two standard sampling techniques--SMOTE and Resample with substitution are used to handle the class imbalance problem. We further used FLDA-based feature selection approach in combination with SMOTE and Resample to select most discriminative metrics. Then the top four classifiers based on performance are used for software fault prediction. The experimentation is carried out over 15 publically available datasets (small, medium and large) which are collected from PROMISE repository. The proposed Resample-FLDA method gives better performance as compared to existing methods in terms of precision, recall, f-measure and area under the curve.},
journal = {J. Supercomput.},
month = sep,
pages = {4568–4602},
numpages = {35},
keywords = {Fault-tolerance, Fisher linear discriminant, Reliability, Robustness, Software fault prediction}
}

@article{10.1007/s11390-019-1958-0,
author = {Chen, Xiang and Zhang, Dun and Cui, Zhan-Qi and Gu, Qing and Ju, Xiao-Lin},
title = {DP-Share: Privacy-Preserving Software Defect Prediction Model Sharing Through Differential Privacy},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1958-0},
doi = {10.1007/s11390-019-1958-0},
abstract = {In current software defect prediction (SDP) research, most previous empirical studies only use datasets provided by PROMISE repository and this may cause a threat to the external validity of previous empirical results. Instead of SDP dataset sharing, SDP model sharing is a potential solution to alleviate this problem and can encourage researchers in the research community and practitioners in the industrial community to share more models. However, directly sharing models may result in privacy disclosure, such as model inversion attack. To the best of our knowledge, we are the first to apply differential privacy (DP) to privacy-preserving SDP model sharing and then propose a novel method DP-Share, since DP mechanisms can prevent this attack when the privacy budget is carefully selected. In particular, DP-Share first performs data preprocessing for the dataset, such as over-sampling for minority instances (i.e., defective modules) and conducting discretization for continuous features to optimize privacy budget allocation. Then, it uses a novel sampling strategy to create a set of training sets. Finally it constructs decision trees based on these training sets and these decision trees can form a random forest (i.e., model). The last phase of DP-Share uses Laplace and exponential mechanisms to satisfy the requirements of DP. In our empirical studies, we choose nine experimental subjects from real software projects. Then, we use AUC (area under ROC curve) as the performance measure and holdout as our model validation technique. After privacy and utility analysis, we find that DP-Share can achieve better performance than a baseline method DF-Enhance in most cases when using the same privacy budget. Moreover, we also provide guidelines to effectively use our proposed method. Our work attempts to fill the research gap in terms of differential privacy for SDP, which can encourage researchers and practitioners to share more SDP models and then effectively advance the state of the art of SDP.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1020–1038},
numpages = {19},
keywords = {software defect prediction, model sharing, differential privacy, cross project defect prediction, empirical study}
}

@article{10.1016/j.infsof.2019.07.003,
author = {Zhou, Tianchi and Sun, Xiaobing and Xia, Xin and Li, Bin and Chen, Xiang},
title = {Improving defect prediction with deep forest},
year = {2019},
issue_date = {Oct 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {114},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.003},
doi = {10.1016/j.infsof.2019.07.003},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {204–216},
numpages = {13},
keywords = {Software defect prediction, Deep forest, Cascade strategy, Empirical evaluation}
}

@inproceedings{10.1145/3028842.3028859,
author = {Gao, Yan and Yang, Chunhui},
title = {Software defect prediction based on manifold learning in subspace selection},
year = {2016},
isbn = {9781450347990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3028842.3028859},
doi = {10.1145/3028842.3028859},
abstract = {Software defects will lead to software running error and system crashes. In order to detect software defect as early as possible at early stage of software development, a series of machine learning approaches have been studied and applied to predict defects in software modules. Unfortunately, the imbalanceof software defect datasets brings great challenge to software defect prediction model training. In this paper, a new manifold learning based subspace learning algorithm, Discriminative Locality Alignment(DLA), is introduced into software defects prediction. Experimental results demonstrate that DLA is consistently superior to LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) in terms of discriminate information extraction and prediction performance. In addition, DLA reveals some attractive intrinsic properties for numeric calculation, e.g. it can overcome the matrix singular problem and small sample size problem in software defect prediction.},
booktitle = {Proceedings of the 1st International Conference on Intelligent Information Processing},
articleno = {17},
numpages = {6},
keywords = {discriminative locality alignment, manifold learning, software defect prediction, support vector machine},
location = {Wuhan, China},
series = {ICIIP '16}
}

@article{10.3233/JIFS-18473,
author = {Malhotra, Ruchika and Sharma, Anjali},
title = {Empirical assessment of feature selection techniques in defect prediction models using web applications},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {36},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-18473},
doi = {10.3233/JIFS-18473},
abstract = {&nbsp;In order to minimize the over-fitting and related factors that are caused by the high dimensionality of the input data in software defect prediction, the attributes are often optimized using various feature selection techniques. However, the comparative performance of these selection techniques in combination with machine learning algorithms remains largely unexplored using web applications. In this work, we investigate the best possible combination of feature selection technique with machine learning algorithms, with the sample space chosen from open source Apache Click and Rave data sets. Our results are based on 945 defect prediction models derived from parametric, non-parametric and ensemble-based machine learning algorithms, for which the metrics are derived from the various filter and threshold-based ranking techniques. Friedman and Nemenyi post-hoc statistical tests are adopted to identify the performance difference of these models. We find that filter-based feature selection in combination with ensemble-based machine learning algorithms not only poise as the best strategy but also yields a maximum feature set redundancy by 94%, with little or no comprise on the performance index.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6567–6578},
numpages = {12},
keywords = {Feature selection, feature ranking, machine learning, web application quality}
}

@inproceedings{10.1145/3382025.3414960,
author = {Str\"{u}der, Stefan and Mukelabai, Mukelabai and Str\"{u}ber, Daniel and Berger, Thorsten},
title = {Feature-oriented defect prediction},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414960},
doi = {10.1145/3382025.3414960},
abstract = {Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {21},
numpages = {12},
keywords = {classification, defect, feature, prediction},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@article{10.1016/j.eswa.2019.113156,
author = {Majd, Amirabbas and Vahidi-Asl, Mojtaba and Khalilian, Alireza and Poorsarvi-Tehrani, Pooria and Haghighi, Hassan},
title = {SLDeep: Statement-level software defect prediction using deep-learning model on static code features},
year = {2020},
issue_date = {Jun 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {147},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113156},
doi = {10.1016/j.eswa.2019.113156},
journal = {Expert Syst. Appl.},
month = jun,
numpages = {14},
keywords = {Defect, Software fault proneness, Machine learning, Fault prediction model, Software metric}
}

@article{10.4018/IJOSSP.2018010101,
author = {Kakkar, Misha and Jain, Sarika and Bansal, Abhay and Grover, P.S.},
title = {Combining Data Preprocessing Methods With Imputation Techniques for Software Defect Prediction},
year = {2018},
issue_date = {January 2018},
publisher = {IGI Global},
address = {USA},
volume = {9},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2018010101},
doi = {10.4018/IJOSSP.2018010101},
abstract = {Software Defect Prediction SDP models are used to predict, whether software is clean or buggy using the historical data collected from various software repositories. The data collected from such repositories may contain some missing values. In order to estimate missing values, imputation techniques are used, which utilizes the complete observed values in the dataset. The objective of this study is to identify the best-suited imputation technique for handling missing values in SDP dataset. In addition to identifying the imputation technique, the authors have investigated for the most appropriate combination of imputation technique and data preprocessing method for building SDP model. In this study, four combinations of imputation technique and data preprocessing methods are examined using the improved NASA datasets. These combinations are used along with five different machine-learning algorithms to develop models. The performance of these SDP models are then compared using traditional performance indicators. Experiment results show that among different imputation techniques, linear regression gives the most accurate imputed value. The combination of linear regression with correlation based feature selector outperforms all other combinations. To validate the significance of data preprocessing methods with imputation the findings are applied to open source projects. It was concluded that the result is in consistency with the above conclusion.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {1–19},
numpages = {19},
keywords = {Feature Selection, Instance Selection, Missing Value Imputation, Software Defect Prediction}
}

@article{10.1016/j.knosys.2015.09.035,
author = {Li, Weiwei and Huang, Zhiqiu and Li, Qing},
title = {Three-way decisions based software defect prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {91},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.09.035},
doi = {10.1016/j.knosys.2015.09.035},
abstract = {Based on a two-stage classification method and a two-stage ranking method on three-way decisions, this paper introduces a three-way decisions framework for cost-sensitive software defect prediction. For the classification problem in software defect prediction, traditional two-way decisions methods usually generate a higher classification error and more decision cost. Here, a two-stage classification method that integrates three-way decisions and ensemble learning to predict software defect is proposed. Experimental results on NASA data sets show that our method can obtain a higher accuracy and a lower decision cost. For the ranking problem in software defect prediction, a two-stage ranking method is introduced. In the first stage, all software modules are classified into three different regions based on three-way decisions. A dominance relation rough set based ranking algorithm is next applied to rank the modules in each region. Comparison experiments with 6 other ranking methods present that our proposed method can obtain a better result on FPA measure.},
journal = {Know.-Based Syst.},
month = jan,
pages = {263–274},
numpages = {12},
keywords = {Software defect classification, Software defect ranking, Three-way decisions}
}

@inproceedings{10.1007/978-3-030-58811-3_62,
author = {Kalouptsoglou, Ilias and Siavvas, Miltiadis and Tsoukalas, Dimitrios and Kehagias, Dionysios},
title = {Cross-Project Vulnerability Prediction Based on Software Metrics and Deep Learning},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_62},
doi = {10.1007/978-3-030-58811-3_62},
abstract = {Vulnerability prediction constitutes a mechanism that enables the identification and mitigation of software vulnerabilities early enough in the development cycle, improving the security of software products, which is an important quality attribute according to ISO/IEC 25010. Although existing vulnerability prediction models have demonstrated sufficient accuracy in predicting the occurrence of vulnerabilities in the software projects with which they have been trained, they have failed to demonstrate sufficient accuracy in cross-project prediction. To this end, in the present paper we investigate whether the adoption of deep learning along with software metrics may lead to more accurate cross-project vulnerability prediction. For this purpose, several machine learning (including deep learning) models are constructed, evaluated, and compared based on a dataset of popular real-world PHP software applications. Feature selection is also applied with the purpose to examine whether it has an impact on cross-project prediction. The results of our analysis indicate that the adoption of software metrics and deep learning may result in vulnerability prediction models with sufficient performance in cross-project vulnerability prediction. Another interesting conclusion is that the performance of the models in cross-project prediction is enhanced when the projects exhibit similar characteristics with respect to their software metrics.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {877–893},
numpages = {17},
keywords = {Software quality, Security, Vulnerability prediction},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-030-87007-2_19,
author = {Tummalapalli, Sahithi and Mittal, Juhi and Kumar, Lov and Murthy Neti, Lalitha Bhanu and Rath, Santanu Kumar},
title = {An Empirical Analysis on the Prediction of Web Service Anti-patterns Using Source Code Metrics and Ensemble Techniques},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_19},
doi = {10.1007/978-3-030-87007-2_19},
abstract = {Today’s software program enterprise uses web services to construct distributed software systems based on the Service Oriented Architecture (SOA) paradigm. The web service description is posted by a web service provider, which may be observed and invoked by a distributed application. Service-Based Systems (SBS) need to conform themselves through years to fit within the new user necessities. These may result in the deterioration of the quality and design of the software systems and might reason the materialization of insufficient solutions called Anti-patterns. Anti-pattern detection using object-oriented source code metrics may be used as part of the software program improvement life cycle to lessen the maintenance of the software
 system and enhance the quality of the software. The work is motivated by developing an automatic predictive model for predicting web services anti-patterns using static evaluations of the source code metrics. The center ideology of this work is to empirically investigate the effectiveness of different variants of data sampling technique, Synthetic Minority Over Sampling TEchnique (SMOTE), and the ensemble learning techniques in the prediction of web service anti-patterns.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {263–276},
numpages = {14},
keywords = {Anti-pattern, WSDL, Ensemble techniques, Code quality},
location = {Cagliari, Italy}
}

@inproceedings{10.4108/icst.bict.2014.257871,
author = {Malhotra, Ruchika and Raje, Rajeev},
title = {An empirical comparison of machine learning techniques for software defect prediction},
year = {2014},
isbn = {9781631900532},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
address = {Brussels, BEL},
url = {https://doi.org/10.4108/icst.bict.2014.257871},
doi = {10.4108/icst.bict.2014.257871},
abstract = {Software systems are exposed to various types of defects. The timely identification of defective classes is essential in early phases of software development to reduce the cost of testing the software. This will guide the software practitioners and researchers for planning of the proper allocation of testing resources. Software metrics can be used in conjunction with defect data to develop models for predicting defective classes. There have been various machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems in less time. However, more studies comparing these techniques are needed to provide evidence so that confidence is established on the performance of one technique over the other. In this paper we address four issues (i) comparison of the machine learning techniques over unpopular used data sets (ii) use of inappropriate performance measures for measuring the performance of defect prediction models (iii) less use of statistical tests and (iv) validation of models from the same data set from which they are trained. To resolve these issues, in this paper, we compare 18 machine learning techniques for investigating the effect of Object-Oriented metrics on defective classes. The results are validated on six releases of the 'MMS' application package of recent widely used mobile operating system -- Android. The overall results of the study indicate the predictive capability of the machine learning techniques and an endorsement of one particular ML technique to predict defects.},
booktitle = {Proceedings of the 8th International Conference on Bioinspired Information and Communications Technologies},
pages = {320–327},
numpages = {8},
keywords = {defect prediction, empirical validation, machine learning, object-oriented metrics},
location = {Boston, Massachusetts},
series = {BICT '14}
}

@article{10.1016/j.scico.2021.102713,
author = {Jain, Shivani and Saha, Anju},
title = {Improving performance with hybrid feature selection and ensemble machine learning techniques for code smell detection},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {212},
number = {C},
issn = {0167-6423},
url = {https://doi.org/10.1016/j.scico.2021.102713},
doi = {10.1016/j.scico.2021.102713},
journal = {Sci. Comput. Program.},
month = dec,
numpages = {34},
keywords = {Code smell, Machine learning, Ensemble machine learning, Hybrid feature selection, Stacking}
}

@inproceedings{10.1145/2723742.2723754,
author = {Muthukumaran, K. and Rallapalli, Akhila and Murthy, N. L. Bhanu},
title = {Impact of Feature Selection Techniques on Bug Prediction Models},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723754},
doi = {10.1145/2723742.2723754},
abstract = {Several change metrics and source code metrics have been introduced and proved to be effective features in building bug prediction models. Researchers performed comparative studies of bug prediction models built using the individual metrics as well as combination of these metrics. In this paper, we investigate whether the prediction accuracy of bug prediction models is improved by applying feature selection techniques. We explore if there is one algorithm amongst ten popular feature selection algorithms that consistently fares better than others across sixteen bench marked open source projects. We also study whether the metrics in best feature subset are consistent across projects.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {120–129},
numpages = {10},
keywords = {Bug prediction, Feature selection, Software Quality},
location = {Bangalore, India},
series = {ISEC '15}
}

@article{10.1155/2019/2384706,
author = {Yang, Xingguang and Yu, Huiqun and Fan, Guisheng and Shi, Kai and Chen, Liqiong and Tramontana, Emiliano},
title = {Local versus Global Models for Just-In-Time Software Defect Prediction},
year = {2019},
issue_date = {2019},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2019},
issn = {1058-9244},
url = {https://doi.org/10.1155/2019/2384706},
doi = {10.1155/2019/2384706},
abstract = {Just-in-time software defect prediction (JIT-SDP) is an active topic in software defect prediction, which aims to identify defect-inducing changes. Recently, some studies have found that the variability of defect data sets can affect the performance of defect predictors. By using local models, it can help improve the performance of prediction models. However, previous studies have focused on module-level defect prediction. Whether local models are still valid in the context of JIT-SDP is an important issue. To this end, we compare the performance of local and global models through a large-scale empirical study based on six open-source projects with 227417 changes. The experiment considers three evaluation scenarios of cross-validation, cross-project-validation, and timewise-cross-validation. To build local models, the experiment uses the k-medoids to divide the training set into several homogeneous regions. In addition, logistic regression and effort-aware linear regression (EALR) are used to build classification models and effort-aware prediction models, respectively. The empirical results show that local models perform worse than global models in the classification performance. However, local models have significantly better effort-aware prediction performance than global models in the cross-validation and cross-project-validation scenarios. Particularly, when the number of clusters k is set to 2, local models can obtain optimal effort-aware prediction performance. Therefore, local models are promising for effort-aware JIT-SDP.},
journal = {Sci. Program.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3383219.3383281,
author = {Khan, Bilal and Iqbal, Danish and Badshah, Sher},
title = {Cross-Project Software Fault Prediction Using Data Leveraging Technique to Improve Software Quality},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383281},
doi = {10.1145/3383219.3383281},
abstract = {Software fault prediction is a process to detect bugs in software projects. Fault prediction in software engineering has attracted much attention from the last decade. The early prognostication of faults in software minimize the cost and effort of errors that come at later stages. Different machine learning techniques have been utilized for fault prediction, that is proven to be utilizable. Despite, the significance of fault prediction most of the companies do not consider fault prediction in practice and do not build useful models due to lack of data or lack of enough data to strengthen the power of fault predictors. However, models trained and tested on less amount of data are difficult to generalize, because they do not consider project size, project differences, and features selection. To overcome these issues, we proposed an instance-based transfer learning through data leveraging using logistic linear regression as a base proposed statistical methodology. In our study, we considered three software projects within the same domain. Finally, we performed a comparative analysis of three different experiments for building models (targeted project). The experimental results of the proposed approach show promising improvements in (SFP).},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {434–438},
numpages = {5},
keywords = {Cross-project, Instance-based learning, Machine learning, Software Quality, Software fault prediction, data leveraging},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/2351676.2351734,
author = {Lu, Huihua and Cukic, Bojan and Culp, Mark},
title = {Software defect prediction using semi-supervised learning with dimension reduction},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351734},
doi = {10.1145/2351676.2351734},
abstract = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {314–317},
numpages = {4},
keywords = {Software fault prediction, dimension reduction, semi-supervised learning, software metrics},
location = {Essen, Germany},
series = {ASE '12}
}

@article{10.1007/s11219-021-09568-9,
author = {Ulan, Maria and L\"{o}we, Welf and Ericsson, Morgan and Wingkvist, Anna},
title = {Copula-based software metrics aggregation},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09568-9},
doi = {10.1007/s11219-021-09568-9},
abstract = {A quality model is a conceptual decomposition of an abstract notion of quality into relevant, possibly conflicting characteristics and further into measurable metrics. For quality assessment and decision making, metrics values are aggregated to characteristics and ultimately to quality scores. Aggregation has often been problematic as quality models do not provide the semantics of aggregation. This makes it hard to formally reason about metrics, characteristics, and quality. We argue that aggregation needs to be interpretable and mathematically well defined in order to assess, to compare, and to improve quality. To address this challenge, we propose a probabilistic approach to aggregation and define quality scores based on joint distributions of absolute metrics values. To evaluate the proposed approach and its implementation under realistic conditions, we conduct empirical studies on bug prediction of ca. 5000 software classes, maintainability of ca. 15000 open-source software systems, and on the information quality of ca. 100000 real-world technical documents. We found that our approach is feasible, accurate, and scalable in performance.},
journal = {Software Quality Journal},
month = dec,
pages = {863–899},
numpages = {37},
keywords = {Quality assessment, Quantitative methods, Software metrics, Aggregation, Multivariate statistical methods, Probabilistic models, Copula}
}

@article{10.1007/s10796-013-9430-0,
author = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri and Wald, Randall},
title = {A comparative study of iterative and non-iterative feature selection techniques for software defect prediction},
year = {2014},
issue_date = {November  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {5},
issn = {1387-3326},
url = {https://doi.org/10.1007/s10796-013-9430-0},
doi = {10.1007/s10796-013-9430-0},
abstract = {Two important problems which can affect the performance of classification models are high-dimensionality (an overabundance of independent features in the dataset) and imbalanced data (a skewed class distribution which creates at least one class with many fewer instances than other classes). To resolve these problems concurrently, we propose an iterative feature selection approach, which repeated applies data sampling (in order to address class imbalance) followed by feature selection (in order to address high-dimensionality), and finally we perform an aggregation step which combines the ranked feature lists from the separate iterations of sampling. This approach is designed to find a ranked feature list which is particularly effective on the more balanced dataset resulting from sampling while minimizing the risk of losing data through the sampling step and missing important features. To demonstrate this technique, we employ 18 different feature selection algorithms and Random Undersampling with two post-sampling class distributions. We also investigate the use of sampling and feature selection without the iterative step (e.g., using the ranked list from a single iteration, rather than combining the lists from multiple iterations), and compare these results from the version which uses iteration. Our study is carried out using three groups of datasets with different levels of class balance, all of which were collected from a real-world software system. All of our experiments use four different learners and one feature subset size. We find that our proposed iterative feature selection approach outperforms the non-iterative approach.},
journal = {Information Systems Frontiers},
month = nov,
pages = {801–822},
numpages = {22},
keywords = {Class imbalance, Date sampling, High dimensionality, Iterative feature selection, Software defect prediction}
}

@article{10.1007/s11390-020-0323-7,
author = {Mhawish, Mohammad Y. and Gupta, Manjari},
title = {Predicting Code Smells and Analysis of Predictions: Using Machine Learning Techniques and Software Metrics},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0323-7},
doi = {10.1007/s11390-020-0323-7},
abstract = {Code smell detection is essential to improve software quality, enhancing software maintainability, and decrease the risk of faults and failures in the software system. In this paper, we proposed a code smell prediction approach based on machine learning techniques and software metrics. The local interpretable model-agnostic explanations (LIME) algorithm was further used to explain the machine learning model’s predictions and interpretability. The datasets obtained from Fontana et al. were reformed and used to build binary-label and multi-label datasets. The results of 10-fold cross-validation show that the performance of tree-based algorithms (mainly Random Forest) is higher compared with kernel-based and network-based algorithms. The genetic algorithm based feature selection methods enhance the accuracy of these machine learning algorithms by selecting the most relevant features in each dataset. Moreover, the parameter optimization techniques based on the grid search algorithm significantly enhance the accuracy of all these algorithms. Finally, machine learning techniques have high potential in predicting the code smells, which contribute to detect these smells and enhance the software’s quality.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1428–1445},
numpages = {18},
keywords = {code smell, code smell detection, feature selection, prediction explanation, parameter optimization}
}

@inproceedings{10.1007/978-3-030-78609-0_28,
author = {Sun, Ying and Sun, Yanfei and Wu, Fei and Jing, Xiao-Yuan},
title = {Deep Adversarial Learning Based Heterogeneous Defect Prediction},
year = {2021},
isbn = {978-3-030-78608-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78609-0_28},
doi = {10.1007/978-3-030-78609-0_28},
abstract = {Cross-project defect prediction (CPDP) is a hot study that predicts defects in the new project by utilizing the model trained on the data from other projects. However, existing CPDP methods usually assume that source and target projects have the same metrics. Heterogeneous defect prediction (HDP) is proposed and has attracted increasing attention, which refers to the metric sets from source and target projects are different in CPDP. HDP conducts prediction model using the instances with heterogeneous metrics from external projects and then use this model to predict defect-prone software instances in source project. However, building HDP methods is challenging including the distribution difference between source and target projects with heterogeneous metrics. In this paper, we propose a Deep adversarial learning based HDP (DHDP) approach. DHDP leverages deep neural network to learn nonlinear transformation for each project to obtain common feature represent, which the heterogeneous data from different projects can be compared directly. DHDP consists of two parts: a discriminator and a classifier that compete with each other. A classifier tries to minimize the similarity across classes and maximize the inter-class similarity. A discriminator tries to distinguish the source of instances that is source or target project on the common feature space. Expensive experiments are performed on 10 public projects from two datasets in terms of F-measure and G-measure. The experimental results show that DHDP gains superior prediction performance improvement compared to a range of competing methods.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part I},
pages = {326–337},
numpages = {12},
keywords = {Adversarial learning, Metric learning, Heterogeneous defect prediction},
location = {Dublin, Ireland}
}

@article{10.1016/j.asoc.2017.05.043,
author = {Arar, mer Faruk and Ayan, Krat},
title = {A feature dependent Naive Bayes approach and its application to the software defect prediction problem},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {59},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2017.05.043},
doi = {10.1016/j.asoc.2017.05.043},
abstract = {Display Omitted In Naive Bayes, features are assumed to be independent and have equal weight. But, In practice, features are interrelated.In this study, features are included for calculation as pairs using the proposed Feature Dependent Naive Bayes (FDNB) method.Eight data sets from the NASA PROMISE repository were used for the software defect prediction problem.Results were compared with other modified NBs. Increased classification performance was found after use of the proposed FDNB. Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes (FDNB) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized NASA PROMISE data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {197–209},
numpages = {13},
keywords = {Data mining, Discretization, Feature independence, Naive Bayes, Software defect prediction}
}

@article{10.1016/j.jss.2019.110402,
author = {Xu, Zhou and Li, Shuai and Xu, Jun and Liu, Jin and Luo, Xiapu and Zhang, Yifeng and Zhang, Tao and Keung, Jacky and Tang, Yutian},
title = {LDFR: Learning deep feature representation for software defect prediction},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110402},
doi = {10.1016/j.jss.2019.110402},
journal = {J. Syst. Softw.},
month = dec,
numpages = {20},
keywords = {Software defect prediction, Deep feature representation, Triplet loss, Weighted cross-entropy loss, Deep neural network, 00-01, 99-00}
}

@inproceedings{10.1109/ISSRE.2014.35,
author = {Lu, Huihua and Kocaguneli, Ekrem and Cukic, Bojan},
title = {Defect Prediction between Software Versions with Active Learning and Dimensionality Reduction},
year = {2014},
isbn = {9781479960330},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISSRE.2014.35},
doi = {10.1109/ISSRE.2014.35},
abstract = {Accurate detection of defects prior to product release helps software engineers focus verification activities on defect prone modules, thus improving the effectiveness of software development. A common scenario is to use the defects from prior releases to build the prediction model for the upcoming release, typically through a supervised learning method. As software development is a dynamic process, fault characteristics in subsequent releases may vary. Therefore, supplementing the defect information from prior releases with limited information about the defects from the current release detected early seems to offer intuitive and practical benefits. We propose active learning as a way to automate the development of models which improve the performance of defect prediction between successive releases. Our results show that the integration of active learning with uncertainty sampling consistently outperforms the corresponding supervised learning approach. We further improve the prediction performance with feature compression techniques, where feature selection or dimensionality reduction is applied to defect data prior to active learning. We observe that dimensionality reduction techniques, particularly multidimensional scaling with random forest similarity, work better than feature selection due to their ability to identify and combine essential information in data set features. We present the improvements offered by this methodology through the prediction of defective modules in the three successive versions of Eclipse.},
booktitle = {Proceedings of the 2014 IEEE 25th International Symposium on Software Reliability Engineering},
pages = {312–322},
numpages = {11},
keywords = {Active learning, Complexity measures, Dimensionality reduction, Machine learning, Software defect prediction},
series = {ISSRE '14}
}

@inproceedings{10.1145/3385032.3385042,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy, N. L. Bhanu},
title = {Prediction of Web Service Anti-patterns Using Aggregate Software Metrics and Machine Learning Techniques},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385042},
doi = {10.1145/3385032.3385042},
abstract = {Service-Oriented Architecture(SOA) can be characterized as an approximately coupled engineering intended to meet the business needs of an association/organization. Service-Based Systems (SBSs) are inclined to continually change to enjoy new client necessities and adjust the execution settings, similar to some other huge and complex frameworks. These changes may lead to the evolution of designs/products with poor Quality of Service (QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns. Anti-patterns makes the evolution and maintenance of the software systems hard and complex. Early identification of modules, classes, or source code regions where anti-patterns are more likely to occur can help in amending and maneuvering testing efforts leading to the improvement of software quality. In this work, we investigate the application of three sampling techniques, three feature selection techniques, and sixteen different classification techniques to develop the models for web service anti-pattern detection. We report the results of an empirical study by evaluating the approach proposed, on a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of five types of web-service anti-patterns. Experimental results demonstrated that SMOTE is the best performing data sampling techniques. The experimental results also reveal that the model developed by considering Uncorrelated Significant Predictors(SUCP) as the input obtained better performance compared to the model developed by other metrics. Experimental results also show that the Least Square Support Vector Machine with Linear(LSLIN) function has outperformed all other classifier techniques.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {8},
numpages = {11},
keywords = {Aggregation measures, Anti-pattern, Class imbalance distribution, Classifiers, Feature Selection, Machine Learning, Service-Based Systems(SBS), Source Code Metrics, WSDL, Web-Services},
location = {Jabalpur, India},
series = {ISEC '20}
}

@article{10.1007/s10664-012-9218-8,
author = {Okutan, Ahmet and Y\i{}ld\i{}z, Olcay Taner},
title = {Software defect prediction using Bayesian networks},
year = {2014},
issue_date = {February  2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9218-8},
doi = {10.1007/s10664-012-9218-8},
abstract = {There are lots of different software metrics discovered and used for defect prediction in the literature. Instead of dealing with so many metrics, it would be practical and easy if we could determine the set of metrics that are most important and focus on them more to predict defectiveness. We use Bayesian networks to determine the probabilistic influential relationships among software metrics and defect proneness. In addition to the metrics used in Promise data repository, we define two more metrics, i.e. NOD for the number of developers and LOCQ for the source code quality. We extract these metrics by inspecting the source code repositories of the selected Promise data repository data sets. At the end of our modeling, we learn the marginal defect proneness probability of the whole software system, the set of most effective metrics, and the influential relationships among metrics and defectiveness. Our experiments on nine open source Promise data repository data sets show that response for class (RFC), lines of code (LOC), and lack of coding quality (LOCQ) are the most effective metrics whereas coupling between objects (CBO), weighted method per class (WMC), and lack of cohesion of methods (LCOM) are less effective metrics on defect proneness. Furthermore, number of children (NOC) and depth of inheritance tree (DIT) have very limited effect and are untrustworthy. On the other hand, based on the experiments on Poi, Tomcat, and Xalan data sets, we observe that there is a positive correlation between the number of developers (NOD) and the level of defectiveness. However, further investigation involving a greater number of projects is needed to confirm our findings.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {154–181},
numpages = {28},
keywords = {Bayesian networks, Defect prediction}
}

@article{10.1016/j.asoc.2020.106686,
author = {Haouari, Ahmed Taha and Souici-Meslati, Labiba and Atil, Fadila and Meslati, Djamel},
title = {Empirical comparison and evaluation of Artificial Immune Systems in inter-release software fault prediction},
year = {2020},
issue_date = {Nov 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {96},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2020.106686},
doi = {10.1016/j.asoc.2020.106686},
journal = {Appl. Soft Comput.},
month = nov,
numpages = {18},
keywords = {Artificial Immune Systems, Software defect prediction, Inter-projects fault prediction, Artificial Immune Recognition System, Neural Network}
}

@inproceedings{10.1145/3368089.3417062,
author = {Suh, Alexander},
title = {Adapting bug prediction models to predict reverted commits at Wayfair},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417062},
doi = {10.1145/3368089.3417062},
abstract = {Researchers have proposed many algorithms to predict software bugs. Given a software entity (e.g., a file or method), these algorithms predict whether the entity is bug-prone. However, since these algorithms cannot identify specific bugs, this does not tend to be particularly useful in practice. In this work, we adapt this prior work to the related problem of predicting whether a commit is likely to be reverted. Given the batch nature of continuous integration deployment at scale, this allows developers to find time-sensitive bugs in production more quickly. The models in this paper are based on features extracted from the revision history of a codebase that are typically used in bug prediction. Our experiments, performed on the three main repositories for the Wayfair website, show that our models can rank reverted commits above 80% of non-reverted commits on average. Moreover, when given to Wayfair developers, our models reduce the amount of time needed to find certain kinds of bugs by 55%. Wayfair continues to use our findings and models today to help find bugs during software deployments.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1251–1262},
numpages = {12},
keywords = {reverted commits, software defect prediction, software deployment},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@article{10.1016/j.neunet.2019.05.022,
author = {Zhao, Haitao and Lai, Zhihui and Chen, Yudong},
title = {Global-and-local-structure-based neural network for fault detection},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {118},
number = {C},
issn = {0893-6080},
url = {https://doi.org/10.1016/j.neunet.2019.05.022},
doi = {10.1016/j.neunet.2019.05.022},
journal = {Neural Netw.},
month = oct,
pages = {43–53},
numpages = {11},
keywords = {Statistical process monitoring, Fault detection, Feedforward neural network, Principal component analysis, Dimension reduction}
}

@article{10.1016/j.jss.2019.110493,
author = {Pascarella, Luca and Palomba, Fabio and Bacchelli, Alberto},
title = {On the performance of method-level bug prediction: A negative result},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {161},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110493},
doi = {10.1016/j.jss.2019.110493},
journal = {J. Syst. Softw.},
month = mar,
numpages = {15},
keywords = {Defect prediction, Empirical software engineering, Mining software repositories}
}

@inproceedings{10.1145/3178212.3178221,
author = {Rizwan, Syed and Tiantian, Wang and Xiaohong, Su and Salahuddin},
title = {Empirical Study on Software Bug Prediction},
year = {2017},
isbn = {9781450354882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178212.3178221},
doi = {10.1145/3178212.3178221},
abstract = {Software defect prediction is a vital research direction in software engineering field. Software defect prediction predicts whether software errors are present in the software by using machine learning analysis on software metrics. It can help software developers to improve the quality of the software. Software defect prediction is usually a binary classification problem, which relies on software metrics and the use of classifiers. There have been many research efforts to improve accuracy in software defect prediction using a variety of classifiers and data preprocessing techniques. However, the "classic classifier validity" and "data preprocessing techniques can enhance the functionality of software defect prediction" has not yet been answered explicitly. Therefore, it is necessary to conduct an empirical analysis to compare these studies. In software defect prediction, the category of interest is a defective module, and the number of defective modules is much less than that of a non-defective module in data. This leads to a category of imbalance problem that reduces the accuracy of the prediction. Therefore, the problem of imbalance is a key problem that needs to be solved in software defect prediction. In this paper, we proposed an experimental model and used the NASA MDP data set to analyze the software defect prediction. Five research questions were defined and analyzed experimentally. In addition to experimental analysis, this paper focuses on the improvement of SMOTE. SMOTE ASMO algorithm has been proposed to overcome the shortcomings of SMOTE.},
booktitle = {Proceedings of the 2017 International Conference on Software and E-Business},
pages = {55–59},
numpages = {5},
keywords = {Classification, Data preprocessing, Defect prediction, SMOTE},
location = {Hong Kong, Hong Kong},
series = {ICSEB '17}
}

@article{10.1016/j.ins.2013.12.031,
author = {Czibula, Gabriela and Marian, Zsuzsanna and Czibula, Istvan Gergely},
title = {Software defect prediction using relational association rule mining},
year = {2014},
issue_date = {April, 2014},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {264},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.12.031},
doi = {10.1016/j.ins.2013.12.031},
abstract = {This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal.},
journal = {Inf. Sci.},
month = apr,
pages = {260–278},
numpages = {19},
keywords = {Association rule, Data mining, Defect prediction, Software engineering}
}

@inproceedings{10.1145/3197231.3198447,
author = {Catolino, Gemma},
title = {Does source code quality reflect the ratings of apps?},
year = {2018},
isbn = {9781450357128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197231.3198447},
doi = {10.1145/3197231.3198447},
abstract = {In the past, bad code quality has been associated with higher bugproneness. At the same time, the main reason why mobile users negatively rate an app is due to the presence of bugs leading to crashes. In this paper, we preliminarily investigate the extent to which code quality metrics can be exploited to predict the commercial success of mobile apps. Key results suggest the existence of a relation between code quality and commercial success; We found that inheritance and information hiding metrics represent important indicators and therefore should be carefully monitored by developers.},
booktitle = {Proceedings of the 5th International Conference on Mobile Software Engineering and Systems},
pages = {43–44},
numpages = {2},
keywords = {metrics, mobile applications, software quality},
location = {Gothenburg, Sweden},
series = {MOBILESoft '18}
}

@article{10.1007/s11219-021-09553-2,
author = {Wu, Jie and Wu , Yingbo and Niu, Nan and Zhou, Min},
title = {MHCPDP: multi-source heterogeneous cross-project defect prediction via multi-source transfer learning and autoencoder},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09553-2},
doi = {10.1007/s11219-021-09553-2},
abstract = {Heterogeneous cross-project defect prediction (HCPDP) is aimed at building a defect prediction model for the target project by reusing datasets from source projects, where the source project datasets and target project dataset have different features. Most existing HCPDP methods only remove redundant or unrelated features without exploring the underlying features of cross-project datasets. Additionally, when the&nbsp;transfer learning method is used in HCPDP, these methods ignore the negative effect of transfer learning. In this paper, we propose a novel HCPDP method called multi-source heterogeneous cross-project defect prediction (MHCPDP). To reduce the gap between the target datasets and the source datasets, MHCPDP uses the autoencoder to extract the intermediate features from the original datasets instead of simply removing redundant and unrelated features and adopts a modified autoencoder algorithm to make instance selection for eliminating irrelevant instances from the source domain datasets. Furthermore, by incorporating multiple source projects to increase the number of source datasets, MHCPDP develops a multi-source transfer learning algorithm to reduce the impact of negative transfers and upgrade the performance of the classifier. We comprehensively evaluate MHCPDP on five open source datasets; our experimental results show that MHCPDP not only has significant improvement in two performance metrics but also overcomes the shortcomings of the conventional HCPDP methods.},
journal = {Software Quality Journal},
month = jun,
pages = {405–430},
numpages = {26},
keywords = {Autoencoder, Heterogeneous cross-project defect prediction, Multi-source transfer learning, Modified autoencoder}
}

@article{10.1007/s10664-018-9679-5,
author = {Kondo, Masanari and Bezemer, Cor-Paul and Kamei, Yasutaka and Hassan, Ahmed E. and Mizuno, Osamu},
title = {The impact of feature reduction techniques on defect prediction models},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9679-5},
doi = {10.1007/s10664-018-9679-5},
abstract = {Defect prediction is an important task for preserving software quality. Most prior work on defect prediction uses software features, such as the number of lines of code, to predict whether a file or commit will be defective in the future. There are several reasons to keep the number of features that are used in a defect prediction model small. For example, using a small number of features avoids the problem of multicollinearity and the so-called `curse of dimensionality'. Feature selection and reduction techniques can help to reduce the number of features in a model. Feature selection techniques reduce the number of features in a model by selecting the most important ones, while feature reduction techniques reduce the number of features by creating new, combined features from the original features. Several recent studies have investigated the impact of feature selection techniques on defect prediction. However, there do not exist large-scale studies in which the impact of multiple feature reduction techniques on defect prediction is investigated. In this paper, we study the impact of eight feature reduction techniques on the performance and the variance in performance of five supervised learning and five unsupervised defect prediction models. In addition, we compare the impact of the studied feature reduction techniques with the impact of the two best-performing feature selection techniques (according to prior work). The following findings are the highlights of our study: (1) The studied correlation and consistency-based feature selection techniques result in the best-performing supervised defect prediction models, while feature reduction techniques using neural network-based techniques (restricted Boltzmann machine and autoencoder) result in the best-performing unsupervised defect prediction models. In both cases, the defect prediction models that use the selected/generated features perform better than those that use the original features (in terms of AUC and performance variance). (2) Neural network-based feature reduction techniques generate features that have a small variance across both supervised and unsupervised defect prediction models. Hence, we recommend that practitioners who do not wish to choose a best-performing defect prediction model for their data use a neural network-based feature reduction technique.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1925–1963},
numpages = {39},
keywords = {Defect prediction, Feature reduction, Feature selection, Neural network, Restricted Boltzmann machine}
}

@article{10.1007/s00521-021-05811-3,
author = {Mehta, Sweta and Patnaik, K. Sridhar},
title = {Improved prediction of software defects using ensemble machine learning techniques},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {16},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05811-3},
doi = {10.1007/s00521-021-05811-3},
abstract = {Software testing process is a crucial part in software development. Generally the errors made by developers get fixed at a later stage of the software development process. This increases the impact of the defect. To prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. Defect prediction process involves classification of software modules into defect prone and non-defect prone. This paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. In this research work, various software metrics are evaluated using feature selection techniques such as Recursive Feature Elimination (RFE), Correlation-based feature selection, Lasso, Ridge, ElasticNet and Boruta. Logistic Regression, Decision Trees, K-nearest neighbor, Support Vector Machines and Ensemble Learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. The proposed model uses combination of Partial Least Square (PLS) Regression and RFE for dimension reduction which is further combined with Synthetic Minority Oversampling Technique due to the imbalanced nature of the used datasets. It has been observed that XGBoost and Stacking Ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work.},
journal = {Neural Comput. Appl.},
month = aug,
pages = {10551–10562},
numpages = {12},
keywords = {Defect prediction, Dimension reduction, Data imbalance, Machine learning algorithms, XGBoost, Stacking ensemble classifier}
}

@article{10.1007/s00521-021-06158-5,
author = {Nevendra, Meetesh and Singh, Pradeep},
title = {Defect count prediction via metric-based convolutional neural network},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {22},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-06158-5},
doi = {10.1007/s00521-021-06158-5},
abstract = {With the increasing complexity and volume of the software, the number of defects in software modules is also increasing consistently, which affects the quality and delivery of software in time and budget. To improve the software quality and timely allocation of resources, defects should be detected at the initial phases of the software development life cycle. However, the existing defect prediction methodology based on high-dimensional and limited data only focuses on predicting defective modules. In contrast, the number of defects present in the software module has not been explored so far, especially using deep neural network. Also, whether deep learning could enhance the performance of defect count prediction is still uninvestigated. To fill this gap, we proposed an improved Convolutional Neural Network model, called metrics-based convolutional neural network (MB-CNN), which combines the advantages of appropriate metrics and an improved CNN method by introducing dropout for regularization between convolutions and dense layer. The proposed method predicts the presented defect count in the software module for homogeneous scenarios as within-version and cross-version. The experimental results show that, on average, across the fourteen real-world defect datasets, the proposed approach improves Li’s CNN architecture by 31% in within-version prediction and 28% in cross-version prediction. Moreover, the Friedman ranking test and Wilcoxon nonparametric test reveal the usefulness of our proposed approach over ten benchmark learning algorithms to predict defect count.},
journal = {Neural Comput. Appl.},
month = nov,
pages = {15319–15344},
numpages = {26},
keywords = {Software defect count prediction, Deep learning, CNN, Cross-project}
}

@article{10.1007/s10462-017-9563-5,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A study on software fault prediction techniques},
year = {2019},
issue_date = {February  2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-017-9563-5},
doi = {10.1007/s10462-017-9563-5},
abstract = {Software fault prediction aims to identify fault-prone software modules by using some underlying properties of the software project before the actual testing process begins. It helps in obtaining desired software quality with optimized cost and effort. Initially, this paper provides an overview of the software fault prediction process. Next, different dimensions of software fault prediction process are explored and discussed. This review aims to help with the understanding of various elements associated with fault prediction process and to explore various issues involved in the software fault prediction. We search through various digital libraries and identify all the relevant papers published since 1993. The review of these papers are grouped into three classes: software metrics, fault prediction techniques, and data quality issues. For each of the class, taxonomical classification of different techniques and our observations have also been presented. The review and summarization in the tabular form are also given. At the end of the paper, the statistical analysis, observations, challenges, and future directions of software fault prediction have been discussed.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {255–327},
numpages = {73},
keywords = {Fault prediction techniques, Software fault datasets, Software fault prediction, Software metrics, Taxonomic classification}
}

@article{10.1016/j.asoc.2016.08.002,
author = {ztrk, Muhammed Maruf and Zengin, Ahmet},
title = {How repeated data points affect bug prediction performance},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.002},
doi = {10.1016/j.asoc.2016.08.002},
abstract = {Graphical abstractDisplay Omitted HighlightsPresents a novel pre-processing algorithm for defect data sets.Discusses the effects of the use of low level metrics in bug prediction.Compares repeated data points industrial versus open-source projects.Provides tips to obtain better bug prediction results. In defect prediction studies, open-source and real-world defect data sets are frequently used. The quality of these data sets is one of the main factors affecting the validity of defect prediction methods. One of the issues is repeated data points in defect prediction data sets. The main goal of the paper is to explore how low-level metrics are derived. This paper also presents a cleansing algorithm that removes repeated data points from defect data sets. The method was applied on 20 data sets, including five open source sets, and area under the curve (AUC) and precision performance parameters have been improved by 4.05% and 6.7%, respectively. In addition, this work discusses how static code metrics should be used in bug prediction. The study provides tips to obtain better defect prediction results.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1051–1061},
numpages = {11},
keywords = {Bug prediction, Repeated data, Software metrics}
}

@article{10.1016/j.procs.2018.05.115,
author = {Singh, Ajmer and Bhatia, Rajesh and Singhrova, Anita},
title = {Taxonomy of machine learning algorithms in software fault prediction using object oriented metrics},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.115},
doi = {10.1016/j.procs.2018.05.115},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {993–1001},
numpages = {9},
keywords = {Software fault prediction, Object Oriented Testing, Object Oriented Coupling, software faults prediction, machine learning}
}

@inproceedings{10.1145/2875913.2875944,
author = {Qing, He and Biwen, Li and Beijun, Shen and Xia, Yong},
title = {Cross-Project Software Defect Prediction Using Feature-Based Transfer Learning},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875944},
doi = {10.1145/2875913.2875944},
abstract = {Cross-project defect prediction is taken as an effective means of predicting software defects when the data shortage exists in the early phase of software development. Unfortunately, the precision of cross-project defect prediction is usually poor, largely because of the differences between the reference and the target projects. Having realized the project differences, this paper proposes CPDP, a feature-based transfer learning approach to cross-project defect prediction. The core insight of CPDP is to (1) filter and transfer highly-correlated data based on data samples in the target projects, and (2) evaluate and choose learning schemas for transferring data sets. Models are then built for predicting defects in the target projects. We have also conducted an evaluation of the proposed approach on PROMISE datasets. The evaluation results show that, the proposed approach adapts to cross-project defect prediction in that f-measure of 81.8% of projects can get improved, and AUC of 54.5% projects improved. It also achieves similar f-measure and AUC as some inner-project defect prediction approaches.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {74–82},
numpages = {9},
keywords = {cross-project defect prediction, feature-based transfer, transfer learning},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1016/j.csi.2017.02.003,
title = {An empirical analysis of the effectiveness of software metrics and fault prediction model for identifying faulty classes},
year = {2017},
issue_date = {August 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {53},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2017.02.003},
doi = {10.1016/j.csi.2017.02.003},
abstract = {Software fault prediction models are used to predict faulty modules at the very early stage of software development life cycle. Predicting fault proneness using source code metrics is an area that has attracted several researchers' attention. The performance of a model to assess fault proneness depends on the source code metrics which are considered as the input for the model. In this work, we have proposed a framework to validate the source code metrics and identify a suitable set of source code metrics with the aim to reduce irrelevant features and improve the performance of the fault prediction model. Initially, we applied a t-test analysis and univariate logistic regression analysis to each source code metric to evaluate their potential for predicting fault proneness. Next, we performed a correlation analysis and multivariate linear regression stepwise forward selection to find the right set of source code metrics for fault prediction. The obtained set of source code metrics are considered as the input to develop a fault prediction model using a neural network with five different training algorithms and three different ensemble methods. The effectiveness of the developed fault prediction models are evaluated using a proposed cost evaluation framework. We performed experiments on fifty six Open Source Java projects. The experimental results reveal that the model developed by considering the selected set of source code metrics using the suggested source code metrics validation framework as the input achieves better results compared to all other metrics. The experimental results also demonstrate that the fault prediction model is best suitable for projects with faulty classes less than the threshold value depending on fault identification efficiency (low 48.89%, median- 39.26%, and high 27.86%). HighlightsFault prediction improve the effectiveness of software quality assurance activities.This paper focus on building an effective fault prediction tool.Fault prediction model using ANN and ensemble methods.We perform experiments on 56 Open Source Java projects.Fault prediction model is best suitable for projects with faulty classes less than the threshold value.},
journal = {Comput. Stand. Interfaces},
month = aug,
pages = {1–32},
numpages = {32}
}

@inproceedings{10.1109/ISCID.2013.199,
author = {Xia, Ye and Yan, Guoying and Si, Qianran},
title = {A Study on the Significance of Software Metrics in Defect Prediction},
year = {2013},
isbn = {9780769550794},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISCID.2013.199},
doi = {10.1109/ISCID.2013.199},
abstract = {In the case of metrics-based software defect prediction, an intelligent selection of metrics plays an important role in improving the model performance. In this paper, we use different ways for feature selection and dimensionality reduction to determine the most important software metrics. Three different classifiers are utilized, namely Na\"{\i}ve Bayes, support vector machine and decision tree. On the publicly NASA data, a comparative experiment results show that instead of 22 or more metrics, less than 10 metrics can get better performance.},
booktitle = {Proceedings of the 2013 Sixth International Symposium on Computational Intelligence and Design - Volume 02},
pages = {343–346},
numpages = {4},
keywords = {classifier, defect prediction, feature selection, software metric},
series = {ISCID '13}
}

@article{10.1007/s00500-021-05994-w,
author = {Guo, Junjun and Wang, Zhengyuan and Li, Haonan and Xue, Yang},
title = {Detecting vulnerability in source code using CNN and LSTM network},
year = {2021},
issue_date = {Jan 2023},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {27},
number = {2},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-05994-w},
doi = {10.1007/s00500-021-05994-w},
abstract = {Automated vulnerability detection has become a research hot spot because it is beneficial for improving software quality and security. The code metric (CM) is one class of important representations of vulnerability in source code. The implicit relationships among different metric attributes have not been sufficiently considered in traditional vulnerability detection based on CMs. In this paper, in view of the local perception capability of convolutional neural network (CNN) and the time-series prediction capability of long short-term memory (LSTM), we propose VulExplore, a compound neural network model for vulnerability detection that consists of a CNN for feature extraction and an LSTM network for deep representation. Moreover, to further indicate the vulnerability features in the source code, we reconstruct a CM dataset that includes two additional important attributes: maintainability index and average number of vulnerabilities committed per line. Our proposed numerical method can obtain both false-negative rate (FNR) and false-positive rate (FPR) under 20% and, meanwhile, achieve recall and precision over 80%, respectively.},
journal = {Soft Comput.},
month = jul,
pages = {1131–1141},
numpages = {11},
keywords = {Vulnerability detection, Code metrics (CMs), Convolutional neural network (CNN), Long short-term memory (LSTM), Source code}
}

@inproceedings{10.1145/3196398.3196431,
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
title = {Deep learning similarities from different representations of source code},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196431},
doi = {10.1145/3196398.3196431},
abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering (SE) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what SE researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning (DL) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how SE tasks can benefit from a DL-based approach, which can automatically learn code similarities from different representations.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {542–553},
numpages = {12},
keywords = {code similarities, deep learning, neural networks},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3318299.3318345,
author = {Li, ZhanJun and Shao, Yan},
title = {A Survey of Feature Selection for Vulnerability Prediction Using Feature-based Machine Learning},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318345},
doi = {10.1145/3318299.3318345},
abstract = {This paper summarized the basic process of software vulnerability prediction using feature-based machine learning for the first time. In addition to sorting out the related types and basis of vulnerability features definition, the advantages and disadvantages of different methods are compared. Finally, this paper analyzed the difficulties and challenges in this research field, and put forward some suggestions for future work.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {36–42},
numpages = {7},
keywords = {Software vulnerability prediction, feature, machine learning},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@article{10.4018/IJOSSP.2017010102,
author = {Alsukhni, Emad and Saifan, Ahmad A. and Alawneh, Hanadi},
title = {A New Data Mining-Based Framework to Test Case Prioritization Using Software Defect Prediction},
year = {2017},
issue_date = {January 2017},
publisher = {IGI Global},
address = {USA},
volume = {8},
number = {1},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2017010102},
doi = {10.4018/IJOSSP.2017010102},
abstract = {Test cases do not have the same importance when used to detect faults in software; therefore, it is more efficient to test the system with the test cases that have the ability to detect the faults. This research proposes a new framework that combines data mining techniques to prioritize the test cases. It enhances fault prediction and detection using two different techniques: 1 the data mining regression classifier that depends on software metrics to predict defective modules, and 2 the k-means clustering technique that is used to select and prioritize test cases to identify the fault early. Our approach of test case prioritization yields good results in comparison with other studies. The authors used the Average Percentage of Faults Detection APFD metric to evaluate the proposed framework, which results in 19.9% for all system modules and 25.7% for defective ones. Our results give us an indication that it is effective to start the testing process with the most defective modules instead of testing all modules arbitrary arbitrarily.},
journal = {Int. J. Open Source Softw. Process.},
month = jan,
pages = {21–41},
numpages = {21},
keywords = {Data Mining, Software Defect Prediction, Software Testing, Test Case Prioritization}
}

@article{10.1145/3467895,
author = {Falessi, Davide and Ahluwalia, Aalok and Penta, Massimiliano DI},
title = {The Impact of Dormant Defects on Defect Prediction: A Study of 19 Apache Projects},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3467895},
doi = {10.1145/3467895},
abstract = {Defect prediction models can be beneficial to prioritize testing, analysis, or code review activities, and has been the subject of a substantial effort in academia, and some applications in industrial contexts. A necessary precondition when creating a defect prediction model is the availability of defect data from the history of projects. If this data is noisy, the resulting defect prediction model could result to be unreliable. One of the causes of noise for defect datasets is the presence of “dormant defects,” i.e., of defects discovered several releases after their introduction. This can cause a class to be labeled as defect-free while it is not, and is, therefore “snoring.” In this article, we investigate the impact of snoring on classifiers' accuracy and the effectiveness of a possible countermeasure, i.e., dropping too recent data from a training set. We analyze the accuracy of 15 machine learning defect prediction classifiers, on data from more than 4,000 defects and 600 releases of 19 open source projects from the Apache ecosystem. Our results show that on average across projects (i) the presence of dormant defects decreases the recall of defect prediction classifiers, and (ii) removing from the training set the classes that in the last release are labeled as not defective significantly improves the accuracy of the classifiers. In summary, this article provides insights on how to create defects datasets by mitigating the negative effect of dormant defects on defect prediction.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {4},
numpages = {26},
keywords = {Defect prediction, fix-inducing changes, dataset bias}
}

@inproceedings{10.1145/3472883.3486995,
author = {Suneja, Sahil and Zheng, Yunhui and Zhuang, Yufan and Laredo, Jim A. and Morari, Alessandro},
title = {Towards Reliable AI for Source Code Understanding},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486995},
doi = {10.1145/3472883.3486995},
abstract = {Cloud maturity and popularity have resulted in Open source software (OSS) proliferation. And, in turn, managing OSS code quality has become critical in ensuring sustainable Cloud growth. On this front, AI modeling has gained popularity in source code understanding tasks, promoted by the ready availability of large open codebases. However, we have been observing certain peculiarities with these black-boxes, motivating a call for their reliability to be verified before offsetting traditional code analysis. In this work, we highlight and organize different reliability issues affecting AI-for-code into three stages of an AI pipeline- data collection, model training, and prediction analysis. We highlight the need for concerted efforts from the research community to ensure credibility, accountability, and traceability for AI-for-code. For each stage, we discuss unique opportunities afforded by the source code and software engineering setting to improve AI reliability.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {403–411},
numpages = {9},
keywords = {explainability, machine learning, reliability, signal awareness},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@article{10.1016/j.compeleceng.2021.107370,
author = {Zheng, Shang and Gai, Jinjing and Yu, Hualong and Zou, Haitao and Gao, Shang},
title = {Training data selection for imbalanced cross-project defect prediction},
year = {2021},
issue_date = {Sep 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {94},
number = {C},
issn = {0045-7906},
url = {https://doi.org/10.1016/j.compeleceng.2021.107370},
doi = {10.1016/j.compeleceng.2021.107370},
journal = {Comput. Electr. Eng.},
month = sep,
numpages = {11},
keywords = {Cross-project software prediction, Data selection, Jensen-Shannon divergence, Relative density}
}

@article{10.1016/j.infsof.2011.09.007,
author = {Ma, Ying and Luo, Guangchun and Zeng, Xue and Chen, Aiguo},
title = {Transfer learning for cross-company software defect prediction},
year = {2012},
issue_date = {March, 2012},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {54},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2011.09.007},
doi = {10.1016/j.infsof.2011.09.007},
abstract = {Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {248–256},
numpages = {9},
keywords = {Different distribution, Machine learning, Naive Bayes, Software defect prediction, Transfer learning}
}

@article{10.1007/s10489-020-02141-0,
author = {Gan, Min and Zhang, Li},
title = {Iteratively local fisher score for feature selection},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02141-0},
doi = {10.1007/s10489-020-02141-0},
abstract = {In machine learning, feature selection is a kind of important dimension reduction techniques, which aims to choose features with the best discriminant ability to avoid the issue of curse of dimensionality for subsequent processing. As a supervised feature selection method, Fisher score (FS) provides a feature evaluation criterion and has been widely used. However, FS ignores the association between features by assessing all features independently and loses the local information for fully connecting within-class samples. In order to solve these issues, this paper proposes a novel feature evaluation criterion based on FS, named iteratively local Fisher score (ILFS). Compared with FS, the new criterion pays more attention to the local structure of data by using K nearest neighbours instead of all samples when calculating the scatters of within-class and between-class. In order to consider the relationship between features, we calculate local Fisher scores of feature subsets instead of scores of single features, and iteratively select the current optimal feature to achieve this idea like sequential forward selection (SFS). Experimental results on UCI and TEP data sets show that the improved algorithm performs well in classification activities compared with some other state-of-the-art methods.},
journal = {Applied Intelligence},
month = aug,
pages = {6167–6181},
numpages = {15},
keywords = {Feature selection, Fisher score, Neighbourhood, Iterative}
}

@article{10.1016/j.procs.2018.05.071,
author = {Pandey, Sushant Kumar and Mishra, Ravi Bhushan and Triphathi, Anil Kumar},
title = {Software Bug Prediction Prototype Using Bayesian Network Classifier: A Comprehensive Model},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.071},
doi = {10.1016/j.procs.2018.05.071},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {1412–1421},
numpages = {10},
keywords = {Bayesian network, bug prediction, classification techniques}
}

@article{10.3233/KES-200029,
author = {Singh, Pradeep and Verma, Shrish},
title = {ACO based comprehensive model for software fault prediction},
year = {2020},
issue_date = {2020},
publisher = {IOS Press},
address = {NLD},
volume = {24},
number = {1},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-200029},
doi = {10.3233/KES-200029},
abstract = {The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization (ACO) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates ACO based mining approach with ROC based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on ACO based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of ACO based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the ACO based learner outperforms other benchmark techniques.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {63–71},
numpages = {9},
keywords = {Software metric, fault prediction, ACO}
}

@inproceedings{10.1145/3273934.3273938,
author = {Amasaki, Sousuke},
title = {Cross-Version Defect Prediction using Cross-Project Defect Prediction Approaches: Does it work?},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273938},
doi = {10.1145/3273934.3273938},
abstract = {Background: Specifying and removing defects before release deserve extra cost for the success of software projects. Long-running projects experience multiple releases, and it is a natural choice to adopt cross-version defect prediction (CVDP) that uses information from older versions. A past study shows that feeding multi older versions data may have a positive influence on the performance. The study also suggests that cross-project defect prediction (CPDP) may fit the situation but one CPDP approach was only examined.Aims: To investigate whether feeding multiple older versions data is effective for CVDP using CPDP approaches. The investigation also involves performance comparisons of the CPDP approaches under CVDP situation. Method: We chose a style of replication of the comparative study on CPDP approaches by Herbold et al. under CVDP situation. Results: Feeding multiple older versions had a positive effect for more than a half CPDP approaches. However, almost all of the CPDP approaches did not perform significantly better than a simple rule-based prediction. Although the best CPDP approach could work better than it and with-in project defect prediction, we found no effect of feeding multiple older versions for it. Conclusions: Feeding multiple older versions could improve CPDP approaches under CVDP situation. However, it did not work for the best CPDP approach in the study.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {32–41},
numpages = {10},
keywords = {Comparative Study, Cross-Project Defect Prediction, Cross-Version Defect Prediction},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3439961.3439979,
author = {Santos, Geanderson and Figueiredo, Eduardo and Veloso, Adriano and Viggiato, Markos and Ziviani, Nivio},
title = {Predicting Software Defects with Explainable Machine Learning},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439979},
doi = {10.1145/3439961.3439979},
abstract = {Most software systems must evolve to cope with stakeholders’ requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-to-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {18},
numpages = {10},
keywords = {NASA datasets, SHAP values, explainable models, software defects},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1007/978-3-030-78612-0_5,
author = {Xu, Haitao and Duan, Ruifeng and Yang, Shengsong and Guo, Lei},
title = {An Empirical Study on Data Sampling for Just-in-Time Defect Prediction},
year = {2021},
isbn = {978-3-030-78611-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-78612-0_5},
doi = {10.1007/978-3-030-78612-0_5},
abstract = {In this paper, the impact of Data Sampling on Just-in-Time defect prediction is explored. We find that there is a significant negative relationship between the class imbalance ratio of the dataset and the performance of the instant software defect prediction model. Secondly although most software defect data are not as unbalanced as expected, a moderate degree of imbalance is sufficient to affect the performance of traditional learning. This means that if the training data for immediate software defects show moderate or more severe imbalances, one need not expect good defect prediction performance and the data sampling approach to balancing the training data can improve the performance of the model. Finally, the empirical approach shows that although the under-sampling method slightly improves model performance, the different sampling methods do not have a substantial impact on the evaluation of immediate software defect prediction models.},
booktitle = {Artificial Intelligence and Security: 7th International Conference, ICAIS 2021, Dublin, Ireland, July 19–23, 2021, Proceedings, Part II},
pages = {54–69},
numpages = {16},
keywords = {Data sampling, Just-in-time defect, Empirical study},
location = {Dublin, Ireland}
}

@article{10.1016/j.asoc.2016.04.032,
author = {Malhotra, Ruchika},
title = {An empirical framework for defect prediction using machine learning techniques with Android software},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.04.032},
doi = {10.1016/j.asoc.2016.04.032},
abstract = {Display Omitted Use of appropriate and large number data sets for comparing 18 ML techniques for defect prediction using object-oriented metrics.Effective performance of the predicted models assessed using appropriate performance measures.Reliability of the results evaluated using statistical test and post-hoc analysis.Validating the predicted models using inter-release validation on various releases of seven application packages of Android software. ContextSoftware defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. ObjectiveThe main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. MethodIn this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. ResultsThe results show that the area under the curve measure for Nave Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. ConclusionThe results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1034–1050},
numpages = {17},
keywords = {Inter-release validation, Machine-learning, Object-oriented metrics, Software defect proneness, Statistical tests}
}

@article{10.1007/s10664-020-09878-9,
author = {Bangash, Abdul Ali and Sahar, Hareem and Hindle, Abram and Ali, Karim},
title = {On the time-based conclusion stability of cross-project defect prediction models},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09878-9},
doi = {10.1007/s10664-020-09878-9},
abstract = {Researchers in empirical software engineering often make claims based on observable data such as defect reports. Unfortunately, in many cases, these claims are generalized beyond the data sets that have been evaluated. Will the researcher’s conclusions hold a year from now for the same software projects? Perhaps not. Recent studies show that in the area of Software Analytics, conclusions over different data sets are usually inconsistent. In this article, we empirically investigate whether conclusions in the area of cross-project defect prediction truly exhibit stability throughout time or not. Our investigation applies a time-aware evaluation approach where models are trained only on the past, and evaluations are executed only on the future. Through this time-aware evaluation, we show that depending on which time period we evaluate defect predictors, their performance, in terms of F-Score, the area under the curve (AUC), and Mathews Correlation Coefficient (MCC), varies and their results are not consistent. The next release of a product, which is significantly different from its prior release, may drastically change defect prediction performance. Therefore, without knowing about the conclusion stability, empirical software engineering researchers should limit their claims of performance within the contexts of evaluation, because broad claims about defect prediction performance might be contradicted by the next upcoming release of a product under analysis.},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5047–5083},
numpages = {37},
keywords = {Conclusion stability, Defect prediction, Time-aware evaluation}
}

@inproceedings{10.1007/978-3-030-38961-1_8,
author = {Sun, Yuanyuan and Xu, Lele and Guo, Lili and Li, Ye and Wang, Yongming},
title = {A Comparison Study of VAE and GAN for Software Fault Prediction},
year = {2019},
isbn = {978-3-030-38960-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-38961-1_8},
doi = {10.1007/978-3-030-38961-1_8},
abstract = {Software fault is an unavoidable problem in software project. How to predict software fault to enhance safety and reliability of system is worth studying. In recent years, deep learning has been widely used in the fields of image, text and voice. However it is seldom applied in the field of software fault prediction. Considering the ability of deep learning, we select the deep learning techniques of VAE and GAN for software fault prediction and compare the performance of them. There is one salient feature of software fault data. The proportion of non-fault data is well above the proportion of fault data. Because of the imbalanced data, it is difficult to get high accuracy to predict software fault. As we known, VAE and GAN are able to generate synthetic samples that obey the distribution of real data. We try to take advantage of their power to generate new fault samples in order to improve the accuracy of software fault prediction. The architectures of VAE and GAN are designed to fit for the high dimensional software fault data. New software fault samples are generated to balance the software fault datasets in order to get better performance for software fault prediction. The models of VAE and GAN are trained on GPU TITAN X. SMOTE is also adopted in order to compare the performance with VAE and GAN. The results in the experiment show that VAE and GAN are useful techniques for software fault prediction and VAE has better performance than GAN on this issue.},
booktitle = {Algorithms and Architectures for Parallel Processing: 19th International Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9–11, 2019, Proceedings, Part II},
pages = {82–96},
numpages = {15},
keywords = {Deep learning, VAE, GAN, Software fault prediction},
location = {Melbourne, VIC, Australia}
}

@article{10.1007/s10664-016-9473-1,
author = {Mahmoud, Anas and Bradshaw, Gary},
title = {Semantic topic models for source code analysis},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9473-1},
doi = {10.1007/s10664-016-9473-1},
abstract = {Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. Despite these advances, applications of topic modeling in software engineering are frequently suboptimal. This can be attributed to the fact that current state-of-the-art topic modeling techniques tend to be data intensive. However, the textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature. This prevents classical topic modeling techniques, typically used to model natural language texts, to generate proper models when applied to source code. Furthermore, the operational complexity and multi-parameter calibration often associated with conventional topic modeling techniques raise important concerns about their feasibility as data analysis models in software engineering. Motivated by these observations, in this paper we propose a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. Ten software systems from different application domains are used to empirically calibrate and configure the proposed approach. The usefulness of generated topics is empirically validated using human judgment. Furthermore, a case study that demonstrates thet operation of the proposed approach in analyzing code evolution is reported. The results show that our approach produces stable, more interpretable, and more expressive topics than classical topic modeling techniques without the necessity for extensive parameter calibration.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1965–2000},
numpages = {36},
keywords = {Clustering, Information theory, Topic modeling}
}

@inproceedings{10.1145/3127005.3127017,
author = {Osman, Haidar and Ghafari, Mohammad and Nierstrasz, Oscar and Lungu, Mircea},
title = {An Extensive Analysis of Efficient Bug Prediction Configurations},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127017},
doi = {10.1145/3127005.3127017},
abstract = {Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable.Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction configurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the field where each aspect is investigated in isolation.Method: We use a cost-aware evaluation scheme to evaluate 60 different bug prediction configuration combinations on five open source Java projects.Results: We find out that the best choices for building a cost-effective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these configuration options results in the most efficient bug predictor across all subject systems.Conclusions: We demonstrate a strong evidence for the interplay among bug prediction configurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate efficient bug predictors.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {107–116},
numpages = {10},
keywords = {Bug Prediction, Effort-Aware Evaluation},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1007/s10664-021-09996-y,
author = {Laaber, Christoph and Basmaci, Mikael and Salza, Pasquale},
title = {Predicting unstable software benchmarks using static source code features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09996-y},
doi = {10.1007/s10664-021-09996-y},
abstract = {Software benchmarks are only as good as the performance measurements they yield. Unstable benchmarks show high variability among repeated measurements, which causes uncertainty about the actual performance and complicates reliable change assessment. However, if a benchmark is stable or unstable only becomes evident after it has been executed and its results are available. In this paper, we introduce a machine-learning-based approach to predict a benchmark’s stability without having to execute it. Our approach relies on 58 statically-computed source code features, extracted for benchmark code and code called by a benchmark, related to (1) meta information, e.g., lines of code (LOC), (2) programming language elements, e.g., conditionals or loops, and (3) potentially performance-impacting standard library calls, e.g., file and network input/output (I/O). To assess our approach’s effectiveness, we perform a large-scale experiment on 4,461 Go benchmarks coming from 230 open-source software (OSS) projects. First, we assess the prediction performance of our machine learning models using 11 binary classification algorithms. We find that Random Forest performs best with good prediction performance from 0.79 to 0.90, and 0.43 to 0.68, in terms of AUC and MCC, respectively. Second, we perform feature importance analyses for individual features and feature categories. We find that 7 features related to meta-information, slice usage, nested loops, and synchronization application programming interfaces (APIs) are individually important for good predictions; and that the combination of all features of the called source code is paramount for our model, while the combination of features of the benchmark itself is less important. Our results show that although benchmark stability is affected by more than just the source code, we can effectively utilize machine learning models to predict whether a benchmark will be stable or not ahead of execution. This enables spending precious testing time on reliable benchmarks, supporting developers to identify unstable benchmarks during development, allowing unstable benchmarks to be repeated more often, estimating stability in scenarios where repeated benchmark execution is infeasible or impossible, and warning developers if new benchmarks or existing benchmarks executed in new environments will be unstable.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {53},
keywords = {Performance testing, Software benchmarking, Performance variability, Source code features, Machine learning for software engineering, Go}
}

@article{10.1155/2019/8391425,
author = {Ren, Jiadong and Zheng, Zhangqi and Liu, Qian and Wei, Zhiyao and Yan, Huaizhi and Chen, Jiageng},
title = {A Buffer Overflow Prediction Approach Based on Software Metrics and Machine Learning},
year = {2019},
issue_date = {2019},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2019},
issn = {1939-0114},
url = {https://doi.org/10.1155/2019/8391425},
doi = {10.1155/2019/8391425},
abstract = {Buffer overflow vulnerability is the most common and serious type of vulnerability in software today, as network security issues have become increasingly critical. To alleviate the security threat, many vulnerability mining methods based on static and dynamic analysis have been developed. However, the current analysis methods have problems regarding high computational time, low test efficiency, low accuracy, and low versatility. This paper proposed a software buffer overflow vulnerability prediction method by using software metrics and a decision tree algorithm. First, the software metrics were extracted from the software source code, and data from the dynamic data stream at the functional level was extracted by a data mining method. Second, a model based on a decision tree algorithm was constructed to measure multiple types of buffer overflow vulnerabilities at the functional level. Finally, the experimental results showed that our method ran in less time than SVM, Bayes, adaboost, and random forest algorithms and achieved 82.53% and 87.51% accuracy in two different data sets. The method presented in this paper achieved the effect of accurately predicting software buffer overflow vulnerabilities in C/C++ and Java programs.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {13}
}

@inproceedings{10.1145/3379247.3379278,
author = {Ahmed, Md. Razu and Ali, Md. Asraf and Ahmed, Nasim and Zamal, Md. Fahad Bin and Shamrat, F.M. Javed Mehedi},
title = {The Impact of Software Fault Prediction in Real-World Application: An Automated Approach for Software Engineering},
year = {2020},
isbn = {9781450376730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379247.3379278},
doi = {10.1145/3379247.3379278},
abstract = {Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold cross-validation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application.},
booktitle = {Proceedings of 2020 6th International Conference on Computing and Data Engineering},
pages = {247–251},
numpages = {5},
keywords = {Defect prediction, Machine learning, Software engineering, Software fault},
location = {Sanya, China},
series = {ICCDE '20}
}

@inproceedings{10.1145/3463274.3463315,
author = {Schnappinger, Markus and Fietzke, Arnaud and Pretschner, Alexander},
title = {Human-level Ordinal Maintainability Prediction Based on Static Code Metrics},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463315},
doi = {10.1145/3463274.3463315},
abstract = {One of the greatest challenges in software quality control is the efficient and effective measurement of maintainability. Thorough expert assessments are precise yet slow and expensive, whereas automated static analysis yields imprecise yet rapid feedback. Several machine learning approaches aim to integrate the advantages of both concepts. However, most prior studies did not adhere to expert judgment and predicted the number of changed lines as a proxy for maintainability, or were biased towards a small group of experts. In contrast, the present study builds on a manually labeled and validated dataset. Prediction is done using static code metrics where we found simple structural metrics such as the size of a class and its methods to yield the highest predictive power towards maintainability. Using just a small set of these metrics, our models can distinguish easy from hard to maintain code with an F-score of 91.3% and AUC of 82.3%. In addition, we perform a more detailed ordinal classification and compare the quality of the classification with the performance of experts. Here, we use the deviations between the individual expert’s ratings and the eventually determined consensus of all experts. In sum, our models achieve the same level of performance as an average human expert. In fact, the obtained accuracy and mean squared error outperform human performance. We hence argue that our models provide an automated and trustworthy prediction of software maintainability.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {160–169},
numpages = {10},
keywords = {Expert Judgment, Machine Learning, Maintainability Prediction, Ordinal Classification, Software Maintainability},
location = {Trondheim, Norway},
series = {EASE '21}
}

@article{10.1504/ijiei.2021.120322,
author = {Lakra, Kirti and Chug, Anuradha},
title = {Application of metaheuristic techniques in software quality prediction: a systematic mapping study},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {4},
issn = {1758-8715},
url = {https://doi.org/10.1504/ijiei.2021.120322},
doi = {10.1504/ijiei.2021.120322},
abstract = {This paper focuses on the systematic review of various metaheuristic techniques employed for analysing different software quality aspects, including fault proneness, defect anticipation, change proneness, maintainability prediction, and software reliability prediction. It is observed that machine learning algorithms are still popular models, but metaheuristic algorithms are also gaining popularity in the field of software quality measurement. This is due to the fact that metaheuristic algorithms are more efficient in solving real-world, search-based, and optimisation problems. Initially, 90 papers were considered and analysed for conducting this study from 2010 to 2020, and 55 studies were shortlisted based on predesigned quality evaluation standards. Resultantly, particle swarm optimisation (PSO), and genetic algorithms came out as the most prominently used metaheuristic techniques for developing software quality models in 36.3% and 27.2% of the shortlisted studies, respectively. The current review will benefit other researchers by providing an insight into the current trends in software quality domain.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {355–399},
numpages = {44},
keywords = {metaheuristic techniques, object-oriented metrics, software quality, software fault proneness, software defect prediction, software change prediction, software reliability prediction, software maintainability prediction, software quality improvement}
}

@article{10.1155/2021/5558561,
author = {Shao, Yanli and Zhao, Jingru and Wang, Xingqi and Wu, Weiwei and Fang, Jinglong and Gao, Honghao},
title = {Research on Cross-Company Defect Prediction Method to Improve Software Security},
year = {2021},
issue_date = {2021},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2021},
issn = {1939-0114},
url = {https://doi.org/10.1155/2021/5558561},
doi = {10.1155/2021/5558561},
abstract = {As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {19}
}

@article{10.1016/j.infsof.2021.106652,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Zhang, Tao and Yang, Dan and Li, Wei},
title = {A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106652},
doi = {10.1016/j.infsof.2021.106652},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {16},
keywords = {Crash localization, Stack trace, Feature selection, Empirical study}
}

@inproceedings{10.1145/3021460.3021481,
author = {Kumar, Lov and Sureka, Ashish},
title = {Using Structured Text Source Code Metrics and Artificial Neural Networks to Predict Change Proneness at Code Tab and Program Organization Level},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021481},
doi = {10.1145/3021460.3021481},
abstract = {Structured Text (ST) is a high-level text-based programming language which is part of the IEC 61131-3 standard. ST is widely used in the domain of industrial automation engineering to create Programmable Logic Controller (PLC) programs. ST is a Domain Specific Language (DSL) which is specialized to the Automation Engineering (AE) application domain. ST has specialized features and programming constructs which are different than general purpose programming languages. We define, develop a tool and compute 10 source code metrics and their correlation with each-other at the Code Tab (CT) and Program Organization Unit (POU) level for two real-world industrial projects at a leading automation engineering company. We study the correlation between the 10 ST source code metrics and their relationship with change proneness at the CT and POU level by creating experimental dataset consisting of different versions of the system. We build predictive models using Artificial Neural Network (ANN) based techniques to predict change proneness of the software. We conduct a series of experiments using various training algorithms and measure the performance of our approach using accuracy and F-measure metrics. We also apply two feature selection techniques to select optimal features aiming to improve the overall accuracy of the classifier.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {172–180},
numpages = {9},
keywords = {Artificial Neural Networks (ANN), Change Proneness Prediction, Machine Learning Applications in Software Engineering, Programmable Logic Controller (PLC) Applications, Source Code Analysis, Source Code Metrics, Structured Text (ST)},
location = {Jaipur, India},
series = {ISEC '17}
}

@article{10.1016/j.eswa.2019.113085,
author = {Pandey, Sushant Kumar and Mishra, Ravi Bhushan and Tripathi, Anil Kumar},
title = {BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques},
year = {2020},
issue_date = {Apr 2020},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.113085},
doi = {10.1016/j.eswa.2019.113085},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {22},
keywords = {Software bug prediction, Classification technique, Software metrics, Deep representation, Boosting, Staked denoising auto-encoder, Heterogeneous Ensemble learning technique}
}

@article{10.1016/j.jss.2021.111031,
author = {Giray, G\"{o}rkem},
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111031},
doi = {10.1016/j.jss.2021.111031},
journal = {J. Syst. Softw.},
month = oct,
numpages = {35},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review}
}

@inproceedings{10.1145/2786805.2786814,
author = {Nam, Jaechang and Kim, Sunghun},
title = {Heterogeneous defect prediction},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786814},
doi = {10.1145/2786805.2786814},
abstract = {Software defect prediction is one of the most active research areas in software engineering. We can build a prediction model with defect data collected from a software project and predict defects in the same project, i.e. within-project defect prediction (WPDP). Researchers also proposed cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. In recent studies, CPDP is proved to be feasible. However, CPDP requires projects that have the same metric set, meaning the metric sets should be identical between projects. As a result, current techniques for CPDP are difficult to apply across projects with heterogeneous metric sets. To address the limitation, we propose heterogeneous defect prediction (HDP) to predict defects across projects with heterogeneous metric sets. Our HDP approach conducts metric selection and metric matching to build a prediction model between projects with heterogeneous metric sets. Our empirical study on 28 subjects shows that about 68% of predictions using our approach outperform or are comparable to WPDP with statistical significance.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {508–519},
numpages = {12},
keywords = {Defect prediction, heterogeneous metrics, quality assurance},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2931037.2931039,
author = {Bowes, David and Hall, Tracy and Harman, Mark and Jia, Yue and Sarro, Federica and Wu, Fan},
title = {Mutation-aware fault prediction},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931039},
doi = {10.1145/2931037.2931039},
abstract = {We introduce mutation-aware fault prediction, which leverages additional guidance from metrics constructed in terms of mutants and the test cases that cover and detect them. We report the results of 12 sets of experiments, applying 4 different predictive modelling techniques to 3 large real-world systems (both open and closed source). The results show that our proposal can significantly (p ≤ 0.05) improve fault prediction performance. Moreover, mutation-based metrics lie in the top 5% most frequently relied upon fault predictors in 10 of the 12 sets of experiments, and provide the majority of the top ten fault predictors in 9 of the 12 sets of experiments.},
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {330–341},
numpages = {12},
keywords = {Empirical Study, Mutation Testing, Software Defect Prediction, Software Fault Prediction, Software Metrics},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.5555/3340730.3340750,
author = {Catolino, Gemma and Di Nucci, Dario and Ferrucci, Filomena},
title = {Cross-project just-in-time bug prediction for mobile apps: an empirical assessment},
year = {2019},
publisher = {IEEE Press},
abstract = {Bug Prediction is an activity aimed at identifying defect-prone source code entities that allows developers to focus testing efforts on specific areas of software systems. Recently, the research community proposed Just-in-Time (JIT) Bug Prediction with the goal of detecting bugs at commit-level. While this topic has been extensively investigated in the context of traditional systems, to the best of our knowledge, only a few preliminary studies assessed the performance of the technique in a mobile environment, by applying the metrics proposed by Kamei et al. in a within-project scenario. The results of these studies highlighted that there is still room for improvement. In this paper, we faced this problem to understand (i) which Kamei et al.'s metrics are useful in the mobile context, (ii) if different classifiers impact the performance of cross-project JIT bug prediction models and (iii) whether the application of ensemble techniques improves the capabilities of the models. To carry out the experiment, we first applied a feature selection technique, i.e., InfoGain, to filter relevant features and avoid models multicollinearity. Then, we assessed and compared the performance of four different well-known classifiers and four ensemble techniques. Our empirical study involved 14 apps and 42,543 commits extracted from the Commit Guru platform. The results show that Naive Bayes achieves the best performance with respect to the other classifiers and in some cases outperforms some well-known ensemble techniques.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {99–110},
numpages = {12},
keywords = {JIT bug prediction, empirical study, metrics},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@inproceedings{10.1007/978-3-030-47426-3_61,
author = {Perera, Kushani and Chan, Jeffrey and Karunasekera, Shanika},
title = {A Framework for Feature Selection to Exploit Feature Group Structures},
year = {2020},
isbn = {978-3-030-47425-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-47426-3_61},
doi = {10.1007/978-3-030-47426-3_61},
abstract = {Filter feature selection methods play an important role in machine learning tasks when low computational costs, classifier independence or simplicity is important. Existing filter methods predominantly focus only on the input data and do not take advantage of the external sources of correlations within feature groups to improve the classification accuracy. We propose a framework which facilitates supervised filter feature selection methods to exploit feature group information from external sources of knowledge and use this framework to incorporate feature group information into minimum Redundancy Maximum Relevance (mRMR) algorithm, resulting in GroupMRMR algorithm. We show that GroupMRMR achieves high accuracy gains over mRMR (up&nbsp;to 35%) and other popular filter methods (up&nbsp;to 50%). GroupMRMR has same computational complexity as that of mRMR, therefore, does not incur additional computational costs. Proposed method has many real world applications, particularly the ones that use genomic, text and image data whose features demonstrate strong group structures.},
booktitle = {Advances in Knowledge Discovery and Data Mining: 24th Pacific-Asia Conference, PAKDD 2020, Singapore, May 11–14, 2020, Proceedings, Part I},
pages = {792–804},
numpages = {13},
keywords = {Filter feature selection, Feature groups, Squared [inline-graphic not available: see fulltext] norm minimisation},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3266237.3266271,
author = {Ara\'{u}jo, Cristiano Werner and Zapalowski, Vanius and Nunes, Ingrid},
title = {Using code quality features to predict bugs in procedural software systems},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266271},
doi = {10.1145/3266237.3266271},
abstract = {A wide range of metrics have been used as features to build bug (or fault) predictors. However, most of the existing predictors focus mostly on object-oriented (OO) systems, either because they rely on OO metrics or were evaluated mainly with OO systems. Procedural software systems (PSS), less addressed in bug prediction research, often suffer from maintainability problems because they typically consist of low-level applications, using for example preprocessors to cope with variability. Previous work evaluated sets of features (composed of static code metrics) proposed in existing approaches in the PSS context. However, explored metrics are limited to those that are part of traditional metric suites, being often associated with structural code properties. A type of information explored to a smaller extent in this context is the output of code quality tools that statically analyse source code, providing hints of code problems. In this paper, we investigate the use of information collected from quality tools to build bug predictors dedicated to PSS. We specify four features derived from code quality tools or associated with poor programming practices and evaluate the effectiveness of these features. Our evaluation shows that our proposed features improve bug predictors in our investigated context.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {bug prediction, code metrics, procedural languanges},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3439961.3439991,
author = {Rivero, Luis and Diniz, Jo\~{a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton},
title = {Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439991},
doi = {10.1145/3439961.3439991},
abstract = {The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {30},
numpages = {10},
keywords = {Methods, Software Processes, Validation, Verification, and Testing, and Tools},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.5555/1332044.1332090,
author = {Catal, Cagatay and Diri, Banu},
title = {Software defect prediction using artificial immune recognition system},
year = {2007},
publisher = {ACTA Press},
address = {USA},
abstract = {Predicting fault-prone modules for software development projects enables companies to reach high reliable systems and minimizes necessary budget, personnel and resource to be allocated to achieve this goal. Researchers have investigated various statistical techniques and machine learning algorithms until now but most of them applied their models to the different datasets which are not public or used different criteria to decide the best predictor model. Artificial Immune Recognition System is a supervised learning algorithm which has been proposed in 2001 for the classification problems and its performance for UCI datasets (University of California machine learning repository) is remarkable.In this paper, we propose a novel software defect prediction model by applying Artificial Immune Recognition System (AIRS) along with the Correlation-Based Feature Selection (CFS) technique. In order to evaluate the performance of the proposed model, we apply it to the five NASA public defect datasets and compute G-mean 1, G-mean 2 and F-measure values to discuss the effectiveness of the model. Experimental results show that AIRS has a great potential for software defect prediction and AIRS along with CFS technique provides relatively better prediction for large scale projects which consist of many modules.},
booktitle = {Proceedings of the 25th Conference on IASTED International Multi-Conference: Software Engineering},
pages = {285–290},
numpages = {6},
keywords = {artificial immune recognition system (AIRS) and correlation-based feature selection, immune systems, quality prediction, software defect prediction},
location = {Innsbruck, Austria},
series = {SE'07}
}

@article{10.1007/s10489-020-02026-2,
author = {Van Nguyen, Sinh and Tran, Ha Manh},
title = {An automated fault detection system for communication networks and distributed systems},
year = {2021},
issue_date = {Aug 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {8},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-02026-2},
doi = {10.1007/s10489-020-02026-2},
abstract = {Automating fault detection in communication networks and distributed systems is a challenging process that usually requires the involvement of supporting tools and the expertise of system operators. Automated event monitoring and correlating systems produce event data that is forwarded to system operators for analyzing error events and creating fault reports. Machine learning methods help not only analyzing event data more precisely but also forecasting possible error events by learning from existing faults. This study introduces an automated fault detection system that assists system operators in detecting and forecasting faults. This system is characterized by the capability of exploiting bug knowledge resources at various online repositories, log events and status parameters from the monitored system; and applying bug analysis and event filtering methods for evaluating events and forecasting faults. The system contains a fault data model to collect bug reports, a feature and semantic filtering method to correlate log events, and machine learning methods to evaluate the severity, priority and relation of log events and forecast the forthcoming critical faults of the monitored system. We have evaluated the prototyping implementation of the proposed system on a high performance computing cluster system and provided analysis with lessons learned.},
journal = {Applied Intelligence},
month = aug,
pages = {5405–5419},
numpages = {15},
keywords = {Fault detection, Automation, Machine learning, Random forest, Bug tracking system}
}

@article{10.1007/s10489-020-01935-6,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {An empirical study of ensemble techniques for software fault prediction},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {6},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-020-01935-6},
doi = {10.1007/s10489-020-01935-6},
abstract = {Previously, many researchers have performed analysis of various techniques for the software fault prediction (SFP). Oddly, the majority of such studies have shown the limited prediction capability and their performance for given software fault datasets was not persistent. In contrast to this, recently, ensemble techniques based SFP models have shown promising and improved results across different software fault datasets. However, many new as well as improved ensemble techniques have been introduced, which are not explored for SFP. Motivated by this, the paper performs an investigation on ensemble techniques for SFP. We empirically assess the performance of seven ensemble techniques namely, Dagging, Decorate, Grading, MultiBoostAB, RealAdaBoost, Rotation Forest, and Ensemble Selection. We believe that most of these ensemble techniques are not used before for SFP. We conduct a series of experiments on the benchmark fault datasets and use three distinct classification algorithms, namely, naive Bayes, logistic regression, and J48 (decision tree) as base learners to the ensemble techniques. Experimental analysis revealed that rotation forest with J48 as the base learner achieved the highest precision, recall, and G-mean 1 values of 0.995, 0.994, and 0.994, respectively and Decorate achieved the highest AUC value of 0.986. Further, results of statistical tests showed used ensemble techniques demonstrated a statistically significant difference in their performance among the used ones for SFP. Additionally, the cost-benefit analysis showed that SFP models based on used ensemble techniques might be helpful in saving software testing cost and effort for twenty out of twenty-eight used fault datasets.},
journal = {Applied Intelligence},
month = jun,
pages = {3615–3644},
numpages = {30},
keywords = {Software fault prediction, Ensemble techniques, PROMISE data repository, Empirical analysis}
}

@inproceedings{10.1145/2532443.2532461,
author = {Chen, Jiaqiang and Liu, Shulong and Chen, Xiang and Gu, Qing and Chen, Daoxu},
title = {Empirical studies on feature selection for software fault prediction},
year = {2013},
isbn = {9781450323697},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2532443.2532461},
doi = {10.1145/2532443.2532461},
abstract = {Classification based software fault prediction methods aim to classify the modules into either fault-prone or non-fault-prone. Feature selection is a preprocess step used to improve the data quality. However most of previous research mainly focus on feature relevance analysis, there is little work focusing on feature redundancy analysis. Therefore we propose a two-stage framework for feature selection to solve this issue. In particular, during the feature relevance phase, we adopt three different relevance measures to obtain the relevant feature subset. Then during the feature redundancy analysis phase, we use a cluster-based method to eliminate redundant features. To verify the effectiveness of our proposed framework, we choose typical real-world software projects, including Eclipse projects and NASA software project KC1. Final empirical result shows the effectiveness of our proposed framework.},
booktitle = {Proceedings of the 5th Asia-Pacific Symposium on Internetware},
articleno = {26},
numpages = {4},
keywords = {feature selection, redundancy analysis, relevance analysis, software fault prediction},
location = {Changsha, China},
series = {Internetware '13}
}

@inproceedings{10.1145/2601248.2601294,
author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel and Dolado, Javier and Riquelme, Jos\'{e} C.},
title = {Preliminary comparison of techniques for dealing with imbalance in software defect prediction},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601294},
doi = {10.1145/2601248.2601294},
abstract = {Imbalanced data is a common problem in data mining when dealing with classification problems, where samples of a class vastly outnumber other classes. In this situation, many data mining algorithms generate poor models as they try to optimize the overall accuracy and perform badly in classes with very few samples. Software Engineering data in general and defect prediction datasets are not an exception and in this paper, we compare different approaches, namely sampling, cost-sensitive, ensemble and hybrid approaches to the problem of defect prediction with different datasets preprocessed differently. We have used the well-known NASA datasets curated by Shepperd et al. There are differences in the results depending on the characteristics of the dataset and the evaluation metrics, especially if duplicates and inconsistencies are removed as a preprocessing step.Further Results and replication package: http://www.cc.uah.es/drg/ease14},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {43},
numpages = {10},
keywords = {data quality, defect prediction, imbalanced data},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@article{10.4018/ijssci.2014040101,
author = {Mishra, Bharavi and Shukla, K.K.},
title = {Software Defect Prediction Based on GUHA Data Mining Procedure and Multi-Objective Pareto Efficient Rule Selection},
year = {2014},
issue_date = {April 2014},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {2},
issn = {1942-9045},
url = {https://doi.org/10.4018/ijssci.2014040101},
doi = {10.4018/ijssci.2014040101},
abstract = {Software defect prediction, if is effective, enables the developers to distribute their testing efforts efficiently and let them focus on defect prone modules. It would be very resource consuming to test all the modules while the defect lies in fraction of modules. Information about fault-proneness of classes and methods can be used to develop new strategies which can help mitigate the overall development cost and increase the customer satisfaction. Several machine learning strategies have been used in recent past to identify defective modules. These models are built using publicly available historical software defect data sets. Most of the proposed techniques are not able to deal with the class imbalance problem efficiently. Therefore, it is necessary to develop a prediction model which consists of small simple and comprehensible rules. Considering these facts, in this paper, the authors propose a novel defect prediction approach named GUHA based Classification Association Rule Mining algorithm G-CARM where "GUHA" stands for General Unary Hypothesis Automaton. G-CARM approach is primarily based on Classification Association Rule Mining, and deploys a two stage process involving attribute discretization, and rule generation using GUHA. GUHA is oldest yet very powerful method of pattern mining. The basic idea of GUHA procedure is to mine the interesting attribute patterns that indicate defect proneness. The new method has been compared against five other models reported in recent literature viz. Naive Bayes, Support Vector Machine, RIPPER, J48 and Nearest Neighbour classifier by using several measures, including AUC and probability of detection. The experimental results indicate that the prediction performance of G-CARM approach is better than other prediction approaches. The authors' approach achieved 76% mean recall and 83% mean precision for defective modules and 93% mean recall and 83% mean precision for non-defective modules on CM1, KC1, KC2 and Eclipse data sets. Further defect rule generation process often generates a large number of rules which require considerable efforts while using these rules as a defect predictor, hence, a rule sub-set selection process is also proposed to select best set of rules according to the requirements. Evolution criteria for defect prediction like sensitivity, specificity, precision often compete against each other. It is therefore, important to use multi-objective optimization algorithms for selecting prediction rules. In this paper the authors report prediction rules that are Pareto efficient in the sense that no further improvements in the rule set is possible without sacrificing some performance criteria. Non-Dominated Sorting Genetic Algorithm has been used to find Pareto front and defect prediction rules.},
journal = {Int. J. Softw. Sci. Comput. Intell.},
month = apr,
pages = {1–29},
numpages = {29},
keywords = {Data Mining, Defect Patterns, Fault Prediction, General Unary Hypothesis Automaton GUHA, Pareto Optimality}
}

@article{10.1504/ijcsm.2021.117600,
author = {Hammad, Mustafa},
title = {Classifying defective software projects based on machine learning and complexity metrics},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {4},
issn = {1752-5055},
url = {https://doi.org/10.1504/ijcsm.2021.117600},
doi = {10.1504/ijcsm.2021.117600},
abstract = {Software defects can lead to software failures or errors at any time. Therefore, software developers and engineers spend a lot of time and effort in order to find possible defects. This paper proposes an automatic approach to predict software defects based on machine learning algorithms. A set of complexity measures values are used to train the classifier. Three public datasets were used to evaluate the ability of mining complexity measures for different software projects to predict possible defects. Experimental results showed that it is possible to min software complexity to build a defect prediction model with a high accuracy rate.},
journal = {Int. J. Comput. Sci. Math.},
month = jan,
pages = {401–412},
numpages = {11},
keywords = {software defects, defect prediction, software metrics, machine learning, complexity, neural networks, na\"{\i}ve Bayes, decision trees, SVM, support vector machine}
}

@inproceedings{10.1145/3377811.3380360,
author = {Li, Ke and Xiang, Zilin and Chen, Tao and Wang, Shuo and Tan, Kay Chen},
title = {Understanding the automated parameter optimization on transfer learning for cross-project defect prediction: an empirical study},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380360},
doi = {10.1145/3377811.3380360},
abstract = {Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is 'not difficult' to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {566–577},
numpages = {12},
keywords = {automated parameter optimization, classification techniques, cross-project defect prediction, transfer learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2245276.2231975,
author = {Banthia, Deepak and Gupta, Atul},
title = {Investigating fault prediction capabilities of five prediction models for software quality},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2231975},
doi = {10.1145/2245276.2231975},
abstract = {Predicting faults in software modules can lead to a high quality and more effective software development process to follow. However, the results of a fault prediction model have to be properly interpreted before incorporating them into any decision making. Most of the earlier studies have used the prediction accuracy as the main criteria to compare amongst competing fault prediction models. However, we show that besides accuracy, other criteria like number of false positives and false negatives can equally be important to choose a candidate model for fault prediction. We have used five NASA software data sets in our experiment. Our results suggest that the performance of Simple Logistic is better than the others on raw data sets whereas the performance of Neural Network was found to be better when we applied dimensionality reduction method on raw data sets. When we used data pre-processing techniques, the prediction accuracy of Random Forest was found to be better in both cases i.e. with and without dimensionality reduction but reliability of Simple Logistic was better than Random Forest because it had less number of fault negatives.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {1259–1261},
numpages = {3},
keywords = {attribute selection, effort estimation, fault prediction, fault prediction models, quality assurance},
location = {Trento, Italy},
series = {SAC '12}
}

@article{10.1007/s10664-021-09966-4,
author = {Tuarob, Suppawong and Assavakamhaenghan, Noppadol and Tanaphantaruk, Waralee and Suwanworaboon, Ponlakit and Hassan, Saeed-Ul and Choetkiertikul, Morakot},
title = {Automatic team recommendation for collaborative software development},
year = {2021},
issue_date = {Jul 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09966-4},
doi = {10.1007/s10664-021-09966-4},
abstract = {In large-scale collaborative software development, building a team of software practitioners can be challenging, mainly due to overloading choices of candidate members to fill in each role. Furthermore, having to understand all members’ diverse backgrounds, and anticipate team compatibility could significantly complicate and attenuate such a team formation process. Current solutions that aim to automatically suggest software practitioners for a task merely target particular roles, such as developers, reviewers, and integrators. While these existing approaches could alleviate issues presented by choice overloading, they fail to address team compatibility while members collaborate. In this paper, we propose RECAST, an intelligent recommendation system that suggests team configurations that satisfy not only the role requirements, but also the necessary technical skills and teamwork compatibility, given task description and a task assignee. Specifically, RECAST uses Max-Logit to intelligently enumerate and rank teams based on the team-fitness scores. Machine learning algorithms are adapted to generate a scoring function that learns from heterogenous features characterizing effective software teams in large-scale collaborative software development. RECAST is evaluated against a state-of-the-art team recommendation algorithm using three well-known open-source software project datasets. The evaluation results are promising, illustrating that our proposed method outperforms the baselines in terms of team recommendation with 646% improvement (MRR) using the exact-match evaluation protocol.},
journal = {Empirical Softw. Engg.},
month = jul,
numpages = {53},
keywords = {Team recommendation, Collaborative software development, Machine learning}
}

@article{10.1016/j.eswa.2020.114022,
author = {Rauber, Thomas Walter and da Silva Loca, Antonio Luiz and Boldt, Francisco de Assis and Rodrigues, Alexandre Loureiros and Varej\~{a}o, Fl\'{a}vio Miguel},
title = {An experimental methodology to evaluate machine learning methods for fault diagnosis based on vibration signals},
year = {2021},
issue_date = {Apr 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2020.114022},
doi = {10.1016/j.eswa.2020.114022},
journal = {Expert Syst. Appl.},
month = apr,
numpages = {18},
keywords = {Fault detection, CWRU bearing fault database, Performance criteria, Classification, Pattern recognition, Machine learning}
}

@inproceedings{10.1145/2372251.2372285,
author = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C.},
title = {Method-level bug prediction},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372285},
doi = {10.1145/2372251.2372285},
abstract = {Researchers proposed a wide range of approaches to build effective bug prediction models that take into account multiple aspects of the software development process. Such models achieved good prediction performance, guiding developers towards those parts of their system where a large share of bugs can be expected. However, most of those approaches predict bugs on file-level. This often leaves developers with a considerable amount of effort to examine all methods of a file until a bug is located. This particular problem is reinforced by the fact that large files are typically predicted as the most bug-prone. In this paper, we present bug prediction models at the level of individual methods rather than at file-level. This increases the granularity of the prediction and thus reduces manual inspection efforts for developers. The models are based on change metrics and source code metrics that are typically used in bug prediction. Our experiments---performed on 21 Java open-source (sub-)systems---show that our prediction models reach a precision and recall of 84% and 88%, respectively. Furthermore, the results indicate that change metrics significantly outperform source code metrics.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {171–180},
numpages = {10},
keywords = {code metrics, fine-grained source code changes, method-level bug prediction},
location = {Lund, Sweden},
series = {ESEM '12}
}

@article{10.1016/j.neucom.2021.08.010,
author = {Ardimento, Pasquale and Aversano, Lerina and Bernardi, Mario Luca and Cimitile, Marta and Iammarino, Martina},
title = {Temporal convolutional networks for just-in-time design smells prediction using fine-grained software metrics},
year = {2021},
issue_date = {Nov 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {463},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.08.010},
doi = {10.1016/j.neucom.2021.08.010},
journal = {Neurocomput.},
month = nov,
pages = {454–471},
numpages = {18},
keywords = {Design smells, Smells prediction, Software quality, Deep learning}
}

@inproceedings{10.1145/2896839.2896843,
author = {Koroglu, Yavuz and Sen, Alper and Kutluay, Doruk and Bayraktar, Akin and Tosun, Yalcin and Cinar, Murat and Kaya, Hasan},
title = {Defect prediction on a legacy industrial software: a case study on software with few defects},
year = {2016},
isbn = {9781450341547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896839.2896843},
doi = {10.1145/2896839.2896843},
abstract = {Context: Building defect prediction models for software projects is helpful for reducing the effort in locating defects. In this paper, we share our experiences in building a defect prediction model for a large industrial software project. We extract product and process metrics to build models and show that we can build an accurate defect prediction model even when 4% of the software is defective.Objective: Our goal in this project is to integrate a defect predictor into the continuous integration (CI) cycle of a large software project and decrease the effort in testing.Method: We present our approach in the form of an experience report. Specifically, we collected data from seven older versions of the software project and used additional features to predict defects of current versions. We compared several classification techniques including Naive Bayes, Decision Trees, and Random Forest and resampled our training data to present the company with the most accurate defect predictor.Results: Our results indicate that we can focus testing efforts by guiding the test team to only 8% of the software where 53% of actual defects can be found. Our model has 90% accuracy.Conclusion: We produce a defect prediction model with high accuracy for a software with defect rate of 4%. Our model uses Random Forest, that which we show has more predictive power than Naive Bayes, Logistic Regression and Decision Trees in our case.},
booktitle = {Proceedings of the 4th International Workshop on Conducting Empirical Studies in Industry},
pages = {14–20},
numpages = {7},
keywords = {defect prediction, experience report, feature selection, process metrics, random forest},
location = {Austin, Texas},
series = {CESI '16}
}

@article{10.1007/s11390-019-1959-z,
author = {Xu, Zhou and Pang, Shuai and Zhang, Tao and Luo, Xia-Pu and Liu, Jin and Tang, Yu-Tian and Yu, Xiao and Xue, Lei},
title = {Cross Project Defect Prediction via Balanced Distribution Adaptation Based Transfer Learning},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1959-z},
doi = {10.1007/s11390-019-1959-z},
abstract = {Defect prediction assists the rational allocation of testing resources by detecting the potentially defective software modules before releasing products. When a project has no historical labeled defect data, cross project defect prediction (CPDP) is an alternative technique for this scenario. CPDP utilizes labeled defect data of an external project to construct a classification model to predict the module labels of the current project. Transfer learning based CPDP methods are the current mainstream. In general, such methods aim to minimize the distribution differences between the data of the two projects. However, previous methods mainly focus on the marginal distribution difference but ignore the conditional distribution difference, which will lead to unsatisfactory performance. In this work, we use a novel balanced distribution adaptation (BDA) based transfer learning method to narrow this gap. BDA simultaneously considers the two kinds of distribution differences and adaptively assigns different weights to them. To evaluate the effectiveness of BDA for CPDP performance, we conduct experiments on 18 projects from four datasets using six indicators (i.e., F-measure, g-means, Balance, AUC, EARecall, and EAF-measure). Compared with 12 baseline methods, BDA achieves average improvements of 23.8%, 12.5%, 11.5%, 4.7%, 34.2%, and 33.7% in terms of the six indicators respectively over four datasets.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1039–1062},
numpages = {24},
keywords = {cross-project defect prediction, transfer learning, balancing distribution, effort-aware indicator}
}

@article{10.1007/s11390-019-1960-6,
author = {Alqmase, Mohammed and Alshayeb, Mohammad and Ghouti, Lahouari},
title = {Threshold Extraction Framework for Software Metrics},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1960-6},
doi = {10.1007/s11390-019-1960-6},
abstract = {Software metrics are used to measure different attributes of software. To practically measure software attributes using these metrics, metric thresholds are needed. Many researchers attempted to identify these thresholds based on personal experiences. However, the resulted experience-based thresholds cannot be generalized due to the variability in personal experiences and the subjectivity of opinions. The goal of this paper is to propose an automated clustering framework based on the expectation maximization (EM) algorithm where clusters are generated using a simplified 3-metric set (LOC, LCOM, and CBO). Given these clusters, different threshold levels for software metrics are systematically determined such that each threshold reflects a specific level of software quality. The proposed framework comprises two major steps: the clustering step where the software quality historical dataset is decomposed into a fixed set of clusters using the EM algorithm, and the threshold extraction step where thresholds, specific to each software metric in the resulting clusters, are estimated using statistical measures such as the mean (μ) and the standard deviation (σ) of each software metric in each cluster. The paper’s findings highlight the capability of EM-based clustering, using a minimum metric set, to group software quality datasets according to different quality levels.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {1063–1078},
numpages = {16},
keywords = {metric threshold, expectation maximization, empirical study}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ unleashed: an open implementation of the SZZ algorithm - featuring example usage in a study of just-in-time bug prediction for the Jenkins project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {SZZ, defect prediction, issue tracking, mining software repositories},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1007/s11219-014-9241-7,
author = {Madeyski, Lech and Jureczko, Marian},
title = {Which process metrics can significantly improve defect prediction models? An empirical study},
year = {2015},
issue_date = {September 2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9241-7},
doi = {10.1007/s11219-014-9241-7},
abstract = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high (  $$p=.016$$ p = . 016 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.76$$ PS = . 76 ), which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics (  $$p=.038$$ p = . 038 ,  $$r=-.29$$ r = - . 29 ,  $$hbox {PS}=.68$$ PS = . 68 ). The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small (  $$p=.038$$ p = . 038 ,  $$r=.06$$ r = . 06 ). Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
journal = {Software Quality Journal},
month = sep,
pages = {393–422},
numpages = {30},
keywords = {Defect prediction models, Process metrics, Product metrics, Software defect prediction, Software metrics}
}

@inproceedings{10.1145/3194104.3194112,
author = {Di Nucci, Dario and Palomba, Fabio and De Lucia, Andrea},
title = {Evaluating the adaptive selection of classifiers for cross-project bug prediction},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194112},
doi = {10.1145/3194104.3194112},
abstract = {Bug prediction models are used to locate source code elements more likely to be defective. One of the key factors influencing their performances is related to the selection of a machine learning method (a.k.a., classifier) to use when discriminating buggy and non-buggy classes. Given the high complementarity of stand-alone classifiers, a recent trend is the definition of ensemble techniques, which try to effectively combine the predictions of different stand-alone machine learners. In a recent work we proposed ASCI, a technique that dynamically selects the right classifier to use based on the characteristics of the class on which the prediction has to be done. We tested it in a within-project scenario, showing its higher accuracy with respect to the Validation and Voting strategy. In this paper, we continue on the line of research, by (i) evaluating ASCI in a global and local cross-project setting and (ii) comparing its performances with those achieved by a stand-alone and an ensemble baselines, namely Naive Bayes and Validation and Voting, respectively. A key finding of our study shows that ASCI is able to perform better than the other techniques in the context of cross-project bug prediction. Moreover, despite local learning is not able to improve the performances of the corresponding models in most cases, it is able to improve the robustness of the models relying on ASCI.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {48–54},
numpages = {7},
keywords = {bug prediction, cross-project, ensemble classifiers},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@article{10.1007/s10664-020-09848-1,
author = {Jiarpakdee, Jirayus and Tantithamthavorn, Chakkrit and Treude, Christoph},
title = {The impact of automated feature selection techniques on the interpretation of defect models},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09848-1},
doi = {10.1007/s10664-020-09848-1},
abstract = {The interpretation of defect models heavily relies on software metrics that are used to construct them. Prior work often uses feature selection techniques to remove metrics that are correlated and irrelevant in order to improve model performance. Yet, conclusions that are derived from defect models may be inconsistent if the selected metrics are inconsistent and correlated. In this paper, we systematically investigate 12 automated feature selection techniques with respect to the consistency, correlation, performance, computational cost, and the impact on the interpretation dimensions. Through an empirical investigation of 14 publicly-available defect datasets, we find that (1) 94–100% of the selected metrics are inconsistent among the studied techniques; (2) 37–90% of the selected metrics are inconsistent among training samples; (3) 0–68% of the selected metrics are inconsistent when the feature selection techniques are applied repeatedly; (4) 5–100% of the produced subsets of metrics contain highly correlated metrics; and (5) while the most important metrics are inconsistent among correlation threshold values, such inconsistent most important metrics are highly-correlated with the Spearman correlation of 0.85–1. Since we find that the subsets of metrics produced by the commonly-used feature selection techniques (except for AutoSpearman) are often inconsistent and correlated, these techniques should be avoided when interpreting defect models. In addition to introducing AutoSpearman which mitigates correlated metrics better than commonly-used feature selection techniques, this paper opens up new research avenues in the automated selection of features for defect models to optimise for interpretability as well as performance.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3590–3638},
numpages = {49},
keywords = {Software analytics, Defect prediction, Model interpretation, Feature selection}
}

@inproceedings{10.1145/2979779.2979783,
author = {Maheshwari, Suchi and Agarwal, Sonali},
title = {Three-way decision based Defect Prediction for Object Oriented Software},
year = {2016},
isbn = {9781450342131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2979779.2979783},
doi = {10.1145/2979779.2979783},
abstract = {Early prediction of defective software module plays critical role in the software project development to reduce the overall development time, budgets and increases the customer satisfaction. The bug prediction based on two-way classification method classifies the software module as defective or non-defective. This method provides good accuracy measure but this metric is not sufficient in case if misclassification cost is concerned. Classifying the defective module as non-defective will lead to higher cost of entire software project at the end. In this study, three-way decision based classification method and Random Forest ensemble are used to predict the defect in Object Oriented Software to reduce the misclassification cost which will lead to avoid the cost overrun. The eclipse bug prediction dataset is used and experimental results show that the decision cost is reduced and accuracy is increased using our proposed method.},
booktitle = {Proceedings of the International Conference on Advances in Information Communication Technology &amp; Computing},
articleno = {4},
numpages = {6},
keywords = {Eclipse Bug Prediction dataset, Na\"{\i}ve Bayes, Random Forest, Software defect prediction, Three-way decision},
location = {Bikaner, India},
series = {AICTC '16}
}

@article{10.1016/j.infsof.2013.09.001,
author = {Finlay, Jacqui and Pears, Russel and Connor, Andy M.},
title = {Data stream mining for predicting software build outcomes using source code metrics},
year = {2014},
issue_date = {February, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.09.001},
doi = {10.1016/j.infsof.2013.09.001},
abstract = {Context: Software development projects involve the use of a wide range of tools to produce a software artifact. Software repositories such as source control systems have become a focus for emergent research because they are a source of rich information regarding software development projects. The mining of such repositories is becoming increasingly common with a view to gaining a deeper understanding of the development process. Objective: This paper explores the concepts of representing a software development project as a process that results in the creation of a data stream. It also describes the extraction of metrics from the Jazz repository and the application of data stream mining techniques to identify useful metrics for predicting build success or failure. Method: This research is a systematic study using the Hoeffding Tree classification method used in conjunction with the Adaptive Sliding Window (ADWIN) method for detecting concept drift by applying the Massive Online Analysis (MOA) tool. Results: The results indicate that only a relatively small number of the available measures considered have any significance for predicting the outcome of a build over time. These significant measures are identified and the implication of the results discussed, particularly the relative difficulty of being able to predict failed builds. The Hoeffding Tree approach is shown to produce a more stable and robust model than traditional data mining approaches. Conclusion: Overall prediction accuracies of 75% have been achieved through the use of the Hoeffding Tree classification method. Despite this high overall accuracy, there is greater difficulty in predicting failure than success. The emergence of a stable classification tree is limited by the lack of data but overall the approach shows promise in terms of informing software development activities in order to minimize the chance of failure.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {183–198},
numpages = {16},
keywords = {Concept drift detection, Data stream mining, Hoeffding tree, Jazz, Software metrics, Software repositories}
}

@inproceedings{10.1145/3094243.3094245,
author = {Pang, Yulei and Xue, Xiaozhen and Wang, Huaying},
title = {Predicting Vulnerable Software Components through Deep Neural Network},
year = {2017},
isbn = {9781450352321},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3094243.3094245},
doi = {10.1145/3094243.3094245},
abstract = {Vulnerabilities need to be detected and removed from software. Although previous studies demonstrated the usefulness of employing prediction techniques in deciding about vulnerabilities of software components, the improvement of effectiveness of these prediction techniques is still a grand challenging research question. This paper employed a technique based on a deep neural network with rectifier linear units trained with stochastic gradient descent method and batch normalization, for predicting vulnerable software components. The features are defined as continuous sequences of tokens in source code files. Besides, a statistical feature selection algorithm is then employed to reduce the feature and search space. We evaluated the proposed technique based on some Java Android applications, and the results demonstrated that the proposed technique could predict vulnerable classes, i.e., software components, with high precision, accuracy and recall.},
booktitle = {Proceedings of the 2017 International Conference on Deep Learning Technologies},
pages = {6–10},
numpages = {5},
keywords = {Android, deep learning, neural network, vulnerability prediction},
location = {Chengdu, China},
series = {ICDLT '17}
}

@article{10.1007/s10586-021-03282-8,
author = {Mustaqeem, Mohd. and Saqib, Mohd.},
title = {Principal component based support vector machine (PC-SVM): a hybrid technique for software defect detection},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-021-03282-8},
doi = {10.1007/s10586-021-03282-8},
abstract = {Defects are the major problems in the current situation and predicting them is also a difficult task. Researchers and scientists have developed many software defects prediction techniques to overcome this very helpful issue. But to some extend there is a need for an algorithm/method to predict defects with more accuracy, reduce time and space complexities. All the previous research conducted on the data without feature reduction lead to the curse of dimensionality. We brought up a machine learning hybrid approach by combining Principal component Analysis (PCA) and Support vector machines (SVM) to overcome the ongoing problem. We have employed PROMISE (CM1: 344 observations, KC1: 2109 observations) data from the directory of NASA to conduct our research. We split the dataset into training (CM1: 240 observations, KC1: 1476 observations) dataset and testing (CM1: 104 observations, KC1: 633 observations) datasets. Using PCA, we find the principal components for feature optimization which reduce the time complexity. Then, we applied SVM for classification due to very native qualities over traditional and conventional methods. We also employed the GridSearchCV method for hyperparameter tuning. In the proposed hybrid model we have found better accuracy (CM1: 95.2%, KC1: 86.6%) than other methods. The proposed model also presents higher evaluation in the terms of other criteria. As a limitation, the only problem with SVM is there is no probabilistic explanation for classification which may very rigid towards classifications. In the future, some other method may also introduce which can overcome this limitation and keep a soft probabilistic based margin for classification on the optimal hyperplane.},
journal = {Cluster Computing},
month = sep,
pages = {2581–2595},
numpages = {15},
keywords = {Software defects detection, PCA, SVM, Feature optimization, Classification, PROMISE dataset}
}

@article{10.1016/j.asoc.2014.11.023,
author = {Malhotra, Ruchika},
title = {A systematic review of machine learning techniques for software fault prediction},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2014.11.023},
doi = {10.1016/j.asoc.2014.11.023},
abstract = {Reviews studies from 1991-2013 to assess application of ML techniques for SFP.Identifies seven categories of the ML techniques.Identifies 64 studies to answer the established research questions.Selects primary studies according to the quality assessment of the studies.Systematic literature review performs the following:Summarize ML techniques for SFP models.Assess performance accuracy and capability of ML techniques for constructing SFP models.Provide comparison between the ML and statistical techniques.Provide comparison of performance accuracy of different ML techniques.Summarize the strength and weakness of the ML techniques.Provides future guidelines to software practitioners and researchers. BackgroundSoftware fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. MethodIn this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. ResultsIn this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. ConclusionBased on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work.},
journal = {Appl. Soft Comput.},
month = feb,
pages = {504–518},
numpages = {15},
keywords = {Machine learning, Software fault proneness, Systematic literature review}
}

@article{10.1155/2020/8858010,
author = {Shen, Zhidong and Chen, Si and Coppolino, Luigi},
title = {A Survey of Automatic Software Vulnerability Detection, Program Repair, and Defect Prediction Techniques},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1939-0114},
url = {https://doi.org/10.1155/2020/8858010},
doi = {10.1155/2020/8858010},
abstract = {Open source software has been widely used in various industries due to its openness and flexibility, but it also brings potential software security problems. Together with the large-scale increase in the number of software and the increase in complexity, the traditional manual methods to deal with these security issues are inefficient and cannot meet the current cyberspace security requirements. Therefore, it is an important research topic for researchers in the field of software security to develop more intelligent technologies to apply to potential security issues in software. The development of deep learning technology has brought new opportunities for the study of potential security issues in software, and researchers have successively proposed many automation methods. In this paper, these automation technologies are evaluated and analysed in detail from three aspects: software vulnerability detection, software program repair, and software defect prediction. At the same time, we point out some problems of these research methods, give corresponding solutions, and finally look forward to the application prospect of deep learning technology in automated software vulnerability detection, automated program repair, and automated defect prediction.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {16}
}

@inproceedings{10.1145/3439961.3439971,
author = {Correia, Jo\~{a}o Lucas and Pereira, Juliana Alves and Mello, Rafael and Garcia, Alessandro and Fonseca, Baldoino and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Kalinowski, Marcos and Cerqueira, Renato and Tiengo, Willy},
title = {Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439971},
doi = {10.1145/3439961.3439971},
abstract = {Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {10},
numpages = {10},
keywords = {Empirical Study, Machine Learning, Practitioner, Software Engineering},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@article{10.1007/s00500-020-05480-9,
author = {Venkatesh, R. and Balasubramanian, C. and Kaliappan, M.},
title = {RETRACTED ARTICLE: Rainfall prediction using
         generative adversarial networks with convolution neural network},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {6},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-020-05480-9},
doi = {10.1007/s00500-020-05480-9},
abstract = {In recent days, deep learning becomes a successful approach to solving
            complex problems and analyzing the huge volume of data. The proposed system developed a
            rainfall prediction system using generative adversarial networks to analyze rainfall
            data of India and predict the future rainfall. The proposed system used a GAN network in
            which long short-term memory (LSTM) network algorithm is used as a generator and
            convolution neural network model is used as a discriminator. LSTM is much suitable to
            predict time series data such as rainfall data. The experimental results reveal that the
            proposed system provides the predicted results with 99% of accuracy. Rainfall prediction
            helps farmers to cultivate their crops and improved their economy as well as country’s
            economy.},
journal = {Soft Comput.},
month = mar,
pages = {4725–4738},
numpages = {14},
keywords = {Convolution neural network, Deep learning, Generative adversarial networks, Long short-term memory networks}
}

@inproceedings{10.1145/3183399.3183402,
author = {Koch, Patrick and Schekotihin, Konstantin and Jannach, Dietmar and Hofer, Birgit and Wotawa, Franz and Schmitz, Thomas},
title = {Combining spreadsheet smells for improved fault prediction},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183402},
doi = {10.1145/3183399.3183402},
abstract = {Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {25–28},
numpages = {4},
keywords = {fault prediction, spreadsheet QA, spreadsheet smells},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1109/SKG.2008.43,
author = {Tian, Yingjie and Chen, Chuanliang and Zhang, Chunhua},
title = {AODE for Source Code Metrics for Improved Software Maintainability},
year = {2008},
isbn = {9780769534015},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SKG.2008.43},
doi = {10.1109/SKG.2008.43},
abstract = {Software metrics are collected at various phases of the whole software development process, in order to assist in monitoring and controlling the software quality. However, software quality control is complicated, because of the complex relationship between these metrics and the attributes of a software development process. To solve this problem, many excellent techniques have been introduced into software maintainability domain. In this paper, we propose a novel classification method--Aggregating One-Dependence Estimators (AODE) to support and enhance our understanding of software metrics and their relationship to software quality. Experiments show that performance of AODE is much better than eight traditional classification methods and it is a promising method for software quality prediction. Furthermore, we present a Symmetrical Uncertainty (SU) based feature selection method to reduce source code metrics taking part in classification, make these classifiers more efficient and keep their performances not undermined meanwhile. Our empirical study shows the promising capability of SU for selecting relevant metrics and preserving original performances of the classifiers.},
booktitle = {Proceedings of the 2008 Fourth International Conference on Semantics, Knowledge and Grid},
pages = {330–335},
numpages = {6},
keywords = {Bayesian Classifier, Software Maintainability, Source Code Metrics, Symmetrical Uncertainty},
series = {SKG '08}
}

@inproceedings{10.1109/ASE.2009.76,
author = {Shivaji, Shivkumar and Jr., E. James Whitehead and Akella, Ram and Kim, Sunghun},
title = {Reducing Features to Improve Bug Prediction},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.76},
doi = {10.1109/ASE.2009.76},
abstract = {Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naive Bayes and Support Vector Machine (SVM) classifiers is characterized.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {600–604},
numpages = {5},
keywords = {Bug prediction, Feature Selection, Machine Learning, Reliability},
series = {ASE '09}
}

@inproceedings{10.5555/2040660.2040688,
author = {Wahyudin, Dindin and Ramler, Rudolf and Biffl, Stefan},
title = {A framework for defect prediction in specific software project contexts},
year = {2008},
isbn = {9783642223853},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software defect prediction has drawn the attention of many researchers in empirical software engineering and software maintenance due to its importance in providing quality estimates and to identify the needs for improvement from project management perspective. However, most defect prediction studies seem valid primarily in a particular context and little concern is given on how to find out which prediction model is well suited for a given project context. In this paper we present a framework for conducting software defect prediction as aid for the project manager in the context of a particular project or organization. The framework has been aligned with practitioners' requirements and is supported by our findings from a systematical literature review on software defect prediction. We provide a guide to the body of existing studies on defect prediction by mapping the results of the systematic literature review to the framework.},
booktitle = {Proceedings of the Third IFIP TC 2 Central and East European Conference on Software Engineering Techniques},
pages = {261–274},
numpages = {14},
keywords = {metric-based defect prediction, software defect prediction, systematical literature review},
location = {Brno, Czech Republic},
series = {CEE-SET'08}
}

@inproceedings{10.1145/3239235.3239248,
author = {Rosenberg, Carl Martin and Moonen, Leon},
title = {Improving problem identification via automated log clustering using dimensionality reduction},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239248},
doi = {10.1145/3239235.3239248},
abstract = {Background: Continuous engineering practices, such as continuous integration and continuous deployment, see increased adoption in modern software development. A frequently reported challenge for adopting these practices is the need to make sense of the large amounts of data that they generate.Goal: We consider the problem of automatically grouping logs of runs that failed for the same underlying reasons, so that they can be treated more effectively, and investigate the following questions: (1) Does an approach developed to identify problems in system logs generalize to identifying problems in continuous deployment logs? (2) How does dimensionality reduction affect the quality of automated log clustering? (3) How does the criterion used for merging clusters in the clustering algorithm affect clustering quality?Method: We replicate and extend earlier work on clustering system log files to assess its generalization to continuous deployment logs. We consider the optional inclusion of one of these dimensionality reduction techniques: Principal Component Analysis (PCA), Latent Semantic Indexing (LSI), and Non-negative Matrix Factorization (NMF). Moreover, we consider three alternative cluster merge criteria (Single Linkage, Average Linkage, and Weighted Linkage), in addition to the Complete Linkage criterion used in earlier work. We empirically evaluate the 16 resulting configurations on continuous deployment logs provided by our industrial collaborator.Results: Our study shows that (1) identifying problems in continuous deployment logs via clustering is feasible, (2) including NMF significantly improves overall accuracy and robustness, and (3) Complete Linkage performs best of all merge criteria analyzed.Conclusions: We conclude that problem identification via automated log clustering is improved by including dimensionality reduction, as it decreases the pipeline's sensitivity to parameter choice, thereby increasing its robustness for handling different inputs.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {16},
numpages = {10},
keywords = {continuous engineering, failure diagnosis, log analysis, log mining},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1007/s10664-011-9173-9,
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
title = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9173-9},
doi = {10.1007/s10664-011-9173-9},
abstract = {Reliably predicting software defects is one of the holy grails of software engineering. Researchers have devised and implemented a plethora of defect/bug prediction approaches varying in terms of accuracy, complexity and the input data they require. However, the absence of an established benchmark makes it hard, if not impossible, to compare approaches. We present a benchmark for defect prediction, in the form of a publicly available dataset consisting of several software systems, and provide an extensive comparison of well-known bug prediction approaches, together with novel approaches we devised. We evaluate the performance of the approaches using different performance indicators: classification of entities as defect-prone or not, ranking of the entities, with and without taking into account the effort to review an entity. We performed three sets of experiments aimed at (1) comparing the approaches across different systems, (2) testing whether the differences in performance are statistically significant, and (3) investigating the stability of approaches across different learners. Our results indicate that, while some approaches perform better than others in a statistically significant manner, external validity in defect prediction is still an open problem, as generalizing results to different contexts/learners proved to be a partially unsuccessful endeavor.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {531–577},
numpages = {47},
keywords = {Change metrics, Defect prediction, Source code metrics}
}

@inproceedings{10.1109/MOBILESoft.2017.58,
author = {Catolino, Gemma},
title = {Just-in-time bug prediction in mobile applications: the domain matters!},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.58},
doi = {10.1109/MOBILESoft.2017.58},
abstract = {Bug prediction allows developers to focus testing efforts on specific areas of software systems. While this topic has been extensively studied for traditional applications, investigations on mobile apps are still missing. In this paper we preliminarily study the effectiveness of a previously defined Just-In-Time bug prediction model applied on five mobile apps. Key results indicate the poor performance of the model and the need of further research on the topic.},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {201–202},
numpages = {2},
keywords = {bug prediction, metrics, mobile applications},
location = {Buenos Aires, Argentina},
series = {MOBILESoft '17}
}

@article{10.5555/3271870.3271878,
title = {Software fault prediction using firefly algorithm},
year = {2018},
issue_date = {January 2018},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {6},
number = {3–4},
issn = {1758-8715},
abstract = {The software fault prediction SFP literature has shown an immense growth of the research studies involving the artificial neural network ANN based fault prediction models. However, the default gradient descent back propagation neural networks BPNNs have a high risk of getting stuck in the local minima of the search space. A class of nature inspired computing methods overcomes this disadvantage of BPNNs and has helped ANNs to evolve into a class of adaptive ANN. In this work, we propose a hybrid SFP model built using firefly algorithm FA and artificial neural network ANN, along with an empirical comparison with GA and PSO based evolutionary methods in optimising the connection weights of ANN. Seven different datasets were involved and MSE and the confusion matrix parameters were used for performance evaluation. The results have shown that FA-ANN model has performed better than the genetic and particle swarm optimised ANN fault prediction models.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {356–377},
numpages = {22}
}

@article{10.1007/s11219-017-9391-5,
author = {Fern\'{a}ndez-Diego, Marta and Gonz\'{a}lez-Ladr\'{o}n-De-Guevara, Fernando},
title = {Application of mutual information-based sequential feature selection to ISBSG mixed data},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9391-5},
doi = {10.1007/s11219-017-9391-5},
abstract = {There is still little research work focused on feature selection (FS) techniques including both categorical and continuous features in Software Development Effort Estimation (SDEE) literature. This paper addresses the problem of selecting the most relevant features from ISBSG (International Software Benchmarking Standards Group) dataset to be used in SDEE. The aim is to show the usefulness of splitting the ranked list of features provided by a mutual information-based sequential FS approach in two, regarding categorical and continuous features. These lists are later recombined according to the accuracy of a case-based reasoning model. Thus, four FS algorithms are compared using a complete dataset with 621 projects and 12 features from ISBSG. On the one hand, two algorithms just consider the relevance, while the remaining two follow the criterion of maximizing relevance and also minimizing redundancy between any independent feature and the already selected features. On the other hand, the algorithms that do not discriminate between continuous and categorical features consider just one list, whereas those that differentiate them use two lists that are later combined. As a result, the algorithms that use two lists present better performance than those algorithms that use one list. Thus, it is meaningful to consider two different lists of features so that the categorical features may be selected more frequently. We also suggest promoting the usage of Application Group, Project Elapsed Time, and First Data Base System features with preference over the more frequently used Development Type, Language Type, and Development Platform.},
journal = {Software Quality Journal},
month = dec,
pages = {1299–1325},
numpages = {27},
keywords = {Feature selection, ISBSG, Mutual information, Software development effort estimation, k-nearest neighbor}
}

@article{10.1016/j.jss.2016.05.015,
author = {Chen, Tse-Hsun and Shang, Weiyi and Nagappan, Meiyappan and Hassan, Ahmed E. and Thomas, Stephen W.},
title = {Topic-based software defect explanation},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {129},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.015},
doi = {10.1016/j.jss.2016.05.015},
abstract = {Some topics are more defect-prone than others.Defect-prone topics are likely to remain so over time.Our topic-based metrics provide additional defect explanatory to baseline metrics.Our metrics outperform state-of-the-art topic-based cohesion and coupling metrics. Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the systems business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret.},
journal = {J. Syst. Softw.},
month = jul,
pages = {79–106},
numpages = {28},
keywords = {Code quality, Cohesion, Coupling, LDA, Metrics, Topic modeling}
}

@article{10.1007/s11390-020-9668-1,
author = {Elmidaoui, Sara and Cheikhi, Laila and Idri, Ali and Abran, Alain},
title = {Machine Learning Techniques for Software Maintainability Prediction: Accuracy Analysis},
year = {2020},
issue_date = {Oct 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-9668-1},
doi = {10.1007/s11390-020-9668-1},
abstract = {Maintaining software once implemented on the end-user side is laborious and, over its lifetime, is most often considerably more expensive than the initial software development. The prediction of software maintainability has emerged as an important research topic to address industry expectations for reducing costs, in particular, maintenance costs. Researchers and practitioners have been working on proposing and identifying a variety of techniques ranging from statistical to machine learning (ML) for better prediction of software maintainability. This review has been carried out to analyze the empirical evidence on the accuracy of software product maintainability prediction (SPMP) using ML techniques. This paper analyzes and discusses the findings of 77 selected studies published from 2000 to 2018 according to the following criteria: maintainability prediction techniques, validation methods, accuracy criteria, overall accuracy of ML techniques, and the techniques offering the best performance. The review process followed the well-known systematic review process. The results show that ML techniques are frequently used in predicting maintainability. In particular, artificial neural network (ANN), support vector machine/regression (SVM/R), regression &amp; decision trees (DT), and fuzzy &amp; neuro fuzzy (FNF) techniques are more accurate in terms of PRED and MMRE. The N-fold and leave-one-out cross-validation methods, and the MMRE and PRED accuracy criteria are frequently used in empirical studies. In general, ML techniques outperformed non-machine learning techniques, e.g., regression analysis (RA) techniques, while FNF outperformed SVM/R, DT, and ANN in most experiments. However, while many techniques were reported superior, no specific one can be identified as the best.},
journal = {J. Comput. Sci. Technol.},
month = oct,
pages = {1147–1174},
numpages = {28},
keywords = {accuracy criterion, accuracy value, machine learning technique, maintainability prediction}
}

@inproceedings{10.1007/978-3-319-47955-2_19,
author = {Murillo-Morera, Juan and Castro-Herrera, Carlos and Arroyo, Javier and Fuentes-Fern\'{a}ndez, Rub\'{e}n},
title = {An Empirical Validation of Learning Schemes Using an Automated Genetic Defect Prediction Framework},
year = {2016},
isbn = {978-3-319-47954-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-47955-2_19},
doi = {10.1007/978-3-319-47955-2_19},
abstract = {Today, it is common for software projects to collect measurement data through development processes. With these data, defect prediction software can try to estimate the defect proneness of a software module, with the objective of assisting and guiding software practitioners. With timely and accurate defect predictions, practitioners can focus their limited testing resources on higher risk areas. This paper reports a benchmarking study that uses a genetic algorithm that automatically generates and compares different learning schemes (preprocessing + attribute selection + learning algorithms). Performance of the software development defect prediction models (using AUC, Area Under the Curve) was validated using NASA-MDP and PROMISE data sets. Twelve data sets from NASA-MDP (8) and PROMISE (4) projects were analyzed running a -fold cross-validation. We used a genetic algorithm to select the components of the learning schemes automatically, and to evaluate and report those with the best performance. In all, 864 learning schemes were studied. The most common learning schemes were: data preprocessors: Log and CoxBox + attribute selectors: Backward Elimination, BestFirst and LinearForwardSelection + learning algorithms: NaiveBayes, NaiveBayesSimple, SimpleLogistic, MultilayerPerceptron, Logistic, LogitBoost, BayesNet, and OneR. The genetic algorithm reported steady performance and runtime among data sets, according to statistical analysis.},
booktitle = {Advances in Artificial Intelligence - IBERAMIA 2016: 15th Ibero-American Conference on AI, San Jos\'{e}, Costa Rica, November 23-25, 2016, Proceedings},
pages = {222–234},
numpages = {13},
keywords = {Software quality, Fault prediction models, Genetic algorithms, Learning schemes, Learning algorithms, Machine learning},
location = {San Jos\'{e}, Costa Rica}
}

@article{10.1145/3408302,
author = {Pantiuchina, Jevgenija and Zampetti, Fiorella and Scalabrino, Simone and Piantadosi, Valentina and Oliveto, Rocco and Bavota, Gabriele and Penta, Massimiliano Di},
title = {Why Developers Refactor Source Code: A Mining-based Study},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3408302},
doi = {10.1145/3408302},
abstract = {Refactoring aims at improving code non-functional attributes without modifying its external behavior. Previous studies investigated the motivations behind refactoring by surveying developers. With the aim of generalizing and complementing their findings, we present a large-scale study quantitatively and qualitatively investigating why developers perform refactoring in open source projects. First, we mine 287,813 refactoring operations performed in the history of 150 systems. Using this dataset, we investigate the interplay between refactoring operations and process (e.g., previous changes/fixes) and product (e.g., quality metrics) metrics. Then, we manually analyze 551 merged pull requests implementing refactoring operations and classify the motivations behind the implemented refactorings (e.g., removal of code duplication). Our results led to (i) quantitative evidence of the relationship existing between certain process/product metrics and refactoring operations and (ii) a detailed taxonomy, generalizing and complementing the ones existing in the literature, of motivations pushing developers to refactor source code.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {29},
numpages = {30},
keywords = {Refactoring, empirical software engineering}
}

@inproceedings{10.1145/2851613.2851788,
author = {das D\^{o}res, Silvia N. and Alves, Luciano and Ruiz, Duncan D. and Barros, Rodrigo C.},
title = {A meta-learning framework for algorithm recommendation in software fault prediction},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851788},
doi = {10.1145/2851613.2851788},
abstract = {Software fault prediction is a significant part of software quality assurance and it is commonly used to detect faulty software modules based on software measurement data. Several machine learning based approaches have been proposed for generating predictive models from collected data, although none has become standard given the specificities of each software project. Hence, we believe that recommending the best algorithm for each project is much more important and useful than developing a single algorithm for being used in any project. For achieving that goal, we propose in this paper a novel framework for recommending machine learning algorithms that is capable of automatically identifying the most suitable algorithm according to the software project that is being considered. Our solution, namely SFP-MLF, makes use of the meta-learning paradigm in order to learn the best learner for a particular project. Results show that the SFP-MLF framework provides both the best single algorithm recommendation and also the best ranking recommendation for the software fault prediction problem.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1486–1491},
numpages = {6},
keywords = {algorithm recommendation, machine learning, meta-learning, software fault prediction, software quality},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1109/ASE.2013.6693087,
author = {Jiang, Tian and Tan, Lin and Kim, Sunghun},
title = {Personalized defect prediction},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693087},
doi = {10.1109/ASE.2013.6693087},
abstract = {Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance.This paper proposes personalized defect prediction--building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java--the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {279–289},
numpages = {11},
keywords = {change classification, machine learning, personalized defect prediction, software reliability},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1007/s10515-017-0220-7,
author = {Li, Zhiqiang and Jing, Xiao-Yuan and Wu, Fei and Zhu, Xiaoke and Xu, Baowen and Ying, Shi},
title = {Cost-sensitive transfer kernel canonical correlation analysis for heterogeneous defect prediction},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0220-7},
doi = {10.1007/s10515-017-0220-7},
abstract = {Cross-project defect prediction (CPDP) refers to predicting defects in a target project using prediction models trained from historical data of other source projects. And CPDP in the scenario where source and target projects have different metric sets is called heterogeneous defect prediction (HDP). Recently, HDP has received much research interest. Existing HDP methods only consider the linear correlation relationship among the features (metrics) of the source and target projects, and such models are insufficient to evaluate nonlinear correlation relationship among the features. So these methods may suffer from the linearly inseparable problem in the linear feature space. Furthermore, existing HDP methods do not take the class imbalance problem into consideration. Unfortunately, the imbalanced nature of software defect datasets increases the learning difficulty for the predictors. In this paper, we propose a new cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) approach for HDP. CTKCCA can not only make the data distributions of source and target projects much more similar in the nonlinear feature space, where the learned features have favorable separability, but also utilize the different misclassification costs for defective and defect-free classes to alleviate the class imbalance problem. We perform the Friedman test with Nemenyi's post-hoc statistical test and the Cliff's delta effect size test for the evaluation. Extensive experiments on 28 public projects from five data sources indicate that: (1) CTKCCA significantly performs better than the related CPDP methods; (2) CTKCCA performs better than the related state-of-the-art HDP methods.},
journal = {Automated Software Engg.},
month = jun,
pages = {201–245},
numpages = {45},
keywords = {Class imbalance, Cost-sensitive learning, Heterogeneous defect prediction, Kernel canonical correlation analysis, Transfer learning}
}

@article{10.1016/j.asoc.2019.02.008,
author = {Juneja, Kapil},
title = {A fuzzy-filtered neuro-fuzzy framework for software fault prediction for inter-version and inter-project evaluation},
year = {2019},
issue_date = {Apr 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {77},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2019.02.008},
doi = {10.1016/j.asoc.2019.02.008},
journal = {Appl. Soft Comput.},
month = apr,
pages = {696–713},
numpages = {18},
keywords = {Defect Prediction, Inter project, Intra project, Classification, Fuzzy}
}

@article{10.1016/j.engappai.2009.10.001,
author = {Pendharkar, Parag C.},
title = {Exhaustive and heuristic search approaches for learning a software defect prediction model},
year = {2010},
issue_date = {February, 2010},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {23},
number = {1},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2009.10.001},
doi = {10.1016/j.engappai.2009.10.001},
abstract = {In this paper, we propose a software defect prediction model learning problem (SDPMLP) where a classification model selects appropriate relevant inputs, from a set of all available inputs, and learns the classification function. We show that the SDPMLP is a combinatorial optimization problem with factorial complexity, and propose two hybrid exhaustive search and probabilistic neural network (PNN), and simulated annealing (SA) and PNN procedures to solve it. For small size SDPMLP, exhaustive search PNN works well and provides an (all) optimal solution(s). However, for large size SDPMLP, the use of exhaustive search PNN approach is not pragmatic and only the SA-PNN allows us to solve the SDPMLP in a practical time limit. We compare the performance of our hybrid approaches with traditional classification algorithms and find that our hybrid approaches perform better than traditional classification algorithms.},
journal = {Eng. Appl. Artif. Intell.},
month = feb,
pages = {34–40},
numpages = {7},
keywords = {Exhaustive search, Heuristics, Probabilistic neural networks, Simulated annealing, Software engineering}
}

@inproceedings{10.1145/3416508.3417118,
author = {Amasaki, Sousuke and Aman, Hirohisa and Yokogawa, Tomoyuki},
title = {An exploratory study on applicability of cross project defect prediction approaches to cross-company effort estimation},
year = {2020},
isbn = {9781450381277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416508.3417118},
doi = {10.1145/3416508.3417118},
abstract = {BACKGROUND: Research on software effort estimation has been active for decades, especially in developing effort estimation models. Effort estimation models need a dataset collected from completed projects similar to a project to be estimated. The similarity suffers from dataset shift, and cross-company software effort estimation (CCSEE) gets an attractive research topic. A recent study on the dataset shift problem examined the applicability and the effectiveness of cross-project defect prediction (CPDP) approaches. It was insufficient to bring a conclusion due to a limited number of examined approaches. AIMS: To investigate the characteristics of CPDP approaches that are applicable and effective for dataset shift problem in effort estimation. METHOD: We first reviewed the characteristics of 24 CPDP approaches to find applicable approaches. Next, we investigated their effectiveness in effort estimation performance with ten dataset configurations. RESULTS: 16 out of 24 CPDP approaches implemented in CrossPare framework were found to be applicable to CCSEE. However, only one approach could improve the effort estimation performance. Most of the others degraded it and were harmful. CONCLUSIONS: Most of the CPDP approaches we examined were helpless for CCSEE.},
booktitle = {Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {71–80},
numpages = {10},
keywords = {cross-company effort estimation, cross-project defect prediction, empirical evaluation},
location = {Virtual, USA},
series = {PROMISE 2020}
}

@inproceedings{10.1145/3387906.3388618,
author = {Cruz, Daniel and Santana, Amanda and Figueiredo, Eduardo},
title = {Detecting bad smells with machine learning algorithms: an empirical study},
year = {2020},
isbn = {9781450379601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387906.3388618},
doi = {10.1145/3387906.3388618},
abstract = {Bad smells are symptoms of bad design choices implemented on the source code. They are one of the key indicators of technical debts, specifically, design debt. To manage this kind of debt, it is important to be aware of bad smells and refactor them whenever possible. Therefore, several bad smell detection tools and techniques have been proposed over the years. These tools and techniques present different strategies to perform detections. More recently, machine learning algorithms have also been proposed to support bad smell detection. However, we lack empirical evidence on the accuracy and efficiency of these machine learning based techniques. In this paper, we present an evaluation of seven different machine learning algorithms on the task of detecting four types of bad smells. We also provide an analysis of the impact of software metrics for bad smell detection using a unified approach for interpreting the models' decisions. We found that with the right optimization, machine learning algorithms can achieve good performance (F1 score) for two bad smells: God Class (0.86) and Refused Parent Bequest (0.67). We also uncovered which metrics play fundamental roles for detecting each bad smell.},
booktitle = {Proceedings of the 3rd International Conference on Technical Debt},
pages = {31–40},
numpages = {10},
keywords = {bad smells detection, empirical software engineering, machine learning, software measurement, software quality},
location = {Seoul, Republic of Korea},
series = {TechDebt '20}
}

@inproceedings{10.1145/2875913.2875922,
author = {Tang, Hao and Lan, Tian and Hao, Dan and Zhang, Lu},
title = {Enhancing Defect Prediction with Static Defect Analysis},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875922},
doi = {10.1145/2875913.2875922},
abstract = {In the software development process, how to develop better software at lower cost has been a major issue of concern. One way that helps is to find more defects as early as possible, on which defect prediction can provide effective guidance. The most popular defect prediction technique is to build defect prediction models based on machine learning. To improve the performance of defect prediction model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined "defect patterns", its result is useful for locating defects. However, defect prediction and static defect analysis are supposed to be two parallel areas due to the differences in research motivation, solution and granularity.In this paper, we present a possible approach to improve the performance of defect prediction with the help of static analysis techniques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of defect prediction models. Based on this approach, we implemented a defect prediction tool and set up experiments to measure the effect of the features.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {43–51},
numpages = {9},
keywords = {Defect, code feature, defect pattern, machine learning, predictive model, static defect analyzer},
location = {Wuhan, China},
series = {Internetware '15}
}

@article{10.1007/s10664-014-9346-4,
author = {Ryu, Duksan and Choi, Okjoo and Baik, Jongmoon},
title = {Value-cognitive boosting with a support vector machine for cross-project defect prediction},
year = {2016},
issue_date = {February  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9346-4},
doi = {10.1007/s10664-014-9346-4},
abstract = {It is well-known that software defect prediction is one of the most important tasks for software quality improvement. The use of defect predictors allows test engineers to focus on defective modules. Thereby testing resources can be allocated effectively and the quality assurance costs can be reduced. For within-project defect prediction (WPDP), there should be sufficient data within a company to train any prediction model. Without such local data, cross-project defect prediction (CPDP) is feasible since it uses data collected from similar projects in other companies. Software defect datasets have the class imbalance problem increasing the difficulty for the learner to predict defects. In addition, the impact of imbalanced data on the real performance of models can be hidden by the performance measures chosen. We investigate if the class imbalance learning can be beneficial for CPDP. In our approach, the asymmetric misclassification cost and the similarity weights obtained from distributional characteristics are closely associated to guide the appropriate resampling mechanism. We performed the effect size A-statistics test to evaluate the magnitude of the improvement. For the statistical significant test, we used Wilcoxon rank-sum test. The experimental results show that our approach can provide higher prediction performance than both the existing CPDP technique and the existing class imbalance technique.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {43–71},
numpages = {29},
keywords = {Boosting, Class imbalance, Cross-project defect prediction, Transfer learning}
}

@inproceedings{10.1145/3318216.3363305,
author = {Soualhia, Mbarka and Fu, Chunyan and Khomh, Foutse},
title = {Infrastructure fault detection and prediction in edge cloud environments},
year = {2019},
isbn = {9781450367332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318216.3363305},
doi = {10.1145/3318216.3363305},
abstract = {As an emerging 5G system component, edge cloud becomes one of the key enablers to provide services such us mission critical, IoT and content delivery applications. However, because of limited fail-over mechanisms in edge clouds, faults (e.g., CPU or HDD faults) are highly undesirable. When infrastructure faults occur in edge clouds, they can accumulate and propagate; leading to severe degradation of system and application performance. It is therefore crucial to identify these faults early on and mitigate them. In this paper, we propose a framework to detect and predict several faults at infrastructure-level of edge clouds using supervised machine learning and statistical techniques. The proposed framework is composed of three main components responsible for: (1) data pre-processing, (2) fault detection, and (3) fault prediction. The results show that the framework allows to timely detect and predict several faults online. For instance, using Support Vector Machine (SVM), Random Forest(RF) and Neural Network(NN)models, the framework is able to detect non-fatal CPU and HDD overload faults with an F1 score of more than 95%. For the prediction, the Convolutional Neural Network (CNN) and Long Short Term Memory (LSTM) have comparable accuracy at 96.47% vs. 96.88% for CPU-overload fault and 85.52% vs. 88.73% for network fault.},
booktitle = {Proceedings of the 4th ACM/IEEE Symposium on Edge Computing},
pages = {222–235},
numpages = {14},
location = {Arlington, Virginia},
series = {SEC '19}
}

@article{10.1016/j.asoc.2016.08.025,
author = {Erturk, Ezgi and Akcapinar Sezer, Ebru},
title = {Iterative software fault prediction with a hybrid approach},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.025},
doi = {10.1016/j.asoc.2016.08.025},
abstract = {Display Omitted To make software fault prediction (SFP) more beneficial, it should be into service at the beginning of the project.A novel prediction methodology based on existing methods (i.e. FIS, ANN) are proposed here.Version based development of software projects are considered to design an iterative prediction approach.Proposed methodology is developed as Eclipse plugin.Experiments show that proposed methodology gives promising results to use SFP in daily routine of software development phases. In this study, we consider a software fault prediction task that can assist a developer during the lifetime of a project. We aim to improve the performance of software fault prediction task while keeping it as applicable. Initial predictions are constructed by Fuzzy Inference Systems (FISs), whereas subsequent predictions are performed by data-driven methods. In this paper, an Artificial Neural Network and Adaptive Neuro Fuzzy Inference System are employed. We propose an iterative prediction model that begins with a FIS when no data are available for the software project and continues with a data-driven method when adequate data become available. To prove the usability of this iterative prediction approach, software fault prediction experiments are performed using expert knowledge for the initial version and information about previous versions for subsequent versions. The datasets employed in this paper comprise different versions of Ant, jEdit, Camel, Xalan, Log4j and Lucene projects from the PROMISE repository. The metrics of the models are common object-oriented metrics, such as coupling between objects, weighted methods per class and response for a class. The results of the models are evaluated according to the receiver operating characteristics with the area under the curve approach. The results indicate that the iterative software fault prediction is successful and can be transformed into a tool that can automatically locate fault-prone modules due to its well-organized information flow. We also implement the proposed methodology as a plugin for the Eclipse environment.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1020–1033},
numpages = {14},
keywords = {Adaptive neuro fuzzy inference system, Artificial neural network, Fuzzy inference systems, Iterative prediction, Software fault prediction}
}

@inproceedings{10.1145/2590748.2590755,
author = {Rathore, Santosh Singh and Gupta, Atul},
title = {A comparative study of feature-ranking and feature-subset selection techniques for improved fault prediction},
year = {2014},
isbn = {9781450327763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2590748.2590755},
doi = {10.1145/2590748.2590755},
abstract = {The quality of a fault prediction model depends on the software metrics that are used to build the prediction model. Feature selection represents a process of selecting a subset of relevant features that may lead to build improved prediction models. Feature selection techniques can be broadly categorized into two subcategories: feature-ranking and feature-subset selection. In this paper, we present a comparative investigation of seven feature-ranking techniques and eight feature-subset selection techniques for improved fault prediction. The performance of these feature selection techniques is evaluated using two popular machine-learning classifiers: Naive Bayes and Random Forest, over fourteen software project's fault-datasets obtained from the PROMISE data repository. The performances were measured using F-measure and AUC values. Our results demonstrated that feature-ranking techniques produced better results compared to feature-subset selection techniques. Among, the feature-ranking techniques used in the study, InfoGain and PCA techniques provided the best performance over all the datasets, while for feature-subset selection techniques ClassifierSubsetEval and Logistic Regression produced better results against their peers.},
booktitle = {Proceedings of the 7th India Software Engineering Conference},
articleno = {7},
numpages = {10},
keywords = {fault prediction, feature selection, feature-ranking, filters, software metrics, wrappers},
location = {Chennai, India},
series = {ISEC '14}
}

@inproceedings{10.1145/3340482.3342744,
author = {Pecorelli, Fabiano and Di Nucci, Dario and De Roover, Coen and De Lucia, Andrea},
title = {On the role of data balancing for machine learning-based code smell detection},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342744},
doi = {10.1145/3340482.3342744},
abstract = {Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics (e.g., code metrics, process metrics) is used to detect smelly code components. However, these techniques suffer of subjective interpretation, low agreement between detectors, and threshold dependability. To overcome these limitations, previous work applied Machine Learning techniques that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine Learning is not always suitable for code smell detection due to the highly unbalanced nature of the problem. In this study we investigate several approaches able to mitigate data unbalancing issues to understand their impact on ML-based approaches for code smell detection. Our findings highlight a number of limitations and open issues with respect to the usage of data balancing in ML-based code smell detection.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {19–24},
numpages = {6},
keywords = {Code Smells, Data Balancing, Machine Learning},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@article{10.1155/2021/6662932,
author = {Gupta, Mansi and Rajnish, Kumar and Bhattacharjee, Vandana and Gou, Jianping},
title = {Impact of Parameter Tuning for Optimizing Deep Neural Network Models for Predicting Software Faults},
year = {2021},
issue_date = {2021},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2021},
issn = {1058-9244},
url = {https://doi.org/10.1155/2021/6662932},
doi = {10.1155/2021/6662932},
abstract = {Deep neural network models built by the appropriate design decisions are crucial to obtain the desired classifier performance. This is especially desired when predicting fault proneness of software modules. When correctly identified, this could help in reducing the testing cost by directing the efforts more towards the modules identified to be fault prone. To be able to build an efficient deep neural network model, it is important that the parameters such as number of hidden layers, number of nodes in each layer, and training details such as learning rate and regularization methods be investigated in detail. The objective of this paper is to show the importance of hyperparameter tuning in developing efficient deep neural network models for predicting fault proneness of software modules and to compare the results with other machine learning algorithms. It is shown that the proposed model outperforms the other algorithms in most cases.},
journal = {Sci. Program.},
month = jan,
numpages = {17}
}

@inproceedings{10.5555/3507788.3507798,
author = {Khan, Md Asif and Azim, Akramul and Liscano, Ramiro and Smith, Kevin and Chang, Yee-Kang and Garcon, Sylvain and Tauseef, Qasim},
title = {Failure prediction using machine learning in IBM WebSphere liberty continuous integration environment},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {The growing complexity and dependencies of software have increased the importance of testing to ensure that frequent changes do not adversely affect existing functionality. Moreover, continuous integration comes with unique challenges associated with maintaining a stable build environment. Several studies have shown that the testing environment becomes more efficient with proper test case prioritization techniques. However, an application's dynamic behavior makes it challenging to derive test case prioritization techniques for achieving optimal results. With the advance of machine learning, the context of an application execution can be analyzed to select and prioritize test suites more efficiently.Test suite prioritization techniques aim to reorder test suites' execution to deliver high quality, maintainable software at lower costs to meet specific objectives such as revealing failures earlier. The state-of-the-art techniques on test prioritization in a continuous integration environment focus on relatively small, single-language, unit-tested projects. This paper compares and analyzes Machine learning-based test suite prioritization technique on two large-scale dataset collected from a continuous integration environment Google and IBM respectively. We optimize hyperparameters and report on experiments' findings by using different machine learning algorithms for test suite prioritization. Our optimized algorithms prioritize test suites with 93% accuracy on average and require 20% fewer test suites to detect 80% of the failures than the test suites prioritized randomly.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {63–72},
numpages = {10},
keywords = {CI, continuous integration, machine learning, test prioritization},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1007/978-3-030-91265-9_11,
author = {Wei, Shaozhi and Mo, Ran and Xiong, Pu and Zhang, Siyuan and Zhao, Yang and Li, Zengyang},
title = {Predicting and Monitoring Bug-Proneness at the Feature Level},
year = {2021},
isbn = {978-3-030-91264-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-91265-9_11},
doi = {10.1007/978-3-030-91265-9_11},
abstract = {Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 7th International Symposium, SETTA 2021, Beijing, China, November 25–27, 2021, Proceedings},
pages = {201–218},
numpages = {18},
keywords = {Code metrics, Machine learning, Feature bug prediction},
location = {Beijing, China}
}

@article{10.1016/j.infsof.2013.05.002,
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
title = {A study of subgroup discovery approaches for defect prediction},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.05.002},
doi = {10.1016/j.infsof.2013.05.002},
abstract = {Context: Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective: In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method: We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results: The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions: The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1810–1822},
numpages = {13},
keywords = {Defect prediction, Imbalanced datasets, Rules, Subgroup discovery}
}

@inproceedings{10.1145/3468264.3468606,
author = {Bogomolov, Egor and Kovalenko, Vladimir and Rebryk, Yurii and Bacchelli, Alberto and Bryksin, Timofey},
title = {Authorship attribution of source code: a language-agnostic approach and applicability in software engineering},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468606},
doi = {10.1145/3468264.3468606},
abstract = {Authorship attribution (i.e., determining who is the author of a piece of source code) is an established research topic. State-of-the-art results for the authorship attribution problem look promising for the software engineering field, where they could be applied to detect plagiarized code and prevent legal issues. With this article, we first introduce a new language-agnostic approach to authorship attribution of source code. Then, we discuss limitations of existing synthetic datasets for authorship attribution, and propose a data collection approach that delivers datasets that better reflect aspects important for potential practical use in software engineering. Finally, we demonstrate that high accuracy of authorship attribution models on existing datasets drastically drops when they are evaluated on more realistic data. We outline next steps for the design and evaluation of authorship attribution models that could bring the research efforts closer to practical use for software engineering.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {932–944},
numpages = {13},
keywords = {Copyrights, Machine learning, Methods of data collection, Security, Software maintenance, Software process},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1016/j.future.2019.09.009,
author = {Lopes, F\'{a}bio and Agnelo, Jo\~{a}o and Teixeira, C\'{e}sar A. and Laranjeiro, Nuno and Bernardino, Jorge},
title = {Automating orthogonal defect classification using machine learning algorithms},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {102},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.09.009},
doi = {10.1016/j.future.2019.09.009},
journal = {Future Gener. Comput. Syst.},
month = jan,
pages = {932–947},
numpages = {16},
keywords = {Software defects, Bug reports, Orthogonal defect classification, Machine learning, Text classification}
}

@article{10.1155/2020/6688075,
author = {Naseem, Rashid and Khan, Bilal and Ahmad, Arshad and Almogren, Ahmad and Jabeen, Saima and Hayat, Bashir and Shah, Muhammad Arif and Uddin, M. Irfan},
title = {Investigating Tree Family Machine Learning Techniques for a Predictive System to Unveil Software Defects},
year = {2020},
issue_date = {2020},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2020},
issn = {1076-2787},
url = {https://doi.org/10.1155/2020/6688075},
doi = {10.1155/2020/6688075},
abstract = {Software defects prediction at the initial period of the software development life cycle remains a critical and important assignment. Defect prediction and correctness leads to the assurance of the quality of software systems and has remained integral to study in the previous years. The quick forecast of imperfect or defective modules in software development can serve the development squad to use the existing assets competently and effectively to provide remarkable software products in a given short timeline. Hitherto, several researchers have industrialized defect prediction models by utilizing statistical and machine learning techniques that are operative and effective approaches to pinpoint the defective modules. Tree family machine learning techniques are well-thought-out to be one of the finest and ordinarily used supervised learning methods. In this study, different tree family machine learning techniques are employed for software defect prediction using ten benchmark datasets. These techniques include Credal Decision Tree (CDT), Cost-Sensitive Decision Forest (CS-Forest), Decision Stump (DS), Forest by Penalizing Attributes (Forest-PA), Hoeffding Tree (HT), Decision Tree (J48), Logistic Model Tree (LMT), Random Forest (RF), Random Tree (RT), and REP-Tree (REP-T). Performance of each technique is evaluated using different measures, i.e., mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), root relative squared error (RRSE), specificity, precision, recall, F-measure (FM), G-measure (GM), Matthew’s correlation coefficient (MCC), and accuracy. The overall outcomes of this paper suggested RF technique by producing best results in terms of reducing error rates as well as increasing accuracy on five datasets, i.e., AR3, PC1, PC2, PC3, and PC4. The average accuracy achieved by RF is 90.2238%. The comprehensive outcomes of this study can be used as a reference point for other researchers. Any assertion concerning the enhancement in prediction through any new model, technique, or framework can be benchmarked and verified.},
journal = {Complex.},
month = jan,
numpages = {21}
}

@article{10.5555/3057337.3057441,
author = {Rathore, Santosh S. and Kumar, Sandeep},
title = {A decision tree logic based recommendation system to select software fault prediction techniques},
year = {2017},
issue_date = {March     2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {99},
number = {3},
issn = {0010-485X},
abstract = {Identifying a reliable fault prediction technique is the key requirement for building effective fault prediction model. It has been found that the performance of fault prediction techniques is highly dependent on the characteristics of the fault dataset. To mitigate this issue, researchers have evaluated and compared a plethora of fault prediction techniques by varying the context in terms of domain information, characteristics of input data, complexity, etc. However, the lack of an accepted benchmark makes it difficult to select fault prediction technique for a particular context of prediction. In this paper, we present a recommendation system that facilitates the selection of appropriate technique(s) to build fault prediction model. First, we have reviewed the literature to elicit the various characteristics of the fault dataset and the appropriateness of the machine learning and statistical techniques for the identified characteristics. Subsequently, we have formalized our findings and built a recommendation system that helps in the selection of fault prediction techniques. We performed an initial appraisal of our presented system and found that proposed recommendation system provides useful hints in the selection of the fault prediction techniques.},
journal = {Computing},
month = mar,
pages = {255–285},
numpages = {31},
keywords = {68N30 Mathematical aspects of software engineering (specification, Decision tree, Recommendation system, Software fault prediction, Software fault prediction techniques, etc.), metrics, requirements, verification}
}

@inproceedings{10.1145/3428757.3429152,
author = {Philipp, Robert and Mladenow, Andreas and Strauss, Christine and V\"{o}lz, Alexander},
title = {Machine Learning as a Service: Challenges in Research and Applications},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429152},
doi = {10.1145/3428757.3429152},
abstract = {This study aims to evaluate the current state of research with regards to Machine Learning as a Service (MLaaS) and to identify challenges and research fields of this novel topic. First, a literature review on a basket of eight leading journals was performed. We motivate this study by identifying a lack of studies in the field of MLaaS. The structured literature review was further extended to established scientific databases relevant in this field. We found 30 contributions on MLaaS. As a result of the analysis we grouped them into four key concepts: Platform, Applications; Performance Enhancements and Challenges. Three of the derived concepts are discussed in detail to identify future research areas and to reveal challenges in research as well as in applications.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications &amp; Services},
pages = {396–406},
numpages = {11},
keywords = {MLaaS, Machine Learning Services, Machine Learning, Machine Learning Platform, Machine Learning as a Service},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1007/11941439_92,
author = {Jin, Xin and Liu, Yi and Ren, Jie and Xu, Anbang and Bie, Rongfang},
title = {Locality preserving projection on source code metrics for improved software maintainability},
year = {2006},
isbn = {3540497870},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11941439_92},
doi = {10.1007/11941439_92},
abstract = {Software project managers commonly use various metrics to assist in the design, maintaining and implementation of large software systems. The ability to predict the quality of a software object can be viewed as a classification problem, where software metrics are the features and expert quality rankings the class labels. In this paper we propose a Gaussian Mixture Model (GMM) based method for software quality classification and use Locality Preserving Projection (LPP) to improve the classification performance. GMM is a generative model which defines the overall data set as a combination of several different Gaussian distributions. LPP is a dimensionality deduction algorithm which can preserve the distance between samples while projecting data to lower dimension. Empirical results on benchmark dataset show that the two methods are effective.},
booktitle = {Proceedings of the 19th Australian Joint Conference on Artificial Intelligence: Advances in Artificial Intelligence},
pages = {877–886},
numpages = {10},
location = {Hobart, Australia},
series = {AI'06}
}

@article{10.1016/j.eswa.2019.03.039,
author = {Chen, Ke and Zhou, Feng-Yu and Yuan, Xian-Feng},
title = {Hybrid particle swarm optimization with spiral-shaped mechanism for feature selection},
year = {2019},
issue_date = {Aug 2019},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2019.03.039},
doi = {10.1016/j.eswa.2019.03.039},
journal = {Expert Syst. Appl.},
month = aug,
pages = {140–156},
numpages = {17},
keywords = {Particle swarm optimization, Feature selection, Classification, Optimization}
}

@article{10.1007/s11042-018-6912-6,
author = {Singh, Rahul Dev and Mittal, Ajay and Bhatia, Rajesh K.},
title = {3D convolutional neural network for object recognition: a review},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {78},
number = {12},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-018-6912-6},
doi = {10.1007/s11042-018-6912-6},
abstract = {Recognition of an object from an image or image sequences is an important task in computer vision. It is an important low-level image processing operation and plays a crucial role in many real-world applications. The challenges involved in object recognition are multi-model, multi-pose, complicated background, and depth variations. Recently developed methods have dealt with these challenges and have reported remarkable results for 3D objects. In this paper, a comprehensive overview of recent advances in 3D object recognition using Convolutional Neural Networks (CNN) has been presented. Along with the latest progress in 3D images, general overview of object recognition of 2D, 2.5D, and 3D images is presented.},
journal = {Multimedia Tools Appl.},
month = jun,
pages = {15951–15995},
numpages = {45},
keywords = {3D images, Convolutional neural network, Deep learning, Object recognition, Supervised learning}
}

@article{10.1007/s42979-021-00872-6,
author = {Sakhrawi, Zaineb and Sellami, Asma and Bouassida, Nadia},
title = {Software Enhancement Effort Prediction Using Machine-Learning Techniques: A Systematic Mapping Study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {6},
url = {https://doi.org/10.1007/s42979-021-00872-6},
doi = {10.1007/s42979-021-00872-6},
abstract = {Accurate prediction of software enhancement effort is a key success in software project management. To increase the accuracy of estimates, several proposals used machine-learning (ML) techniques for predicting the software project effort. However, there is no clear evidence for determining which techniques to select for predicting more accurate effort within the context of enhancement projects. This paper aims to present a systematic mapping study (SMS) related to the use of ML techniques for predicting software enhancement effort (SEME). A SMS was performed by reviewing relevant papers from 1995 through 2020. We followed well-known guidelines. We selected 30 relevant studies; 19 from journals and 11 conferences proceedings through 4 search engines. Some of the key findings indicate that (1) there is relatively little activity in the area of SEME, (2) most of the successful studies cited focused on regression problems for enhancement maintenance effort prediction, (3) SEME is the dependent variable the most commonly used in software enhancement project planning, and the enhancement size (or the functional change size) is the most used independent variables, (4) several private datasets were used in the selected studies, and there is a growing demand for the use of commonly published datasets, and (5) only single models were employed for SEME prediction. Results indicate that much more work is needed to develop repositories in all prediction models. Based on the findings obtained in this SMS, estimators should be aware that SEME using ML techniques as part of non-algorithmic models demonstrated increased accuracy prediction over the algorithmic models. The use of ML techniques generally provides a reasonable accuracy when using the enhancement functional size as independent variables.},
journal = {SN Comput. Sci.},
month = sep,
numpages = {15},
keywords = {Systematic mapping study (SMS), Functional change (FC), Software enhancement effort (SEME) prediction, Machine learning (ML)}
}

@inproceedings{10.1109/ICSE43902.2021.00138,
author = {Wang, Song and Shrestha, Nishtha and Subburaman, Abarna Kucheri and Wang, Junjie and Wei, Moshi and Nagappan, Nachiappan},
title = {Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00138},
doi = {10.1109/ICSE43902.2021.00138},
abstract = {Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically-orientated and have fundamentally different nature and construction from general software projects.In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely-used machine learning libraries with two popular unit test case generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1548–1560},
numpages = {13},
keywords = {Empirical software engineering, test case generation, testing machine learning libraries},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3377816.3381734,
author = {Byun, Taejoon and Rayadurgam, Sanjai},
title = {Manifold for machine learning assurance},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381734},
doi = {10.1145/3377816.3381734},
abstract = {The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–100},
numpages = {4},
keywords = {machine learning testing, neural networks, variational autoencoder},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@article{10.1007/s11334-017-0295-0,
author = {Shatnawi, Raed},
title = {The application of ROC analysis in threshold identification, data imbalance and metrics selection for software fault prediction},
year = {2017},
issue_date = {September 2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {2–3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-017-0295-0},
doi = {10.1007/s11334-017-0295-0},
abstract = {Software engineers have limited resources and need metrics analysis tools to investigate software quality such as fault-proneness of modules. There are a large number of software metrics available to investigate quality. However, not all metrics are strongly correlated with faults. In addition, software fault data are imbalanced and affect quality assessment tools such as fault prediction or threshold values that are used to identify risky modules. Software quality is investigated for three purposes. First, the receiver operating characteristics (ROC) analysis is used to identify threshold values to identify risky modules. Second, the ROC analysis is investigated for imbalanced data. Third, the ROC analysis is considered for feature selection. This work validated the use of ROC to identify thresholds for four metrics (WMC, CBO, RFC and LCOM). The ROC results after sampling the data are not significantly different from before sampling. The ROC analysis selects the same metrics (WMC, CBO and RFC) in most datasets, while other techniques have a large variation in selecting metrics.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {201–217},
numpages = {17},
keywords = {Fault-proneness models, Feature selection, Imbalanced data, ROC analysis, Software metrics}
}

@article{10.1007/s10515-021-00285-y,
author = {Goyal, Somya},
title = {Predicting the Defects using Stacked Ensemble Learner with Filtered Dataset},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00285-y},
doi = {10.1007/s10515-021-00285-y},
abstract = {Software defect prediction is a crucial software project management activity to enhance the software quality. It aids the development team to forecast about which modules need extra attention for testing; which part of software is more prone to errors and faults; before the commencement of testing phase. It helps to reduce the testing cost and hence the overall development cost of the software. Though, it ensures in-time delivery of good quality end-product, but there is one major hinderance in making this prediction. This is the class imbalance issue in the training data. Data imbalance in class distribution adversely affects the performance of classifiers. This paper proposes a K-nearest neighbour (KNN) filtering-based data pre-processing technique for stacked ensemble classifier to handle class imbalance issue. First, nearest neighbour-based filtering is applied to filter out the overlapped data-points to reduce Imbalanced Ratio, then, the processed data with static code metrics is supplied to stacked ensemble for prediction. The stacking is achieved with five base classifiers namely Artificial Neural Network, Decision Tree, Na\"{\i}ve Bayes, K-nearest neighbour (KNN) and Support Vector Machine. A comparative analysis among 30 classifiers (5 data pre-processing techniques * 6 prediction techniques) is made. In the experiments, five public datasets from NASA repository namely CM1, JM1, KC1, KC2 and PC1 are used. In total 150 prediction models (5 data pre-processing techniques * 6 classification techniques * 5 datasets) are proposed and their performances are assessed in terms of measures namely Receiver Operator Curve, Area under the Curve and accuracy. The statistical analysis shows that proposed stacked ensemble classifier with KNN filtering performs best among all the predictors independent of datasets.},
journal = {Automated Software Engg.},
month = nov,
numpages = {81},
keywords = {Software quality, Defect prediction, Data pre-processing, Class imbalance, Artificial neural networks (ANN), Stacked ensembles, Decision trees, Nearest neighbour, Support vector machine, ROC and AUC}
}

@phdthesis{10.5555/2520283,
author = {Shivaji, Shivkumar},
advisor = {Whitehead, E. James},
title = {Efficient bug prediction and fix suggestions},
year = {2013},
isbn = {9781303021435},
publisher = {University of California at Santa Cruz},
address = {USA},
abstract = {Bugs are a well known Achilles' heel of software development. In the last few years, machine learning techniques to combat software bugs have become popular. However, results of these techniques are not good enough for practical adoption. In addition, most techniques do not provide reasons for why a code change is a bug. Furthermore, suggestions to fix the bug would be greatly beneficial. An added bonus would be engaging humans to improve the bug and fix prediction process. In this dissertation, a step-by-step procedure which effectively predicts buggy code changes (Bug Prognosticator), produces bug fix suggestions (Fix Suggester), and utilizes human feedback is presented. Each of these steps can be used independently, but combining them allows more effective management of bugs. These techniques are tested on many open source and a large commercial project. Human feedback was used to understand and improve the performance of the techniques. Feedback was primarily gathered from industry participants in order to assess practical suitability. The Bug Prognosticator explores feature selection techniques and classifiers to improve results of code change bug prediction. The optimized Bug Prognosticator is able to achieve an average 97% precision and 70% recall when evaluated on eleven projects, ten open source and one commercial. The Fix Suggester uses the Bug Prognosticator and statistical analysis of keyword term frequencies to suggest unordered fix keywords to a code change predicted to be buggy. The suggestions are validated against actual bug fixes to confirm their utility. The Fix Suggester is able to achieve 46.9% precision and 38.9% recall on its predicted fix tokens. This is a reasonable start to the difficult problem of predicting the contents of a bug fix. To improve the efficiency of the Bug Prognosticator and the Fix Suggester, active learning is employed on willing human participants. Developers aid the Bug Prognosticator and the Fix Suggester on code changes that machines find hard to evaluate. The developer's feedback is used to enhance the performance of the Bug Prognosticator and the Fix Suggester. In addition, a user study is performed to gauge the utility of the Fix Suggester. The dissertation concludes with a discussion of future work and challenges faced by the techniques. Given the success of statistical defect prediction techniques, more industrial exposure would benefit researchers and software practitioners.},
note = {AAI3558011}
}

@article{10.1007/s11554-017-0714-3,
author = {Haider, Khurram Zeeshan and Malik, Kaleem Razzaq and Khalid, Shehzad and Nawaz, Tabassam and Jabbar, Sohail},
title = {Deepgender: real-time gender classification using deep learning for smartphones},
year = {2019},
issue_date = {Feb 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {16},
number = {1},
issn = {1861-8200},
url = {https://doi.org/10.1007/s11554-017-0714-3},
doi = {10.1007/s11554-017-0714-3},
abstract = {Face recognition, expression identification, age determination, racial binding and gender classification are common examples of image processing computerization. Gender classification is very straightforward for us like we can tell by the person’s hair, nose, eyes, mouth and skin whether that person is a male or female with a relatively high degree of confidence and accuracy; however, can we program a computer to perform just as well at gender classification? The very problem is the main focus of this research. The conventional sequence for recent real-time facial image processing consists of five steps: face detection, noise removal, face alignment, feature representation and classification. With the aim of human gender classification, face alignment and feature vector extraction stages have been re-examined keeping in view the application of the system on smartphones. Face alignment has been made by spotting out 83 facial landmarks and 3-D facial model with the purpose of applying affine transformation. Furthermore, ‘feature representation’ is prepared through proposed modification in multilayer deep neural network, and hence we name it Deepgender. This convolutional deep neural network consists of some locally connected hidden layers without common weights of kernels as previously followed in legacy layered architecture. This specific case study involves deep learning as four convolutional layers, three max-pool layers (for downsizing of unrelated data), two fully connected layers (connection of outcome to all inputs) and a single layer of ‘multinomial logistic regression.’ Training has been made using CAS-PEAL and FEI which contain 99,594 face images of 1040 people and 2800 face images of 200 individuals, respectively. These images are either in different poses or taken under uncontrolled conditions which are close to real-time input facial image for gender classification application. The proposed system ‘Deepgender’ has registered 98% accuracy by combined use of both databases with the specific preprocess procedure, i.e., exhibiting alignment before resizing. Experiments suggest that accuracy is nearly 100% with frontal and nonblurred facial images. State-of-the-art steps have been taken to overcome memory and battery constraints in mobiles.},
journal = {J. Real-Time Image Process.},
month = feb,
pages = {15–29},
numpages = {15},
keywords = {Real-time systems, Machine learning, Image processing, Deep learning, Gender classification, Advanced mobile applications}
}

@inproceedings{10.1145/3358331.3358376,
author = {Easttom, Chuck},
title = {A Methodological Approach to Weaponizing Machine Learning},
year = {2019},
isbn = {9781450372022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358331.3358376},
doi = {10.1145/3358331.3358376},
abstract = {The current literature is replete with studies involving the use of machine learning algorithms for defensive security implementations. For example, machine learning has been utilized to enhance antivirus software and intrusion detection systems. The use of machine learning in defensive cybersecurity operations is well documented. However, there is a substantial gap in the literature on the offensive use of machine learning. Particularly, use of machine learning algorithms to enhance cyber warfare operations. Cyber components to modern conflicts, whether those conflicts are cyber or kinetic warfare, are a fact of the modern international political landscape. It is a natural progression to explore applications of machine learning to cyber warfare, particularly weaponized malware.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Advanced Manufacturing},
articleno = {45},
numpages = {5},
keywords = {Weaponized malware, cyber warfare, machine learning, weaponized malware},
location = {Dublin, Ireland},
series = {AIAM 2019}
}

@inproceedings{10.1145/3021460.3021491,
author = {Malhotra, Ruchika},
title = {Software Quality Predictive Modeling: An Effective Assessment of Experimental Data},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021491},
doi = {10.1145/3021460.3021491},
abstract = {A major problem faced by software project managers is to develop good quality software products within tight schedules and budget constraints [1]. Predictive modeling, in the context of software engineering relates to construction of models for estimation of software quality attributes such as defect-proneness, maintainability and effort amongst others. For developing such models, software metrics act as predictor variables as they signify various design characteristics of a software such as coupling, cohesion, inheritance and polymorphism. A number of techniques such as statistical and machine learning are available for developing predictive models.However, conducting effective empirical studies, which develop successful predictive models, is not possible if proper research methodology and steps are not followed. This work introduces a successful stepwise procedure for efficient application of various techniques to predictive modeling. A number of research issues which are important to be addressed while conducting empirical studies such as data collection, validation method, use of statistical tests, use of an effective performance evaluator etc. are also discussed with the help of an example.The tutorial presents an overview of the research process and methodology followed in an empirical research [2]. All steps that are needed to perform an effective empirical study are described. The tutorial would demonstrate the research methodology with the help of an example based on a data set for defect prediction.In this work we focus on various research issues that are stated below:RQ1: Which repositories are available for extracting software engineering data?RQ2: What type of data pre-processing and feature selection techniques should be used before developing predictive models?RQ3: Which possible tools are freely available for mining and analysis of data for developing software quality predictive models?RQ4: Which techniques are available for developing software quality predictive models?RQ5: Which metrics should be used for performance evaluation for models developed for software?RQ6: Which statistical tests can be effectively used for hypothesis testing using search-based techniques?RQ7: How can we effectively use search-based techniques for predictive modeling?RQ8: What are possible fitness functions while using search-based techniques for predictive modeling?RQ9: How would researchers account for the stochastic nature of search-based techniques?The reasons for relevance of this study are manifold. Empirical validation of OO metrics is a critical research area in the present day scenario, with a large number of academicians and research practitioners working towards this direction to predict software quality attributes in the early phases of software development. Thus, we explore the various steps involved in development of an effective software quality predictive model using a modeling technique with an example data set. Performing successful empirical studies in software engineering is important for the following reasons:• To identify defective classes at the initial phases of software development so that more resources can be allocated to these classes to remove errors.• To analyze the metrics which are important for predicting software quality attributes and to use them as quality benchmarks so that the software process can be standardized and delivers effective products.• To efficiently plan testing, walkthroughs, reviews and inspection activities so that limited resources can be properly planned to provide good quality software.• To use and adapt different techniques (statistical, machine learning &amp; search-based) in predicting software quality attributes.• To analyze existing trends for software quality predictive modeling and suggest future directions for researchers.• To document the research methodology so that effective replicated studies can be performed with ease.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {215–216},
numpages = {2},
keywords = {Empirical Validation, Object-oriented metrics, Search-based techniques, Software quality predictive modeling},
location = {Jaipur, India},
series = {ISEC '17}
}

@article{10.1007/s10664-015-9400-x,
author = {Kamei, Yasutaka and Fukushima, Takafumi and Mcintosh, Shane and Yamashita, Kazuhiro and Ubayashi, Naoyasu and Hassan, Ahmed E.},
title = {Studying just-in-time defect prediction using cross-project models},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9400-x},
doi = {10.1007/s10664-015-9400-x},
abstract = {Unlike traditional defect prediction models that identify defect-prone modules, Just-In-Time (JIT) defect prediction models identify defect-inducing changes. As such, JIT defect models can provide earlier feedback for developers, while design decisions are still fresh in their minds. Unfortunately, similar to traditional defect models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this limitation in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from other projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT models in a cross-project context. Through an empirical study on 11 open source projects, we find that while JIT models rarely perform well in a cross-project context, their performance tends to improve when using approaches that: (1) select models trained using other projects that are similar to the testing project, (2) combine the data of several other projects to produce a larger pool of training data, and (3) combine the models of several other projects to produce an ensemble model. Our findings empirically confirm that JIT models learned using other projects are a viable solution for projects with limited historical data. However, JIT models tend to perform best in a cross-project context when the data used to learn them are carefully selected.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2072–2106},
numpages = {35},
keywords = {Defect prediction, Empirical study, Just-in-time prediction}
}

@article{10.1007/s11219-020-09520-3,
author = {Rantala, Leevi and M\"{a}ntyl\"{a}, Mika},
title = {Predicting technical debt from commit contents: reproduction and extension with automated feature selection},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09520-3},
doi = {10.1007/s11219-020-09520-3},
abstract = {Self-admitted technical debt refers to sub-optimal development solutions that are expressed in written code comments or commits. We reproduce and improve on a prior work by Yan et al. (2018) on detecting commits that introduce self-admitted technical debt. We use multiple natural language processing methods: Bag-of-Words, topic modeling, and word embedding vectors. We study 5 open-source projects. Our NLP approach uses logistic Lasso regression from Glmnet to automatically select best predictor words. A manually labeled dataset from prior work that identified self-admitted technical debt from code level commits serves as ground truth. Our approach achieves + 0.15 better area under the ROC curve performance than a prior work, when comparing only commit message features, and + 0.03 better result overall when replacing manually selected features with automatically selected words. In both cases, the improvement was statistically significant (p &lt; 0.0001). Our work has four main contributions, which are comparing different NLP techniques for SATD detection, improved results over previous work, showing how to generate generalizable predictor words when using multiple repositories, and producing a list of words correlating with SATD. As a concrete result, we release a list of the predictor words that correlate positively with SATD, as well as our used datasets and scripts to enable replication studies and to aid in the creation of future classifiers.},
journal = {Software Quality Journal},
month = dec,
pages = {1551–1579},
numpages = {29},
keywords = {Natural language processing, Latent Dirichlet allocation, Logistic regression, Word embeddings, Topic modeling, Data mining}
}

@article{10.1016/j.infsof.2019.106241,
author = {Perkusich, Mirko and Chaves e Silva, Lenardo and Costa, Alexandre and Ramos, Felipe and Saraiva, Renata and Freire, Arthur and Dilorenzo, Ednaldo and Dantas, Emanuel and Santos, Danilo and Gorg\^{o}nio, Kyller and Almeida, Hyggo and Perkusich, Angelo},
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106241},
doi = {10.1016/j.infsof.2019.106241},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {19},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence}
}

@article{10.1016/j.infsof.2019.106214,
author = {Alsolai, Hadeel and Roper, Marc},
title = {A systematic literature review of machine learning techniques for software maintainability prediction},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106214},
doi = {10.1016/j.infsof.2019.106214},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {25},
keywords = {Systematic literature review, Software maintainability prediction, Machine learning, Metric, Dataset}
}

@article{10.1007/s10515-011-0091-2,
author = {Liparas, Dimitris and Angelis, Lefteris and Feldt, Robert},
title = {Applying the Mahalanobis-Taguchi strategy for software defect diagnosis},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0091-2},
doi = {10.1007/s10515-011-0091-2},
abstract = {The Mahalanobis-Taguchi (MT) strategy combines mathematical and statistical concepts like Mahalanobis distance, Gram-Schmidt orthogonalization and experimental designs to support diagnosis and decision-making based on multivariate data. The primary purpose is to develop a scale to measure the degree of abnormality of cases, compared to "normal" or "healthy" cases, i.e. a continuous scale from a set of binary classified cases. An optimal subset of variables for measuring abnormality is then selected and rules for future diagnosis are defined based on them and the measurement scale. This maps well to problems in software defect prediction based on a multivariate set of software metrics and attributes. In this paper, the MT strategy combined with a cluster analysis technique for determining the most appropriate training set, is described and applied to well-known datasets in order to evaluate the fault-proneness of software modules. The measurement scale resulting from the MT strategy is evaluated using ROC curves and shows that it is a promising technique for software defect diagnosis. It compares favorably to previously evaluated methods on a number of publically available data sets. The special characteristic of the MT strategy that it quantifies the level of abnormality can also stimulate and inform discussions with engineers and managers in different defect prediction situations.},
journal = {Automated Software Engg.},
month = jun,
pages = {141–165},
numpages = {25},
keywords = {Fault-proneness, Mahalanobis-Taguchi strategy, Software defect prediction, Software testing}
}

@article{10.1007/s10664-019-09769-8,
author = {Ochodek, Miroslaw and Hebig, Regina and Meding, Wilhelm and Frost, Gert and Staron, Miroslaw},
title = {Recognizing lines of code violating company-specific coding guidelines using machine learning: A Method and Its Evaluation},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09769-8},
doi = {10.1007/s10664-019-09769-8},
abstract = {Software developers in big and medium-size companies are working with millions of lines of code in their codebases. Assuring the quality of this code has shifted from simple defect management to proactive assurance of internal code quality. Although static code analysis and code reviews have been at the forefront of research and practice in this area, code reviews are still an effort-intensive and interpretation-prone activity. The aim of this research is to support code reviews by automatically recognizing company-specific code guidelines violations in large-scale, industrial source code. In our action research project, we constructed a machine-learning-based tool for code analysis where software developers and architects in big and medium-sized companies can use a few examples of source code lines violating code/design guidelines (up to 700 lines of code) to train decision-tree classifiers to find similar violations in their codebases (up to 3 million lines of code). Our action research project consisted of (i) understanding the challenges of two large software development companies, (ii) applying the machine-learning-based tool to detect violations of Sun’s and Google’s coding conventions in the code of three large open source projects implemented in Java, (iii) evaluating the tool on evolving industrial codebase, and (iv) finding the best learning strategies to reduce the cost of training the classifiers. We were able to achieve the average accuracy of over 99% and the average F-score of 0.80 for open source projects when using ca. 40K lines for training the tool. We obtained a similar average F-score of 0.78 for the industrial code but this time using only up to 700 lines of code as a training dataset. Finally, we observed the tool performed visibly better for the rules requiring to understand a single line of code or the context of a few lines (often allowing to reach the F-score of 0.90 or higher). Based on these results, we could observe that this approach can provide modern software development companies with the ability to use examples to teach an algorithm to recognize violations of code/design guidelines and thus increase the number of reviews conducted before the product release. This, in turn, leads to the increased quality of the final software.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {220–265},
numpages = {46},
keywords = {Measurement, Machine learning, Action research, Code reviews}
}

@article{10.1016/j.eswa.2014.10.025,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {A comparison of some soft computing methods for software fault prediction},
year = {2015},
issue_date = {March 2015},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {42},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2014.10.025},
doi = {10.1016/j.eswa.2014.10.025},
abstract = {Software fault prediction is implemented with ANN, SVM and ANFIS.First ANFIS implementation is applied to solve fault prediction problem.Parameters are discussed in neuro fuzzy approach.Experiments show that the application of ANFIS to the software fault prediction problem is highly reasonable. The main expectation from reliable software is the minimization of the number of failures that occur when the program runs. Determining whether software modules are prone to fault is important because doing so assists in identifying modules that require refactoring or detailed testing. Software fault prediction is a discipline that predicts the fault proneness of future modules by using essential prediction metrics and historical fault data. This study presents the first application of the Adaptive Neuro Fuzzy Inference System (ANFIS) for the software fault prediction problem. Moreover, Artificial Neural Network (ANN) and Support Vector Machine (SVM) methods, which were experienced previously, are built to discuss the performance of ANFIS. Data used in this study are collected from the PROMISE Software Engineering Repository, and McCabe metrics are selected because they comprehensively address the programming effort. ROC-AUC is used as a performance measure. The results achieved were 0.7795, 0.8685, and 0.8573 for the SVM, ANN and ANFIS methods, respectively.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {1872–1879},
numpages = {8},
keywords = {Adaptive neuro fuzzy systems, Artificial Neural Networks, McCabe metrics, Software fault prediction, Support Vector Machines}
}

@article{10.1016/j.ins.2008.12.001,
author = {Catal, Cagatay and Diri, Banu},
title = {Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {179},
number = {8},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2008.12.001},
doi = {10.1016/j.ins.2008.12.001},
abstract = {Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used.},
journal = {Inf. Sci.},
month = mar,
pages = {1040–1058},
numpages = {19},
keywords = {Artificial Immune Systems, J48, Machine learning, Naive Bayes, Random Forests, Software fault prediction}
}

@inproceedings{10.5555/1689250.1689300,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Wang, Huanjing},
title = {An empirical investigation of filter attribute selection techniques for software quality classification},
year = {2009},
isbn = {9781424441143},
publisher = {IEEE Press},
abstract = {Attribute selection is an important activity in data preprocessing for software quality modeling and other data mining problems. The software quality models have been used to improve the fault detection process. Finding faulty components in a software system during early stages of software development process can lead to a more reliable final product and can reduce development and maintenance costs. It has been shown in some studies that prediction accuracy of the models improves when irrelevant and redundant features are removed from the original data set. In this study, we investigated four filter attribute selection techniques, Automatic Hybrid Search (AHS), Rough Sets (RS), Kolmogorov-Smirnov (KS) and Probabilistic Search (PS) and conducted the experiments by using them on a very large telecommunications software system. In order to evaluate their classification performance on the smaller subsets of attributes selected using different approaches, we built several classification models using five different classifiers. The empirical results demonstrated that by applying an attribution selection approach we can build classification models with an accuracy comparable to that built with a complete set of attributes. Moreover, the smaller subset of attributes has less than 15 percent of the complete set of attributes. Therefore, the metrics collection, model calibration, model validation, and model evaluation times of future software development efforts of similar systems can be significantly reduced. In addition, we demonstrated that our recently proposed attribute selection technique, KS, outperformed the other three attribute selection techniques.},
booktitle = {Proceedings of the 10th IEEE International Conference on Information Reuse &amp; Integration},
pages = {272–277},
numpages = {6},
location = {Las Vegas, Nevada, USA},
series = {IRI'09}
}

@article{10.1016/j.jpdc.2017.06.022,
author = {Hussain, Shahid and Keung, Jacky and Khan, Arif Ali and Ahmad, Awais and Cuomo, Salvatore and Piccialli, Francesco and Jeon, Gwanggil and Akhunzada, Adnan},
title = {Implications of deep learning for the automation of design patterns organization},
year = {2018},
issue_date = {Jul 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2017.06.022},
doi = {10.1016/j.jpdc.2017.06.022},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {256–266},
numpages = {11},
keywords = {Design patterns, Deep learning, Feature set, Performance, Classifiers}
}

@inproceedings{10.1109/MSR.2019.00052,
author = {Bangash, Abdul Ali and Sahar, Hareem and Chowdhury, Shaiful and Wong, Alexander William and Hindle, Abram and Ali, Karim},
title = {What do developers know about machine learning: a study of ML discussions on StackOverflow},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00052},
doi = {10.1109/MSR.2019.00052},
abstract = {Machine learning, a branch of Artificial Intelligence, is now popular in software engineering community and is successfully used for problems like bug prediction, and software development effort estimation. Developers' understanding of machine learning, however, is not clear, and we require investigation to understand what educators should focus on, and how different online programming discussion communities can be more helpful. We conduct a study on Stack Overflow (SO) machine learning related posts using the SOTorrent dataset. We found that some machine learning topics are significantly more discussed than others, and others need more attention. We also found that topic generation with Latent Dirichlet Allocation (LDA) can suggest more appropriate tags that can make a machine learning post more visible and thus can help in receiving immediate feedback from sites like SO.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {260–264},
numpages = {5},
keywords = {machine learning, stackoverflow, topic modeling},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/2972958.2972964,
author = {Hosseini, Seyedrebvar and Turhan, Burak and M\"{a}ntyl\"{a}, Mika},
title = {Search Based Training Data Selection For Cross Project Defect Prediction},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972964},
doi = {10.1145/2972958.2972964},
abstract = {Context: Previous studies have shown that steered training data or dataset selection can lead to better performance for cross project defect prediction (CPDP). On the other hand, data quality is an issue to consider in CPDP.Aim: We aim at utilising the Nearest Neighbor (NN)-Filter, embedded in a genetic algorithm, for generating evolving training datasets to tackle CPDP, while accounting for potential noise in defect labels.Method: We propose a new search based training data (i.e., instance) selection approach for CPDP called GIS (Genetic Instance Selection) that looks for solutions to optimize a combined measure of F-Measure and GMean, on a validation set generated by (NN)-filter. The genetic operations consider the similarities in features and address possible noise in assigned defect labels. We use 13 datasets from PROMISE repository in order to compare the performance of GIS with benchmark CPDP methods, namely (NN)-filter and naive CPDP, as well as with within project defect prediction (WPDP).Results: Our results show that GIS is significantly better than (NN)-Filter in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.697) and GMean (p -- value ≪ 0.001, Cohen's d = 0.946). It also outperforms the naive CPDP approach in terms of F-Measure (p -- value ≪ 0.001, Cohen's d = 0.753) and GMean (p -- value ≪ 0.001, Cohen's d = 0.994). In addition, the performance of our approach is better than that of WPDP, again considering F-Measure (p -- value ≪ 0.001, Cohen's d = 0.227) and GMean (p -- value ≪ 0.001, Cohen's d = 0.595) values.Conclusions: We conclude that search based instance selection is a promising way to tackle CPDP. Especially, the performance comparison with the within project scenario encourages further investigation of our approach. However, the performance of GIS is based on high recall in the expense of low precision. Using different optimization goals, e.g. targeting high precision, would be a future direction to investigate.},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
keywords = {Cross Project Defect Prediction, Genetic Algorithms, Instance Selection, Search Based Optimization, Training Data Selection},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@article{10.1007/s11219-019-09463-4,
author = {Ali, Sadia and Hafeez, Yaser and Hussain, Shariq and Yang, Shunkun},
title = {Enhanced regression testing technique for agile software development and continuous integration strategies},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09463-4},
doi = {10.1007/s11219-019-09463-4},
abstract = {To survive in competitive marketplaces, most organizations have adopted agile methodologies to facilitate continuous integration and faster application delivery and rely on regression testing during application development to validate the quality and reliability of the software after changes have been made. Consequently, for large projects with cost and time constraints, it is extremely difficult to determine which test cases to run at the end of each release. In this paper, a test case prioritization and selection approach is proposed to improve the quality of releases. From existing literature, we analyzed prevailing problems and proposed solution relevant to regression testing in agile practices. The proposed approach is based on two phases. First, test cases are prioritized by clustering those test cases that frequently change. In case of a tie, test cases are prioritized based on their respective failure frequencies and coverage criteria. Second, test cases with a higher frequency of failure or coverage criteria are selected. The proposed technique was validated by an empirical study on three industrial subjects. The results show that the method successfully selects an optimal test suite and increases the fault detection rate (i.e., more than 90% in the case of proposed technique and less than 50% in other techniques), which reduces the number of irrelevant test cases and avoids detecting duplicate faults. The results of evaluation metrics illustrate that the proposed technique significantly outperform (i.e., between 91 and 97%) as compared to other existing regression testing techniques (i.e., between 52 and 68%). Therefore, our model enhances the test case prioritization and selection with the ability for earlier and high fault detection. Thus, pruning out irrelevant test cases and redundant faults and enhancing the regression testing process for agile applications.},
journal = {Software Quality Journal},
month = jun,
pages = {397–423},
numpages = {27},
keywords = {Regression testing, Agile methodology, Agile environment, Test case prioritization, Test suite selection, Frequency of change}
}

@article{10.1007/s11334-015-0258-2,
author = {Abdi, Yousef and Parsa, Saeed and Seyfari, Yousef},
title = {A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction},
year = {2015},
issue_date = {December  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-015-0258-2},
doi = {10.1007/s11334-015-0258-2},
abstract = {Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization (DSMOPSO). Because of discrete search space, we modified this algorithm and named it DSMOPSO-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the PROMISE repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {289–301},
numpages = {13},
keywords = {Classification rules, DSMOPSO-D, Fault prediction, Imbalanced data sets, Kernel k-means, Multi-objective particle swarm optimization}
}

@inproceedings{10.1145/3379177.3388905,
author = {Liu, Hanyan and Eksmo, Samuel and Risberg, Johan and Hebig, Regina},
title = {Emerging and Changing Tasks in the Development Process for Machine Learning Systems},
year = {2020},
isbn = {9781450375122},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379177.3388905},
doi = {10.1145/3379177.3388905},
abstract = {Integrating machine learning components in software systems is a task more and more companies are confronted with. However, there is not much knowledge today on how the software development process needs to change, when such components are integrated into a software system. We performed an interview study with 16 participants, focusing on emerging and changing task. The results uncover a set of 25 tasks associated to different software development phases, such as requirements engineering or deployment. We are just starting to understand the implications of using machine-learning components on the software development process. This study allows some first insights into how widespread the required process changes are.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {125–134},
numpages = {10},
keywords = {Challenges, Machine learning, Roles, Software process},
location = {Seoul, Republic of Korea},
series = {ICSSP '20}
}

@inproceedings{10.5555/2227134.2227137,
author = {Wang, Qi and Zhu, Jie and Yu, Bo},
title = {Feature selection and clustering in software quality prediction},
year = {2007},
publisher = {BCS Learning &amp; Development Ltd.},
address = {Swindon, GBR},
abstract = {Software quality prediction models use the software metrics and fault data collected from previous software releases or similar projects to predict the quality of software components in development. Previous research has shown that this kind of models can yield predictions with impressive accuracy. However, building accurate software quality prediction model is still challenging for following two reasons. Firstly, the outliers in software data often have a disproportionate effect on the overalls predictive ability of the model. Secondly, not all collected software metrics should be used to construct model because of the curse of dimension. To resolve these two problems, we present a new software quality prediction model based on genetic algorithm (GA) in which outlier detection and feature selection are executed simultaneously. The experimental results illustrate this model performs better than some latest raised software quality prediction models based on S-PLUS and TreeDisc. Furthermore, the clustered software components and selected features are easier for software engineers and data analysts to study and interpret.},
booktitle = {Proceedings of the 11th International Conference on Evaluation and Assessment in Software Engineering},
pages = {21–32},
numpages = {12},
keywords = {clustering, feature selection, genetic algorithm, software quality prediction},
location = {UK},
series = {EASE'07}
}

@article{10.1504/IJIIDS.2015.070825,
author = {Abaei, Golnoush and Mashinchi, M. Reza and Selamat, Ali},
title = {Software fault prediction using BP-based crisp artificial neural networks},
year = {2015},
issue_date = {July 2015},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1751-5858},
url = {https://doi.org/10.1504/IJIIDS.2015.070825},
doi = {10.1504/IJIIDS.2015.070825},
abstract = {Early fault detection for software reduces the cost of developments. Fault level can be predicted through learning mechanisms. Conventionally, precise metrics measure the fault level and crisp artificial neural networks CANNs perform the learning. However, the performance of CANNs depends on complexities of data and learning algorithm. This paper considers these two complexities to predict the fault level of software. We apply the principle component analysis PCA to reduce the dimensionality of data, and employ the correlation-based feature selection CFS to select the best features. CANNs, then, predict the fault level of software using back propagation BP algorithm as a learning mechanism. To investigate the performance of BP-based CANNs, we analyse varieties of dimensionality reduction. The results reveal the superiority of PCA to CFS in terms of accuracy.},
journal = {Int. J. Intell. Inf. Database Syst.},
month = jul,
pages = {15–31},
numpages = {17}
}

@article{10.1016/j.knosys.2017.04.014,
author = {Arcelli Fontana, Francesca and Zanoni, Marco},
title = {Code smell severity classification using machine learning techniques},
year = {2017},
issue_date = {July 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {128},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2017.04.014},
doi = {10.1016/j.knosys.2017.04.014},
abstract = {Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.880.96, measured through Spearmans .},
journal = {Know.-Based Syst.},
month = jul,
pages = {43–58},
numpages = {16},
keywords = {Code smell severity, Code smells detection, Machine learning, Ordinal classification, Refactoring prioritization}
}

@article{10.3233/KES-190421,
author = {Panigrahi, Rasmita and Kuanar, Sanjay K. and Kumar, Lov and Padhy, Neelamadhab and Satapathy, Suresh Chandra},
title = {Software reusability metrics prediction and cost estimation by using machine learning algorithms},
year = {2019},
issue_date = {2019},
publisher = {IOS Press},
address = {NLD},
volume = {23},
number = {4},
issn = {1327-2314},
url = {https://doi.org/10.3233/KES-190421},
doi = {10.3233/KES-190421},
abstract = {In this research, a highly robust and efficient software design optimization model has been proposed for object-oriented programming based software solutions while considering the importance of quality and reliability. Due to a piece of information that software component reusability has allowed cost and time-efficient software design. The software reusability metrics prediction and cost estimation play a vital role in the software industry. Software quality prediction is an important feature that can be achieved a novel machine learning approach. It is a process of gathering and analyzing recurring patterns in software metrics. Machine learning techniques play a crucial role in intelligent decision making and proactive forecasting. This paper focuses on analyzing software reusability and cost estimation metrics by providing the data set. In the present world software, cost estimation and reusability prediction problem has been resolved using various newly developed methods. This paper emphasizes to solve the novel machine learning algorithms as well as improved Output layer self-connection recurrent neural networks (OLSRNN) with kernel fuzzy c-means clustering (KFCM). The investigational results confirmed the competence of the proposed method for solving software reusability and cost estimation.},
journal = {Int. J. Know.-Based Intell. Eng. Syst.},
month = jan,
pages = {317–328},
numpages = {12},
keywords = {Object-Oriented Metrics, software reusability metrics, machine learning techniques, software cost estimation}
}

@article{10.1155/2021/6627588,
author = {Xie, Yuan and Zhao, Jisheng and Qiang, Baohua and Mi, Luzhong and Tang, Chenghua and Li, Longge and Xue, Xingsi},
title = {Attention Mechanism-Based CNN-LSTM Model for Wind Turbine Fault Prediction Using SSN Ontology Annotation},
year = {2021},
issue_date = {2021},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
volume = {2021},
issn = {1530-8669},
url = {https://doi.org/10.1155/2021/6627588},
doi = {10.1155/2021/6627588},
abstract = {The traditional model for wind turbine fault prediction is not sensitive to the time sequence data and cannot mine the deep connection between the time series data, resulting in poor generalization ability of the model. To solve this problem, this paper proposes an attention mechanism-based CNN-LSTM model. The semantic sensor data annotated by SSN ontology is used as input data. Firstly, CNN extracts features to get high-level feature representation from input data. Then, the latent time sequence connection of features in different time periods is learned by LSTM. Finally, the output of LSTM is input into the attention mechanism module to obtain more fault-related target information, which improves the efficiency, accuracy, and generalization ability of the model. In addition, in the data preprocessing stage, the random forest algorithm analyzes the feature correlation degree of the data to get the features of high correlation degree with the wind turbine fault, which further improves the efficiency, accuracy, and generalization ability of the model. The model is validated on the icing fault dataset of No. 21 wind turbine and the yaw dataset of No. 4 wind turbine. The experimental results show that the proposed model has better efficiency, accuracy, and generalization ability than RNN, LSTM, and XGBoost.},
journal = {Wirel. Commun. Mob. Comput.},
month = jan,
numpages = {12}
}

@article{10.1109/TSE.2012.43,
author = {Shivaji, Shivkumar and Whitehead, E. James and Akella, Ram and Kim, Sunghun},
title = {Reducing Features to Improve Code Change-Based Bug Prediction},
year = {2013},
issue_date = {April 2013},
publisher = {IEEE Press},
volume = {39},
number = {4},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2012.43},
doi = {10.1109/TSE.2012.43},
abstract = {Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.},
journal = {IEEE Trans. Softw. Eng.},
month = apr,
pages = {552–569},
numpages = {18}
}

@article{10.5555/3546258.3546367,
author = {Perry, Ronan and Mischler, Gavin and Guo, Richard and Lee, Theodore and Chang, Alexander and Koul, Arman and Franz, Cameron and Richard, Hugo and Carmichael, Iain and Ablin, Pierre and Gramfort, Alexandre and Vogelstein, Joshua T.},
title = {mvlearn: multiview machine learning in Python},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {As data are generated more and more from multiple disparate sources, multiview data sets, where each sample has features in distinct views, have grown in recent years. However, no comprehensive package exists that enables non-specialists to use these methods easily. mvlearn is a Python library which implements the leading multiview machine learning methods. Its simple API closely follows that of scikit-learn for increased ease-of-use. The package can be installed from Python Package Index (PyPI) and the conda package manager and is released under the MIT open-source license.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {109},
numpages = {7},
keywords = {multiview, machine learning, python, multi-modal, multi-table, multi-block}
}

@article{10.1007/s00607-020-00809-6,
author = {Gupta, Deepak and Ahlawat, Anil K. and Sharma, Arun and Rodrigues, Joel J. P. C.},
title = {Feature selection and evaluation for software usability model using modified moth-flame optimization},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {102},
number = {6},
issn = {0010-485X},
url = {https://doi.org/10.1007/s00607-020-00809-6},
doi = {10.1007/s00607-020-00809-6},
abstract = {This paper introduces a nature-inspired optimized algorithm called modified moth-flame optimization (MMFO) for usability feature selection. To determine quality of software usability plays a significant role. This model contains various usability factors that are divided into several features, which have some characteristics, thus making a hierarchical model. Here, the authors have introduced MMFO (Modified Moth-flame optimization algorithm) for the selection of usability features to get an optimal solution MMFO is an extension of moth-flame optimization algorithm (MFO), which is based on the navigation method of moths called transverse orientation and to the best of our knowledge; this algorithm is introduced in software engineering practices. The selected features and accuracy of proposed MMFO is compared with the original MFO and other related optimization techniques. The results shows that the proposed nature-inspired optimization algorithm outperforms the other related optimizers as it generates a fewer number of selected features and having low accuracy.},
journal = {Computing},
month = jun,
pages = {1503–1520},
numpages = {18},
keywords = {Moth-flame optimization, Software usability model, Optimization, Feature selection, 91C05}
}

@inproceedings{10.1109/MSR.2019.00019,
author = {Ahluwalia, Aalok and Falessi, Davide and Di Penta, Massimiliano},
title = {Snoring: a noise in defect prediction datasets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00019},
doi = {10.1109/MSR.2019.00019},
abstract = {In order to develop and train defect prediction models, researchers rely on datasets in which a defect is often attributed to a release where the defect itself is discovered. However, in many circumstances, it can happen that a defect is only discovered several releases after its introduction. This might introduce a bias in the dataset, i.e., treating the intermediate releases as defect-free and the latter as defect-prone. We call this phenomenon as "sleeping defects". We call "snoring" the phenomenon where classes are affected by sleeping defects only, that would be treated as defect-free until the defect is discovered. In this paper we analyze, on data from 282 releases of six open source projects from the Apache ecosystem, the magnitude of the sleeping defects and of the snoring classes. Our results indicate that 1) on all projects, most of the defects in a project slept for more than 20% of the existing releases, and 2) in the majority of the projects the missing rate is more than 25% even if we remove the last 50% of releases.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {63–67},
numpages = {5},
keywords = {dataset bias, defect prediction, fix-inducing changes},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1007/s10586-019-02917-1,
author = {Mohammed, Bashir and Awan, Irfan and Ugail, Hassan and Younas, Muhammad},
title = {Failure prediction using machine learning in a virtualised HPC system and application},
year = {2019},
issue_date = {Mar 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-019-02917-1},
doi = {10.1007/s10586-019-02917-1},
abstract = {Failure is an increasingly important issue in high performance computing and cloud systems. As large-scale systems continue to grow in scale and complexity, mitigating the impact of failure and providing accurate predictions with sufficient lead time remains a challenging research problem. Traditional existing fault-tolerance strategies such as regular check-pointing and replication are not adequate because of the emerging complexities of high performance computing systems. This necessitates the importance of having an effective as well as proactive failure management approach in place aimed at minimizing the effect of failure within the system. With the advent of machine learning techniques, the ability to learn from past information to predict future pattern of behaviours makes it possible to predict potential system failure more accurately. Thus, in this paper, we explore the predictive abilities of machine learning by applying a number of algorithms to improve the accuracy of failure prediction. We have developed a failure prediction model using time series and machine learning, and performed comparison based tests on the prediction accuracy. The primary algorithms we considered are the support vector machine (SVM), random forest (RF), k-nearest neighbors (KNN), classification and regression trees (CART) and linear discriminant analysis (LDA). Experimental results indicates that the average prediction accuracy of our model using SVM when predicting failure is 90% accurate and effective compared to other algorithms. This finding implies that our method can effectively predict all possible future system and application failures within the system.},
journal = {Cluster Computing},
month = jun,
pages = {471–485},
numpages = {15},
keywords = {Cloud computing, Failure, High performance computing, Machine learning}
}

@inproceedings{10.1145/3143434.3143456,
author = {Hosni, Mohamed and Idri, Ali and Abran, Alain},
title = {Investigating heterogeneous ensembles with filter feature selection for software effort estimation},
year = {2017},
isbn = {9781450348539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3143434.3143456},
doi = {10.1145/3143434.3143456},
abstract = {Ensemble Effort Estimation (EEE) consists on predicting the software development effort by combining more than one single estimation technique. EEE has recently been investigated in software development effort estimation (SDEE) in order to improve the estimation accuracy. The overall results suggested that the EEE yield better prediction accuracy than single techniques. On the other hand, feature selection (FS) methods have been used in the area of SDEE for the purpose of reducing the dimensionality of a dataset size by eliminating the irrelevant and redundant features. Thus, the SDEE techniques are trained on a dataset with relevant features which can lead to improving the accuracy of their estimations. This paper aims at investigating the impact of two Filter feature selection methods: Correlation based Feature Selection (CFS) and RReliefF on the estimation accuracy of Heterogeneous (HT) ensembles. Four machine learning techniques (K-Nearest Neighbor, Support Vector Regression, Multilayer Perceptron and Decision Trees) were used as base techniques for the HT ensembles of this study. We evaluate the accuracy of these HT ensembles when their base techniques were trained on datasets preprocessed by the two feature selection methods. The HT ensembles use three combination rules: average, median, and inverse ranked weighted mean. The evaluation was carried out by means of eight unbiased accuracy measures through the leave-one-out-cross validation (LOOCV) technique over six datasets. The overall results suggest that all the attributes of most datasets used are relevant for building an accurate predictive technique since the ensembles constructed without features selection outperformed in general the ones using features selection. As for the combination rule, the median generally produces better results than the other two used in this empirical study.},
booktitle = {Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement},
pages = {207–220},
numpages = {14},
keywords = {accuracy, ensemble effort estimation, features selection, filter, machine learning},
location = {Gothenburg, Sweden},
series = {IWSM Mensura '17}
}

@inproceedings{10.1145/2786805.2786813,
author = {Jing, Xiaoyuan and Wu, Fei and Dong, Xiwei and Qi, Fumin and Xu, Baowen},
title = {Heterogeneous cross-company defect prediction by unified metric representation and CCA-based transfer learning},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786813},
doi = {10.1145/2786805.2786813},
abstract = {Cross-company defect prediction (CCDP) learns a prediction model by using training data from one or multiple projects of a source company and then applies the model to the target company data. Existing CCDP methods are based on the assumption that the data of source and target companies should have the same software metrics. However, for CCDP, the source and target company data is usually heterogeneous, namely the metrics used and the size of metric set are different in the data of two companies. We call CCDP in this scenario as heterogeneous CCDP (HCCDP) task. In this paper, we aim to provide an effective solution for HCCDP. We propose a unified metric representation (UMR) for the data of source and target companies. The UMR consists of three types of metrics, i.e., the common metrics of the source and target companies, source-company specific metrics and target-company specific metrics. To construct UMR for source company data, the target-company specific metrics are set as zeros, while for UMR of the target company data, the source-company specific metrics are set as zeros. Based on the unified metric representation, we for the first time introduce canonical correlation analysis (CCA), an effective transfer learning method, into CCDP to make the data distributions of source and target companies similar. Experiments on 14 public heterogeneous datasets from four companies indicate that: 1) for HCCDP with partially different metrics, our approach significantly outperforms state-of-the-art CCDP methods; 2) for HCCDP with totally different metrics, our approach obtains comparable prediction performances in contrast with within-project prediction results. The proposed approach is effective for HCCDP.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {496–507},
numpages = {12},
keywords = {Heterogeneous cross-company defect prediction (HCCDP), canonical correlation analysis (CCA), common metrics, company-specific metrics, unified metric representation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1016/j.infsof.2019.01.008,
author = {Meqdadi, Omar and Alhindawi, Nouh and Alsakran, Jamal and Saifan, Ahmad and Migdadi, Hatim},
title = {Mining software repositories for adaptive change commits using machine learning techniques},
year = {2019},
issue_date = {May 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {109},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.01.008},
doi = {10.1016/j.infsof.2019.01.008},
journal = {Inf. Softw. Technol.},
month = may,
pages = {80–91},
numpages = {12},
keywords = {Code change metrics, Adaptive maintenance, Commit types, Maintenance classification, Machine learning}
}

@inproceedings{10.1109/ICPC.2019.00023,
author = {Pecorelli, Fabiano and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea},
title = {Comparing heuristic and machine learning approaches for metric-based code smell detection},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00023},
doi = {10.1109/ICPC.2019.00023},
abstract = {Code smells represent poor implementation choices performed by developers when enhancing source code. Their negative impact on source code maintainability and comprehensibility has been widely shown in the past and several techniques to automatically detect them have been devised. Most of these techniques are based on heuristics, namely they compute a set of code metrics and combine them by creating detection rules; while they have a reasonable accuracy, a recent trend is represented by the use of machine learning where code metrics are used as predictors of the smelliness of code artefacts. Despite the recent advances in the field, there is still a noticeable lack of knowledge of whether machine learning can actually be more accurate than traditional heuristic-based approaches. To fill this gap, in this paper we propose a large-scale study to empirically compare the performance of heuristic-based and machine-learning-based techniques for metric-based code smell detection. We consider five code smell types and compare machine learning models with Decor, a state-of-the-art heuristic-based approach. Key findings emphasize the need of further research aimed at improving the effectiveness of both machine learning and heuristic approaches for code smell detection: while Decor generally achieves better performance than a machine learning baseline, its precision is still too low to make it usable in practice.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {93–104},
numpages = {12},
keywords = {code smells detection, empirical study, heuristics, machine learning},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@article{10.1504/IJDATS.2016.075971,
author = {Erturk, Ezgi and Sezer, Ebru Akcapinar},
title = {Software fault prediction using Mamdani type fuzzy inference system},
year = {2016},
issue_date = {April 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {8},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2016.075971},
doi = {10.1504/IJDATS.2016.075971},
abstract = {High quality software requires the occurrence of minimum number of failures while software runs. Software fault prediction is the determining whether software modules are prone to fault or not. Identification of the modules or code segments which need detailed testing, editing or, reorganising can be possible with the help of software fault prediction systems. In literature, many studies present models for software fault prediction using some soft computing methods which use training/testing phases. As a result, they require historical data to build models. In this study, to eliminate this drawback, Mamdani type fuzzy inference system FIS is applied for the software fault prediction problem. Several FIS models are produced and assessed with ROC-AUC as performance measure. The results achieved are ranging between 0.7138 and 0.7304; they are encouraging us to try FIS with the different software metrics and data to demonstrate general FIS performance on this problem.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = apr,
pages = {14–28},
numpages = {15}
}

@article{10.1016/j.compag.2021.106382,
author = {Maldaner, Leonardo Felipe and Molin, Jos\'{e} Paulo and Canata, Tatiana Fernanda and Martello, Maur\'{\i}cio},
title = {A system for plant detection using sensor fusion approach based on machine learning model},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {189},
number = {C},
issn = {0168-1699},
url = {https://doi.org/10.1016/j.compag.2021.106382},
doi = {10.1016/j.compag.2021.106382},
journal = {Comput. Electron. Agric.},
month = oct,
numpages = {11},
keywords = {Precision agriculture, Sugarcane, Machine learning, Optic sensor, Ultrasonic sensor, Data fusion}
}

@inproceedings{10.1145/2020390.2020406,
author = {Paikari, Elham and Sun, Bo and Ruhe, Guenther and Livani, Emadoddin},
title = {Customization support for CBR-based defect prediction},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020406},
doi = {10.1145/2020390.2020406},
abstract = {Background: The prediction performance of a case-based reasoning (CBR) model is influenced by the combination of the following parameters: (i) similarity function, (ii) number of nearest neighbor cases, (iii) weighting technique used for attributes, and (iv) solution algorithm. Each combination of the above parameters is considered as an instantiation of the general CBR-based prediction method. The selection of an instantiation for a new data set with specific characteristics (such as size, defect density and language) is called customization of the general CBR method.Aims: For the purpose of defect prediction, we approach the question which combinations of parameters works best at which situation. Three more specific questions were studied:(RQ1) Does one size fit all? Is one instantiation always the best?(RQ2) If not, which individual and combined parameter settings occur most frequently in generating the best prediction results?(RQ3) Are there context-specific rules to support the customization?Method: In total, 120 different CBR instantiations were created and applied to 11 data sets from the PROMISE repository. Predictions were evaluated in terms of their mean magnitude of relative error (MMRE) and percentage Pred(α) of objects fulfilling a prediction quality level α. For the third research question, dependency network analysis was performed.Results: Most frequent parameter options for CBR instantiations were neural network based sensitivity analysis (as the weighting technique), un-weighted average (as the solution algorithm), and maximum number of nearest neighbors (as the number of nearest neighbors). Using dependency network analysis, a set of recommendations for customization was provided.Conclusion: An approach to support customization is provided. It was confirmed that application of context-specific rules across groups of similar data sets is risky and produces poor results.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {16},
numpages = {10},
keywords = {case-based reasoning, customization, defect prediction, dependency network analysis, instantiation},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@inproceedings{10.1109/ICTAI.2011.172,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Impact of Data Sampling on Stability of Feature Selection for Software Measurement Data},
year = {2011},
isbn = {9780769545967},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2011.172},
doi = {10.1109/ICTAI.2011.172},
abstract = {Software defect prediction can be considered a binary classification problem. Generally, practitioners utilize historical software data, including metric and fault data collected during the software development process, to build a classification model and then employ this model to predict new program modules as either fault-prone (fp) or not-fault-prone (nfp). Limited project resources can then be allocated according to the prediction results by (for example) assigning more reviews and testing to the modules predicted to be potentially defective. Two challenges often come with the modeling process: (1) high-dimensionality of software measurement data and (2) skewed or imbalanced distributions between the two types of modules (fp and nfp) in those datasets. To overcome these problems, extensive studies have been dedicated towards improving the quality of training data. The commonly used techniques are feature selection and data sampling. Usually, researchers focus on evaluating classification performance after the training data is modified. The present study assesses a feature selection technique from a different perspective. We are more interested in studying the stability of a feature selection method, especially in understanding the impact of data sampling techniques on the stability of feature selection when using the sampled data. Some interesting findings are found based on two case studies performed on datasets from two real-world software projects.},
booktitle = {Proceedings of the 2011  IEEE 23rd International Conference on Tools with Artificial Intelligence},
pages = {1004–1011},
numpages = {8},
keywords = {data sampling, defect prediction, feature selection, software metrics, stability},
series = {ICTAI '11}
}

@article{10.1007/s11219-012-9180-0,
author = {\c{C}al\i{}kl\i{}, G\"{u}l and Bener, Ay\c{s}e Ba\c{s}ar},
title = {Influence of confirmation biases of developers on software quality: an empirical study},
year = {2013},
issue_date = {June      2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9180-0},
doi = {10.1007/s11219-012-9180-0},
abstract = {The thought processes of people have a significant impact on software quality, as software is designed, developed and tested by people. Cognitive biases, which are defined as patterned deviations of human thought from the laws of logic and mathematics, are a likely cause of software defects. However, there is little empirical evidence to date to substantiate this assertion. In this research, we focus on a specific cognitive bias, confirmation bias, which is defined as the tendency of people to seek evidence that verifies a hypothesis rather than seeking evidence to falsify a hypothesis. Due to this confirmation bias, developers tend to perform unit tests to make their program work rather than to break their code. Therefore, confirmation bias is believed to be one of the factors that lead to an increased software defect density. In this research, we present a metric scheme that explores the impact of developers' confirmation bias on software defect density. In order to estimate the effectiveness of our metric scheme in the quantification of confirmation bias within the context of software development, we performed an empirical study that addressed the prediction of the defective parts of software. In our empirical study, we used confirmation bias metrics on five datasets obtained from two companies. Our results provide empirical evidence that human thought processes and cognitive aspects deserve further investigation to improve decision making in software development for effective process management and resource allocation.},
journal = {Software Quality Journal},
month = jun,
pages = {377–416},
numpages = {40},
keywords = {Confirmation bias, Defect prediction, Human factors, Software psychology}
}

@article{10.1007/s10515-011-0090-3,
author = {He, Zhimin and Shu, Fengdi and Yang, Ye and Li, Mingshu and Wang, Qing},
title = {An investigation on the feasibility of cross-project defect prediction},
year = {2012},
issue_date = {June      2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-011-0090-3},
doi = {10.1007/s10515-011-0090-3},
abstract = {Software defect prediction helps to optimize testing resources allocation by identifying defect-prone modules prior to testing. Most existing models build their prediction capability based on a set of historical data, presumably from the same or similar project settings as those under prediction. However, such historical data is not always available in practice. One potential way of predicting defects in projects without historical data is to learn predictors from data of other projects. This paper investigates defect predictions in the cross-project context focusing on the selection of training data. We conduct three large-scale experiments on 34 data sets obtained from 10 open source projects. Major conclusions from our experiments include: (1) in the best cases, training data from other projects can provide better prediction results than training data from the same project; (2) the prediction results obtained using training data from other projects meet our criteria for acceptance on the average level, defects in 18 out of 34 cases were predicted at a Recall greater than 70% and a Precision greater than 50%; (3) results of cross-project defect predictions are related with the distributional characteristics of data sets which are valuable for training data selection. We further propose an approach to automatically select suitable training data for projects without historical data. Prediction results provided by the training data selected by using our approach are comparable with those provided by training data from the same project.},
journal = {Automated Software Engg.},
month = jun,
pages = {167–199},
numpages = {33},
keywords = {Cross-project, Data characteristics, Defect prediction, Machine learning, Training data}
}

@article{10.1007/s10586-018-1923-7,
author = {Viji, C. and Rajkumar, N. and Duraisamy, S.},
title = {Prediction of software fault-prone classes using an unsupervised hybrid SOM algorithm},
year = {2019},
issue_date = {Jan 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {1386-7857},
url = {https://doi.org/10.1007/s10586-018-1923-7},
doi = {10.1007/s10586-018-1923-7},
abstract = {In software engineering fault proneness prediction is one of the important fields for quality measurement using multiple code metrics. The metrics thresholds are very practical in measuring the code quality for fault proneness prediction. It helps to improvise the software quality in short time with very low cost. Many researchers are in the race to develop a measuring attribute for the software quality using various methodologies. Currently so many fault proneness prediction models are available. Among that most of the methods are used to identify the faults either by data history or by special supervising algorithms. In most of the real time cases the fault data bases may not be available so that the process becomes tedious. This article proposes a hybrid model for identifying the faults in the software models and also we proposed coupling model along with the algorithm so that the metrics are used to identify the faults and the coupling model couples the metrics and the faults for the developed system software.},
journal = {Cluster Computing},
month = jan,
pages = {133–143},
numpages = {11},
keywords = {Software metrics, Fault prediction, Coupling, Fault proneness, ANN}
}

@article{10.1016/j.jss.2015.10.011,
author = {Cerpa, Narciso and Bardeen, Matthew and Astudillo, C\'{e}sar A. and Verner, June},
title = {Evaluating different families of prediction methods for estimating software project outcomes},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {112},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.10.011},
doi = {10.1016/j.jss.2015.10.011},
abstract = {We compare classifiers using AUC when predicting software project outcome.Attribute selection using Information Gain improves our classifiers performance.Statistical and ensemble classifiers are robust for predicting project outcome.Random Forest is the most appropriate technique for determining project outcome.Best prediction is achieved with team dynamics, process, and estimation attributes. Software has been developed since the 1960s but the success rate of development projects is still low. Classification models have been used to predict defects and effort estimation, but little work has been done to predict the outcome of these projects. Previous research shows that it is possible to predict outcome using classifiers based on key variables during development, but it is not clear which techniques provide more accurate predictions. We benchmark classifiers from different families to determine the outcome of a software project and identify variables that influence it. A survey-based empirical investigation was used to examine variables contributing to project outcome. Classification models were built and tested to identify the best classifiers for this data by comparing their AUC values. We reduce the dimensionality of the data with Information Gain and build models with the same techniques. We use Information Gain and classification techniques to identify key attributes and their relative importance. We find that four classification techniques provide good results for survey data, regardless of dimensionality reduction. We conclude that Random Forest is the most appropriate technique for predicting project outcome. We identified key attributes which are related to communication, estimation, and process review.},
journal = {J. Syst. Softw.},
month = feb,
pages = {48–64},
numpages = {17},
keywords = {AUC analysis, Classification techniques, Project outcome predictors}
}

@article{10.1007/s11761-016-0202-9,
author = {Kumar, Lov and Krishna, Aneesh and Rath, Santanu Ku.},
title = {The impact of feature selection on maintainability prediction of service-oriented applications},
year = {2017},
issue_date = {June      2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {2},
issn = {1863-2386},
url = {https://doi.org/10.1007/s11761-016-0202-9},
doi = {10.1007/s11761-016-0202-9},
abstract = {Service-oriented development methodologies are very often considered for distributed system development. The quality of service-oriented computing can be best assessed by the use of software metrics that are considered to design the prediction model. Feature selection technique is a process of selecting a subset of features that may lead to build improved prediction models. Feature selection techniques can be broadly classified into two subclasses such as feature ranking and feature subset selection technique. In this study, eight different types of feature ranking and four different types of feature subset selection techniques have been considered for improving the performance of a prediction model focusing on maintainability criterion. The performance of these feature selection techniques is evaluated using support vector machine with different types of kernels over a case study, i.e., five different versions of eBay Web service. The performances are measured using accuracy and F-measure value. The results show that maintainability of the service-oriented computing paradigm can be predicted by using object-oriented metrics. The results also show that it is possible to find a small subset of object-oriented metrics which helps to predict maintainability with higher accuracy and also reduces the value of misclassification errors.},
journal = {Serv. Oriented Comput. Appl.},
month = jun,
pages = {137–161},
numpages = {25},
keywords = {Feature selection techniques, Kernel function, Maintainability, Object-oriented metrics, SVM, Service-oriented computing}
}

@inproceedings{10.5555/951951.952272,
author = {Khoshgoftaar, Taghi M. and Nguyen, Laurent and Gao, Kehan and Rajeevalochanam, Jayanth},
title = {Application of an Attribute Selection Method to CBR-Based Software Quality Classification},
year = {2003},
isbn = {0769520383},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This study investigates the attribute selection problem for reducing the number of software metrics (program attributes) used by a case-based reasoning (CBR) software quality classification model. The metrics are selected using the Kolmogorov-Smirnov (K-S) two sample test. The "Modified Expected Cost of Misclassification" measure, recently proposed by ourresearch team, is used as a performance measure to select, evaluate, and compare classification models. The attribute selection procedure presented in this paper can assist a software development organization in determining the software metrics that are better indicators of software quality. By reducing the number of software metrics to be collected during the development process, the metrics data collection task can be simplified. Moreover, reducing the number of metrics would result in reducing the computation time of a cbr model. Using an empirical case study of a real-world software system, it is shown that with a reduced number of metrics the cbr technique is capable of yielding useful software quality classification models. Moreover, their performances were better than or similar to cbr models calibrated without attribute selection.},
booktitle = {Proceedings of the 15th IEEE International Conference on Tools with Artificial Intelligence},
pages = {47},
keywords = {Kolmogorov-Smirnov two-sample test, case-based reasoning, modified expected cost of misclassification, software metrics selection, software quality},
series = {ICTAI '03}
}

@article{10.1145/3324916,
author = {Ren, Xiaoxue and Xing, Zhenchang and Xia, Xin and Lo, David and Wang, Xinyu and Grundy, John},
title = {Neural Network-based Detection of Self-Admitted Technical Debt: From Performance to Explainability},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3324916},
doi = {10.1145/3324916},
abstract = {Technical debt is a metaphor to reflect the tradeoff software engineers make between short-term benefits and long-term stability. Self-admitted technical debt (SATD), a variant of technical debt, has been proposed to identify debt that is intentionally introduced during software development, e.g., temporary fixes and workarounds. Previous studies have leveraged human-summarized patterns (which represent n-gram phrases that can be used to identify SATD) or text-mining techniques to detect SATD in source code comments. However, several characteristics of SATD features in code comments, such as vocabulary diversity, project uniqueness, length, and semantic variations, pose a big challenge to the accuracy of pattern or traditional text-mining-based SATD detection, especially for cross-project deployment. Furthermore, although traditional text-mining-based method outperforms pattern-based method in prediction accuracy, the text features it uses are less intuitive than human-summarized patterns, which makes the prediction results hard to explain. To improve the accuracy of SATD prediction, especially for cross-project prediction, we propose a Convolutional Neural Network-- (CNN) based approach for classifying code comments as SATD or non-SATD. To improve the explainability of our model’s prediction results, we exploit the computational structure of CNNs to identify key phrases and patterns in code comments that are most relevant to SATD. We have conducted an extensive set of experiments with 62,566 code comments from 10 open-source projects and a user study with 150 comments of another three projects. Our evaluation confirms the effectiveness of different aspects of our approach and its superior performance, generalizability, adaptability, and explainability over current state-of-the-art traditional text-mining-based methods for SATD classification.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {15},
numpages = {45},
keywords = {Self-admitted technical debt, convolutional neural network, cross project prediction, model adaptability, model explainability, model generalizability}
}

@article{10.1007/s10270-020-00856-9,
author = {Pilarski, Sebastian and Staniszewski, Martin and Bryan, Matthew and Villeneuve, Frederic and Varr\'{o}, D\'{a}niel},
title = {Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {20},
number = {3},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-020-00856-9},
doi = {10.1007/s10270-020-00856-9},
abstract = {The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers.},
journal = {Softw. Syst. Model.},
month = jun,
pages = {685–709},
numpages = {25},
keywords = {Prediction-at-runtime, Machine learning, Neural networks, Automated deployment, Code generation, Gas turbine engines}
}

@article{10.4018/ijsi.2014100105,
author = {Abaei, Golnoush and Selamat, Ali},
title = {Increasing the Accuracy of Software Fault Prediction using Majority Ranking Fuzzy Clustering},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {2},
number = {4},
issn = {2166-7160},
url = {https://doi.org/10.4018/ijsi.2014100105},
doi = {10.4018/ijsi.2014100105},
abstract = {Despite proposing many software fault prediction models, this area has yet to be explored as still there is a room for stable and consistent model with better performance. In this paper, a new method is proposed to increase the accuracy of fault prediction based on the notion of fuzzy clustering and majority ranking. The authors investigated the effect of irrelevant and inconsistent modules on software fault prediction and tried to decrease it by designing a new framework, in which the entire project modules are clustered. The obtained results showed that fuzzy clustering could decrease the negative effect of irrelevant modules on prediction performance. Eight data sets from NASA and Turkish white-goods software is employed to evaluate our model. Performance evaluation in terms of false positive rate, false negative rate, and overall error showed the superiority of our model compared to other predicting models. The authors proposed majority ranking fuzzy clustering approach showed between 3% to 18% and 1% to 4% improvement in false negative rate and overall error, respectively, compared with other available proposed models (ACF and ACN) in more than half of the testing cases. According to the results, our systems can be used to guide testing effort by identifying fault prone modules to improve the quality of software development and software testing in a limited time and budget.},
journal = {Int. J. Softw. Innov.},
month = oct,
pages = {60–71},
numpages = {12},
keywords = {Available Proposed Models, Fuzzy Clustering, Majority Ranking, NASA, Software Fault Prediction}
}

@inproceedings{10.1109/ASE.2015.56,
author = {Nam, Jaechang and Kim, Sunghun},
title = {CLAMI: defect prediction on unlabeled datasets},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.56},
doi = {10.1109/ASE.2015.56},
abstract = {Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort.In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {452–463},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3460319.3464811,
author = {Zohdinasab, Tahereh and Riccio, Vincenzo and Gambi, Alessio and Tonella, Paolo},
title = {DeepHyperion: exploring the feature space of deep learning-based systems through illumination search},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464811},
doi = {10.1145/3460319.3464811},
abstract = {Deep Learning (DL) has been successfully applied to a wide range of application domains, including safety-critical ones. Several DL testing approaches have been recently proposed in the literature but none of them aims to assess how different interpretable features of the generated inputs affect the system's behaviour.  In this paper, we resort to Illumination Search to find the highest-performing test cases (i.e., misbehaving and closest to misbehaving), spread across the cells of a map representing the feature space of the system. We introduce a methodology that guides the users of our approach in the tasks of identifying and quantifying the dimensions of the feature space for a given domain. We developed DeepHyperion, a search-based tool for DL systems that illuminates, i.e., explores at large, the feature space, by providing developers with an interpretable feature map where automatically generated inputs are placed along with information about the exposed behaviours.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {79–90},
numpages = {12},
keywords = {deep learning, search based software engineering, self-driving cars, software testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1007/s42979-019-0004-1,
author = {Tran, Ha Manh and Le, Son Thanh and Nguyen, Sinh Van and Ho, Phong Thanh},
title = {An Analysis of Software Bug Reports Using Machine Learning Techniques},
year = {2019},
issue_date = {Jan 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {1},
number = {1},
url = {https://doi.org/10.1007/s42979-019-0004-1},
doi = {10.1007/s42979-019-0004-1},
abstract = {Bug tracking systems manage bug reports for assuring the quality of software products. A bug report (alsoreferred as trouble, problem, ticket or defect) contains several features for problem management and resolution purposes. Severity and priority are two essential features of a bug report that define the effect level and fixing order of the bug. Determining these features is challenging and depends heavily on human being, e.g., software developers or system operators, especially for assessing a large number of error and warning events occurring on software products or network services. This study first proposes a comparison of machine learning techniques for assessing severity and priority for software bug reports and then chooses an approach of using optimal decision trees, or random forest, for further investigation. This approach aims at constructing multiple decision trees based on the subsets of the existing bug dataset and features, and then selecting the best decision trees to assess the severity and priority of new bugs. The approach can be applied for detecting and forecasting faults in large, complex communication networks and distributed systems today. We have presented the applicability of random forest for bug report analysis and performed several experiments on software bug datasets obtained from open source bug tracking systems. Random forest yields an average accuracy score of 0.75 that can be sufficient for assisting system operators in determining these features. We have provided some analysis of the experimental results.},
journal = {SN Comput. Sci.},
month = jun,
numpages = {11},
keywords = {Network fault detection, Fault management, Machine learning, Data analytics, Software bug report}
}

@book{10.5555/2911053,
author = {Mistrik, Ivan and Soley, Richard M. and Ali, Nour and Grundy, John and Tekinerdogan, Bedir},
title = {Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year = {2015},
isbn = {0128023015},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrows) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality.Focused on quality assurance at all levels of software design and developmentCovers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systemsExplains likely trade-offs from design decisions in the context of complex software system engineering and quality assuranceIncludes practical case studies of software quality assurance for complex, adaptive and context-critical systems}
}

@article{10.1016/j.future.2019.11.042,
author = {Loreti, Daniela and Lippi, Marco and Torroni, Paolo},
title = {Parallelizing Machine Learning as a service for the end-user},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.11.042},
doi = {10.1016/j.future.2019.11.042},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {275–286},
numpages = {12},
keywords = {Machine Learning as a service, Parallelization, MapReduce}
}

@article{10.1016/j.infsof.2010.05.009,
author = {Oliveira, Adriano L. I. and Braga, Petronio L. and Lima, Ricardo M. F. and Corn\'{e}lio, M\'{a}rcio L.},
title = {GA-based method for feature selection and parameters optimization for machine learning regression applied to software effort estimation},
year = {2010},
issue_date = {November, 2010},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {52},
number = {11},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.05.009},
doi = {10.1016/j.infsof.2010.05.009},
abstract = {Context: In software industry, project managers usually rely on their previous experience to estimate the number men/hours required for each software project. The accuracy of such estimates is a key factor for the efficient application of human resources. Machine learning techniques such as radial basis function (RBF) neural networks, multi-layer perceptron (MLP) neural networks, support vector regression (SVR), bagging predictors and regression-based trees have recently been applied for estimating software development effort. Some works have demonstrated that the level of accuracy in software effort estimates strongly depends on the values of the parameters of these methods. In addition, it has been shown that the selection of the input features may also have an important influence on estimation accuracy. Objective: This paper proposes and investigates the use of a genetic algorithm method for simultaneously (1) select an optimal input feature subset and (2) optimize the parameters of machine learning methods, aiming at a higher accuracy level for the software effort estimates. Method: Simulations are carried out using six benchmark data sets of software projects, namely, Desharnais, NASA, COCOMO, Albrecht, Kemerer and Koten and Gray. The results are compared to those obtained by methods proposed in the literature using neural networks, support vector machines, multiple additive regression trees, bagging, and Bayesian statistical models. Results: In all data sets, the simulations have shown that the proposed GA-based method was able to improve the performance of the machine learning methods. The simulations have also demonstrated that the proposed method outperforms some recent methods reported in the recent literature for software effort estimation. Furthermore, the use of GA for feature selection considerably reduced the number of input features for five of the data sets used in our analysis. Conclusions: The combination of input features selection and parameters optimization of machine learning methods improves the accuracy of software development effort. In addition, this reduces model complexity, which may help understanding the relevance of each input feature. Therefore, some input parameters can be ignored without loss of accuracy in the estimations.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1155–1166},
numpages = {12},
keywords = {Feature selection, Genetic algorithms, Regression, Software effort estimation, Support vector regression}
}

@article{10.1016/j.jss.2016.04.058,
author = {Idri, Ali and Abnane, Ibtissam and Abran, Alain},
title = {Missing data techniques in analogy-based software development effort estimation},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.04.058},
doi = {10.1016/j.jss.2016.04.058},
abstract = {Evaluating Analogy effort estimation techniques using three missing data approaches.Classical/Fuzzy Analogy with KNN give better accuracy than with toleration/deletion.Fuzzy Analogy generates more accurate estimates than Classical Analogy in all casesMissingness mechanisms have different negative impacts on the accuracy of Analogy. Missing Data (MD) is a widespread problem that can affect the ability to use data to construct effective software development effort prediction systems. This paper investigates the use of missing data (MD) techniques with two analogy-based software development effort estimation techniques: Classical Analogy and Fuzzy Analogy. More specifically, we analyze the predictive performance of these two analogy-based techniques when using toleration, deletion or k-nearest neighbors (KNN) imputation techniques. A total of 1512 experiments were conducted involving seven data sets, three MD techniques (toleration, deletion and KNN imputation), three missingness mechanisms (MCAR: missing completely at random, MAR: missing at random, NIM: non-ignorable missing), and MD percentages from 10 percent to 90 percent. The results suggest that Fuzzy Analogy generates more accurate estimates in terms of the Standardized Accuracy measure (SA) than Classical Analogy regardless of the MD technique, the data set used, the missingness mechanism or the MD percentage. Moreover, this study found that the use of KNN imputation, rather than toleration or deletion, may improve the prediction accuracy of both analogy-based techniques. However, toleration, deletion and KNN imputation are affected by the missingness mechanism and the MD percentage, both of which have a strong negative impact upon effort prediction accuracy.},
journal = {J. Syst. Softw.},
month = jul,
pages = {595–611},
numpages = {17},
keywords = {Analogy-based software development effort estimation, Imputation, Missing data}
}

@article{10.1109/TSE.2012.20,
author = {Dejaeger, Karel and Verbraken, Thomas and Baesens, Bart},
title = {Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers},
year = {2013},
issue_date = {February 2013},
publisher = {IEEE Press},
volume = {39},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2012.20},
doi = {10.1109/TSE.2012.20},
abstract = {Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Dem\v{s}ar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.},
journal = {IEEE Trans. Softw. Eng.},
month = feb,
pages = {237–257},
numpages = {21},
keywords = {Bayesian methods, Bayesian networks, Capability maturity model, Machine learning, Measurement, Predictive models, Probability distribution, Software, Software fault prediction, classification, comprehensibility}
}

@inproceedings{10.1109/ICMLA.2013.110,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {An Empirical Study on Wrapper-Based Feature Selection for Software Engineering Data},
year = {2013},
isbn = {9780769551449},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2013.110},
doi = {10.1109/ICMLA.2013.110},
abstract = {Software metrics give valuable information for understanding and predicting the quality of software modules, and thus it is important to select the right software metrics for building software quality classification models. In this paper we focus on wrapper-based feature (metric) selection techniques, which evaluate the merit of feature subsets based on the performance of classification models. We seek to understand the relationship between the internal learner used inside wrappers and the external learner for building the final classification model. We perform experiments using four consecutive releases of a very large telecommunications system, which include 42 software metrics (and with defect data collected for every program module). Our results demonstrate that (1) the best performance is never found when the internal and external learner match, (2)the best performance is usually found by using NB (Na\"{\i}ve Bayes) inside the wrapper unless SVM (Support Vector Machine) is external learner, (3) LR (Logistic Regression) is often the best learner to use for building classification models regardless of which learner was used inside the wrapper.},
booktitle = {Proceedings of the 2013 12th International Conference on Machine Learning and Applications - Volume 02},
pages = {84–89},
numpages = {6},
keywords = {learner, software quality prediction model, wrapper-based feature selection},
series = {ICMLA '13}
}

@article{10.1016/j.eswa.2010.09.136,
author = {Chiu, Nan-Hsing},
title = {Combining techniques for software quality classification: An integrated decision network approach},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.09.136},
doi = {10.1016/j.eswa.2010.09.136},
abstract = {Accurately predicting fault-prone modules is a major problem in quality control of a software system during software development. Selecting an appropriate suggestion from various software quality classification models is a difficult decision for software project managers. In this paper, an integrated decision network is proposed to combine the well-known software quality classification models in providing the summarized suggestion. A particle swarm optimization algorithm is used to search for suitable combinations among the software quality classification models in the integrated decision network. The experimental results show that the proposed integrated decision network outperforms the independent software quality classification models. It also provides an appropriate summary for decision makers.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4618–4625},
numpages = {8},
keywords = {Decision support systems, Integrated decision network, Particle swarm optimization, Software project management, Software quality classification}
}

@article{10.1016/j.ins.2021.05.008,
author = {Zhang, Nana and Ying, Shi and Ding, Weiping and Zhu, Kun and Zhu, Dandan},
title = {WGNCS: A robust hybrid cross-version defect model via multi-objective optimization and deep enhanced feature representation},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {570},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.008},
doi = {10.1016/j.ins.2021.05.008},
journal = {Inf. Sci.},
month = sep,
pages = {545–576},
numpages = {32},
keywords = {Cross-version defect prediction, Multi-objective feature selection, Deep learning techniques, Wasserstein GAN with Gradient Penalty, Convolutional neural network}
}

@inproceedings{10.1145/2070821.2070823,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {Software fault localization using feature selection},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070823},
doi = {10.1145/2070821.2070823},
abstract = {Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code -- the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults in most subject programs of the Siemens suite, which have previously been used to evaluate several fault localization techniques.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {RELIEF, automated debugging, fault localization, feature selection, machine learning, statistical debugging},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1007/s11219-014-9240-8,
author = {Galinac Grbac, Tihana and Car, \v{Z}eljka and Huljeni\'{c}, Darko},
title = {A quality cost reduction model for large-scale software development},
year = {2015},
issue_date = {June      2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-014-9240-8},
doi = {10.1007/s11219-014-9240-8},
abstract = {Understanding quality costs is recognized as a prerequisite for decreasing the variability of the success of software development projects. This paper presents an empirical quality cost reduction (QCR) model to support the decision-making process for additional investment in the early phases of software verification. The main idea of the QCR model is to direct additional investment into software units that have some fault-slip potential in their later verification phases, with the aim of reducing costs and increasing product quality. The fault-slip potential of a software unit within a system is determined by analogy with historical projects. After a preliminary study on a sample of software units, which proves that we can lower quality costs with additional investment into particular verification activities, we examine the effectiveness of the proposed QCR model using real project data. The results show that applying the model produces a positive business case, meaning that the model lowers quality costs and increases quality, resulting in economic benefit. The potential to reduce quality costs is growing significantly with the evolution of software systems and the reuse of their software units. The proposed model is the result of a research project performed at Ericsson.},
journal = {Software Quality Journal},
month = jun,
pages = {363–390},
numpages = {28},
keywords = {Control model, Fault detection, Large-scale software, Quality cost, Verification}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {56},
numpages = {36},
keywords = {Software vulnerability analysis, data-mining, machine-learning, review, software security, software vulnerability discovery, survey}
}

@inproceedings{10.1145/3416505.3423563,
author = {Palma, Stefano Dalla and Mohammadi, Majid and Di Nucci, Dario and Tamburri, Damian A.},
title = {Singling the odd ones out: a novelty detection approach to find defects in infrastructure-as-code},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423563},
doi = {10.1145/3416505.3423563},
abstract = {Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {31–36},
numpages = {6},
keywords = {Defect Prediction, Infrastructure-as-Code, Novelty Detection},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@article{10.1145/3450288,
author = {Lo, Sin Kit and Lu, Qinghua and Wang, Chen and Paik, Hye-Young and Zhu, Liming},
title = {A Systematic Literature Review on Federated Machine Learning: From a Software Engineering Perspective},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3450288},
doi = {10.1145/3450288},
abstract = {Federated learning is an emerging machine learning paradigm where clients train models locally and formulate a global model based on the local model updates. To identify the state-of-the-art in federated learning and explore how to develop federated learning systems, we perform a systematic literature review from a software engineering perspective, based on 231 primary studies. Our data synthesis covers the lifecycle of federated learning system development that includes background understanding, requirement analysis, architecture design, implementation, and evaluation. We highlight and summarise the findings from the results and identify future trends to encourage researchers to advance their current work.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {95},
numpages = {39},
keywords = {Federated learning, distributed learning, edge learning, privacy, software engineering, systematic literature review}
}

@article{10.1007/s11219-019-09490-1,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu and Mensah, Solomon and Amankwah, Richard and Kudjo, Christopher},
title = {The effect of Bellwether analysis on software vulnerability severity prediction models},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09490-1},
doi = {10.1007/s11219-019-09490-1},
abstract = {Vulnerability severity prediction (VSP) models provide useful insight for vulnerability prioritization and software maintenance. Previous studies have proposed a variety of machine learning algorithms as an important paradigm for VSP. However, to the best of our knowledge, there are no other existing research studies focusing on investigating how a subset of features can be used to improve VSP. To address this deficiency, this paper presents a general framework for VSP using the Bellwether analysis (i.e., exemplary data). First, we apply the natural language processing techniques to the textual descriptions of software vulnerability. Next, we developed an algorithm termed Bellvul to identify and select an exemplary subset of data (referred to as Bellwether) to be considered as the training set to yield improved prediction accuracy against the growing portfolio, within-project cases, and the k-fold cross-validation subset. Finally, we assessed the performance of four machine learning algorithms, namely, deep neural network, logistic regression, k-nearest neighbor, and random forest using the sampled instances. The prediction results of the suggested models and the benchmark techniques were assessed based on the standard classification evaluation metrics such as precision, recall, and F-measure. The experimental result shows that the Bellwether approach achieves F-measure ranging from 14.3% to 97.8%, which is an improvement over the benchmark techniques. In conclusion, the proposed approach is a promising research direction for assisting software engineers when seeking to predict instances of vulnerability records that demand much attention prior to software release.},
journal = {Software Quality Journal},
month = dec,
pages = {1413–1446},
numpages = {34},
keywords = {Bellwether, Software vulnerability, Feature selection, Machine learning algorithms, Severity}
}

@inproceedings{10.1145/2597073.2597075,
author = {Fukushima, Takafumi and Kamei, Yasutaka and McIntosh, Shane and Yamashita, Kazuhiro and Ubayashi, Naoyasu},
title = {An empirical study of just-in-time defect prediction using cross-project models},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597075},
doi = {10.1145/2597073.2597075},
abstract = {Prior research suggests that predicting defect-inducing changes, i.e., Just-In-Time (JIT) defect prediction is a more practical alternative to traditional defect prediction techniques, providing immediate feedback while design decisions are still fresh in the minds of developers. Unfortunately, similar to traditional defect prediction models, JIT models require a large amount of training data, which is not available when projects are in initial development phases. To address this flaw in traditional defect prediction, prior work has proposed cross-project models, i.e., models learned from older projects with sufficient history. However, cross-project models have not yet been explored in the context of JIT prediction. Therefore, in this study, we empirically evaluate the performance of JIT cross-project models. Through a case study on 11 open source projects, we find that in a JIT cross-project context: (1) high performance within-project models rarely perform well; (2) models trained on projects that have similar correlations between predictor and dependent variables often perform well; and (3) ensemble learning techniques that leverage historical data from several other projects (e.g., voting experts) often perform well. Our findings empirically confirm that JIT cross-project models learned using other projects are a viable solution for projects with little historical data. However, JIT cross-project models perform best when the data used to learn them is carefully selected.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {172–181},
numpages = {10},
keywords = {Empirical study, software quality},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.1016/j.eswa.2007.02.012,
author = {Bibi, S. and Tsoumakas, G. and Stamelos, I. and Vlahavas, I.},
title = {Regression via Classification applied on software defect estimation},
year = {2008},
issue_date = {April, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {3},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.02.012},
doi = {10.1016/j.eswa.2007.02.012},
abstract = {In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {2091–2101},
numpages = {11},
keywords = {ISBSG data set, Machine learning, Regression via Classification, Software fault estimation, Software metrics, Software quality}
}

@inproceedings{10.1145/3106237.3106256,
author = {Fu, Wei and Menzies, Tim},
title = {Easy over hard: a case study on deep learning},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106256},
doi = {10.1145/3106237.3106256},
abstract = {While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.  For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method.  We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {49–60},
numpages = {12},
keywords = {SVM, Search based software engineering, data analytics for software engineering, deep learning, differential evolution, parameter tuning, software analytic},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3493244.3493262,
author = {e Silva, Jonathan Messias and de Lima J\'{u}nior, Manoel Limeira},
title = {Prediction of Pull Requests Review Time in Open Source Projects},
year = {2021},
isbn = {9781450395533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493244.3493262},
doi = {10.1145/3493244.3493262},
abstract = {In open-source projects that receive large amounts of pull requests, the tasks of maintaining quality and prioritizing code review have become a complex task. In this sense, several works explored data on pull requests in order to provide useful information. Although, the review time was treated as the interval between the submission and the integration of the pull requests, that is, the lifetime itself. Since December 2016, a feature allows requesting reviews to one or more specific reviewers, which, together with the review status, allowed to establish the period closest to the effective code review time, the interval between the review request and the last review with approval status. In this context, the main objective of this work is to predict the review time of pull requests. Furthermore, the lifetime and acceptance of pull requests with and without review time were compared and the CFS (Correlation-based Feature Selection) attribute selection strategy was used to identify those most relevant to the forecast. The results of the experiments indicate that the SMO (Sequential Minimal Optimization) algorithm had the smallest error, averaging 8,504 minutes (approximately 5,9 days) and that the presence of approvals in the review requests has a positive influence on both the acceptance and in the pull request lifetime.},
booktitle = {Proceedings of the XX Brazilian Symposium on Software Quality},
articleno = {11},
numpages = {10},
keywords = {Distributed software development, lifetime, pull request, review time},
location = {Virtual Event, Brazil},
series = {SBQS '21}
}

@inproceedings{10.1109/GrC.2010.104,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Van Hulse, Jason},
title = {A Comparative Study of Threshold-Based Feature Selection Techniques},
year = {2010},
isbn = {9780769541617},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/GrC.2010.104},
doi = {10.1109/GrC.2010.104},
abstract = {Given high-dimensional software measurement data, researchers and practitioners often use feature (metric) selection techniques to improve the performance of software quality classification models. This paper presents our newly proposed threshold-based feature selection techniques, comparing the performance of these techniques by building classification models using five commonly used classifiers. In order to evaluate the effectiveness of different feature selection techniques, the models are evaluated using eight different performance metrics separately since a given performance metric usually captures only one aspect of the classification performance. All experiments are conducted on three Eclipse data sets with different levels of class imbalance. The experiments demonstrate that the choice of a performance metric may significantly influence the results. In this study, we have found four distinct patterns when utilizing eight performance metrics to order 11 threshold-based feature selection techniques. Moreover, performances of the software quality models either improve or remain unchanged despite the removal of over 96% of the software metrics (attributes).},
booktitle = {Proceedings of the 2010 IEEE International Conference on Granular Computing},
pages = {499–504},
numpages = {6},
keywords = {classification, performance metrics, software metrics, threshold-based feature selection technique},
series = {GRC '10}
}

@article{10.1504/ijaip.2019.101983,
author = {Kumar, Reddi Kiran and Rao, S.V. Achuta},
title = {Severity of defect: an optimised prediction},
year = {2019},
issue_date = {2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {13},
number = {3–4},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2019.101983},
doi = {10.1504/ijaip.2019.101983},
abstract = {To assure the quality of software an important activity is performed namely software defect prediction (SDP). Historical databases are used to detect software defects using different machine learning techniques. Conversely, there are disadvantages like testing becomes expensive, poor quality and so the product is unreliable for use. This paper classifies the severity of defects by using a method based on optimised neural network (NN). In full search space, a solution is found by many meta-heuristic optimisations and global search ability has been used. Hence, high-quality solutions are finding within a reasonable period of time. SDP performance is improved by the combination of meta-heuristic optimisation methods. For class imbalance problem, meta-heuristic optimisation methods such as genetic algorithm (GA) and shuffled frog leaping algorithm (SFLA) are applied. The above method is based on SFLA and the experimental outputs show that it can do better than Leven berg Marquardt based NN system (LM-NN).},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {334–345},
numpages = {11},
keywords = {software defect prediction, SDP, severity, neural network, Levenberg Marquardt, LM, shuffled frog and fuzzy classifier}
}

@article{10.1007/s11219-008-9058-3,
author = {Khoshgoftaar, Taghi M. and Rebours, Pierre and Seliya, Naeem},
title = {Software quality analysis by combining multiple projects and learners},
year = {2009},
issue_date = {March     2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9058-3},
doi = {10.1007/s11219-008-9058-3},
abstract = {When building software quality models, the approach often consists of training data mining learners on a single fit dataset. Typically, this fit dataset contains software metrics collected during a past release of the software project that we want to predict the quality of. In order to improve the predictive accuracy of such quality models, it is common practice to combine the predictive results of multiple learners to take advantage of their respective biases. Although multi-learner classifiers have been proven to be successful in some cases, the improvement is not always significant because the information in the fit dataset sometimes can be insufficient. We present an innovative method to build software quality models using majority voting to combine the predictions of multiple learners induced on multiple training datasets. To our knowledge, no previous study in software quality has attempted to take advantage of multiple software project data repositories which are generally spread across the organization. In a large scale empirical study involving seven real-world datasets and seventeen learners, we show that, on average, combining the predictions of one learner trained on multiple datasets significantly improves the predictive performance compared to one learner induced on a single fit dataset. We also demonstrate empirically that combining multiple learners trained on a single training dataset does not significantly improve the average predictive accuracy compared to the use of a single learner induced on a single fit dataset.},
journal = {Software Quality Journal},
month = mar,
pages = {25–49},
numpages = {25},
keywords = {Cost of misclassification, Data mining, Majority voting, Multiple learners, Multiple software metrics repositories, Software quality classification model}
}

@inproceedings{10.1145/3345629.3345632,
author = {Polisetty, Sravya and Miranskyy, Andriy and Ba\c{s}ar, Ay\c{s}e},
title = {On Usefulness of the Deep-Learning-Based Bug Localization Models to Practitioners},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345632},
doi = {10.1145/3345629.3345632},
abstract = {Background: Developers spend a significant amount of time and effort to localize bugs. In the literature, many researchers proposed state-of-the-art bug localization models to help developers localize bugs easily. The practitioners, on the other hand, expect a bug localization tool to meet certain criteria, such as trustworthiness, scalability, and efficiency. The current models are not capable of meeting these criteria, making it harder to adopt these models in practice. Recently, deep-learning-based bug localization models have been proposed in the literature. They show a better performance than the state-of-the-art models.Aim: In this research, we would like to investigate whether deep learning models meet the expectations of practitioners or not.Method: We constructed a Convolution Neural Network and a Simple Logistic model to examine their effectiveness in localizing bugs. We train these models on five open source projects written in Java and compare their performance with the performance of other state-of-the-art models trained on these datasets.Results: Our experiments show that although the deep learning models perform better than classic machine learning models, they meet the adoption criteria set by the practitioners only partially.Conclusions: This work provides evidence that the practitioners should be cautious while using the current state of the art models for production-level use-cases. It also highlights the need for standardization of performance benchmarks to ensure that bug localization models are assessed equitably and realistically.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {16–25},
numpages = {10},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/1368088.1368114,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368114},
doi = {10.1145/1368088.1368114},
abstract = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, Na\"{\i}ve Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: &gt;75% percentage of correctly classified files, a recall of &gt;80%, and a false positive rate &lt;30%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {181–190},
numpages = {10},
keywords = {cost-sensitive classification, defect prediction, software metrics},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@article{10.5555/3546258.3546297,
author = {Tauzin, Guillaume and Lupo, Umberto and Tunstall, Lewis and P\'{e}rez, Julian Burella and Caorsi, Matteo and Medina-Mardones, Anibal M. and Dassatti, Alberto and Hess, Kathryn},
title = {giotto-tda: a topological data analysis toolkit for machine learning and data exploration},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {We introduce giotto-tda, a Python library that integrates high-performance topological data analysis with machine learning via a scikit-learn-compatible API and state-of-the-art C++ implementations. The library's ability to handle various types of data is rooted in a wide range of preprocessing techniques, and its strong focus on data exploration and interpretability is aided by an intuitive plotting API.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {39},
numpages = {6},
keywords = {topological data analysis, persistent homology, mapper, machine learning, data exploration, python}
}

@inproceedings{10.1145/1414004.1414063,
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
title = {Analysis of the reliability of a subset of change metrics for defect prediction},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414063},
doi = {10.1145/1414004.1414063},
abstract = {In this paper, we describe an experiment, which analyzes the relative importance and stability of change metrics for predicting defects for 3 releases of the Eclipse project. The results indicate that out of 18 change metrics 3 metrics contain most information about software defects. Moreover, those 3 metrics remain stable across 3 releases of the Eclipse project. A comparative analysis with the full model shows that the prediction accuracy is not too much affected by using a subset of 3 metrics and the recall even improves.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {309–311},
numpages = {3},
keywords = {defect prediction, feature selection, software metrics},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1016/j.eswa.2008.10.027,
author = {Catal, Cagatay and Diri, Banu},
title = {A systematic review of software fault prediction studies},
year = {2009},
issue_date = {May, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2008.10.027},
doi = {10.1016/j.eswa.2008.10.027},
abstract = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
journal = {Expert Syst. Appl.},
month = may,
pages = {7346–7354},
numpages = {9},
keywords = {Automated fault prediction models, Expert systems, Machine learning, Method-level metrics, Public datasets}
}

@article{10.1007/s10664-008-9064-x,
author = {Hewett, Rattikorn and Kijsanayothin, Phongphun},
title = {On modeling software defect repair time},
year = {2009},
issue_date = {April     2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9064-x},
doi = {10.1007/s10664-008-9064-x},
abstract = {The ability to predict the time required to repair software defects is important for both software quality management and maintenance. Estimated repair times can be used to improve the reliability and time-to-market of software under development. This paper presents an empirical approach to predicting defect repair times by constructing models that use well-established machine learning algorithms and defect data from past software defect reports. We describe, as a case study, the analysis of defect reports collected during the development of a large medical software system. Our predictive models give accuracies as high as 93.44%, despite the limitations of the available data. We present the proposed methodology along with detailed experimental results, which include comparisons with other analytical modeling approaches.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {165–186},
numpages = {22},
keywords = {Data mining, Defect report analysis, Quality assurance, Software testing, Testing management}
}

@article{10.5555/2938006.2938019,
author = {Arcelli Fontana, Francesca and M\"{a}ntyl\"{a}, Mika V. and Zanoni, Marco and Marino, Alessandro},
title = {Comparing and experimenting machine learning techniques for code smell detection},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {3},
issn = {1382-3256},
abstract = {Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (&gt;96 %), and only a hundred training examples are needed to reach at least 95 % accuracy.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1143–1191},
numpages = {49},
keywords = {Benchmark for code smell detection, Code smells detection, Machine learning techniques}
}

@article{10.1016/j.jksuci.2018.04.012,
author = {Atoum, Issa},
title = {A novel framework for measuring software quality-in-use based on semantic similarity and sentiment analysis of software reviews},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {32},
number = {1},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.04.012},
doi = {10.1016/j.jksuci.2018.04.012},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = jan,
pages = {113–125},
numpages = {13},
keywords = {ISO25010, Quality in use, Sentiment analysis, Software quality, Text similarity}
}

@article{10.1016/j.asoc.2015.07.006,
author = {Jin, Cong and Jin, Shu-Wei},
title = {Prediction approach of software fault-proneness based on hybrid artificial neural network and quantum particle swarm optimization},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {35},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.07.006},
doi = {10.1016/j.asoc.2015.07.006},
abstract = {We present a hybrid method using ANN and QPSO for software fault-prone prediction.ANN is used for the classification of software modules.QPSO is controlled more easily than PSO. The identification of a module's fault-proneness is very important for minimizing cost and improving the effectiveness of the software development process. How to obtain the correlation between software metrics and module's fault-proneness has been the focus of much research. This paper presents the application of hybrid artificial neural network (ANN) and Quantum Particle Swarm Optimization (QPSO) in software fault-proneness prediction. ANN is used for classifying software modules into fault-proneness or non fault-proneness categories, and QPSO is applied for reducing dimensionality. The experiment results show that the proposed prediction approach can establish the correlation between software metrics and modules' fault-proneness, and is very simple because its implementation requires neither extra cost nor expert's knowledge. Proposed prediction approach can provide the potential software modules with fault-proneness to software developers, so developers only need to focus on these software modules, which may minimize effort and cost of software maintenance.},
journal = {Appl. Soft Comput.},
month = oct,
pages = {717–725},
numpages = {9},
keywords = {ANN, Fault-prone prediction, QPSO, Software metrics}
}

@inproceedings{10.1145/2597008.2597156,
author = {Zapalowski, Vanius and Nunes, Ingrid and Nunes, Daltro Jos\'{e}},
title = {Revealing the relationship between architectural elements and source code characteristics},
year = {2014},
isbn = {9781450328791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597008.2597156},
doi = {10.1145/2597008.2597156},
abstract = {Understanding how a software system is structured, i.e. its architecture, is crucial for software comprehension. It allows developers to understand an implemented system and reason about how non-functional requirements are addressed. Yet, many systems lack any architectural documentation, or it is often outdated due to software evolution. In current practice, the process of recovering a system's architecture relies primarily on developer knowledge. Although existing architecture recovery approaches can help to identify architectural elements, these approaches require improvement to identify architectural concepts of a system automatically. Towards this goal, we analyze the usefulness of adopting different code-level characteristics to group elements into architectural modules. Our main contributions are an evaluation of the relationships between different sets of characteristics and their corresponding accuracies, and the evaluation results, which help us to understand which characteristics reveal information about the source code structure. Our experiment shows that an identified set of characteristics achieves an average accuracy of 80%, which indicates the usefulness of the considered characteristics for architecture recovery and thus to improving software comprehension.},
booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
pages = {14–25},
numpages = {12},
keywords = {Software architecture, architecture reconstruction, architecture recovery, source code characteristics},
location = {Hyderabad, India},
series = {ICPC 2014}
}

@inproceedings{10.1145/3236024.3236082,
author = {Ma, Shiqing and Liu, Yingqi and Lee, Wen-Chuan and Zhang, Xiangyu and Grama, Ananth},
title = {MODE: automated neural network model debugging via state differential analysis and input selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236082},
doi = {10.1145/3236024.3236082},
abstract = {Artificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {175–186},
numpages = {12},
keywords = {Debugging, Deep Neural Network, Differential Analysis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1145/3341145,
author = {Duc, Thang Le and Leiva, Rafael Garc\'{\i}a and Casari, Paolo and \"{O}stberg, Per-Olov},
title = {Machine Learning Methods for Reliable Resource Provisioning in Edge-Cloud Computing: A Survey},
year = {2019},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3341145},
doi = {10.1145/3341145},
abstract = {Large-scale software systems are currently designed as distributed entities and deployed in cloud data centers. To overcome the limitations inherent to this type of deployment, applications are increasingly being supplemented with components instantiated closer to the edges of networks—a paradigm known as edge computing. The problem of how to efficiently orchestrate combined edge-cloud applications is, however, incompletely understood, and a wide range of techniques for resource and application management are currently in use.This article investigates the problem of reliable resource provisioning in joint edge-cloud environments, and surveys technologies, mechanisms, and methods that can be used to improve the reliability of distributed applications in diverse and heterogeneous network environments. Due to the complexity of the problem, special emphasis is placed on solutions to the characterization, management, and control of complex distributed applications using machine learning approaches. The survey is structured around a decomposition of the reliable resource provisioning problem into three categories of techniques: workload characterization and prediction, component placement and system consolidation, and application elasticity and remediation. Survey results are presented along with a problem-oriented discussion of the state-of-the-art. A summary of identified challenges and an outline of future research directions are presented to conclude the article.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {94},
numpages = {39},
keywords = {Reliability, autoscaling, cloud computing, consolidation, distributed systems, edge computing, machine learning, optimization, placement, remediation}
}

@inproceedings{10.1145/1414004.1414066,
author = {Tosun, Ayse and Turhan, Burak and Bener, Ayse},
title = {Ensemble of software defect predictors: a case study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414066},
doi = {10.1145/1414004.1414066},
abstract = {In this paper, we present a defect prediction model based on ensemble of classifiers, which has not been fully explored so far in this type of research. We have conducted several experiments on public datasets. Our results reveal that ensemble of classifiers considerably improve the defect detection capability compared to Naive Bayes algorithm. We also conduct a cost-benefit analysis for our ensemble, where it turns out that it is enough to inspect 32% of the code on the average, for detecting 76% of the defects.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {318–320},
numpages = {3},
keywords = {defect prediction, ensemble of classifiers, static code attributes},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1016/j.ins.2013.04.027,
author = {Pizzi, Nick J.},
title = {A fuzzy classifier approach to estimating software quality},
year = {2013},
issue_date = {August, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {241},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2013.04.027},
doi = {10.1016/j.ins.2013.04.027},
abstract = {With the increasing sophistication of today's software systems, it is often difficult to estimate the overall quality of underlying software components with respect to attributes such as complexity, utility, and extensibility. Many metrics exist in the software engineering literature that attempt to quantify, with varying levels of accuracy, a large swath of qualitative attributes. However, the overall quality of a software object may manifest itself in ways that the simple interpretation of metrics fails to identify. A better strategy is to determine the best, possibly non-linear, subset of many software metrics for accurately estimating software quality. This strategy may be couched in terms of a problem of classification, that is, determine a mapping from a set of software metrics to a set of class labels representing software quality. We implement this strategy using a fuzzy classification approach. The software metrics are automatically computed and presented as features (input) to a classifier, while the class labels (output) are assigned via an expert's (software architect) thorough assessment of the quality of individual software objects. A large collection of classifiers is presented with subsets of the software metric features. Subsets are selected stochastically using a fuzzy logic based sampling method. The classifiers then predict the quality, specifically the class label, of each software object. Fuzzy integration is applied to the results from the most accurate individual classifiers. We empirically evaluate this approach using software objects from a sophisticated algorithm development framework used to develop biomedical data analysis systems. We demonstrate that the sampling method attenuates the effects of confounding features, and the aggregated classification results using fuzzy integration are superior to the predictions from the respective best individual classifiers.},
journal = {Inf. Sci.},
month = aug,
pages = {1–11},
numpages = {11},
keywords = {Computational intelligence, Fuzzy logic, Pattern classification, Software engineering, Software metric}
}

@article{10.1007/s11219-020-09546-7,
author = {Oyetoyan, Tosin Daniel and Morrison, Patrick},
title = {An improved text classification modelling approach to identify security messages in heterogeneous projects},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09546-7},
doi = {10.1007/s11219-020-09546-7},
abstract = {Security remains under-addressed in many organisations, illustrated by the number of large-scale software security breaches. Preventing breaches can begin during software development if attention is paid to security during the software’s design and implementation. One approach to security assurance during software development is to examine communications between developers as a means of studying the security concerns of the project. Prior research has investigated models for classifying project communication messages (e.g., issues or commits) as security related or not. A known problem is that these models are project-specific, limiting their use by other projects or organisations. We investigate whether we can build a generic classification model that can generalise across projects. We define a set of security keywords by extracting them from relevant security sources, dividing them into four categories: asset, attack/threat, control/mitigation, and implicit. Using different combinations of these categories and including them in the training dataset, we built a classification model and evaluated it on industrial, open-source, and research-based datasets containing over 45 different products. Our model based on harvested security keywords as a feature set shows average recall from 55 to 86%, minimum recall from 43 to 71% and maximum recall from 60 to 100%. An average f-score between 3.4 and 88%, an average g-measure of at least 66% across all the dataset, and an average AUC of ROC from 69 to 89%. In addition, models that use externally sourced features outperformed models that use project-specific features on average by a margin of 26–44% in recall, 22–50% in g-measure, 0.4–28% in f-score, and 15–19% in AUC of ROC. Further, our results outperform a state-of-the-art prediction model for security bug reports in all cases. We find using sound statistical and effect size tests that (1) using harvested security keywords as features to train a text classification model improve classification models and generalise to other projects significantly. (2) Including features in the training dataset before model construction improve classification models significantly. (3) Different security categories represent predictors for different projects. Finally, we introduce new and promising approaches to construct models that can generalise across different independent projects.},
journal = {Software Quality Journal},
month = jun,
pages = {509–553},
numpages = {45},
keywords = {Security, Classification model, Text classification, Software repository, Machine learning}
}

@inproceedings{10.1145/2499393.2499398,
author = {Calikli, Gul and Bener, Ayse},
title = {An algorithmic approach to missing data problem in modeling human aspects in software development},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499398},
doi = {10.1145/2499393.2499398},
abstract = {Background: In our previous research, we built defect prediction models by using confirmation bias metrics. Due to confirmation bias developers tend to perform unit tests to make their programs run rather than breaking their code. This, in turn, leads to an increase in defect density. The performance of prediction model that is built using confirmation bias was as good as the models that were built with static code or churn metrics.Aims: Collection of confirmation bias metrics may result in partially "missing data" due to developers' tight schedules, evaluation apprehension and lack of motivation as well as staff turnover. In this paper, we employ Expectation-Maximization (EM) algorithm to impute missing confirmation bias data.Method: We used four datasets from two large-scale companies. For each dataset, we generated all possible missing data configurations and then employed Roweis' EM algorithm to impute missing data. We built defect prediction models using the imputed data. We compared the performances of our proposed models with the ones that used complete data.Results: In all datasets, when missing data percentage is less than or equal to 50% on average, our proposed model that used imputed data yielded performance results that are comparable with the performance results of the models that used complete data.Conclusions: We may encounter the "missing data" problem in building defect prediction models. Our results in this study showed that instead of discarding missing or noisy data, in our case confirmation bias metrics, we can use effective techniques such as EM based imputation to overcome this problem.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {10},
numpages = {10},
keywords = {confirmation bias, expectation maximisation (EM) algorithm, handling missing data, software defect prediction},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@article{10.1007/s10664-008-9103-7,
author = {Turhan, Burak and Menzies, Tim and Bener, Ay\c{s}e B. and Di Stefano, Justin},
title = {On the relative value of cross-company and within-company data for defect prediction},
year = {2009},
issue_date = {October   2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9103-7},
doi = {10.1007/s10664-008-9103-7},
abstract = {We propose a practical defect prediction approach for companies that do not track defect related data. Specifically, we investigate the applicability of cross-company (CC) data for building localized defect predictors using static code features. Firstly, we analyze the conditions, where CC data can be used as is. These conditions turn out to be quite few. Then we apply principles of analogy-based learning (i.e. nearest neighbor (NN) filtering) to CC data, in order to fine tune these models for localization. We compare the performance of these models with that of defect predictors learned from within-company (WC) data. As expected, we observe that defect predictors learned from WC data outperform the ones learned from CC data. However, our analyses also yield defect predictors learned from NN-filtered CC data, with performance close to, but still not better than, WC data. Therefore, we perform a final analysis for determining the minimum number of local defect reports in order to learn WC defect predictors. We demonstrate in this paper that the minimum number of data samples required to build effective defect predictors can be quite small and can be collected quickly within a few months. Hence, for companies with no local defect data, we recommend a two-phase approach that allows them to employ the defect prediction process instantaneously. In phase one, companies should use NN-filtered CC data to initiate the defect prediction process and simultaneously start collecting WC (local) data. Once enough WC data is collected (i.e. after a few months), organizations should switch to phase two and use predictors learned from WC data.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {540–578},
numpages = {39},
keywords = {Cross-company, Defect prediction, Learning, Metrics (product metrics), Nearest-neighbor filtering, Within-company}
}

@article{10.1016/j.jss.2009.12.023,
author = {de Carvalho, Andr\'{e} B. and Pozo, Aurora and Vergilio, Silvia Regina},
title = {A symbolic fault-prediction model based on multiobjective particle swarm optimization},
year = {2010},
issue_date = {May, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.12.023},
doi = {10.1016/j.jss.2009.12.023},
abstract = {In the literature the fault-proneness of classes or methods has been used to devise strategies for reducing testing costs and efforts. In general, fault-proneness is predicted through a set of design metrics and, most recently, by using Machine Learning (ML) techniques. However, some ML techniques cannot deal with unbalanced data, characteristic very common of the fault datasets and, their produced results are not easily interpreted by most programmers and testers. Considering these facts, this paper introduces a novel fault-prediction approach based on Multiobjective Particle Swarm Optimization (MOPSO). Exploring Pareto dominance concepts, the approach generates a model composed by rules with specific properties. These rules can be used as an unordered classifier, and because of this, they are more intuitive and comprehensible. Two experiments were accomplished, considering, respectively, fault-proneness of classes and methods. The results show interesting relationships between the studied metrics and fault prediction. In addition to this, the performance of the introduced MOPSO approach is compared with other ML algorithms by using several measures including the area under the ROC curve, which is a relevant criterion to deal with unbalanced data.},
journal = {J. Syst. Softw.},
month = may,
pages = {868–882},
numpages = {15},
keywords = {Fault prediction, Multiobjective, Particle swarm optimization, Rule learning algorithm}
}

@inproceedings{10.1145/1985374.1985386,
author = {M\i{}s\i{}rl\i{}, Ayse Tosun and \c{C}a\u{g}layan, Bora and Miranskyy, Andriy V. and Bener, Ay\c{s}e and Ruffolo, Nuzio},
title = {Different strokes for different folks: a case study on software metrics for different defect categories},
year = {2011},
isbn = {9781450305938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985374.1985386},
doi = {10.1145/1985374.1985386},
abstract = {Defect prediction has been evolved with variety of metric sets, and defect types. Researchers found code, churn, and network metrics as significant indicators of defects. However, all metric sets may not be informative for all defect categories such that only one metric type may represent majority of a defect category. Our previous study showed that defect category sensitive prediction models are more successful than general models, since each category has different characteristics in terms of metrics. We extend our previous work, and propose specialized prediction models using churn, code, and network metrics with respect to three defect categories. Results show that churn metrics are the best for predicting all defects. The strength of correlation for code and network metrics varies with defect category: Network metrics have higher correlations than code metrics for defects reported during functional testing and in the field, and vice versa for defects reported during system testing.},
booktitle = {Proceedings of the 2nd International Workshop on Emerging Trends in Software Metrics},
pages = {45–51},
numpages = {7},
keywords = {churn metrics, network metrics, software defect prediction, static code metrics},
location = {Waikiki, Honolulu, HI, USA},
series = {WETSoM '11}
}

@article{10.5555/2946645.3053452,
author = {Bischl, Bernd and Lang, Michel and Kotthoff, Lars and Schiffner, Julia and Richter, Jakob and Studerus, Erich and Casalicchio, Giuseppe and Jones, Zachary M.},
title = {mlr: machine learning in R},
year = {2016},
issue_date = {January 2016},
publisher = {JMLR.org},
volume = {17},
number = {1},
issn = {1532-4435},
abstract = {The MLR package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5938–5942},
numpages = {5},
keywords = {R, benchmarking, data mining, feature selection, hyperparameter tuning, machine learning, model selection, visualization}
}

@inproceedings{10.1145/3236024.3236055,
author = {Hu, Gang and Zhu, Linjie and Yang, Junfeng},
title = {AppFlow: using machine learning to synthesize robust, reusable UI tests},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236055},
doi = {10.1145/3236024.3236055},
abstract = {UI testing is known to be difficult, especially as today’s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an “add to cart” test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides “smoke testing” requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {269–282},
numpages = {14},
keywords = {UI recognition, UI testing, machine learning, mobile testing, test reuse, test synthesis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1007/s10664-010-9151-7,
author = {Kpodjedo, Segla and Ricca, Filippo and Galinier, Philippe and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
title = {Design evolution metrics for defect prediction in object oriented systems},
year = {2011},
issue_date = {February  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-010-9151-7},
doi = {10.1007/s10664-010-9151-7},
abstract = {Testing is the most widely adopted practice to ensure software quality. However, this activity is often a compromise between the available resources and software quality. In object-oriented development, testing effort should be focused on defective classes. Unfortunately, identifying those classes is a challenging and difficult activity on which many metrics, techniques, and models have been tried. In this paper, we investigate the usefulness of elementary design evolution metrics to identify defective classes. The metrics include the numbers of added, deleted, and modified attributes, methods, and relations. The metrics are used to recommend a ranked list of classes likely to contain defects for a system. They are compared to Chidamber and Kemerer's metrics on several versions of Rhino and of ArgoUML. Further comparison is conducted with the complexity metrics computed by Zimmermann et al. on several releases of Eclipse. The comparisons are made according to three criteria: presence of defects, number of defects, and defect density in the top-ranked classes. They show that the design evolution metrics, when used in conjunction with known metrics, improve the identification of defective classes. In addition, they show that the design evolution metrics make significantly better predictions of defect density than other metrics and, thus, can help in reducing the testing effort by focusing test activity on a reduced volume of code.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {141–175},
numpages = {35},
keywords = {Defect prediction, Design evolution metrics, Error tolerant graph matching}
}

@article{10.1007/s11219-021-09550-5,
author = {Alkharabsheh, Khalid and Crespo, Yania and Fern\'{a}ndez-Delgado, Manuel and Viqueira, Jos\'{e} R. and Taboada, Jos\'{e} A.},
title = {Exploratory study of the impact of project domain and size category on the detection of the God class design smell},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09550-5},
doi = {10.1007/s11219-021-09550-5},
abstract = {Design smell detection has proven to be an efficient strategy to improve software quality and consequently decrease maintainability expenses. This work explores the influence of the&nbsp;information  about&nbsp;project context expressed as project domain and size category information, on the automatic detection of the god class design smell by machine learning techniques. A set of experiments using eight classifiers to detect god classes was conducted on a dataset containing 12, 587 classes from 24 Java projects. The results show that classifiers change their behavior when they are used on datasets that differ in these kinds of project information. The results show that god class design smell detection can be improved by feeding machine learning classifiers with this project context information.},
journal = {Software Quality Journal},
month = jun,
pages = {197–237},
numpages = {41},
keywords = {Design smell detection, Machine learning, Software metrics, Project context information, God class}
}

@article{10.1007/s11219-012-9183-x,
author = {Khatibi Bardsiri, Vahid and Jawawi, Dayang Norhayati and Hashim, Siti Zaiton and Khatibi, Elham},
title = {A PSO-based model to increase the accuracy of software development effort estimation},
year = {2013},
issue_date = {September 2013},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-012-9183-x},
doi = {10.1007/s11219-012-9183-x},
abstract = {Development effort is one of the most important metrics that must be estimated in order to design the plan of a project. The uncertainty and complexity of software projects make the process of effort estimation difficult and ambiguous. Analogy-based estimation (ABE) is the most common method in this area because it is quite straightforward and practical, relying on comparison between new projects and completed projects to estimate the development effort. Despite many advantages, ABE is unable to produce accurate estimates when the importance level of project features is not the same or the relationship among features is difficult to determine. In such situations, efficient feature weighting can be a solution to improve the performance of ABE. This paper proposes a hybrid estimation model based on a combination of a particle swarm optimization (PSO) algorithm and ABE to increase the accuracy of software development effort estimation. This combination leads to accurate identification of projects that are similar, based on optimizing the performance of the similarity function in ABE. A framework is presented in which the appropriate weights are allocated to project features so that the most accurate estimates are achieved. The suggested model is flexible enough to be used in different datasets including categorical and non-categorical project features. Three real data sets are employed to evaluate the proposed model, and the results are compared with other estimation models. The promising results show that a combination of PSO and ABE could significantly improve the performance of existing estimation models.},
journal = {Software Quality Journal},
month = sep,
pages = {501–526},
numpages = {26},
keywords = {Analogy-based estimation, Development effort estimation, Particle swarm optimization, Software project}
}

@article{10.5555/2734212.2734215,
author = {Ronchieri, E. and Canaparo, M. and Salomoni, D.},
title = {A Software Quality Model by Using Discriminant Analysis Predictive Technique},
year = {2014},
issue_date = {October 2014},
publisher = {IOS Press},
address = {NLD},
volume = {18},
number = {4},
issn = {1092-0617},
abstract = {Scientific computing infrastructures need reliable software within pressing deadlines due to communities¡¯ requirements they support. Concerning the Grid, computing researchers have been developing software projects without exploiting solutions for discovering defects early enough in the implementation process. This has led to spending energy maintaining and correcting software once released. Achieving high reliability is therefore one of the most important challenges to be faced in the Grid context during the software development life cycle. Although developers perceive quality improvement solutions as limiting factors to their productivity, in our opinion enhancing quality enables us to eliminate mistakes and, as a consequence, reduce costs and delays; the software quality models and metrics represent the mainstream to reach high reliability by balancing both effort and results. In this paper, we aim to provide an extension of a mathematical model that connects software best practices with a set of metrics to periodically predict the quality at any stage of code development and to determine its problems at any early phase. For data statistical properties, we used a risk-threshold-based discriminant analysis technique to analyze the defined model and to detect fault-prone and non fault-prone components. We gathered input data for this model from several European Middleware Initiative packages having different scopes and characteristics, whilst outputs were derived from measures of all specified metrics. At the end we attempted to understand if the model is a true picture of the software under evaluation.},
journal = {J. Integr. Des. Process Sci.},
month = oct,
pages = {25–29},
numpages = {5},
keywords = {Discriminant Analysis, Fault-Prone Modules, Predictive Methods, Software Metrics}
}

@proceedings{10.1145/3001867,
title = {FOSD 2016: Proceedings of the 7th International Workshop on Feature-Oriented Software Development},
year = {2016},
isbn = {9781450346474},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@inproceedings{10.1145/3387904.3389255,
author = {Roy, Devjeet and Fakhoury, Sarah and Lee, John and Arnaoudova, Venera},
title = {A Model to Detect Readability Improvements in Incremental Changes},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389255},
doi = {10.1145/3387904.3389255},
abstract = {Identifying source code that has poor readability allows developers to focus maintenance efforts on problematic code. Therefore, the effort to develop models that can quantify the readability of a piece of source code has been an area of interest for software engineering researchers for several years. However, recent research questions the usefulness of these readability models in practice. When applying these models to readability improvements that are made in practice, i.e., commits, they are unable to capture these incremental improvements, despite a clear perceived improvement by the developers. This results in a discrepancy between the models we have built to measure readability, and the actual perception of readability in practice.In this work, we propose a model that is able to detect incremental readability improvements made by developers in practice with an average precision of 79.2% and an average recall of 67% on an unseen test set. We then investigate the metrics that our model associates with developer perceived readability improvements as well as non-readability changes. Finally, we compare our model to existing state-of-the-art readability models, which our model outperforms by at least 23% in terms of precision and 42% in terms of recall.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {25–36},
numpages = {12},
keywords = {Code quality, Machine learning, Source code readability},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.1007/s10664-012-9230-z,
author = {Linares-V\'{a}squez, Mario and Mcmillan, Collin and Poshyvanyk, Denys and Grechanik, Mark},
title = {On using machine learning to automatically classify software applications into domain categories},
year = {2014},
issue_date = {June      2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-012-9230-z},
doi = {10.1007/s10664-012-9230-z},
abstract = {Software repositories hold applications that are often categorized to improve the effectiveness of various maintenance tasks. Properly categorized applications allow stakeholders to identify requirements related to their applications and predict maintenance problems in software projects. Manual categorization is expensive, tedious, and laborious --- this is why automatic categorization approaches are gaining widespread importance. Unfortunately, for different legal and organizational reasons, the applications' source code is often not available, thus making it difficult to automatically categorize these applications. In this paper, we propose a novel approach in which we use Application Programming Interface (API) calls from third-party libraries for automatic categorization of software applications that use these API calls. Our approach is general since it enables different categorization algorithms to be applied to repositories that contain both source code and bytecode of applications, since API calls can be extracted from both the source code and byte-code. We compare our approach to a state-of-the-art approach that uses machine learning algorithms for software categorization, and conduct experiments on two large Java repositories: an open-source repository containing 3,286 projects and a closed-source repository with 745 applications, where the source code was not available. Our contribution is twofold: we propose a new approach that makes it possible to categorize software projects without any source code using a small number of API calls as attributes, and furthermore we carried out a comprehensive empirical evaluation of automatic categorization approaches.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {582–618},
numpages = {37},
keywords = {Closed-source, Machine learning, Open-source, Software categorization}
}

@inproceedings{10.5555/1888258.1888293,
author = {Eichinger, Frank and Krogmann, Klaus and Klug, Roland and B\"{o}hm, Klemens},
title = {Software-defect localisation by mining dataflow-enabled call graphs},
year = {2010},
isbn = {364215879X},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Defect localisation is essential in software engineering and is an important task in domain-specific data mining. Existing techniques building on call-graph mining can localise different kinds of defects. However, these techniques focus on defects that affect the controlflow and are agnostic regarding the dataflow. In this paper, we introduce dataflowenabled call graphs that incorporate abstractions of the dataflow. Building on these graphs, we present an approach for defect localisation. The creation of the graphs and the defect localisation are essentially data mining problems, making use of discretisation, frequent subgraph mining and feature selection. We demonstrate the defect-localisation qualities of our approach with a study on defects introduced into Weka. As a result, defect localisation now works much better, and a developer has to investigate on average only 1.5 out of 30 methods to fix a defect.},
booktitle = {Proceedings of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: Part I},
pages = {425–441},
numpages = {17},
location = {Barcelona, Spain},
series = {ECML PKDD'10}
}

@article{10.1016/j.eswa.2010.10.024,
author = {Catal, Cagatay},
title = {Review: Software fault prediction: A literature review and current trends},
year = {2011},
issue_date = {April, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2010.10.024},
doi = {10.1016/j.eswa.2010.10.024},
abstract = {Software engineering discipline contains several prediction approaches such as test effort prediction, correction cost prediction, fault prediction, reusability prediction, security prediction, effort prediction, and quality prediction. However, most of these prediction approaches are still in preliminary phase and more research should be conducted to reach robust models. Software fault prediction is the most popular research area in these prediction approaches and recently several research centers started new projects on this area. In this study, we investigated 90 software fault prediction papers published between year 1990 and year 2009 and then we categorized these papers according to the publication year. This paper surveys the software engineering literature on software fault prediction and both machine learning based and statistical based approaches are included in this survey. Papers explained in this article reflect the outline of what was published so far, but naturally this is not a complete review of all the papers published so far. This paper will help researchers to investigate the previous studies from metrics, methods, datasets, performance evaluation metrics, and experimental results perspectives in an easy and effective manner. Furthermore, current trends are introduced and discussed.},
journal = {Expert Syst. Appl.},
month = apr,
pages = {4626–4636},
numpages = {11},
keywords = {Automated fault prediction models, Expert systems, Machine learning, Software engineering, Software quality engineering, Statistical methods}
}

@inproceedings{10.1109/ICMLA.2012.60,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {An Empirical Study on the Stability of Feature Selection for Imbalanced Software Engineering Data},
year = {2012},
isbn = {9780769549132},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICMLA.2012.60},
doi = {10.1109/ICMLA.2012.60},
abstract = {In software quality modeling, software metrics are collected during the software development cycle. However, not all metrics are relevant to the class attribute (software quality). Metric (feature) selection has become the cornerstone of many software quality classification problems. Selecting software metrics that are important for software quality classification is a necessary and critical step before the model training process. Recently, the robustness (e.g., stability) of feature selection techniques has been studied, to examine the sensitivity of these techniques to changes (adding/removing program modules to/from their dataset). This work provides an empirical study regarding the stability of feature selection techniques across six software metrics datasets with varying levels of class balance. In this work eighteen feature selection techniques are evaluated. Moreover, three factors, feature subset size, degree of perturbation, and class balance of datasets, are considered in this study to evaluate stability of feature selection techniques. Experimental results show that these factors affect the stability of feature selection techniques as one might expect. We found that with few exceptions, feature ranking based on highly imbalanced datasets are less stable than based on slightly imbalanced data. Results also show that making smaller changes to the datasets has less impact on the stability of feature ranking techniques. Overall, we conclude that a careful understanding of one's dataset (and certain choices of metric selection technique) can help practitioners build more reliable software quality models.},
booktitle = {Proceedings of the 2012 11th International Conference on Machine Learning and Applications - Volume 01},
pages = {317–323},
numpages = {7},
keywords = {feature ranking, imbalanced data, stability, subsample},
series = {ICMLA '12}
}

@inproceedings{10.1109/ICTAI.2009.20,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Gao, Kehan and Seliya, Naeem},
title = {High-Dimensional Software Engineering Data and Feature Selection},
year = {2009},
isbn = {9780769539201},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICTAI.2009.20},
doi = {10.1109/ICTAI.2009.20},
abstract = {Software metrics collected during project development play a critical role in software quality assurance. A software practitioner is very keen on learning which software metrics to focus on for software quality prediction. While a concise set of software metrics is often desired, a typical project collects a very large number of metrics. Minimal attention has been devoted to finding the minimum set of software metrics that have the same predictive capability as a larger set of metrics – we strive to answer that question in this paper. We present a comprehensive comparison between seven commonly-used filter-based feature ranking techniques (FRT) and our proposed hybrid feature selection (HFS) technique. Our case study consists of a very highdimensional (42 software attributes) software measurement data set obtained from a large telecommunications system. The empirical analysis indicates that HFS performs better than FRT; however, the Kolmogorov-Smirnov feature ranking technique demonstrates competitive performance. For the telecommunications system, it is found that only 10% of the software attributes are sufficient for effective software quality prediction.},
booktitle = {Proceedings of the 2009 21st IEEE International Conference on Tools with Artificial Intelligence},
pages = {83–90},
numpages = {8},
keywords = {feature ranking, high-dimensional data, hybrid feature selection, quality prediction, software metrics},
series = {ICTAI '09}
}

@article{10.1016/j.asoc.2016.08.012,
author = {Idri, Ali and Hosni, Mohamed and Abran, Alain},
title = {Improved estimation of software development effort using Classical and Fuzzy Analogy ensembles},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.012},
doi = {10.1016/j.asoc.2016.08.012},
abstract = {Display Omitted Ensemble effort estimation based on Classical and Fuzzy Analogy.Evaluation solo Classical and Fuzzy Analogy.Evaluating Classical and Fuzzy Analogy ensembles.Comparing solo and ensemble classical and Fuzzy Analogy techniques.Fuzzy Analogy ensembles achieve better performance than with Classical Analogy ensembles. Delivering an accurate estimate of software development effort plays a decisive role in successful management of a software project. Therefore, several effort estimation techniques have been proposed including analogy based techniques. However, despite the large number of proposed techniques, none has outperformed the others in all circumstances and previous studies have recommended generating estimation from ensembles of various single techniques rather than using only one solo technique. Hence, this paper proposes two types of homogeneous ensembles based on single Classical Analogy or single Fuzzy Analogy for the first time. To evaluate this proposal, we conducted an empirical study with 100/60 variants of Classical/Fuzzy Analogy techniques respectively. These variants were assessed using standardized accuracy and effect size criteria over seven datasets. Thereafter, these variants were clustered using the Scott-Knott statistical test and ranked using four unbiased errors measures. Moreover, three linear combiners were used to combine the single estimates. The results show that there is no best single Classical/Fuzzy Analogy technique across all datasets, and the constructed ensembles (Classical/Fuzzy Analogy ensembles) are often ranked first and their performances are, in general, higher than the single techniques. Furthermore, Fuzzy Analogy ensembles achieve better performance than Classical Analogy ensembles and there is no best Classical/Fuzzy ensemble across all datasets and no evidence concerning the best combiner.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {990–1019},
numpages = {30},
keywords = {Analogy, Ensemble effort estimation, Fuzzy logic, Software development effort estimation}
}

@article{10.1016/j.knosys.2015.11.013,
author = {Yijing, Li and Haixiang, Guo and Xiao, Liu and Yanan, Li and Jinling, Li},
title = {Adapted ensemble classification algorithm based on multiple classifier system and feature selection for classifying multi-class imbalanced data},
year = {2016},
issue_date = {February 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {94},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.11.013},
doi = {10.1016/j.knosys.2015.11.013},
abstract = {Learning from imbalanced data, where the number of observations in one class is significantly rarer than in other classes, has gained considerable attention in the data mining community. Most existing literature focuses on binary imbalanced case while multi-class imbalanced learning is barely mentioned. What's more, most proposed algorithms treated all imbalanced data consistently and aimed to handle all imbalanced data with a versatile algorithm. In fact, the imbalanced data varies in their imbalanced ratio, dimension and the number of classes, the performances of classifiers for learning from different types of datasets are different. In this paper we propose an adaptive multiple classifier system named of AMCS to cope with multi-class imbalanced learning, which makes a distinction among different kinds of imbalanced data. The AMCS includes three components, which are, feature selection, resampling and ensemble learning. Each component of AMCS is selected discriminatively for different types of imbalanced data. We consider two feature selection methods, three resampling mechanisms, five base classifiers and five ensemble rules to construct a selection pool, the adapting criterion of choosing each component from the selection pool to frame AMCS is analyzed through empirical study. In order to verify the effectiveness of AMCS, we compare AMCS with several state-of-the-art algorithms, the results show that AMCS can outperform or be comparable with the others. At last, AMCS is applied in oil-bearing reservoir recognition. The results indicate that AMCS makes no mistake in recognizing characters of layers for oilsk81-oilsk85 well logging data which is collected in Jianghan oilfield of China.},
journal = {Know.-Based Syst.},
month = feb,
pages = {88–104},
numpages = {17},
keywords = {Adaptive learning, Imbalanced data, Multiple classifier system, Oil reservoir}
}

@article{10.4018/IRMJ.2017070101,
author = {Chevers, Delroy A. and Grant, Gerald G.},
title = {Information Systems Quality and Success in Canadian Software Development Firms},
year = {2017},
issue_date = {July 2017},
publisher = {IGI Global},
address = {USA},
volume = {30},
number = {3},
issn = {1040-1628},
url = {https://doi.org/10.4018/IRMJ.2017070101},
doi = {10.4018/IRMJ.2017070101},
abstract = {For years, firms have been investing millions of dollars in information systems IS to gain operational and strategic benefits. However, in most cases these expected benefits have not been realized because the software development community has been plagued with the delivery of low quality and unsuccessful information systems. Duggan and Reichgelt's information systems quality model was adapted with minor modifications to explore the impact of process maturity and people on IS quality in Canadian software development firms. The study also investigated the impact of IS quality on IS success. Using PLS-Graph as the statistical tool, it was discovered that people skills and contribution had the greatest impact on IS quality and that IS quality impacted IS success. These findings are important to both IS practitioners and researchers in their desire to deliver high quality and successful information systems in Canada.},
journal = {Inf. Resour. Manage. J.},
month = jul,
pages = {1–25},
numpages = {25},
keywords = {Capability Maturity Model Integration, Information Systems Quality, Information Systems Success, Process Maturity, Software Process Improvement}
}

@article{10.1016/j.jss.2016.01.003,
author = {Kumar, Lov and Rath, Santanu Ku.},
title = {Hybrid functional link artificial neural network approach for predicting maintainability of object-oriented software},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {121},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.01.003},
doi = {10.1016/j.jss.2016.01.003},
abstract = {Among all quality parameters, Maintainability is more important to achieve success.This paper focus on maintainability of software using object oriented metrics.Hybrid approach of neural network is used to design a model for prediction.Two feature selection techniques are used to select best set of metrics.We achieved better prediction rate for maintainability as compared to others. In present day, software development methodology is mostly based on object-oriented paradigm. With the increase in the number of these software system, their effective maintenance aspects becomes a crucial factor. Most of the maintainability prediction models in literature are based on techniques such as regression analysis and simple neural network. In this paper, three artificial intelligence techniques (AI) such as hybrid approach of functional link artificial neural network (FLANN) with genetic algorithm (GA), particle swarm optimization (PSO) and clonal selection algorithm (CSA), i.e., FLANN-Genetic (FGA and AFGA), FLANN-PSO (FPSO and MFPSO), FLANN-CSA (FCSA) are applied to design a model for predicting maintainability. These three AI techniques are applied to predict maintainability on two case studies such as Quality Evaluation System (QUES) and User Interface System (UIMS). This paper also focuses on the effectiveness of feature reduction techniques such as rough set analysis (RSA) and principal component analysis (PCA) when they are applied for predicting maintainability. The results show that feature reduction techniques are very effective in obtaining better results while using FLANN-Genetic.},
journal = {J. Syst. Softw.},
month = nov,
pages = {170–190},
numpages = {21},
keywords = {Artificial neural network, CK metrics suite, Maintainability}
}

@inproceedings{10.5555/1882011.1882054,
author = {Luo, Yunfeng and Ben, Kerong and Mi, Lei},
title = {Software metrics reduction for fault-proneness prediction of software modules},
year = {2010},
isbn = {3642156711},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {It would be valuable to use metrics to identify the fault-proneness of software modules. However, few research works are on how to select appropriate metrics for fault-proneness prediction currently. We conduct a large-scale comparative experiment of nine different software metrics reduction methods over eleven public-domain data sets from the NASA metrics data repository. The Naive Bayes data miner, with a log-filtering preprocessor on the numeric data, is utilized to construct the prediction model. Comparisons are based on the analysis of variance. Our conclusion is that, reduction methods of software metrics are important to build adaptable and robust software fault-proneness prediction models. Given our results on Naive Bayes and log-filtering, discrete wavelet transformation outperforms other reduction methods, and correlationbased feature selection with genetic search algorithm and information gain can also obtain better predicted performance.},
booktitle = {Proceedings of the 2010 IFIP International Conference on Network and Parallel Computing},
pages = {432–441},
numpages = {10},
keywords = {analysis of variance, metrics reduction, software fault-proneness},
location = {Zhengzhou, China},
series = {NPC'10}
}

@article{10.1016/j.jss.2007.05.035,
author = {Gondra, Iker},
title = {Applying machine learning to software fault-proneness prediction},
year = {2008},
issue_date = {February, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.05.035},
doi = {10.1016/j.jss.2007.05.035},
abstract = {The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module's fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA's Metrics Data Program data repository.},
journal = {J. Syst. Softw.},
month = feb,
pages = {186–195},
numpages = {10},
keywords = {Fault-proneness, Machine learning, Neural network, Sensitivity analysis, Software metrics, Software testing, Support vector machine}
}

@article{10.4018/IJAEC.2015100104,
author = {Saadi, Maryam Hassani and Bardsiri, Vahid Khatibi and Ziaaddini, Fahimeh},
title = {The Application of Meta-Heuristic Algorithms to Improve the Performance of Software Development Effort Estimation Models},
year = {2015},
issue_date = {October 2015},
publisher = {IGI Global},
address = {USA},
volume = {6},
number = {4},
issn = {1942-3594},
url = {https://doi.org/10.4018/IJAEC.2015100104},
doi = {10.4018/IJAEC.2015100104},
abstract = {One of the major activities in effective and efficient production of software projects is the precise estimation of software development effort. Estimation of the effort in primary steps of software development is one of the most important challenges in managing software projects. Some reasons for these challenges such as: discordant software projects, the complexity of the manufacturing process, special role of human and high level of obscure and unusual features of software projects can be noted. Predicting the necessary efforts to develop software using meta-heuristic optimization algorithms has made significant progressions in this field. These algorithms have the potent to be used in estimation of the effort of the software. The necessity to increase estimation precision urged the authors to survey the efficiency of some meta-heuristic optimization algorithms and their effects on the software projects. To do so, in this paper, they investigated the effect of combining various optimization algorithms such as genetic algorithm, particle swarm optimization algorithm and ant colony algorithm on different models such as COCOMO, estimation based on analogy, machine learning methods and standard estimation models. These models have employed various data sets to evaluate the results such as COCOMO, Desharnais, NASA, Kemerer, CF, DPS, ISBSG and Koten &amp; Gary. The results of this survey can be used by researchers as a primary reference.},
journal = {Int. J. Appl. Evol. Comput.},
month = oct,
pages = {39–68},
numpages = {30},
keywords = {Datasets, Effort Estimation, Meta-Heuristic Optimization Algorithm, Software Project, Survey}
}

@article{10.1007/s11219-011-9132-0,
author = {Gao, Kehan and Khoshgoftaar, Taghi M. and Seliya, Naeem},
title = {Predicting high-risk program modules by selecting the right software measurements},
year = {2012},
issue_date = {March     2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9132-0},
doi = {10.1007/s11219-011-9132-0},
abstract = {A timely detection of high-risk program modules in high-assurance software is critical for avoiding the high consequences of operational failures. While software risk can initiate from external sources, such as management or outsourcing, software quality is adversely affected when internal software risks are realized, such as improper practice of standard software processes or lack of a defined software quality infrastructure. Practitioners employ various techniques to identify and rectify high-risk or low-quality program modules. Effectiveness of detecting such modules is affected by the software measurements used, making feature selection an important step during software quality prediction. We use a wrapper-based feature ranking technique to select the optimal set of software metrics to build defect prediction models. We also address the adverse effects of class imbalance (very few low-quality modules compared to high-quality modules), a practical problem observed in high-assurance systems. Applying a data sampling technique followed by feature selection is a relatively unique contribution of our work. We present a comprehensive investigation on the impact of data sampling followed by attribute selection on the defect predictors built with imbalanced data. The case study data are obtained from several real-world high-assurance software projects. The key results are that attribute selection is more efficient when applied after data sampling, and defect prediction performance generally improves after applying data sampling and feature selection.},
journal = {Software Quality Journal},
month = mar,
pages = {3–42},
numpages = {40},
keywords = {Data sampling, Feature selection, Imbalanced data, Performance metrics, Software quality classification, Wrapper-based feature ranking}
}

@article{10.1007/s10664-011-9165-9,
author = {Shin, Yonghee and Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {On the use of calling structure information to improve fault prediction},
year = {2012},
issue_date = {August    2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {17},
number = {4–5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9165-9},
doi = {10.1007/s10664-011-9165-9},
abstract = {Previous studies have shown that software code attributes, such as lines of source code, and history information, such as the number of code changes and the number of faults in prior releases of software, are useful for predicting where faults will occur. In this study of two large industrial software systems, we investigate the effectiveness of adding information about calling structure to fault prediction models. Adding calling structure information to a model based solely on non-calling structure code attributes modestly improved prediction accuracy. However, the addition of calling structure information to a model that included both history and non-calling structure code attributes produced no improvement.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {390–423},
numpages = {34},
keywords = {Calling structure attributes, Empirical study, Negative binomial model, Software faults}
}

@inproceedings{10.1145/2884781.2884783,
author = {Kim, Miryung and Zimmermann, Thomas and DeLine, Robert and Begel, Andrew},
title = {The emerging role of data scientists on software development teams},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884783},
doi = {10.1145/2884781.2884783},
abstract = {Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {96–107},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1007/s10489-009-0193-8,
author = {Hewett, Rattikorn},
title = {Mining software defect data to support software testing management},
year = {2011},
issue_date = {April     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {2},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-009-0193-8},
doi = {10.1007/s10489-009-0193-8},
abstract = {Achieving high quality software would be easier if effective software development practices were known and deployed in appropriate contexts. Because our theoretical knowledge of the underlying principles of software development is far from complete, empirical analysis of past experience in software projects is essential for acquiring useful software practices. As advances in software technology continue to facilitate automated tracking and data collection, more software data become available. Our research aims to develop methods to exploit such data for improving software development practices.This paper proposes an empirical approach, based on the analysis of defect data, that provides support for software testing management in two ways: (1) construction of a predictive model for defect repair times, and (2) a method for assessing testing quality across multiple releases. The approach employs data mining techniques including statistical methods and machine learning. To illustrate the proposed approach, we present a case study using the defect reports created during the development of three releases of a large medical software system, produced by a large well-established software company. We validate our proposed testing quality assessment using a statistical test at a significance level of 0.1. Despite the limitations of the available data, our predictive models give accuracies as high as 93%.},
journal = {Applied Intelligence},
month = apr,
pages = {245–257},
numpages = {13},
keywords = {Data mining, Defect report, Quality assurance, Software testing management}
}

@article{10.1016/j.jss.2010.11.920,
author = {Xie, Xiaoyuan and Ho, Joshua W. K. and Murphy, Christian and Kaiser, Gail and Xu, Baowen and Chen, Tsong Yueh},
title = {Testing and validating machine learning classifiers by metamorphic testing},
year = {2011},
issue_date = {April, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.11.920},
doi = {10.1016/j.jss.2010.11.920},
abstract = {Abstract: Machine learning algorithms have provided core functionality to many application domains - such as bioinformatics, computational linguistics, etc. However, it is difficult to detect faults in such applications because often there is no ''test oracle'' to verify the correctness of the computed outputs. To help address the software quality, in this paper we present a technique for testing the implementations of machine learning classification algorithms which support such applications. Our approach is based on the technique ''metamorphic testing'', which has been shown to be effective to alleviate the oracle problem. Also presented include a case study on a real-world machine learning application framework, and a discussion of how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also conduct mutation analysis and cross-validation, which reveal that our method has high effectiveness in killing mutants, and that observing expected cross-validation result alone is not sufficiently effective to detect faults in a supervised classification program. The effectiveness of metamorphic testing is further confirmed by the detection of real faults in a popular open-source classification program.},
journal = {J. Syst. Softw.},
month = apr,
pages = {544–558},
numpages = {15},
keywords = {Machine learning, Metamorphic testing, Oracle problem, Test oracle, Validation, Verification}
}

@inproceedings{10.1145/1294948.1294953,
author = {Bernstein, Abraham and Ekanayake, Jayalath and Pinzger, Martin},
title = {Improving defect prediction using temporal features and non linear models},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294953},
doi = {10.1145/1294948.1294953},
abstract = {Predicting the defects in the next release of a large software system is a very valuable asset for the project manger to plan her resources. In this paper we argue that temporal features (or aspects) of the data are central to prediction performance. We also argue that the use of non-linear models, as opposed to traditional regression, is necessary to uncover some of the hidden interrelationships between the features and the defects and maintain the accuracy of the prediction in some cases.Using data obtained from the CVS and Bugzilla repositories of the Eclipse project, we extract a number of temporal features, such as the number of revisions and number of reported issues within the last three months. We then use these data to predict both the location of defects (i.e., the classes in which defects will occur) as well as the number of reported bugs in the next month of the project. To that end we use standard tree-based induction algorithms in comparison with the traditional regression.Our non-linear models uncover the hidden relationships between features and defects, and present them in easy to understand form. Results also show that using the temporal features our prediction model can predict whether a source file will have a defect with an accuracy of 99% (area under ROC curve 0.9251) and the number of defects with a mean absolute error of 0.019 (Spearman's correlation of 0.96).},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {11–18},
numpages = {8},
keywords = {decision tree learner, defect prediction, mining software repository},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.5555/2486788.2486991,
author = {Haiduc, Sonia and De Rosa, Giuseppe and Bavota, Gabriele and Oliveto, Rocco and De Lucia, Andrea and Marcus, Andrian},
title = {Query quality prediction and reformulation for source code search: the refoqus tool},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Developers search source code frequently during their daily tasks, to find pieces of code to reuse, to find where to implement changes, etc. Code search based on text retrieval (TR) techniques has been widely used in the software engineering community during the past decade. The accuracy of the TR-based search results depends largely on the quality of the query used. We introduce Refoqus, an Eclipse plugin which is able to automatically detect the quality of a text retrieval query and to propose reformulations for it, when needed, in order to improve the results of TR-based code search. A video of Refoqus is found online at http://www.youtube.com/watch?v=UQlWGiauyk4.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1307–1310},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1016/j.jss.2009.06.055,
author = {Arisholm, Erik and Briand, Lionel C. and Johannessen, Eivind B.},
title = {A systematic and comprehensive investigation of methods to build and evaluate fault prediction models},
year = {2010},
issue_date = {January, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {83},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.06.055},
doi = {10.1016/j.jss.2009.06.055},
abstract = {This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases - both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost.},
journal = {J. Syst. Softw.},
month = jan,
pages = {2–17},
numpages = {16},
keywords = {Cost-effectiveness, Fault prediction models, Verification}
}

@inproceedings{10.1145/2597008.2597147,
author = {Hossen, Md Kamal and Kagdi, Huzefa and Poshyvanyk, Denys},
title = {Amalgamating source code authors, maintainers, and change proneness to triage change requests},
year = {2014},
isbn = {9781450328791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597008.2597147},
doi = {10.1145/2597008.2597147},
abstract = {The paper presents an approach, namely iMacPro, to recommend developers who are most likely to implement incoming change requests. iMacPro amalgamates the textual similarity between the given change request and source code, change proneness information, authors, and maintainers of a software system. Latent Semantic Indexing (LSI) and a lightweight analysis of source code, and its commits from the software repository, are used. The basic premise of iMacPro is that the authors and maintainers of the relevant source code, which is change prone, to a given change request are most likely to best assist with its resolution. iMacPro unifies these sources in a unique way to perform its task, which was not investigated and reported in the literature previously.  An empirical study on three open source systems, ArgoUML, JabRef, and jEdit , was conducted to assess the effectiveness of iMacPro. A number of change requests from these systems were used in the evaluated benchmark. Recall values for top one, five, and ten recommended developers are reported. Furthermore, a comparative study with a previous approach that uses the source-code authorship information for developer recommendation was performed. Results show that iMacPro could provide recall gains from 30% to 180% over its subjected competitor with statistical significance.},
booktitle = {Proceedings of the 22nd International Conference on Program Comprehension},
pages = {130–141},
numpages = {12},
keywords = {Change Request, Expert Developer Recommendation, Software Maintenance, Triaging},
location = {Hyderabad, India},
series = {ICPC 2014}
}

@article{10.1007/s11219-020-09525-y,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {An empirical study on predictability of software maintainability using imbalanced data},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09525-y},
doi = {10.1007/s11219-020-09525-y},
abstract = {In software engineering predictive modeling, early prediction of software modules or classes that possess high maintainability effort is a challenging task. Many prediction models are constructed to predict the maintainability of software classes or modules by applying various machine learning (ML) techniques. If the software modules or classes need&nbsp;high maintainability, effort would be reduced&nbsp;in a dataset, and&nbsp;there would be imbalanced data to train the model. The imbalanced datasets make&nbsp;ML techniques bias their predictions towards low maintainability effort or majority classes, and minority class instances get discarded as noise by the machine learning (ML) techniques. In this direction, this paper presents empirical work to improve the performance of software maintainability prediction (SMP) models developed with ML techniques using imbalanced data. For developing the models, the imbalanced data is pre-processed by applying data resampling methods. Fourteen data resampling methods, including oversampling, undersampling, and hybrid resampling, are used in the study. The study results recommend that the safe-level synthetic minority oversampling technique (Safe-Level-SMOTE) is a useful method to deal with the imbalanced datasets and to develop competent prediction models to forecast software maintainability.},
journal = {Software Quality Journal},
month = dec,
pages = {1581–1614},
numpages = {34},
keywords = {Software maintainability prediction, Machine learning, Data resampling, Imbalanced learning}
}

@article{10.5555/1953048.2078195,
author = {Pedregosa, Fabian and Varoquaux, Ga\"{e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'{E}douard},
title = {Scikit-learn: Machine Learning in Python},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2825–2830},
numpages = {6}
}

@article{10.1145/2556777,
author = {Zhou, Yuming and Xu, Baowen and Leung, Hareton and Chen, Lin},
title = {An in-depth study of the potentially confounding effect of class size in fault prediction},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2556777},
doi = {10.1145/2556777},
abstract = {Background. The extent of the potentially confounding effect of class size in the fault prediction context is not clear, nor is the method to remove the potentially confounding effect, or the influence of this removal on the performance of fault-proneness prediction models. Objective. We aim to provide an in-depth understanding of the effect of class size on the true associations between object-oriented metrics and fault-proneness. Method. We first employ statistical methods to examine the extent of the potentially confounding effect of class size in the fault prediction context. After that, we propose a linear regression-based method to remove the potentially confounding effect. Finally, we empirically investigate whether this removal could improve the prediction performance of fault-proneness prediction models. Results. Based on open-source software systems, we found: (a) the confounding effect of class size on the associations between object-oriented metrics and fault-proneness in general exists; (b) the proposed linear regression-based method can effectively remove the confounding effect; and (c) after removing the confounding effect, the prediction performance of fault prediction models with respect to both ranking and classification can in general be significantly improved. Conclusion. We should remove the confounding effect of class size when building fault prediction models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {10},
numpages = {51},
keywords = {Metrics, class size, confounding effect, fault, prediction}
}

@inproceedings{10.1145/2568225.2568260,
author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
title = {An exploratory study of the pull-based software development model},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568260},
doi = {10.1145/2568225.2568260},
abstract = {The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workflow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, first on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We find that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and find that technical ones are only a small minority.},
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {345–355},
numpages = {11},
keywords = {Pull-based development, distributed software development, empirical software engineering, pull request},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3183440.3183457,
author = {Catolino, Gemma},
title = {Effort-oriented methods and tools for software development and maintenance for mobile apps},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183457},
doi = {10.1145/3183440.3183457},
abstract = {The present research project aims to propose methods and tools for mobile applications development and maintenance that rely on effort information (estimations). Specifically, we will focus on two main challenges to overcome existing work: (i) conceiving effort estimation approaches that can be applied earlier in the development cycle and evolve through the development process (ii) prioritizing development and maintenance tasks by relying on effort estimation information.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {450–451},
numpages = {2},
keywords = {effort estimation, maintenance and evolution, mobile apps},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1016/j.infsof.2015.07.004,
author = {Huang, Jianglin and Li, Yan-Fu and Xie, Min},
title = {An empirical analysis of data preprocessing for machine learning-based software cost estimation},
year = {2015},
issue_date = {November 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {67},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.07.004},
doi = {10.1016/j.infsof.2015.07.004},
abstract = {ContextDue to the complex nature of software development process, traditional parametric models and statistical methods often appear to be inadequate to model the increasingly complicated relationship between project development cost and the project features (or cost drivers). Machine learning (ML) methods, with several reported successful applications, have gained popularity for software cost estimation in recent years. Data preprocessing has been claimed by many researchers as a fundamental stage of ML methods; however, very few works have been focused on the effects of data preprocessing techniques. ObjectiveThis study aims for an empirical assessment of the effectiveness of data preprocessing techniques on ML methods in the context of software cost estimation. MethodIn this work, we first conduct a literature survey of the recent publications using data preprocessing techniques, followed by a systematic empirical study to analyze the strengths and weaknesses of individual data preprocessing techniques as well as their combinations. ResultsOur results indicate that data preprocessing techniques may significantly influence the final prediction. They sometimes might have negative impacts on prediction performance of ML methods. ConclusionIn order to reduce prediction errors and improve efficiency, a careful selection is necessary according to the characteristics of machine learning methods, as well as the datasets used for software cost estimation.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {108–127},
numpages = {20},
keywords = {Case selection, Data preprocessing, Feature selection, Missing-data treatments, Scaling, Software cost estimation}
}

@article{10.1007/s00500-021-06048-x,
author = {Rathore, Santosh S.},
title = {An exploratory analysis of regression methods for predicting faults in software systems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {25},
number = {23},
issn = {1432-7643},
url = {https://doi.org/10.1007/s00500-021-06048-x},
doi = {10.1007/s00500-021-06048-x},
abstract = {The use of regression methods, for instance, linear regression, decision tree regression, etc., has been used earlier to build software fault prediction (SFP) models. However, these methods showed limited SFP performance with higher misclassification errors. In previous works, issues such as multicollinearity, feature scaling, and imbalance distribution of faulty and non-faulty modules in the dataset have not been considered reasonably, which might be a potential cause behind the poor prediction performance of these regression methods. Motivated from it, in this paper, we investigate the impact of 15 different regression methods for the faults count prediction in the software system and report their interpretation for fault models. We consider different fault data quality issues, and a comprehensive assessment of the regression methods is presented to handle these issues. We believe that many used regression methods have not been explored before for the SFP by considering different data quality issues. In the presented study, 44 fault datasets and their versions are used that are collected from the PROMISE software data repository are used to validate the performance of the regression methods, and absolute relative error (ARE), root mean square error (RSME), and fault-percentile-average (FPA) are used as the performance measures. For the model building, five different scenarios are considered, (1) original dataset without preprocessing; (2) standardized processed dataset; (3) balanced dataset; (4) non-multicollinearity processed dataset; (5) balanced+non-multicollinearity processed dataset. Experimental results showed that overall kernel-based regression methods, KernelRidge and SVR (Support vector regression, both linear and nonlinear kernels), yielded the best performance for predicting the fault counts compared to other methods. Other regression methods, in particular NNR (Nearest neighbor regression), RFR (Random forest regression), and GBR (Gradient boosting regression), are performed significantly accurately. Further, results showed that applying standardization and handling multicollinearity in the fault dataset helped improve regression methods’ performance. It is concluded that regression methods are promising for building software fault prediction models.},
journal = {Soft Comput.},
month = dec,
pages = {14841–14872},
numpages = {32},
keywords = {Software fault prediction, Regression methods, PROMISE data repository, Empirical study}
}

@inproceedings{10.1109/ASE.2011.6100072,
author = {Menzies, Tim and Butcher, Andrew and Marcus, Andrian and Zimmermann, Thomas and Cok, David},
title = {Local vs. global models for effort estimation and defect prediction},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100072},
doi = {10.1109/ASE.2011.6100072},
abstract = {Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules.},
booktitle = {Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {343–351},
numpages = {9},
series = {ASE '11}
}

@inproceedings{10.1145/3472674.3473983,
author = {Lomio, Francesco and Jurvansuu, Sampsa and Taibi, Davide},
title = {Metrics selection for load monitoring of service-oriented system},
year = {2021},
isbn = {9781450386258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472674.3473983},
doi = {10.1145/3472674.3473983},
abstract = {Background. Complex software systems produce a large amount of data depicting their internal state and activities. The data can be monitored to make estimations and predictions of the status of the system, helping taking preventative actions in case of impending malfunctions and failures. However, a complex system may reveal thousands of internal metrics, which makes it a non-trivial task to decide which metrics are the most important to monitor. Objective. In this work we aim at finding a subset of metrics to collect and analyse for the monitoring of the load in a Service-oriented system. Method. We use a performance test bench tool to generate load of different intensities on the target system, which is a specific service-oriented application platform. The numeric metrics data collected from the system is combined with the load intensity at each moment. The combined data is used to analyse which metrics are best at estimating the load of the system. By using a regression analysis it was possible to rank the metrics by their ability to measure the load of the system. Results. The results show that (1) the use of machine learning regressor allows to correctly measure the load of a service-oriented system, and (2) the most important metrics are related to network traffic and request counts, as well as memory usage and disk activity. Conclusion. The results help with the designs of efficient monitoring tool. In addition, further investigation should be focused on exploring more precise machine learning model to further improve the metric selection process.},
booktitle = {Proceedings of the 5th International Workshop on Machine Learning Techniques for Software Quality Evolution},
pages = {31–36},
numpages = {6},
keywords = {Machine Learning, Metrics Selection, Monitoring, Service-Oriented System},
location = {Athens, Greece},
series = {MaLTESQuE 2021}
}

@article{10.5555/2698256.2698490,
author = {Pizzi, Nick J.},
title = {Software quality prediction using fuzzy integration: a case study},
year = {2008},
issue_date = {January   2008},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {12},
number = {1},
issn = {1432-7643},
abstract = {Given the complexity of many contemporary software systems, it is often difficult to gauge the overall quality of their underlying software components. A potential technique to automatically evaluate such qualitative attributes is to use software metrics as quantitative predictors. In this case study, an aggregation technique based on fuzzy integration is presented that combines the predicted qualitative assessments from multiple classifiers. Multiple linear classifiers are presented with randomly selected subsets of automatically generated software metrics describing components from a sophisticated biomedical data analysis system. The external reference test is a software developer's thorough assessment of complexity, maintainability, and usability, which is used to assign corresponding quality class labels to each system component. The aggregated qualitative predictions using fuzzy integration are shown to be superior to the predictions from the respective best single classifiers.},
journal = {Soft Comput.},
month = jan,
pages = {67–76},
numpages = {10}
}

@article{10.1016/j.neucom.2011.08.040,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Software measurement data reduction using ensemble techniques},
year = {2012},
issue_date = {September, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {92},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2011.08.040},
doi = {10.1016/j.neucom.2011.08.040},
abstract = {Software defect prediction models are used to identify program modules that are high-risk, or likely to have a high number of faults. These models are built using software metrics which are collected during the software development process. Various techniques and approaches have been created for improving fault predictions. One of these is feature (metric) selection. Choosing the most important features is important to improve the effectiveness of defect predictors. However, using a single feature subset selection method may generate local optima. Ensembles of feature selection methods attempt to combine multiple feature selection methods instead of using a single one. In this paper, we present a comprehensive empirical study examining 17 different ensembles of feature ranking techniques (rankers) including six commonly used feature ranking techniques, the signal-to-noise filter technique, and 11 threshold-based feature ranking techniques. This study utilized 16 real-world software measurement data sets of different sizes and built 54,400 classification models using four well known classifiers. The main conclusion is that ensembles of very few rankers are very effective and even better than ensembles of many or all rankers.},
journal = {Neurocomput.},
month = sep,
pages = {124–132},
numpages = {9},
keywords = {Defect prediction, Ensembles of feature ranking techniques, Feature selection}
}

@article{10.14778/3229863.3229865,
author = {Boehm, Matthias and Reinwald, Berthold and Hutchison, Dylan and Sen, Prithviraj and Evfimievski, Alexandre V. and Pansare, Niketan},
title = {On optimizing operator fusion plans for large-scale machine learning in systemML},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3229865},
doi = {10.14778/3229863.3229865},
abstract = {Many machine learning (ML) systems allow the specification of ML algorithms by means of linear algebra programs, and automatically generate efficient execution plans. The opportunities for fused operators---in terms of fused chains of basic operators---are ubiquitous, and include fewer materialized intermediates, fewer scans of inputs, and sparsity exploitation across operators. However, existing fusion heuristics struggle to find good plans for complex operator DAGs or hybrid plans of local and distributed operations. In this paper, we introduce an exact yet practical cost-based optimization framework for fusion plans and describe its end-to-end integration into Apache SystemML. We present techniques for candidate exploration and selection of fusion plans, as well as code generation of local and distributed operations over dense, sparse, and compressed data. Our experiments in SystemML show end-to-end performance improvements of up to 22x, with negligible compilation overhead.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {1755–1768},
numpages = {14}
}

@article{10.1016/j.future.2019.12.016,
author = {Ortin, Francisco and Rodriguez-Prieto, Oscar and Pascual, Nicolas and Garcia, Miguel},
title = {Heterogeneous tree structure classification to label Java programmers according to their expertise level},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2019.12.016},
doi = {10.1016/j.future.2019.12.016},
journal = {Future Gener. Comput. Syst.},
month = apr,
pages = {380–394},
numpages = {15},
keywords = {Big code, Machine learning, Syntax patterns, Abstract syntax trees, Programmer expertise, Decision trees, Big data}
}

@article{10.1007/s10489-021-02324-3,
author = {\c{S}ahin, Canan Batur and Dinler, \"{O}zlem Batur and Abualigah, Laith},
title = {Prediction of software vulnerability based deep symbiotic genetic algorithms: Phenotyping of dominant-features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {11},
issn = {0924-669X},
url = {https://doi.org/10.1007/s10489-021-02324-3},
doi = {10.1007/s10489-021-02324-3},
abstract = {The detection of software vulnerabilities is considered a vital problem in the software security area for a long time. Nowadays, it is challenging to manage software security due to its increased complexity and diversity. So, vulnerability detection applications play a significant part in software development and maintenance. The ability of the forecasting techniques in vulnerability detection is still weak. Thus, one of the efficient defining features methods that have been used to determine the software vulnerabilities is the metaheuristic optimization methods. This paper proposes a novel software vulnerability prediction model based on using a deep learning method and SYMbiotic Genetic algorithm. We are first to apply Diploid Genetic algorithms with deep learning networks on software vulnerability prediction to the best of our knowledge. In this proposed method, a deep SYMbiotic-based genetic algorithm model (DNN-SYMbiotic GAs) is used by learning the phenotyping of dominant-features for software vulnerability prediction problems. The proposed method aimed at increasing the detection abilities of vulnerability patterns with vulnerable components in the software. Comprehensive experiments are conducted on several benchmark datasets; these datasets are taken from Drupal, Moodle, and PHPMyAdmin projects. The obtained results revealed that the proposed method (DNN-SYMbiotic GAs) enhanced vulnerability prediction, which reflects improving software quality prediction.},
journal = {Applied Intelligence},
month = nov,
pages = {8271–8287},
numpages = {17},
keywords = {Deep learning, Software vulnerability, Genetic algorithms, Symbiotic learning, Dominance mechanism}
}

@inproceedings{10.1109/FLOSS.2009.5071357,
author = {Caglayan, Bora and Bener, Ayse and Koch, Stefan},
title = {Merits of using repository metrics in defect prediction for open source projects},
year = {2009},
isbn = {9781424437207},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FLOSS.2009.5071357},
doi = {10.1109/FLOSS.2009.5071357},
abstract = {Many corporate code developers are the beta testers of open source software.They continue testing until they are sure that they have a stable version to build their code on. In this respect defect predictors play a critical role to identify defective parts of the software. Performance of a defect predictor is determined by correctly finding defective parts of the software without giving any false alarms. Having high false alarms means testers/ developers would inspect bug free code unnecessarily. Therefore in this research we focused on decreasing the false alarm rates by using repository metrics. We conducted experiments on the data sets of Eclipse project. Our results showed that repository metrics decreased the false alarm rates on the average to 23% from 32% corresponding up to 907 less files to inspect.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Emerging Trends in Free/Libre/Open Source Software Research and Development},
pages = {31–36},
numpages = {6},
series = {FLOSS '09}
}

@inproceedings{10.1145/3383219.3383268,
author = {Lenz, Luca and Felderer, Michael and Schwedes, Sascha and M\"{u}ller, Kai},
title = {Explainable Priority Assessment of Software-Defects using Categorical Features at SAP HANA},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383268},
doi = {10.1145/3383219.3383268},
abstract = {We want to automate priority assessment of software defects. To do so we provide a tool which uses an explainability-driven framework and classical machine learning algorithms to keep the decisions transparent. Differing from other approaches we only use objective and categorical fields from the bug tracking system as features. This makes our approach lightweight and extremely fast. We perform binary classification with priority labels corresponding to deadlines. Additionally, we evaluate the tool on real data to ensure good performance in the practical use case.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {366–367},
numpages = {2},
keywords = {bug priority, defect assessment, machine learning, software quality},
location = {Trondheim, Norway},
series = {EASE '20}
}

@article{10.1016/j.eswa.2009.02.064,
author = {Chiu, Nan-Hsing},
title = {An early software-quality classification based on improved grey relational classifier},
year = {2009},
issue_date = {September, 2009},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {36},
number = {7},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2009.02.064},
doi = {10.1016/j.eswa.2009.02.064},
abstract = {The inherent uncertainty and incomplete information of the software development process presents particular challenges for identifying fault-prone modules and providing a preferred model early enough in a development cycle in order to guide software enhancement efforts effectively. Grey relational analysis (GRA) of grey system theory is a well known approach that is utilized for generalizing estimates under small sample and uncertain conditions. This paper examines the potential benefits for providing an early software-quality classification based on improved grey relational classifier. The particle swarm optimization (PSO) approach is adopted to explore the best fit of weights on software metrics in the GRA approach for deriving a classifier with preferred balance of misclassification rates. We have demonstrated our approach by using the data from the medical information system dataset. Empirical results show that the proposed approach provides a preferred balance of misclassification rates than the grey relational classifiers without using PSO. It also outperforms the widely used classifiers of classification and regression trees (CART) and C4.5 approaches.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {10727–10734},
numpages = {8},
keywords = {Decision support systems, Grey relational analysis, Particle swarm optimization, Software-quality classification}
}

@article{10.1007/s11219-020-09529-8,
author = {Al-Hawari, Assem and Najadat, Hassan and Shatnawi, Raed},
title = {Classification of application reviews into software maintenance tasks using data mining techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09529-8},
doi = {10.1007/s11219-020-09529-8},
abstract = {Mobile application reviews are considered a rich source of information for software engineers to provide a general understanding of user requirements and technical feedback to avoid main programming issues. Previous researches have used traditional data mining techniques to classify user reviews into several software maintenance tasks. In this paper, we aim to use associative classification (AC) algorithms to investigate the performance of different classifiers to classify reviews into several software maintenance tasks. Also, we proposed a new AC approach for review mining (ACRM). Review classification needs preprocessing steps to apply natural language preprocessing and text analysis. Also, we studied the influence of two feature selection techniques (information gain and chi-square) on classifiers. Association rules give a better understanding of users’ intent since they discover the hidden patterns in words and features that are related to one of the maintenance tasks, and present it as class association rules (CARs). For testing the classifiers, we used two datasets that classify reviews into four different maintenance tasks. Results show that the highest accuracy was achieved by AC algorithms for both datasets. ACRM has the highest precision, recall, F-score, and accuracy. Feature selection helps improving the classifiers’ performance significantly.},
journal = {Software Quality Journal},
month = sep,
pages = {667–703},
numpages = {37},
keywords = {Associative classification, Software reviews mining, Interesting measures}
}

@inproceedings{10.1145/1188895.1188910,
author = {Di Fatta, Giuseppe and Leue, Stefan and Stegantova, Evghenia},
title = {Discriminative pattern mining in software fault detection},
year = {2006},
isbn = {1595935843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1188895.1188910},
doi = {10.1145/1188895.1188910},
abstract = {We present a method to enhance fault localization for software systems based on a frequent pattern mining algorithm. Our method is based on a large set of test cases for a given set of programs in which faults can be detected. The test executions are recorded as function call trees. Based on test oracles the tests can be classified into successful and failing tests. A frequent pattern mining algorithm is used to identify frequent subtrees in successful and failing test executions. This information is used to rank functions according to their likelihood of containing a fault. The ranking suggests an order in which to examine the functions during fault analysis. We validate our approach experimentally using a subset of Siemens benchmark programs.},
booktitle = {Proceedings of the 3rd International Workshop on Software Quality Assurance},
pages = {62–69},
numpages = {8},
keywords = {automated debugging, fault isolation},
location = {Portland, Oregon},
series = {SOQUA '06}
}

@article{10.1007/s10664-021-10024-2,
author = {Zamani, Shayan and Hemmati, Hadi},
title = {A pragmatic approach for hyper-parameter tuning in search-based test case generation},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10024-2},
doi = {10.1007/s10664-021-10024-2},
abstract = {Search-based test case generation, which is the application of meta-heuristic search for generating test cases, has been studied a lot in the literature, lately. Since, in theory, the performance of meta-heuristic search methods is highly dependent on their hyper-parameters, there is a need to study hyper-parameter tuning in this domain. In this paper, we propose a new metric (“Tuning Gain”), which estimates how cost-effective tuning a particular class is. We then predict “Tuning Gain” using static features of source code classes. Finally, we prioritize classes for tuning, based on the estimated “Tuning Gains” and spend the tuning budget only on the highly-ranked classes. To evaluate our approach, we exhaustively analyze 1,200 hyper-parameter configurations of a well-known search-based test generation tool (EvoSuite) for 250 classes of 19 projects from benchmarks such as SF110 and SBST2018 tool competition. We used a tuning approach called Meta-GA and compared the tuning results with and without the proposed class prioritization. The results show that for a low tuning budget, prioritizing classes outperforms the alternatives in terms of extra covered branches (10 times more than a traditional global tuning). In addition, we report the impact of different features of our approach such as search space size, tuning budgets, tuning algorithms, and the number of classes to tune, on the final results.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {35},
keywords = {Search-based testing, Hyper-parameter tuning, Test case generation, Source code metrics}
}

@article{10.1016/j.jss.2016.02.035,
author = {Procaccianti, Giuseppe and Fern\'{a}ndez, H\'{e}ctor and Lago, Patricia},
title = {Empirical evaluation of two best practices for energy-efficient software development},
year = {2016},
issue_date = {July 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {117},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.02.035},
doi = {10.1016/j.jss.2016.02.035},
abstract = {We empirically studied the energy impact of two best practices for energy-efficient software.We applied the practices on two widely used software applications, MySQL Server and Apache Webserver.Each practice successfully reduced the energy consumption of our test environment.The hardware resource usage of the modified applications is more energy-proportional.Software design and implementation choices significantly affect energy efficiency. Background. Energy efficiency is an increasingly important property of software. A large number of empirical studies have been conducted on the topic. However, current state-of-the-Art does not provide empirically-validated guidelines for developing energy-efficient software.Aim. This study aims at assessing the impact, in terms of energy savings, of best practices for achieving software energy efficiency, elicited from previous work. By doing so, it identifies which resources are affected by the practices and the possible trade-offs with energy consumption.Method. We performed an empirical experiment in a controlled environment, where we applied two different Green Software practices to two software applications, namely query optimization in MySQL Server and usage of "sleep" instruction in the Apache web server. We then performed a comparison of the energy consumption at system-level and at resource-level, before and after applying the practice.Results. Our results show that both practices are effective in improving software energy efficiency, reducing consumption up to 25%. We observe that after applying the practices, resource usage is more energy-proportional i.e., increasing CPU usage increases energy consumption in an almost linear way. We also provide our reflections on empirical experimentation in software energy efficiency.Conclusions. Our contribution shows that significant improvements in software energy efficiency can be gained by applying best practices during design and development. Future work will be devoted to further validate best practices, and to improve their reusability.},
journal = {J. Syst. Softw.},
month = jul,
pages = {185–198},
numpages = {14},
keywords = {Best practices, Energy efficiency, Software engineering}
}

@article{10.1016/j.eswa.2017.04.014,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Towards an ensemble based system for predicting the number of software faults},
year = {2017},
issue_date = {October 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {82},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.04.014},
doi = {10.1016/j.eswa.2017.04.014},
abstract = {Paper presents ensemble based system for the prediction of number of software faults.System is based on the heterogeneous ensemble method.System uses three fault prediction techniques as base learners for the ensemble.Results are verified on Eclipse datasets. Software fault prediction using different techniques has been done by various researchers previously. It is observed that the performance of these techniques varied from dataset to dataset, which make them inconsistent for fault prediction in the unknown software project. On the other hand, use of ensemble method for software fault prediction can be very effective, as it takes the advantage of different techniques for the given dataset to come up with better prediction results compared to individual technique. Many works are available on binary class software fault prediction (faulty or non-faulty prediction) using ensemble methods, but the use of ensemble methods for the prediction of number of faults has not been explored so far. The objective of this work is to present a system using the ensemble of various learning techniques for predicting the number of faults in given software modules. We present a heterogeneous ensemble method for the prediction of number of faults and use a linear combination rule and a non-linear combination rule based approaches for the ensemble. The study is designed and conducted for different software fault datasets accumulated from the publicly available data repositories. The results indicate that the presented system predicted number of faults with higher accuracy. The results are consistent across all the datasets. We also use prediction at level l (Pred(l)), and measure of completeness to evaluate the results. Pred(l) shows the number of modules in a dataset for which average relative error value is less than or equal to a threshold value l. The results of prediction at level l analysis and measure of completeness analysis have also confirmed the effectiveness of the presented system for the prediction of number of faults. Compared to the single fault prediction technique, ensemble methods produced improved performance for the prediction of number of software faults. Main impact of this work is to allow better utilization of testing resources helping in early and quick identification of most of the faults in the software system.},
journal = {Expert Syst. Appl.},
month = oct,
pages = {357–382},
numpages = {26},
keywords = {Empirical study, Genetic programming, Gradient boosting, Linear regression, Promise repository, Software fault prediction techniques}
}

@inproceedings{10.1109/ICPC.2019.00043,
author = {Schnappinger, Markus and Osman, Mohd Hafeez and Pretschner, Alexander and Fietzke, Arnaud},
title = {Learning a classifier for prediction of maintainability based on static analysis tools},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00043},
doi = {10.1109/ICPC.2019.00043},
abstract = {Static Code Analysis Tools are a popular aid to monitor and control the quality of software systems. Still, these tools only provide a large number of measurements that have to be interpreted by the developers in order to obtain insights about the actual quality of the software. In cooperation with professional quality analysts, we manually inspected source code from three different projects and evaluated its maintainability. We then trained machine learning algorithms to predict the human maintainability evaluation of program classes based on code metrics. The code metrics include structural metrics such as nesting depth, cloning information and abstractions like the number of code smells. We evaluated this approach on a dataset of more than 115,000 Lines of Code. Our model is able to predict up to 81% of the threefold labels correctly and achieves a precision of 80%. Thus, we believe this is a promising contribution towards automated maintainability prediction. In addition, we analyzed the attributes in our created dataset and identified the features with the highest predictive power, i.e. code clones, method length, and the number of alerts raised by the tool Teamscale. This insight provides valuable help for users needing to prioritize tool measurements.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {243–248},
numpages = {6},
keywords = {code comprehension, maintenance tools, software maintenance, software quality, static code analysis},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1109/IWoR.2019.00015,
author = {Sae-Lim, Natthawute and Hayashi, Shinpei and Saeki, Motoshi},
title = {Toward proactive refactoring: an exploratory study on decaying modules},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IWoR.2019.00015},
doi = {10.1109/IWoR.2019.00015},
abstract = {Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells.To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.},
booktitle = {Proceedings of the 3rd International Workshop on Refactoring},
pages = {39–46},
numpages = {8},
keywords = {code quality, code smell, refactoring},
location = {Montreal, Quebec, Canada},
series = {IWOR '19}
}

@inproceedings{10.5555/2394450.2394484,
author = {Catal, Cagatay and Diri, Banu},
title = {Software fault prediction with object-oriented metrics based artificial immune recognition system},
year = {2007},
isbn = {3540734597},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Software testing is a time-consuming and expensive process. Software fault prediction models are used to identify fault-prone classes automatically before system testing. These models can reduce the testing duration, project risks, resource and infrastructure costs. In this study, we propose a novel fault prediction model to improve the testing process. Chidamber-Kemerer Object-Oriented metrics and method-level metrics such as Halstead and McCabe are used as independent metrics in our Artificial Immune Recognition System based model. According to this study, class-level metrics based model which applies AIRS algorithm can be used successfully for fault prediction and its performance is higher than J48 based approach. A fault prediction tool which uses this model can be easily integrated into the testing process.},
booktitle = {Proceedings of the 8th International Conference on Product-Focused Software Process Improvement},
pages = {300–314},
numpages = {15},
location = {Riga, Latvia},
series = {PROFES'07}
}

@article{10.5555/2372179.2372185,
author = {Puuronen, Seppo and Tsymbal, Alexey},
title = {Local Feature Selection with Dynamic Integration of Classifiers},
year = {2001},
issue_date = {January 2001},
publisher = {IOS Press},
address = {NLD},
volume = {47},
number = {1–2},
issn = {0169-2968},
abstract = {Multidimensional data is often feature space heterogeneous so that individual features have unequal importance in different sub areas of the feature space. This motivates to search for a technique that provides a strategic splitting of the instance space being able to identify the best subset of features for each instance to be classified. Our technique applies the wrapper approach where a classification algorithm is used as an evaluation function to differentiate between different feature subsets. In order to make the feature selection local, we apply the recent technique for dynamic integration of classifiers. This allows to determine which classifier and which feature subset should be used for each new instance. Decision trees are used to help to restrict the number of feature combinations analyzed. For each new instance we consider only those feature combinations that include the features present in the path taken by the new instance in the decision tree built on the whole feature set. We evaluate our technique on data sets from the UCI machine learning repository. In our experiments, we use the C4.5 algorithm as the learning algorithm for base classifiers and for the decision trees that guide the local feature selection. The experiments show some advantages of the local feature selection with dynamic integration of classifiers in comparison with the selection of one feature subset for the whole space.},
journal = {Fundam. Inf.},
month = jan,
pages = {91–117},
numpages = {27},
keywords = {Feature selection, data mining, dynamic integration, ensemble of classifiers, machine learning}
}

@article{10.1504/IJDATS.2017.10003991,
title = {Software fault proneness prediction: a comparative study between bagging, boosting, and stacking ensemble and base learner methods},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {1},
issn = {1755-8050},
url = {https://doi.org/10.1504/IJDATS.2017.10003991},
doi = {10.1504/IJDATS.2017.10003991},
abstract = {Modules with defects might be the prime reason for decreasing the software quality and increasing the cost of maintenance. Therefore, the prediction of faulty modules of systems under test at early stages contributes to the overall quality of software products. In this research three symmetric ensemble methods: bagging, boosting and stacking are used to predict faulty modules based on evaluating the performance of 11 base learners. The results reveal that the defect prediction performance of the base learner classifier and ensemble learner classifiers is the same for na\"{\i}ve Bayes, Bayes net, PART, random forest, IB1, VFI, decision table, and NB tree base learners, the case was different for boosted SMO, bagged J48 and boosted and bagged random tree. In addition the results showed that the random forest classifier is one of the most significant classifiers that should be stacked with other classifiers to gain the better fault prediction.},
journal = {Int. J. Data Anal. Tech. Strateg.},
month = jan,
pages = {1–16},
numpages = {16}
}

@phdthesis{10.5555/2125736,
author = {Vivanco, Rodrigo},
title = {Improving predictive models of software quality using search-based metric selection and decision trees},
year = {2010},
isbn = {9780494711217},
publisher = {University of Manitoba},
address = {CAN},
abstract = {Software engineering is a human centric endeavour where the majority of the effort is spent understanding and modifying source code. The ability to automatically identify potentially problematic components would assist developers and project managers to make the best use of limited resources when taking mitigating actions such as detailed code inspections, more exhaustive testing, refactoring or reassignment to more experienced developers. Predictive models can be used to discover poor quality components via structural information from the design and/or source code. There exist many traditional source code metrics to capture the size, algorithmic complexity, cohesion and coupling of modules. Object-oriented systems have introduced additional structural concepts such as encapsulation and inheritance, providing even more ways to capture and measure different aspects of coupling, cohesion, complexity and size. An important question to answer is:  Which metrics should be used with a model for a particular predictive objective__ __ In machine learning, large dimensional feature spaces may contain inputs that are irrelevant or redundant. Feature selection is the process of identifying a subset of features that improve a classifier's discriminatory performance. In analysis of software system, the features used are source code metrics. In this work, an analysis tool has been developed that implements a parallel genetic algorithm (GA) as a search-based metric selection strategy. A comparative study has been carried out between GA, the Chidamber and Kemerer metrics suite (for an objected-oriented dataset), and principal component analysis (PCA) as metric selection strategies with different datasets. Program comprehension is important for programmers and the first dataset evaluated uses source code inspections as a subjective measure of cognitively complexity that degrade program understanding. Predicting the likely location of system failures is important in order to improve a system's reliability. The second dataset uses an objective measure of faults found in system modules in order to predict fault-prone components. The aim of this research has been to advance the current state of the art in predictive models of software quality by exploring the efficacy of a search-based approach in selecting appropriate metrics subsets for various predictive objectives. Results show that a search-based strategy, such as GA, performs well as a metric selection strategy when used with a linear discriminant analysis classifier. When predicting cognitive complex classes, GA achieved an F-value of 0.845 compared to an F-value of 0.740 using principal component analysis, and 0.750 when using only the CK metrics suite. By examining the GA chosen metrics with a white box predictive model (decision tree classifier) additional insights into the structural properties of a system that degrade product quality were observed. Source code metrics have been designed for human understanding and program comprehension and predictive models for cognitive complexity perform well with just source code metrics. Models for fault prone modules do not perform as well when using only source code metrics and need additional non-source code information, such module modification history or testing history.},
note = {AAINR71121}
}

@article{10.1016/j.knosys.2016.12.017,
author = {Rathore, Santosh Singh and Kumar, Sandeep},
title = {Linear and non-linear heterogeneous ensemble methods to predict the number of faults in software systems},
year = {2017},
issue_date = {March 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {119},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2016.12.017},
doi = {10.1016/j.knosys.2016.12.017},
abstract = {This paper expands the use of ensemble methods for the prediction of number of faults unlikely the earlier works on ensemble methods that focused on predicting software modules as faulty or non-faulty.This paper investigates the usage of both heterogeneous ensemble methods as well as homogeneous ensemble methods for the prediction of number of faults.We present two linear combination rules and two non-linear combination rules for combining the outputs of the base learners in the ensemble.In addition, we assess the performance of ensemble methods under two different scenarios, intra-release prediction and inter-releases prediction.The experiments are performed over five open-source software systems with their fifteen releases, collected from the PROMISE data repository. Several classification techniques have been investigated and evaluated earlier for the software fault prediction. These techniques have produced different prediction accuracy for the different software systems and none of the technique has always performed consistently better across different domains. On the other hand, software fault prediction using ensemble methods can be very effective, as they take the advantage of each participating technique for the given dataset and try to come up with better prediction results compared to the individual techniques. Many works are available for classifying software modules being faulty or non-faulty using the ensemble methods. These works are only specifying that whether a given software module is faulty or not, but number of faults in that module are not predicted by them. The use of ensemble methods for the prediction of number of faults has not been explored so far. To fulfill this gap, this paper presents ensemble methods for the prediction of number of faults in the given software modules. The experimental study is designed and conducted for five open-source software projects with their fifteen releases, collected from the PROMISE data repository. The results are evaluated under two different scenarios, intra-release prediction and inter-releases prediction. The prediction accuracy of ensemble methods is evaluated using absolute error, relative error, prediction at level l, and measure of completeness performance measures. Results show that the presented ensemble methods yield improved prediction accuracy over the individual fault prediction techniques under consideration. Further, the results are consistent for all the used datasets. The evidences obtained from the prediction at level l and measure of completeness analysis have also confirmed the effectiveness of the proposed ensemble methods for predicting the number of faults.},
journal = {Know.-Based Syst.},
month = mar,
pages = {232–256},
numpages = {25},
keywords = {Ensemble methods, Heterogeneous ensemble, Prediction of number of faults, Software fault prediction}
}

@article{10.1016/j.infsof.2016.10.006,
author = {Jiang, Jing and Yang, Yun and He, Jiahuan and Blanc, Xavier and Zhang, Li},
title = {Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development},
year = {2017},
issue_date = {April 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {84},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.10.006},
doi = {10.1016/j.infsof.2016.10.006},
abstract = {Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests.Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach.Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu etal. Then we compare precision and recall of different approaches.Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects.Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {48–62},
numpages = {15},
keywords = {Attribute selection, Commenter recommendation, Pull-based software development, Reviewer recommendation}
}

@article{10.1155/2016/7658207,
author = {Tomar, Divya and Agarwal, Sonali},
title = {Prediction of defective software modules using class imbalance learning},
year = {2016},
issue_date = {January 2016},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2016},
issn = {1687-9724},
url = {https://doi.org/10.1155/2016/7658207},
doi = {10.1155/2016/7658207},
abstract = {Software defect predictors are useful to maintain the high quality of software products effectively. The early prediction of defective software modules can help the software developers to allocate the available resources to deliver high quality software products. The objective of software defect prediction system is to find as many defective software modules as possible without affecting the overall performance. The learning process of a software defect predictor is difficult due to the imbalanced distribution of software modules between defective and nondefective classes. Misclassification cost of defective software modules generally incurs much higher cost than the misclassification of nondefective one. Therefore, on considering the misclassification cost issue, we have developed a software defect prediction system using Weighted Least Squares Twin Support Vector Machine (WLSTSVM). This system assigns higher misclassification cost to the data samples of defective classes and lower cost to the data samples of nondefective classes. The experiments on eight software defect prediction datasets have proved the validity of the proposed defect prediction system. The significance of the results has been tested via statistical analysis performed by using nonparametric Wilcoxon signed rank test.},
journal = {Appl. Comp. Intell. Soft Comput.},
month = jan,
articleno = {6},
numpages = {1}
}

@inproceedings{10.1145/3460319.3464840,
author = {Pan, Cong and Pradel, Michael},
title = {Continuous test suite failure prediction},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464840},
doi = {10.1145/3460319.3464840},
abstract = {Continuous integration advocates to run the test suite of a project frequently, e.g., for every code change committed to a shared repository. This process imposes a high computational cost and sometimes also a high human cost, e.g., when developers must wait for the test suite to pass before a change appears in the main branch of the shared repository. However, only 4% of all test suite invocations turn a previously passing test suite into a failing test suite. The question arises whether running the test suite for each code change is really necessary. This paper presents continuous test suite failure prediction, which reduces the cost of continuous integration by predicting whether a particular code change should trigger the test suite at all. The core of the approach is a machine learning model based on features of the code change, the test suite, and the development history. We also present a theoretical cost model that describes when continuous test suite failure prediction is worthwhile. Evaluating the idea with 15k test suite runs from 242 open-source projects shows that the approach is effective at predicting whether running the test suite is likely to reveal a test failure. Moreover, we find that our approach improves the AUC over baselines that use features proposed for just-in-time defect prediction and test case failure prediction by 13.9% and 2.9%, respectively. Overall, continuous test suite failure prediction can significantly reduce the cost of continuous integration.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {553–565},
numpages = {13},
keywords = {continuous integration, continuous test suite failure prediction, cost model, machine learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1016/j.infsof.2010.12.007,
author = {Heckman, Sarah and Williams, Laurie},
title = {A systematic literature review of actionable alert identification techniques for automated static code analysis},
year = {2011},
issue_date = {April, 2011},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {53},
number = {4},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2010.12.007},
doi = {10.1016/j.infsof.2010.12.007},
abstract = {ContextAutomated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice. ObjectiveThe goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT). MethodRelevant studies about AAITs were gathered via a systematic literature review. ResultsWe selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics. ConclusionThe selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program's history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics.},
journal = {Inf. Softw. Technol.},
month = apr,
pages = {363–387},
numpages = {25},
keywords = {Actionable alert identification, Actionable alert prediction, Automated static analysis, Systematic literature review, Unactionable alert mitigation, Warning prioritization}
}

@article{10.5555/1239046.1239048,
author = {Khoshgoftaar, Taghi M. and Zhong, Shi and Joshi, Vedang},
title = {Enhancing software quality estimation using ensemble-classifier based noise filtering},
year = {2005},
issue_date = {January 2005},
publisher = {IOS Press},
address = {NLD},
volume = {9},
number = {1},
issn = {1088-467X},
abstract = {This paper presents a technique that improves the accuracy of classification models by enhancing the quality of training data. The idea is to eliminate instances that are likely to be noisy, and train classification models on "clean" data. Our approach uses 25 different classification techniques to create an ensemble classifier to filter noise. Using a relatively large number of base-level classifiers in the ensemble filter helps achieve different levels of desired noise removal conservativeness with several possible levels of filtering. It also provides a high degree of confidence in the noise elimination procedure as the results are less likely to get influenced by (possible) inappropriate learning bias of a few algorithms with 25 base-level classifiers than with a relatively smaller number of base-level classifiers. An empirical case study with software measurement data of a high assurance software project demonstrates the effectiveness of our noise elimination approach in improving classification accuracies. The similarities among predictions from the 25 classifiers are also investigated, and preliminary results suggest that the 25 classifiers may be effectively reduced to 13.},
journal = {Intell. Data Anal.},
month = jan,
pages = {3–27},
numpages = {25}
}

@article{10.1145/3127360.3127368,
author = {Kumar, Lov and Behera, Ranjan Kumar and Rath, Santanu and Sureka, Ashish},
title = {Transfer Learning for Cross-Project Change-Proneness Prediction in Object-Oriented Software Systems: A Feasibility Analysis},
year = {2017},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3127360.3127368},
doi = {10.1145/3127360.3127368},
abstract = {Change-prone classes or modules are defined as regions of the source code which are more likely to change as a result of a software development of maintenance activity. Automatic identification of change-prone classes are useful for the software development team as they can focus their testing efforts on areas within the source code which are more likely to change. Several machine learning techniques have been proposed for predicting change-prone classes based on the application of source code metrics as indicators. However, most of the work has focused on within-project training and model building. There are several real word scenario in which sufficient training dataset is not available for model building such as in the case of a new project. Cross-project prediction is an approach which consists of training a model from dataset belonging to one project and testing it on dataset belonging to a different project. Cross-project change-proneness prediction is relatively unexplored.We propose a machine learning based approach for cross-project change-proneness prediction. We conduct experiments on 10 open-source Eclipse plug-ins and demonstrate the effectiveness of our approach. We frame several research questions comparing the performance of within project and cross project prediction and also propose a Genetic Algorithm (GA) based approach for identifying the best set of source code metrics. We conclude that for within project experimental setting, Random Forest (RF) technique results in the best precision. In case of cross-project change-proneness prediction, our analysis reveals that the NDTF ensemble method performs higher than other individual classifiers (such as decision tree and logistic regression) and ensemble methods in the experimental dataset. We conduct a comparison of within-project, cross-project without GA and cross-project with GA and our analysis reveals that cross-project with GA performs best followed by within-project and then cross-project without GA.},
journal = {SIGSOFT Softw. Eng. Notes},
month = sep,
pages = {1–11},
numpages = {11}
}

@inproceedings{10.1145/3293882.3338985,
author = {Kudjo, Patrick Kwaku and Chen, Jinfu},
title = {A cost-effective strategy for software vulnerability prediction based on bellwether analysis},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338985},
doi = {10.1145/3293882.3338985},
abstract = {Vulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1%-98.5%.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {424–427},
numpages = {4},
keywords = {Bellwether, Machine learning, Software vulnerability, Tuning},
location = {Beijing, China},
series = {ISSTA 2019}
}

@article{10.1007/s10270-017-0600-2,
author = {Hartmann, Thomas and Moawad, Assaad and Fouquet, Francois and Le Traon, Yves},
title = {The next evolution of MDE: a seamless integration of machine learning into domain modeling},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0600-2},
doi = {10.1007/s10270-017-0600-2},
abstract = {Machine learning algorithms are designed to resolve unknown behaviors by extracting commonalities over massive datasets. Unfortunately, learning such global behaviors can be inaccurate and slow for systems composed of heterogeneous elements, which behave very differently, for instance as it is the case for cyber-physical systems and Internet of Things applications. Instead, to make smart decisions, such systems have to continuously refine the behavior on a per-element basis and compose these small learning units together. However, combining and composing learned behaviors from different elements is challenging and requires domain knowledge. Therefore, there is a need to structure and combine the learned behaviors and domain knowledge together in a flexible way. In this paper we propose to weave machine learning into domain modeling. More specifically, we suggest to decompose machine learning into reusable, chainable, and independently computable small learning units, which we refer to as microlearning units. These microlearning units are modeled together with and at the same level as the domain data. We show, based on a smart grid case study, that our approach can be significantly more accurate than learning a global behavior, while the performance is fast enough to be used for live learning.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1285–1304},
numpages = {20},
keywords = {Cyber-physical systems, Domain modeling, Live learning, Metamodeling, Model-driven engineering, Smart grids}
}

@article{10.1007/s10664-015-9401-9,
author = {Jonsson, Leif and Borg, Markus and Broman, David and Sandahl, Kristian and Eldh, Sigrid and Runeson, Per},
title = {Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts},
year = {2016},
issue_date = {August    2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9401-9},
doi = {10.1007/s10664-015-9401-9},
abstract = {Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1533–1578},
numpages = {46},
keywords = {Bug assignment, Bug reports, Classification, Ensemble learning, Industrial scale; Large scale, Machine learning}
}

@article{10.1016/j.jss.2011.09.009,
author = {Rodr\'{\i}guez, D. and Sicilia, M. A. and Garc\'{\i}a, E. and Harrison, R.},
title = {Empirical findings on team size and productivity in software development},
year = {2012},
issue_date = {March, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.09.009},
doi = {10.1016/j.jss.2011.09.009},
abstract = {The size of software project teams has been considered to be a driver of project productivity. Although there is a large literature on this, new publicly available software repositories allow us to empirically perform further research. In this paper we analyse the relationships between productivity, team size and other project variables using the International Software Benchmarking Standards Group (ISBSG) repository. To do so, we apply statistical approaches to a preprocessed subset of the ISBSG repository to facilitate the study. The results show some expected correlations between productivity, effort and time as well as corroborating some other beliefs concerning team size and productivity. In addition, this study concludes that in order to apply statistical or data mining techniques to these type of repositories extensive preprocessing of the data needs to be performed due to ambiguities, wrongly recorded values, missing values, unbalanced datasets, etc. Such preprocessing is a difficult and error prone activity that would need further guidance and information that is not always provided in the repository.},
journal = {J. Syst. Softw.},
month = mar,
pages = {562–570},
numpages = {9},
keywords = {Effort estimation datasets, ISBSG repository, Productivity, Team size}
}

@article{10.1007/s11219-021-09547-0,
author = {Azzeh, Mohammad and Nassif, Ali Bou and Mart\'{\i}n, Cuauht\'{e}moc L\'{o}pez},
title = {Empirical analysis on productivity prediction and locality for use case points method},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {29},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-021-09547-0},
doi = {10.1007/s11219-021-09547-0},
abstract = {Use case points (UCP) method has been around for over two decades. Although there was a substantial criticism concerning the algebraic construction and factor assessment of UCP, it remains an efficient early size estimation method. Predicting software effort from UCP is still an ever-present challenge. The earlier version of UCP method suggested using productivity as a cost driver, where fixed or a few pre-defined productivity ratios have been widely agreed. While this approach was successful when not enough historical data is available, it is no longer acceptable because software projects are different in terms of development aspects. Therefore, it is better to understand the relationship between productivity and other UCP variables. This paper examines the impact of data locality approaches on productivity and effort prediction from multiple UCP variables. The environmental factors are used as partitioning factors to produce local homogeneous data either based on their influential levels or using clustering algorithms. Different machine learning methods, including solo and ensemble methods, are used to construct productivity and effort prediction models based on the local data. The results demonstrate that the prediction models that are created based on local data surpass models that use entire data. Also, the results show that conforming to&nbsp;the hypothetical assumption between productivity and environmental factors is not necessarily a requirement for the&nbsp;success of locality.},
journal = {Software Quality Journal},
month = jun,
pages = {309–336},
numpages = {28},
keywords = {Use case points, Productivity, Effort estimation, Data locality}
}

@inproceedings{10.1145/3387904.3389281,
author = {Zhang, Jinglei and Xie, Rui and Ye, Wei and Zhang, Yuhan and Zhang, Shikun},
title = {Exploiting Code Knowledge Graph for Bug Localization via Bi-directional Attention},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389281},
doi = {10.1145/3387904.3389281},
abstract = {Bug localization automatic localize relevant source files given a natural language description of bug within a software project. For a large project containing hundreds and thousands of source files, developers need cost lots of time to understand bug reports generated by quality assurance and localize these buggy source files. Traditional methods are heavily depending on the information retrieval technologies which rank the similarity between source files and bug reports in lexical level. Recently, deep learning based models are used to extract semantic information of code with significant improvements for bug localization. However, programming language is a highly structural and logical language, which contains various relations within and cross source files. Thus, we propose KGBugLocator to utilize knowledge graph embeddings to extract these interrelations of code, and a keywords supervised bi-directional attention mechanism regularize model with interactive information between source files and bug reports. With extensive experiments on four different projects, we prove our model can reach the new the-state-of-art(SOTA) for bug localization.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {219–229},
numpages = {11},
keywords = {bug localization, code representation, deep learning, knowledge graph},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@article{10.34768/amcs-2021-0046,
author = {Piundefinedta, Piotr and Szmuc, Tomasz},
title = {Applications of Rough Sets in Big Data Analysis: An Overview},
year = {2021},
issue_date = {Dec 2021},
publisher = {Walter de Gruyter &amp; Co.},
address = {USA},
volume = {31},
number = {4},
issn = {1641-876X},
url = {https://doi.org/10.34768/amcs-2021-0046},
doi = {10.34768/amcs-2021-0046},
abstract = {Big data, artificial intelligence and the Internet of things (IoT) are still very popular areas in current research and industrial applications. Processing massive amounts of data generated by the IoT and stored in distributed space is not a straightforward task and may cause many problems. During the last few decades, scientists have proposed many interesting approaches to extract information and discover knowledge from data collected in database systems or other sources. We observe a permanent development of machine learning algorithms that support each phase of the data mining process, ensuring achievement of better results than before. Rough set theory (RST) delivers a formal insight into information, knowledge, data reduction, uncertainty, and missing values. This formalism, formulated in the 1980s and developed by several researches, can serve as a theoretical basis and practical background for dealing with ambiguities, data reduction, building ontologies, etc. Moreover, as a mature theory, it has evolved into numerous extensions and has been transformed through various incarnations, which have enriched expressiveness and applicability of the related tools. The main aim of this article is to present an overview of selected applications of RST in big data analysis and processing. Thousands of publications on rough sets have been contributed; therefore, we focus on papers published in the last few years. The applications of RST are considered from two main perspectives: direct use of the RST concepts and tools, and jointly with other approaches, i.e., fuzzy sets, probabilistic concepts, and deep learning. The latter hybrid idea seems to be very promising for developing new methods and related tools as well as extensions of the application area.},
journal = {Int. J. Appl. Math. Comput. Sci.},
month = dec,
pages = {659–683},
numpages = {25},
keywords = {rough sets theory, big data analysis, deep learning, data mining, tools}
}

@inproceedings{10.1145/1868328.1868341,
author = {Caglayan, Bora and Tosun, Ayse and Miranskyy, Andriy and Bener, Ayse and Ruffolo, Nuzio},
title = {Usage of multiple prediction models based on defect categories},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868341},
doi = {10.1145/1868328.1868341},
abstract = {Background: Most of the defect prediction models are built for two purposes: 1) to detect defective and defect-free modules (binary classification), and 2) to estimate the number of defects (regression analysis). It would also be useful to give more information on the nature of defects so that software managers can plan their testing resources more effectively.Aims: In this paper, we propose a defect prediction model that is based on defect categories.Method: We mined the version history of a large-scale enterprise software product to extract churn and static code metrics. and grouped them into three defect categories according to different testing phases. We built a learning-based model for each defect category. We compared the performance of our proposed model with a general one. We conducted statistical techniques to evaluate the relationship between defect categories and software metrics. We also tested our hypothesis by replicating the empirical work on Eclipse data.Results: Our results show that building models that are sensitive to defect categories is cost-effective in the sense that it reveals more information and increases detection rates (pd) by 10% keeping the false alarms (pf) constant.Conclusions: We conclude that slicing defect data and categorizing it for use in a defect prediction model would enable practitioners to take immediate actions. Our results on Eclipse replication showed that haphazard categorization of defects is not worth the effort.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {8},
numpages = {9},
keywords = {defect categories, defect prediction, software quality},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@article{10.1145/3479575,
author = {Smith, Micah J. and Cito, J\"{u}rgen and Lu, Kelvin and Veeramachaneni, Kalyan},
title = {Enabling Collaborative Data Science Development with the Ballet Framework},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW2},
url = {https://doi.org/10.1145/3479575},
doi = {10.1145/3479575},
abstract = {While the open-source software development model has led to successful large-scale collaborations in building software systems, data science projects are frequently developed by individuals or small teams. We describe challenges to scaling data science collaborations and present a conceptual framework and ML programming model to address them. We instantiate these ideas in Ballet, the first lightweight framework for collaborative, open-source data science through a focus on feature engineering, and an accompanying cloud-based development environment. Using our framework, collaborators incrementally propose feature definitions to a repository which are each subjected to software and ML performance validation and can be automatically merged into an executable feature engineering pipeline. We leverage Ballet to conduct a case study analysis of an income prediction problem with 27 collaborators, and discuss implications for future designers of collaborative projects.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {431},
numpages = {39},
keywords = {collaborative framework, data science, feature definition, feature engineering, feature validation, machine learning, mutual information, streaming feature selection}
}

@article{10.1007/s11063-021-10607-6,
author = {Kassaymeh, Sofian and Abdullah, Salwani and Al-Laham, Mohamad and Alweshah, Mohammed and Al-Betar, Mohammed Azmi and Othman, Zalinda},
title = {Salp Swarm Optimizer for Modeling Software Reliability Prediction Problems},
year = {2021},
issue_date = {Dec 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {6},
issn = {1370-4621},
url = {https://doi.org/10.1007/s11063-021-10607-6},
doi = {10.1007/s11063-021-10607-6},
abstract = {In this paper, software effort prediction (SEP) and software test prediction (STP) (i.e., software reliability problems) are tackled by integrating the salp swarm algorithm (SSA) with a backpropagation neural network (BPNN). Software effort and test prediction problems are common in software engineering and arise when seeking to determine the actual software resources needed to develop a project. BPNN is the most popular prediction algorithm used in the literature. The performance of BPNN depends totally on the initial parameter values such as weight and biases. The main objective of this paper is to integrate SSA with the BPNN to find the optimal weight for every training cycle and thereby improve prediction accuracy. The proposed method, abbreviated as SSA-BPNN, is tested on twelve SEP datasets and two STP datasets. All datasets vary in terms of complexity and size. The results obtained by SSA-BPNN are evaluated according to twelve performance measures: MSE, RMSE, RAE, RRSE, MAE, MRE, MMRE, MdMRE, VAF(%), R2(%), ED, and MD. First, the results obtained by BPNN with SSA (i.e., SSA-BPNN) and without SSA are compared. The evaluation of the results indicates that SSA-BPNN performs better than BPNN for all datasets. In the comparative evaluation, the results of SSA-BPNN are compared against thirteen state-of-the-art methods using the same SEP and STP problem datasets. The evaluation of the results reveals that the proposed method outperforms the comparative methods for almost all datasets, both SEP and STP, in the case of most performance measures. In conclusion, integrating SSA with BPNN is a very powerful approach for solving software reliability problems that can be used widely to yield accurate prediction results.},
journal = {Neural Process. Lett.},
month = dec,
pages = {4451–4487},
numpages = {37},
keywords = {Machine learning, Salp swarm optimizer, Backpropagation neural network, Software reliability problems, Software effort estimation, Software test estimation}
}

@article{10.1007/s10515-021-00287-w,
author = {Gadelha, Guilherme and Ramalho, Franklin and Massoni, Tiago},
title = {Traceability recovery between bug reports and test cases-a Mozilla Firefox case study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-021-00287-w},
doi = {10.1007/s10515-021-00287-w},
abstract = {Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retrieval (IR) techniques have been proposed for the task, but they differ considerably in input parameters and results. It is difficult to assess results when those techniques are applied in isolation, usually in small or medium-sized software projects. Recently, multilayered approaches to machine learning, in special Deep Learning (DL), have achieved success in text classification through their capacity to model complex relationships among data. In this article, we apply several IR and DL techniques for investing automatic traceability between bug reports and manual test cases, using historical data from the Mozilla Firefox’s Quality Assurance (QA) team. In this case study, we assess the following IR techniques: LSI, LDA, and BM25, in addition to a DL architecture called Convolutional Neural Networks (CNNs), through the use of Word Embeddings. In this context of traceability, we observe poor performances from three out of the four studied techniques. Only the LSI technique presented acceptable results, standing out even over the state-of-the-art BM25 technique. The obtained results suggest that the semi-automatic application of the LSI technique – with an appropriate combination of thresholds – may be feasible for real-world software projects.},
journal = {Automated Software Engg.},
month = nov,
numpages = {46},
keywords = {Bug reports, System features, Test cases, Traceability, Information retrieval, Deep learning}
}

@article{10.1007/s10515-017-0229-y,
author = {Nizamani, Zeeshan Ahmed and Liu, Hui and Chen, David Matthew and Niu, Zhendong},
title = {Automatic approval prediction for software enhancement requests},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0229-y},
doi = {10.1007/s10515-017-0229-y},
abstract = {Software applications often receive a large number of enhancement requests that suggest developers to fulfill additional functions. Such requests are usually checked manually by the developers, which is time consuming and tedious. Consequently, an approach that can automatically predict whether a new enhancement report will be approved is beneficial for both the developers and enhancement suggesters. With the approach, according to their available time, the developers can rank the reports and thus limit the number of reports to evaluate from large collection of low quality enhancement requests that are unlikely to be approved. The approach can help developers respond to the useful requests more quickly. To this end, we propose a multinomial naive Bayes based approach to automatically predict whether a new enhancement report is likely to be approved or rejected. We acquire the enhancement reports of open-source software applications from Bugzilla for evaluation. Each report is preprocessed and modeled as a vector. Using these vectors with their corresponding approval status, we train a Bayes based classifier. The trained classifier predicts approval or rejection of the new enhancement reports. We apply different machine learning and neural network algorithms, and it turns out that the multinomial naive Bayes classifier yields the highest accuracy with the given dataset. The proposed approach is evaluated with 40,000 enhancement reports from 35 open source applications. The results of tenfold cross validation suggest that the average accuracy is up to 89.25%.},
journal = {Automated Software Engg.},
month = jun,
pages = {347–381},
numpages = {35},
keywords = {Document classification, Machine learning, Multinomial naive Bayes, Software enhancements}
}

@article{10.4018/jossp.2012040103,
author = {Chaturvedi, K. K. and Singh, V.B.},
title = {An Empirical Comparison of Machine Learning Techniques in Predicting the Bug Severity of Open and Closed Source Projects},
year = {2012},
issue_date = {April 2012},
publisher = {IGI Global},
address = {USA},
volume = {4},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/jossp.2012040103},
doi = {10.4018/jossp.2012040103},
abstract = {Bug severity is the degree of impact that a defect has on the development or operation of a component or system, and can be classified into different levels based on their impact on the system. Identification of severity level can be useful for bug triager in allocating the bug to the concerned bug fixer. Various researchers have attempted text mining techniques in predicting the severity of bugs, detection of duplicate bug reports and assignment of bugs to suitable fixer for its fix. In this paper, an attempt has been made to compare the performance of different machine learning techniques namely Support vector machine SVM, probability based Na\"{\i}ve Bayes NB, Decision Tree based J48 A Java implementation of C4.5, rule based Repeated Incremental Pruning to Produce Error Reduction RIPPER and Random Forests RF learners in predicting the severity level 1 to 5 of a reported bug by analyzing the summary or short description of the bug reports. The bug report data has been taken from NASA's PITS Projects and Issue Tracking System datasets as closed source and components of Eclipse, Mozilla &amp; GNOME datasets as open source projects. The analysis has been carried out in RapidMiner and STATISTICA data mining tools. The authors measured the performance of different machine learning techniques by considering i the value of accuracy and F-Measure for all severity level and ii number of best cases at different threshold level of accuracy and F-Measure.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {32–59},
numpages = {28},
keywords = {10-fold Cross Validation, Bug Repositories, Bug Severity, Multiclass Classification, Supervised Classification, Text Mining}
}

@inproceedings{10.1145/3433210.3453115,
author = {Meng, Dongyu and Guerriero, Michele and Machiry, Aravind and Aghakhani, Hojjat and Bose, Priyanka and Continella, Andrea and Kruegel, Christopher and Vigna, Giovanni},
title = {Bran: Reduce Vulnerability Search Space in Large Open Source Repositories by Learning Bug Symptoms},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3453115},
doi = {10.1145/3433210.3453115},
abstract = {Software is continually increasing in size and complexity, and therefore, vulnerability discovery would benefit from techniques that identify potentially vulnerable regions within large code bases, as this allows for easing vulnerability detection by reducing the search space. Previous work has explored the use of conventional code-quality and complexity metrics in highlighting suspicious sections of (source) code. Recently, researchers also proposed to reduce the vulnerability search space by studying code properties with neural networks. However, previous work generally failed in leveraging the rich metadata that is available for long-running, large code repositories.In this paper, we present an approach, named Bran, to reduce the vulnerability search space by combining conventional code metrics with fine-grained repository metadata. Bran locates code sections that are more likely to contain vulnerabilities in large code bases, potentially improving the efficiency of both manual and automatic code audits. In our experiments on four large code bases, Bran successfully highlights potentially vulnerable functions, outperforming several baselines, including state-of-art vulnerability prediction tools. We also assess Bran's effectiveness in assisting automated testing tools. We use Bran to guide syzkaller, a known kernel fuzzer, in fuzzing a recent version of the Linux kernel. The guided fuzzer identifies 26 bugs (10 are zero-day flaws), including arbitrary writes and reads.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {731–743},
numpages = {13},
keywords = {machine learning, static analysis, vulnerabilities},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@article{10.1016/j.ins.2021.05.041,
author = {Kluska, Jacek and Madera, Michal},
title = {Extremely simple classifier based on fuzzy logic and gene expression programming},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {571},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2021.05.041},
doi = {10.1016/j.ins.2021.05.041},
journal = {Inf. Sci.},
month = sep,
pages = {560–579},
numpages = {20},
keywords = {Machine learning, Data mining, Fuzzy rule-based classifier, Gene expression programming, Interpretability}
}

@article{10.1007/s11334-020-00383-2,
author = {Althar, Raghavendra Rao and Samanta, Debabrata},
title = {The realist approach for evaluation of computational intelligence in software engineering},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-020-00383-2},
doi = {10.1007/s11334-020-00383-2},
abstract = {Secured software development must employ a security mindset across software engineering practices. Software security must be considered during the requirements phase so that it is included throughout the development phase. Do the requirements gathering team get the proper input from the technical team? This paper unearths some of the data sources buried within software development phases and describes the potential approaches to understand them. Concepts such as machine learning and deep learning are explored to understand the data sources and explore how these learnings can be provided to the requirements gathering team. This knowledge system will help bring objectivity in the conversations between the requirements gathering team and the customer's business team. A literature review is also done to secure requirements management and identify the possible gaps in providing future research direction to enhance our understanding. Feature engineering in the landscape of software development is explored to understand the data sources. Experts offer their insight on the root cause of the lack of security focus in requirements gathering practices. The core theme is statistical modeling of all the software artifacts that hold information related to the software development life cycle. Strengthening of some traditional methods like threat modeling is also a key area explored. Subjectivity involved in these approaches can be made more objective.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {17–27},
numpages = {11},
keywords = {Software engineering, Data science, Computational intelligence, Software requirements management, Threat modeling}
}

@article{10.1007/s10664-013-9241-4,
author = {Khatibi Bardsiri, Vahid and Jawawi, Dayang Norhayati and Hashim, Siti Zaiton and Khatibi, Elham},
title = {A flexible method to estimate the software development effort based on the classification of projects and localization of comparisons},
year = {2014},
issue_date = {August    2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-013-9241-4},
doi = {10.1007/s10664-013-9241-4},
abstract = {The estimation of software development effort has been centralized mostly on the accuracy of estimates through dealing with heterogeneous datasets regardless of the fact that the software projects are inherently complex and uncertain. In particular, Analogy Based Estimation (ABE), as a widely accepted estimation method, suffers a great deal from the problem of inconsistent and non-normal datasets because it is a comparison-based method and the quality of comparisons strongly depends on the consistency of projects. In order to overcome this problem, prior studies have suggested the use of weighting methods, outlier elimination techniques and various types of soft computing methods. However the proposed methods have reduced the complexity and uncertainty of projects, the results are not still convincing and the methods are limited to a special domain of software projects, which causes the generalization of methods to be impossible. Localization of comparison and weighting processes through clustering of projects is the main idea behind this paper. A hybrid model is proposed in which the software projects are divided into several clusters based on key attributes (development type, organization type and development platform). A combination of ABE and Particle Swarm Optimization (PSO) algorithm is used to design a weighting system in which the project attributes of different clusters are given different weights. Instead of comparing a new project with all the historical projects, it is only compared with the projects located in the related clusters based on the common attributes. The proposed method was evaluated through three real datasets that include a total of 505 software projects. The performance of the proposed model was compared with other well-known estimation methods and the promising results showed that the proposed localization can considerably improve the accuracy of estimates. Besides the increase in accuracy, the results also certified that the proposed method is flexible enough to be used in a wide range of software projects.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {857–884},
numpages = {28},
keywords = {ABE, Clustering, Effort estimation, Localization, PSO}
}

@inproceedings{10.1145/3419604.3419809,
author = {Miloudi, Chaymae and Cheikhi, Laila and Idri, Ali},
title = {A Review of Open Source Software Maintenance Effort Estimation},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419809},
doi = {10.1145/3419604.3419809},
abstract = {Open Source Software (OSS) is gaining interests of software engineering community as well as practitioners from industry with the growth of the internet. Studies in estimating maintenance effort (MEE) of such software product have been published in the literature in order to provide better estimation. The aim of this study is to provide a review of studies related to maintenance effort estimation for open source software (OSSMEE). To this end, a set of 60 primary empirical studies are selected from six electronic databases and a discussion is provided according to eight research questions (RQs) related to: publication year, publication source, datasets (OSS projects), metrics (independent variables), techniques, maintenance effort (dependent variable), validation methods, and accuracy criteria used in the empirical validation. This study has found that popular OSS projects have been used, Linear Regression, Na\"{\i}ve Bayes and k Nearest Neighbors were frequently used, and bug resolution was the most used regarding the estimation of maintenance effort for the future releases. A set of gaps are identified and recommendations for researchers are also provided.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {41},
numpages = {7},
keywords = {Datasets, Empirical, Maintenance effort estimation, Open source software, Review, metrics, techniques},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1007/978-3-030-58811-3_67,
author = {ElGhondakly, Roaa and Moussa, Sherin and Badr, Nagwa},
title = {Handling Faults in Service Oriented Computing: A Comprehensive Study},
year = {2020},
isbn = {978-3-030-58810-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-58811-3_67},
doi = {10.1007/978-3-030-58811-3_67},
abstract = {Recently, service-oriented computing paradigms have become a trending development direction, in which software systems are built using a set of loosely coupled services distributed over multiple locations through a service-oriented architecture. Such systems encounter different challenges, as integration, performance, reliability, availability, etc., which made all associated testing activities to be another major challenge to avoid their faults and system failures. Services are considered the substantial element in service-oriented computing. Thus, the quality of services and the service dependability in a web service composition have become essential to manage faults within these software systems. Many studies addressed web service faults from diverse perspectives. In this paper, a comprehensive study is conducted to investigate the different perspectives to manipulate web service faults, including fault tolerance, fault injection, fault prediction and fault localization. An extensive comparison is provided, highlighting the main research gaps, challenges and limitations of each perspective for web services. An analytical discussion is then followed to suggest future research directions that can be adopted to face such obstacles by improving fault handling capabilities for an efficient testing in service-oriented computing systems.},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part IV},
pages = {947–959},
numpages = {13},
keywords = {Fault tolerance, Fault prediction, Fault injection, Quality of Service, Service testing, Service oriented computing},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/267895.267916,
author = {Ebert, Christof},
title = {Experiences with criticality predictions in software development},
year = {1997},
isbn = {3540635319},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1145/267895.267916},
doi = {10.1145/267895.267916},
booktitle = {Proceedings of the 6th European SOFTWARE ENGINEERING Conference Held Jointly with the 5th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {278–293},
numpages = {16},
keywords = {classification, criticality prediction, data analysis, eomplexity, quality models, software metrics},
location = {Zurich, Switzerland},
series = {ESEC '97/FSE-5}
}

@article{10.1016/j.neucom.2020.01.120,
author = {Malhotra, Ruchika and Lata, Kusum},
title = {An empirical study to investigate the impact of data resampling techniques on the performance of class maintainability prediction models},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {459},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2020.01.120},
doi = {10.1016/j.neucom.2020.01.120},
journal = {Neurocomput.},
month = oct,
pages = {432–453},
numpages = {22},
keywords = {Maintainability prediction, Imbalanced data, Data resampling techniques, Machine learning techniques, Search-based techniques, Object-oriented metrics}
}

@article{10.1007/s11219-006-7597-z,
author = {Khoshgoftaar, Taghi M. and Seliya, Naeem and Sundaresh, Nandini},
title = {An empirical study of predicting software faults with case-based reasoning},
year = {2006},
issue_date = {June      2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-006-7597-z},
doi = {10.1007/s11219-006-7597-z},
abstract = {The resources allocated for software quality assurance and improvement have not increased with the ever-increasing need for better software quality. A targeted software quality inspection can detect faulty modules and reduce the number of faults occurring during operations. We present a software fault prediction modeling approach with case-based reasoning (CBR), a part of the computational intelligence field focusing on automated reasoning processes. A CBR system functions as a software fault prediction model by quantifying, for a module under development, the expected number of faults based on similar modules that were previously developed. Such a system is composed of a similarity function, the number of nearest neighbor cases used for fault prediction, and a solution algorithm. The selection of a particular similarity function and solution algorithm may affect the performance accuracy of a CBR-based software fault prediction system. This paper presents an empirical study investigating the effects of using three different similarity functions and two different solution algorithms on the prediction accuracy of our CBR system. The influence of varying the number of nearest neighbor cases on the performance accuracy is also explored. Moreover, the benefits of using metric-selection procedures for our CBR system is also evaluated. Case studies of a large legacy telecommunications system are used for our analysis. It is observed that the CBR system using the Mahalanobis distance similarity function and the inverse distance weighted solution algorithm yielded the best fault prediction. In addition, the CBR models have better performance than models based on multiple linear regression.},
journal = {Software Quality Journal},
month = jun,
pages = {85–111},
numpages = {27},
keywords = {Case-based reasoning, Similarity functions, Software fault prediction, Software metrics, Software quality, Solution algorithm}
}

@article{10.5555/3455716.3455928,
author = {P\"{o}lsterl, Sebastian},
title = {scikit-survival: a library for time-to-event analysis built on top of scikit-learn},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {scikit-survival is an open-source Python package for time-to-event analysis fully compatible with scikit-learn. It provides implementations of many popular machine learning techniques for time-to-event analysis, including penalized Cox model, Random Survival Forest, and Survival Support Vector Machine. In addition, the library includes tools to evaluate model performance on censored time-to-event data. The documentation contains installation instructions, interactive notebooks, and a full description of the API. scikit-survival is distributed under the GPL-3 license with the source code and detailed instructions available at https://github.com/sebp/scikit-survival},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {212},
numpages = {6},
keywords = {time-to-event analysis, survival analysis, censored data, Python}
}

@inproceedings{10.1109/MSR.2017.4,
author = {Rajbahadur, Gopi Krishnan and Wang, Shaowei and Kamei, Yasutaka and Hassan, Ahmed E.},
title = {The impact of using regression models to build defect classifiers},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.4},
doi = {10.1109/MSR.2017.4},
abstract = {It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers).In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches; ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance.Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {135–145},
numpages = {11},
keywords = {bug prediction, classification via regression, discretization, model interpretation, non-discretization, random forest},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@article{10.5555/3337636.3337642,
title = {Threshold-based empirical validation of object-oriented metrics on different severity levels},
year = {2019},
issue_date = {January 2019},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {2–3},
issn = {1758-8715},
abstract = {Software metrics has become desideratum for the fault-proneness, reusability and effort prediction. To enhance and intensify the sufficiency of object-oriented OO metrics, it is crucial to perceive the relationship between OO metrics and fault-proneness at distinct severity levels. This paper characterise on the investigation of the software parts with higher probability of occurrence of faults. We examined the effect of thresholds on the OO metrics and build the predictive model based on those threshold values. This paper also instanced on the empirical validation of threshold values calculated for the OO metrics for predicting faults at different severity levels and builds the statistical model using logistic regression. This paper depicts the detection of fault-proneness by extracting the relevant OO metrics and focus on those projects that falls outside the specified risk level for allocating the more resources to them. We presented the effects of threshold values at different risk levels and also validated results on the KC1 dataset using machine learning and different classifiers.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {231–262},
numpages = {32}
}

@article{10.1016/j.asoc.2021.107259,
author = {Kang, Yanzhe and Jia, Ning and Cui, Runbang and Deng, Jiang},
title = {A graph-based semi-supervised reject inference framework considering imbalanced data distribution for consumer credit scoring},
year = {2021},
issue_date = {Jul 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {105},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2021.107259},
doi = {10.1016/j.asoc.2021.107259},
journal = {Appl. Soft Comput.},
month = jul,
numpages = {19},
keywords = {Financial technology, Credit scoring, Reject inference, Imbalanced learning, Semi-supervised learning, Label spreading}
}

@article{10.1007/s11219-010-9128-1,
author = {M\i{}s\i{}rl\i{}, Ay\c{s}e Tosun and Bener, Ay\c{s}e Ba\c{s}ar and Turhan, Burak},
title = {An industrial case study of classifier ensembles for locating software defects},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9128-1},
doi = {10.1007/s11219-010-9128-1},
abstract = {As the application layer in embedded systems dominates over the hardware, ensuring software quality becomes a real challenge. Software testing is the most time-consuming and costly project phase, specifically in the embedded software domain. Misclassifying a safe code as defective increases the cost of projects, and hence leads to low margins. In this research, we present a defect prediction model based on an ensemble of classifiers. We have collaborated with an industrial partner from the embedded systems domain. We use our generic defect prediction models with data coming from embedded projects. The embedded systems domain is similar to mission critical software so that the goal is to catch as many defects as possible. Therefore, the expectation from a predictor is to get very high probability of detection (pd). On the other hand, most embedded systems in practice are commercial products, and companies would like to lower their costs to remain competitive in their market by keeping their false alarm (pf) rates as low as possible and improving their precision rates. In our experiments, we used data collected from our industry partners as well as publicly available data. Our results reveal that ensemble of classifiers significantly decreases pf down to 15% while increasing precision by 43% and hence, keeping balance rates at 74%. The cost-benefit analysis of the proposed model shows that it is enough to inspect 23% of the code on local datasets to detect around 70% of defects.},
journal = {Software Quality Journal},
month = sep,
pages = {515–536},
numpages = {22},
keywords = {Defect prediction, Embedded software, Ensemble of classifiers, Static code attributes}
}

@article{10.1007/s11219-011-9149-4,
author = {G\"{u}ldali, Baris and Funke, Holger and Sauer, Stefan and Engels, Gregor},
title = {TORC: test plan optimization by requirements clustering},
year = {2011},
issue_date = {December  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9149-4},
doi = {10.1007/s11219-011-9149-4},
abstract = {Acceptance testing is a time-consuming task for complex software systems that have to fulfill a large number of requirements. To reduce this effort, we have developed a widely automated method for deriving test plans from requirements that are expressed in natural language. It consists of three stages: annotation, clustering, and test plan specification. The general idea is to exploit redundancies and implicit relationships in requirements specifications. Multi-viewpoint techniques based on RM-ODP (Reference Model for Open Distributed Processing) are employed for specifying the requirements. We then use linguistic analysis techniques, requirements clustering algorithms, and pattern-based requirements collection to reduce the total effort of testing against the requirements specification. In particular, we use linguistic analysis for extracting and annotating the actor, process and object of a requirements statement. During clustering, a similarity function is computed as a measure for the overlap of requirements. In the test plan specification stage, our approach provides capabilities for semi-automatically deriving test plans and acceptance criteria from the clustered informal textual requirements. Two patterns are applied to compute a suitable order of test activities. The generated test plans consist of a sequence of test steps and asserts that are executed or checked in the given order. We also present the supporting prototype tool TORC, which is available open source. For the evaluation of the approach, we have conducted a case study in the field of acceptance testing of a national electronic identification system. In summary, we report on lessons learned how linguistic analysis and clustering techniques can help testers in understanding the relations between requirements and for improving test planning.},
journal = {Software Quality Journal},
month = dec,
pages = {771–799},
numpages = {29},
keywords = {Acceptance criteria, Acceptance testing, Linguistic analysis, Requirements clustering, Test planning}
}

@inproceedings{10.1109/MSR.2019.00018,
author = {Kiehn, Max and Pan, Xiangyi and Camci, Fatih},
title = {Empirical study in using version histories for change risk classification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00018},
doi = {10.1109/MSR.2019.00018},
abstract = {Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {58–62},
numpages = {5},
keywords = {change risk, code ownership, file metrics, machine learning},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@article{10.1016/j.datak.2008.10.005,
author = {Turhan, Burak and Bener, Ayse},
title = {Analysis of Naive Bayes' assumptions on software fault data: An empirical study},
year = {2009},
issue_date = {February, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {68},
number = {2},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2008.10.005},
doi = {10.1016/j.datak.2008.10.005},
abstract = {Software defect prediction is important for reducing test times by allocating testing resources effectively. In terms of predicting the defects in software, Naive Bayes outperforms a wide range of other methods. However, Naive Bayes assumes the 'independence' and 'equal importance' of attributes. In this work, we analyze these assumptions of Naive Bayes using public software defect data from NASA. Our analysis shows that independence assumption is not harmful for software defect data with PCA pre-processing. Our results also indicate that assigning weights to static code attributes may increase the prediction performance significantly, while removing the need for feature subset selection.},
journal = {Data Knowl. Eng.},
month = feb,
pages = {278–290},
numpages = {13},
keywords = {Empirical study, Naive Bayes, Software defect prediction}
}

@article{10.1007/s10664-021-10038-w,
author = {Hasan, Masum and Iqbal, Anindya and Islam, Mohammad Rafid Ul and Rahman, A.J.M. Imtiajur and Bosu, Amiangshu},
title = {Using a balanced scorecard to identify opportunities to improve code review effectiveness: an industrial experience report},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10038-w},
doi = {10.1007/s10664-021-10038-w},
abstract = {Peer code review is a widely adopted software engineering practice to ensure code quality and ensure software reliability in both the commercial and open-source software projects. Due to the large effort overhead associated with practicing code reviews, project managers often wonder, if their code reviews are effective and if there are improvement opportunities in that respect. Since project managers at Samsung Research Bangladesh (SRBD) were also intrigued by these questions, this research developed, deployed, and evaluated a production-ready solution using the Balanced SCorecard (BSC) strategy that SRBD managers can use in their day-to-day management to monitor individual developer’s, a particular project’s or the entire organization’s code review effectiveness. Following the four-step framework of the BSC strategy, we– 1) defined the operation goals of this research, 2) defined a set of metrics to measure the effectiveness of code reviews, 3) developed an automated mechanism to measure those metrics, and 4) developed and evaluated a monitoring application to inform the key stakeholders. Our automated model to identify useful code reviews achieves 7.88% and 14.39% improvement in terms of accuracy and minority class F1 score respectively over the models proposed in prior studies. It also outperforms human evaluators from SRBD, that the model replaces, by a margin of 25.32% and 23.84% respectively in terms of accuracy and minority class F1 score. In our post-deployment survey, SRBD developers and managers indicated that they found our solution as useful and it provided them with important insights to help their decision makings.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {34},
keywords = {Code review, Software development, Usefulness, Productivity, Tool development}
}

@article{10.1016/j.peva.2012.09.004,
author = {Cotroneo, Domenico and Natella, Roberto and Pietrantuono, Roberto},
title = {Predicting aging-related bugs using software complexity metrics},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {70},
number = {3},
issn = {0166-5316},
url = {https://doi.org/10.1016/j.peva.2012.09.004},
doi = {10.1016/j.peva.2012.09.004},
abstract = {Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs.},
journal = {Perform. Eval.},
month = mar,
pages = {163–178},
numpages = {16},
keywords = {Aging-related bugs, Fault prediction, Software aging, Software complexity metrics}
}

@article{10.1016/j.ins.2011.01.039,
author = {Rodr\'{\i}guez, D. and Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S.},
title = {Searching for rules to detect defective modules: A subgroup discovery approach},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2011.01.039},
doi = {10.1016/j.ins.2011.01.039},
abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases.},
journal = {Inf. Sci.},
month = may,
pages = {14–30},
numpages = {17},
keywords = {Defect prediction, Imbalanced datasets, Rules, Subgroup discovery}
}

@article{10.1016/j.jksuci.2017.07.006,
author = {Lal, Sangeeta and Sardana, Neetu and Sureka, Ashish},
title = {Three-level learning for improving cross-project logging prediction for if-blocks},
year = {2019},
issue_date = {Oct 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {31},
number = {4},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2017.07.006},
doi = {10.1016/j.jksuci.2017.07.006},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = oct,
pages = {481–496},
numpages = {16}
}

@article{10.1016/j.patcog.2016.08.023,
author = {Zhang, Xiuzhen and Li, Yuxuan and Kotagiri, Ramamohanarao and Wu, Lifang and Tari, Zahir and Cheriet, Mohamed},
title = {KRNN},
year = {2017},
issue_date = {February 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {62},
number = {C},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2016.08.023},
doi = {10.1016/j.patcog.2016.08.023},
abstract = {Imbalanced classification is a challenging problem. Re-sampling and cost-sensitive learning are global strategies for generality-oriented algorithms such as the decision tree, targeting inter-class imbalance. We research local strategies for the specificity-oriented learning algorithms like the k Nearest Neighbour (KNN) to address the within-class imbalance issue of positive data sparsity. We propose an algorithm k Rare-class Nearest Neighbour, or KRNN, by directly adjusting the induction bias of KNN. We propose to form dynamic query neighbourhoods, and to further adjust the positive posterior probability estimation to bias classification towards the rare class. We conducted extensive experiments on thirty real-world and artificial datasets to evaluate the performance of KRNN. Our experiments showed that KRNN significantly improved KNN for classification of the rare class, and often outperformed re-sampling and cost-sensitive learning strategies with generality-oriented base learners. HighlightsNearest neighbour classification algorithm for accurate rare-class classification.Dynamic nearest neighbourhood formulation.Adjusted posterior class probability estimation biased for the rare class.},
journal = {Pattern Recogn.},
month = feb,
pages = {33–44},
numpages = {12},
keywords = {Cost-sensitive learning, Imbalanced classification, KNN, Nearest neighbour classification, Re-sampling}
}

@article{10.1007/s11219-019-09446-5,
author = {Honfi, D\'{a}vid and Micskei, Zolt\'{a}n},
title = {Classifying generated white-box tests: an exploratory study},
year = {2019},
issue_date = {Sep 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09446-5},
doi = {10.1007/s11219-019-09446-5},
abstract = {White-box test generation analyzes the code of the system under test, selects relevant test inputs, and captures the observed behavior of the system as expected values in the tests. However, if there is a fault in the implementation, this fault could get encoded in the assertions (expectations) of the tests. The fault is only recognized if the developer, who is using test generation, is also aware of the real expected behavior. Otherwise, the fault remains silent both in the test and in the implementation. A common assumption is that developers using white-box test generation techniques need to inspect the generated tests and their assertions, and to validate whether the tests encode any fault or represent the real expected behavior. Our goal is to provide insights about how well developers perform in this classification task. We designed an exploratory study to investigate the performance of developers. We also conducted an internal replication to increase the validity of the results. The two studies were carried out in a laboratory setting with 106 graduate students altogether. The tests were generated in four open-source projects. The results were analyzed quantitatively (binary classification metrics and timing measurements) and qualitatively (by observing and coding the activities of participants from screen captures and detailed logs). The results showed that participants tend to incorrectly classify tests encoding both expected and faulty behavior (with median misclassification rate 20%). The time required to classify one test varied broadly with an average of 2 min. This classification task is an essential step in white-box test generation that notably affects the real fault detection capability of such tools. We recommended a conceptual framework to describe the classification task and suggested taking this problem into account when using or evaluating white-box test generators.},
journal = {Software Quality Journal},
month = sep,
pages = {1339–1380},
numpages = {42},
keywords = {Software testing, White-box test generation, Empirical study, Test classification}
}

@inproceedings{10.1145/3177457.3191709,
author = {Ren, Yidan and Zhu, Zhengzhou and Chen, Xiangzhou and Ding, Huixia and Zhang, Geng},
title = {Research on Defect Detection Technology of Trusted Behavior Decision Tree Based on Intelligent Data Semantic Analysis of Massive Data},
year = {2018},
isbn = {9781450363396},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177457.3191709},
doi = {10.1145/3177457.3191709},
abstract = {With the rapid development of information technology, software systems' scales and complexity are showing a trend of expansion. The users' needs for the software security, software security reliability and software stability are growing increasingly. At present, the industry has applied machine learning methods to the fields of defect detection to repair and improve software defects through the massive data intelligent semantic analysis or code scanning. The model in machine learning is faced with big difficulty of model building, understanding, and the poor visualization in the field of traditional software defect detection. In view of the above problems, we present a point of view that intelligent semantic analysis technology based on massive data, and using the trusted behavior decision tree model to analyze the soft behavior by layered detection technology. At the same time, it is equipped related test environment to compare the tested software. The result shows that the defect detection technology based on intelligent semantic analysis of massive data is superior to other techniques at the cost of building time and error reported ratio.},
booktitle = {Proceedings of the 10th International Conference on Computer Modeling and Simulation},
pages = {168–175},
numpages = {8},
keywords = {Massive data, decision tree, intelligent semantic analysis, software defect detection},
location = {Sydney, Australia},
series = {ICCMS '18}
}

@inproceedings{10.1145/3377811.3380411,
author = {Zhao, Dehai and Xing, Zhenchang and Chen, Chunyang and Xu, Xiwei and Zhu, Liming and Li, Guoqiang and Wang, Jinshui},
title = {Seenomaly: vision-based linting of GUI animation effects against design-don't guidelines},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380411},
doi = {10.1145/3377811.3380411},
abstract = {GUI animations, such as card movement, menu slide in/out, snackbar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can "see" the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by "seeing" lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and real-world GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1286–1297},
numpages = {12},
keywords = {GUI animation, design guidelines, lint, unsupervised learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3278142.3278147,
author = {Falessi, Davide and Moede, Max Jason},
title = {Facilitating feasibility analysis: the pilot defects prediction dataset maker},
year = {2018},
isbn = {9781450360562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278142.3278147},
doi = {10.1145/3278142.3278147},
abstract = {Our industrial experience in institutionalizing defect prediction models in the software industry shows that the first step is to measure prediction metrics and defects to assess the feasibility of the tool, i.e., if the accuracy of the defect prediction tool is higher than of a random predictor. However, computing prediction metrics is time consuming and error prone. Thus, the feasibility analysis has a cost which needs some initial investment by the potential clients. This initial investment acts as a barrier for convincing potential clients of the benefits of institutionalizing a software prediction model. To reduce this barrier, in this paper we present the Pilot Defects Prediction Dataset Maker (PDPDM), a desktop application for measuring metrics to use for defect prediction. PDPDM receives as input the repository’s information of a software project, and it provides as output, in an easy and replicable way, a dataset containing a set of 17 well-defined product and process metrics, that have been shown to be useful for defect prediction, such as size and smells. PDPDM avoids the use of outdated datasets and it allows researchers and practitioners to create defect datasets without the need to write any lines of code.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
pages = {15–18},
numpages = {4},
keywords = {Defects prediction},
location = {Lake Buena Vista, FL, USA},
series = {SWAN 2018}
}

@article{10.1007/s10515-020-00270-x,
author = {Richter, Cedric and H\"{u}llermeier, Eyke and Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Algorithm selection for software validation based on graph kernels},
year = {2020},
issue_date = {Jun 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1–2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-020-00270-x},
doi = {10.1007/s10515-020-00270-x},
abstract = {Algorithm selection is the task of choosing an algorithm from a given set of candidate algorithms when faced with a particular problem instance. Algorithm selection via machine learning (ML) has recently been successfully applied for various problem classes, including computationally hard problems such as SAT. In this paper, we study algorithm selection for software validation, i.e., the task of choosing a software validation tool for a given validation instance. A validation instance consists of a program plus properties to be checked on it. The application of machine learning techniques to this task first of all requires an appropriate representation of software. To this end,
we propose a dedicated kernel function, which compares two programs in terms of their similarity, thus making the algorithm selection task amenable to kernel-based machine learning methods. Our kernel operates on a graph representation of source code mixing elements of control-flow and program-dependence graphs with abstract syntax trees.
Thus, given two such representations as input, the kernel function yields a real-valued score that can be interpreted as a degree of similarity. We experimentally evaluate our kernel in two learning scenarios, namely a classification and a ranking problem: (1) selecting between a verification and a testing tool for bug finding (i.e., property violation), and (2) ranking several verification tools,
from presumably best to worst, for property proving. The evaluation, which is based on data sets from the annual software verification competition SV-COMP, demonstrates our kernel to generalize well and to achieve rather high prediction accuracy, both for the classification and the ranking task.},
journal = {Automated Software Engg.},
month = jun,
pages = {153–186},
numpages = {34},
keywords = {Algorithm selection, Software validation, Machine learning, Graph kernels, Verification, Testing}
}

@inproceedings{10.5555/3433701.3433786,
author = {Yazdi, Amirhessam and Lin, Xing and Yang, Lei and Yan, Feng},
title = {SEFEE: lightweight storage error forecasting in large-scale enterprise storage systems},
year = {2020},
isbn = {9781728199986},
publisher = {IEEE Press},
abstract = {With the rapid growth in scale and complexity, today's enterprise storage systems need to deal with significant amounts of errors. Existing proactive methods mainly focus on machine learning techniques trained using SMART measurements. However, such methods are usually expensive to use in practice and can only be applied to a limited types of errors with a limited scale. We collected more than 23-million storage events from 87 deployed NetApp-ONTAP systems managing 14,371 disks for two years and propose a lightweight training-free storage error forecasting method SEFEE. SEFEE employs Tensor Decomposition to directly analyze storage error-event logs and perform online error prediction for all error types in all storage nodes. SEFEE explores hidden spatio-temporal information that is deeply embedded in the global scale of storage systems to achieve record breaking error forecasting accuracy with minimal prediction overhead.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {64},
numpages = {14},
keywords = {error prediction, lightweight forecasting, storage failures, tensor decomposition, training-free prediction},
location = {Atlanta, Georgia},
series = {SC '20}
}

@article{10.1007/s10664-019-09778-7,
author = {Titcheu Chekam, Thierry and Papadakis, Mike and Bissyand\'{e}, Tegawend\'{e} F. and Le Traon, Yves and Sen, Koushik},
title = {Selecting fault revealing mutants},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09778-7},
doi = {10.1007/s10664-019-09778-7},
abstract = {Mutant selection refers to the problem of choosing, among a large number of mutants, the (few) ones that should be used by the testers. In view of this, we investigate the problem of selecting the fault revealing mutants, i.e., the mutants that are killable and lead to test cases that uncover unknown program faults. We formulate two variants of this problem: the fault revealing mutant selection and the fault revealing mutant prioritization. We argue and show that these problems can be tackled through a set of ‘static’ program features and propose a machine learning approach, named FaRM, that learns to select and rank killable and fault revealing mutants. Experimental results involving 1,692 real faults show the practical benefits of our approach in both examined problems. Our results show that FaRM achieves a good trade-off between application cost and effectiveness (measured in terms of faults revealed). We also show that FaRM outperforms all the existing mutant selection methods, i.e., the random mutant sampling, the selective mutation and defect prediction (mutating the code areas pointed by defect prediction). In particular, our results show that with respect to mutant selection, our approach reveals 23% to 34% more faults than any of the baseline methods, while, with respect to mutant prioritization, it achieves higher average percentage of revealed faults with a median difference between 4% and 9% (from the random mutant orderings).},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {434–487},
numpages = {54},
keywords = {Mutation testing, Machine learning, Mutant selection, Mutant prioritization}
}

@inproceedings{10.1109/ICSE43902.2021.00027,
author = {Tufano, Rosalia and Pascarella, Luca and Tufano, Michele and Poshyvanyk, Denys and Bavota, Gabriele},
title = {Towards Automating Code Review Activities},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00027},
doi = {10.1109/ICSE43902.2021.00027},
abstract = {Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code.Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the contributor and the reviewer sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language.The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31% of cases. While these results are encouraging, more research is needed to make these models usable by developers.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {163–174},
numpages = {12},
keywords = {Code Review, Deep Learning, Empirical Software Engineering},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3106237.3117766,
author = {Volf, Zahy and Shmueli, Edi},
title = {Screening heuristics for project gating systems},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117766},
doi = {10.1145/3106237.3117766},
abstract = {Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {872–877},
numpages = {6},
keywords = {Continuous Integration, Machine Learning, Project Gating},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@article{10.1155/2018/6791683,
author = {Ji, Haijin and Huang, Song and Gutierrez, Pedro Antonio},
title = {Kernel Entropy Component Analysis with Nongreedy L1-Norm Maximization},
year = {2018},
issue_date = {2018},
publisher = {Hindawi Limited},
address = {London, GBR},
volume = {2018},
issn = {1687-5265},
url = {https://doi.org/10.1155/2018/6791683},
doi = {10.1155/2018/6791683},
abstract = {Kernel entropy component analysis (KECA) is a newly proposed dimensionality reduction (DR) method, which has showed superiority in many pattern analysis issues previously solved by principal component analysis (PCA). The optimized KECA (OKECA) is a state-of-the-art variant of KECA and can return projections retaining more expressive power than KECA. However, OKECA is sensitive to outliers and accused of its high computational complexities due to its inherent properties of L2-norm. To handle these two problems, we develop a new extension to KECA, namely, KECA-L1, for DR or feature extraction. KECA-L1 aims to find a more robust kernel decomposition matrix such that the extracted features retain information potential as much as possible, which is measured by L1-norm. Accordingly, we design a nongreedy iterative algorithm which has much faster convergence than OKECA’s. Moreover, a general semisupervised classifier is developed for KECA-based methods and employed into the data classification. Extensive experiments on data classification and software defect prediction demonstrate that our new method is superior to most existing KECA- and PCA-based approaches. Code has been also made publicly available.},
journal = {Intell. Neuroscience},
month = jan,
numpages = {9}
}

@inproceedings{10.1109/ICPC.2019.00050,
author = {Fucci, Davide and Girardi, Daniela and Novielli, Nicole and Quaranta, Luigi and Lanubile, Filippo},
title = {A replication study on code comprehension and expertise using lightweight biometric sensors},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00050},
doi = {10.1109/ICPC.2019.00050},
abstract = {Code comprehension has been recently investigated from physiological and cognitive perspectives using medical imaging devices. Floyd et al. (i.e., the original study) used fMRI to classify the type of comprehension tasks performed by developers and relate their results to their expertise. We replicate the original study using lightweight biometrics sensors. Our study participants---28 undergrads in computer science---performed comprehension tasks on source code and natural language prose. We developed machine learning models to automatically identify what kind of tasks developers are working on leveraging their brain-, heart-, and skin-related signals. The best improvement over the original study performance is achieved using solely the heart signal obtained through a single device (BAC 87% vs. 79.1%). Differently from the original study, we did not observe a correlation between the participants' expertise and the classifier performance (τ = 0.16, p = 0.31). Our findings show that lightweight biometric sensors can be used to accurately recognize comprehension tasks opening interesting scenarios for research and practice.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {311–322},
numpages = {12},
keywords = {biometric sensors, machine learning, software development tasks},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/2961111.2962601,
author = {Soltanifar, Behjat and Erdem, Atakan and Bener, Ayse},
title = {Predicting Defectiveness of Software Patches},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962601},
doi = {10.1145/2961111.2962601},
abstract = {Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted.Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission.Method: We built three models using three different machine learning algorithms: Logistic Regression, Na\~{A}undefinedve Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P).Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy.Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {22},
numpages = {10},
keywords = {Code Review Quality, Code review, Defect Prediction, Software Patch Defectiveness},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {Software product line testing, dimensionality reduction, many-objective problems},
location = {Natal, Brazil},
series = {SAST '20}
}

@article{10.1007/s11219-019-09481-2,
author = {Wu, Xiaoxue and Zheng, Wei and Pu, Minchao and Chen, Jie and Mu, Dejun},
title = {Invalid bug reports complicate the software aging situation},
year = {2020},
issue_date = {Mar 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-019-09481-2},
doi = {10.1007/s11219-019-09481-2},
abstract = {Symptoms of software aging include performance degradation and failure occurrence increasing when software systems run for a period of time. Therefore, software aging is closely related to system performance. Understanding and analyzing performance issues in the software system is critical to mastering software aging information. Instead of focusing on normal valid bug reports (VBRs), this paper advocates the usage of invalid bug reports (IBRs) to capture software aging signals. We use performance bugs that are highly related to software aging as an example to construct a binary classification model for bug report classification. We conduct a rigorous evaluation of the constructed models via different performance measures (i.e., recall, precision, F1-score, AUC). Then, the model is used to predict the performance bug reports (PBRs) in IBRs, and a manual analysis of the prediction results is conducted to identify aging-related bug reports (ABRs). The final results show that the ratio of PBRs in IBRs ranges from 4.9 to 42.18% for the two real open-source projects HDFS and HBase when considering five different classifiers. Among these five classifiers, Support Vector Machine (SVM) classifier can achieve the best performance. The ratios of PBRs in IBRs by using this classifier are 11.1% and 15.35% for these two datasets and the performances in terms of F1-score are 85% and 74%. Further analysis of the predicted PBRs of IBRs in the project HDFS is conducted through a manual user case study; some surprising findings revealing the relationship between IBRs, PBRs, and ABRs are presented: (1) Around 50% of the PBRs in IBRs are related to software aging; (2) components that undertake major tasks are more prone to aging problems; (3) more than 50% ARBs lead to timeout, 33% ARBs are caused by improper control of memory or threats, and 29% ARBs are caused by inappropriate management of file operation or disk usage; (4) hard to reproduce is the major reason that ARBs are usually closed as invalid because many aging-related bugs would temporarily disappear by restarting the system.},
journal = {Software Quality Journal},
month = mar,
pages = {195–220},
numpages = {26},
keywords = {Software aging, Invalid bug report, Performance-related bug report, Aging related bug report, Text mining}
}

@article{10.5555/3455716.3455901,
author = {Bonald, Thomas and De Lara, Nathan and Lutz, Quentin and Charpentier, Bertrand},
title = {Scikit-network: graph analysis in Python},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Scikit-network is a Python package inspired by scikit-learn for the analysis of large graphs. Graphs are represented by their adjacency matrix in the sparse CSR format of SciPy. The package provides state-of-the-art algorithms for ranking, clustering, classifying, embedding and visualizing the nodes of a graph. High performance is achieved through a mix of fast matrix-vector products (using SciPy), compiled code (using Cython) and parallel processing. The package is distributed under the BSD license, with dependencies limited to NumPy and SciPy. It is compatible with Python 3.6 and newer. Source code, documentation and installation instructions are available online.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {185},
numpages = {6},
keywords = {graph analysis, sparse matrices, python, cython, scipy}
}

@inproceedings{10.1145/3377811.3380369,
author = {Bertolino, Antonia and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
title = {Learning-to-rank vs ranking-to-learn: strategies for regression testing in continuous integration},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380369},
doi = {10.1145/3377811.3380369},
abstract = {In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1–12},
numpages = {12},
keywords = {continuous integration, machine learning, regression testing, test prioritization, test selection},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2857705.2857750,
author = {Younis, Awad and Malaiya, Yashwant and Anderson, Charles and Ray, Indrajit},
title = {To Fear or Not to Fear That is the Question: Code Characteristics of a Vulnerable Functionwith an Existing Exploit},
year = {2016},
isbn = {9781450339353},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2857705.2857750},
doi = {10.1145/2857705.2857750},
abstract = {Not all vulnerabilities are equal. Some recent studies have shown that only a small fraction of vulnerabilities that have been reported has actually been exploited. Since finding and addressing potential vulnerabilities in a program can take considerable time and effort, recently effort has been made to identify code that is more likely to be vulnerable. This paper tries to identify the attributes of the code containing a vulnerability that makes the code more likely to be exploited. We examine 183 vulnerabilities from the National Vulnerability Database for Linux Kernel and Apache HTTP server. These include eighty-two vulnerabilities that have been found to have an exploit according to the Exploit Database. We characterize the vulnerable functions that have no exploit and the ones that have an exploit using eight metrics. The results show that the difference between a vulnerability that has no exploit and the one that has an exploit can potentially be characterized using the chosen software metrics. However, predicting exploitation of vulnerabilities is more complex than predicting just the presence of vulnerabilities and further research is needed using metrics that consider security domain knowledge for enhancing the predictability of vulnerability exploits.},
booktitle = {Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy},
pages = {97–104},
numpages = {8},
keywords = {data mining and machine learning, exploitability, exploits, feature selection, prediction, software metrics., software security, vulnerabilities severity},
location = {New Orleans, Louisiana, USA},
series = {CODASPY '16}
}

@article{10.1016/j.infsof.2019.106204,
author = {Theisen, Christopher and Williams, Laurie},
title = {Better together: Comparing vulnerability prediction models},
year = {2020},
issue_date = {Mar 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {119},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.106204},
doi = {10.1016/j.infsof.2019.106204},
journal = {Inf. Softw. Technol.},
month = mar,
numpages = {12},
keywords = {Security, Vulnerabilities, Prediction model, Software engineering}
}

@article{10.1504/ijiei.2021.118275,
author = {Mittal, Shruti and Nagpal, Chander Kumar},
title = {Reinforcement learning based predictive analytics framework for survival in stock market},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {3},
issn = {1758-8715},
url = {https://doi.org/10.1504/ijiei.2021.118275},
doi = {10.1504/ijiei.2021.118275},
abstract = {Contemporary research in stock market domain is limited to forecasting of the stock price from one day to one week. Such small period predictions cannot be of much help for continuous gainful survival in the stock market. In fact, there has to be predictive analytics framework which analyses the current situation in the holistic manner and provides the appropriate advice for selling/buying/no action along with the quantity resulting in significant gain for the user/investor. The proposed framework generates various reinforcement signals by applying statistical and machine learning techniques on historical data and studies their impact on the stock prices by analysing future data. The outcome of the process has been used to generate rewards, through the use of fuzzy logic, for various actions in a given state of the environment. Fully automated implementation of the proposed framework can help both institutional and common investor in taking the rational decision.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {294–327},
numpages = {33},
keywords = {predictive analytics, statistical learning, machine learning, stock market predictions, reinforcement learning, fuzzy sets and logic, finite state machine, fuzzy rule base, stock fundamental, stock technical analysis, single value decomposition}
}

@article{10.1007/s11219-020-09540-z,
author = {Harrison, Rachel},
title = {In this issue},
year = {2020},
issue_date = {Dec 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {28},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-020-09540-z},
doi = {10.1007/s11219-020-09540-z},
journal = {Software Quality Journal},
month = dec,
pages = {1411–1412},
numpages = {2}
}

@inproceedings{10.5555/3306127.3331796,
author = {Cuccu, Giuseppe and Togelius, Julian and Cudr\'{e}-Mauroux, Philippe},
title = {Playing Atari with Six Neurons},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable---and occasionally superior---to state-of-the-art techniques which use two orders of magnitude more neurons.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {998–1006},
numpages = {9},
keywords = {evolutionary algorithms, game playing, learning agent capabilities, neuroevolution},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1016/j.procs.2018.05.194,
author = {Kukkar, Ashima and Mohana, Rajni},
title = {A Supervised Bug Report Classification with Incorporate and Textual field Knowledge},
year = {2018},
issue_date = {2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {132},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2018.05.194},
doi = {10.1016/j.procs.2018.05.194},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {352–361},
numpages = {10},
keywords = {Natural Language Processing, K-nearest neighbor, Incorporate, textual fields based prediction, Software maintenance, development, Bug Triaging System}
}

@article{10.1007/s10664-017-9522-4,
author = {Huang, Qiao and Shihab, Emad and Xia, Xin and Lo, David and Li, Shanping},
title = {Identifying self-admitted technical debt in open source projects using text mining},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9522-4},
doi = {10.1007/s10664-017-9522-4},
abstract = {Technical debt is a metaphor to describe the situation in which long-term code quality is traded for short-term goals in software projects. Recently, the concept of self-admitted technical debt (SATD) was proposed, which considers debt that is intentionally introduced, e.g., in the form of quick or temporary fixes. Prior work on SATD has shown that source code comments can be used to successfully detect SATD, however, most current state-of-the-art classification approaches of SATD rely on manual inspection of the source code comments. In this paper, we proposed an automated approach to detect SATD in source code comments using text mining. In our approach, we utilize feature selection to select useful features for classifier training, and we combine multiple classifiers from different source projects to build a composite classifier that identifies SATD comments in a target project. We investigate the performance of our approach on 8 open source projects that contain 212,413 comments. Our experimental results show that, on every target project, our approach outperforms the state-of-the-art and the baselines approaches in terms of F1-score. The F1-score achieved by our approach ranges between 0.518 - 0.841, with an average of 0.737, which improves over the state-of-the-art approach proposed by Potdar and Shihab by 499.19%. When compared with the text mining-based baseline approaches, our approach significantly improves the average F1-score by at least 58.49%. When compared with a natural language processing-based baseline, our approach also significantly improves its F1-score by 27.95%. Our proposed approach can be used by project personnel to effectively identify SATD with minimal manual effort.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {418–451},
numpages = {34},
keywords = {Source code comments, Technical debt, Text mining}
}

@article{10.1016/j.jss.2021.111044,
author = {Pereira, Juliana Alves and Acher, Mathieu and Martin, Hugo and J\'{e}z\'{e}quel, Jean-Marc and Botterweck, Goetz and Ventresque, Anthony},
title = {Learning software configuration spaces: A systematic literature review},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111044},
doi = {10.1016/j.jss.2021.111044},
journal = {J. Syst. Softw.},
month = dec,
numpages = {29},
keywords = {Systematic literature review, Software product lines, Machine learning, Configurable systems}
}

@inproceedings{10.1109/ASE.2019.00099,
author = {Nafi, Kawser Wazed and Kar, Tonny Shekha and Roy, Banani and Roy, Chanchal K. and Schneider, Kevin A.},
title = {CLCDSA: cross language code clone detection using syntactical features and API documentation},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00099},
doi = {10.1109/ASE.2019.00099},
abstract = {Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is twofold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1026–1037},
numpages = {12},
keywords = {API documentation, Word2Vector, code clone, source code syntax},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3340422.3343639,
author = {Kumar, Lov and Hota, Chinmay and Mahindru, Arvind and Neti, Lalita Bhanu Murthy},
title = {Android Malware Prediction Using Extreme Learning Machine with Different Kernel Functions},
year = {2019},
isbn = {9781450368490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340422.3343639},
doi = {10.1145/3340422.3343639},
abstract = {Android is currently the most popular smartphone platform which occupied 88% of global sale by the end of 2nd quarter 2018. With the popularity of these applications, it is also inviting cybercriminals to develop malware application for accessing important information from smartphones. The major objective of cybercriminals to develop Malware apps or Malicious apps to threaten the organization privacy data, user privacy data, and device integrity. Early identification of such malware apps can help the android user to save private data and device integrity. In this study, features extracted from intermediate code representations obtained using decompilation of APK file are used for providing requisite input data to develop the models for predicting android malware applications. These models are trained using extreme learning with multiple kernel functions ans also compared with the model trained using most frequently used classifiers like linear regression, decision tree, polynomial regression, and logistic regression. This paper also focuses on the effectiveness of data sampling techniques for balancing data and feature selection methods for selecting right sets of significant uncorrelated metrics. The high-value of accuracy and AUC confirm the predicting capability of data sampling, sets of metrics, and training algorithms to malware and normal applications.},
booktitle = {Proceedings of the 15th Asian Internet Engineering Conference},
pages = {33–40},
numpages = {8},
keywords = {Artificial neural network, Genetics algorithm, Maintainability, Object-Oriented Metrics, Parallel Computing},
location = {Phuket, Thailand},
series = {AINTEC '19}
}

@article{10.1016/j.ins.2019.01.047,
author = {Azmi, Mohamed and Runger, George C. and Berrado, Abdelaziz},
title = {Interpretable regularized class association rules algorithm for classification in a categorical data space},
year = {2019},
issue_date = {May 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {483},
number = {C},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2019.01.047},
doi = {10.1016/j.ins.2019.01.047},
journal = {Inf. Sci.},
month = may,
pages = {313–331},
numpages = {19},
keywords = {Classification, Ensemble learning, Association rules, Pruning, Regularization, Class association rules}
}

@inproceedings{10.1007/978-3-030-86130-8_20,
author = {Xiao, Yunhao and Xiao, Xi and Tian, Fang and Hu, Guangwu},
title = {A LambdaMart-Based High-Accuracy Approach for Software Automatic Fault Localization},
year = {2021},
isbn = {978-3-030-86129-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-86130-8_20},
doi = {10.1007/978-3-030-86130-8_20},
abstract = {Software debugging or fault localization is a very significant task in software development and maintenance, which directly determines the quality of software. Traditional methods of fault localization rely on manual investigation, which takes too much time in large-scale software development. To mitigate this problem, many automatic fault localization techniques have been proposed which can effectively lighten the burden of programmers. However, the quality of these techniques is not enough to meet the practical requirements. In order to improve the accuracy of fault localization, we propose LBFL, a LambdaMart-based high-accuracy approach for software automatic fault localization, which can integrate software’s diversified features and achieve very high accuracy. To realize that, LBFL first extracts the static and dynamic features and normalizes them. Then these features are gathered on LambdaMart algorithm for training. Finally, LBFL sorts the code statements according to the model and generates a list which can help developers to locate faults. Exhaustive experiments indicate that LBFL can locate 76 faults in Top-1, which has at least 217% improvements over nine single techniques and has 55% improvements over ABFL approach on the Defects4J dataset.},
booktitle = {Wireless Algorithms, Systems, and Applications: 16th International Conference, WASA 2021, Nanjing, China, June 25–27, 2021, Proceedings, Part II},
pages = {249–261},
numpages = {13},
keywords = {Fault localization, Software engineering, Learning to rank, LambdaMart},
location = {Nanjing, China}
}

@inproceedings{10.1007/978-3-030-87007-2_16,
author = {Tummalapalli, Sahithi and Kumar, Lov and Murthy Neti, Lalitha Bhanu and Kocher, Vipul and Padmanabhuni, Srinivas},
title = {A Novel Approach for the Detection of Web Service Anti-Patterns Using Word Embedding Techniques},
year = {2021},
isbn = {978-3-030-87006-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-87007-2_16},
doi = {10.1007/978-3-030-87007-2_16},
abstract = {An anti-pattern is defined as a standard but ineffective solution to solve a problem. Anti-patterns in software design make it hard for software maintenance and development by making source code very complicated for understanding. Various studies revealed that the presence of anti-patterns in web services leads to maintenance and evolution-related problems. Identification of anti-patterns at the design level helps in reducing efforts, resources, and costs. This makes the identification of anti-patterns an exciting issue for researchers. This work introduces a novel approach for detecting anti-patterns using text metrics extracted from the Web Service Description Language (WSDL) file. The framework used in this paper builds on the presumption that text metrics extracted at the web service level have been considered as a predictor for anti-patterns. This paper empirically investigates the effectiveness of three feature selection techniques and the original features, three data sampling techniques, the original data, four word embedding techniques, and nine classifier techniques in detecting web service anti-patterns. Data Sampling techniques are employed to counter the class imbalance problem suffered by the data set. The results confirm the predictive ability of text metrics obtained by different word embedding techniques in predicting anti-patterns.},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part VII},
pages = {217–230},
numpages = {14},
keywords = {Web service, Word embedding techniques, Machine learning, Classifier techniques, Class imbalance, Anti-pattern, Text metrics},
location = {Cagliari, Italy}
}

@inproceedings{10.1007/978-3-030-73128-1_1,
author = {Abbas, Muhammad and Ferrari, Alessio and Shatnawi, Anas and Enoiu, Eduard Paul and Saadatmand, Mehrdad},
title = {Is Requirements Similarity a Good Proxy for Software Similarity? An Empirical Investigation in Industry},
year = {2021},
isbn = {978-3-030-73127-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-73128-1_1},
doi = {10.1007/978-3-030-73128-1_1},
abstract = {[Context and Motivation] Content-based recommender systems for requirements are typically built on the assumption that similar requirements can be used as proxies to retrieve similar software. When a new requirement is proposed by a stakeholder, natural language processing (NLP)-based similarity metrics can be exploited to retrieve existing requirements, and in turn identify previously developed code. [Question/problem] Several NLP approaches for similarity computation are available, and there is little empirical evidence on the adoption of an effective technique in recommender systems specifically oriented to requirements-based code reuse. [Principal ideas/results] This study compares different state-of-the-art NLP approaches and correlates the similarity among requirements with the similarity of their source code. The evaluation is conducted on real-world requirements from two industrial projects in the railway domain. Results show that requirements similarity computed with the traditional tf-idf approach has the highest correlation with the actual software similarity in the considered context. Furthermore, results indicate a moderate positive correlation with Spearman’s rank correlation coefficient of more than 0.5. [Contribution] Our work is among the first ones to explore the relationship between requirements similarity and software similarity. In addition, we also identify a suitable approach for computing requirements similarity that reflects software similarity well in an industrial context. This can be useful not only in recommender systems but also in other requirements engineering tasks in which similarity computation is relevant, such as tracing and categorization.},
booktitle = {Requirements Engineering:  Foundation  for Software Quality: 27th International Working Conference, REFSQ 2021, Essen, Germany, April 12–15, 2021, Proceedings},
pages = {3–18},
numpages = {16},
keywords = {Requirements similarity, Software similarity, Correlation}
}

@article{10.1016/j.jss.2018.05.065,
author = {J\'{u}nior, Manoel Limeira de Lima and Soares, Daric\'{e}lio Moreira and Plastino, Alexandre and Murta, Leonardo},
title = {Automatic assignment of integrators to pull requests: The importance of selecting appropriate attributes},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.05.065},
doi = {10.1016/j.jss.2018.05.065},
journal = {J. Syst. Softw.},
month = oct,
pages = {181–196},
numpages = {16},
keywords = {Integrator assignment, Pull-based software development, Distributed software development}
}

@article{10.1007/s11219-017-9376-4,
author = {Melo, Silvana Morita and Souza, Simone Do and Sarmanho, Felipe Santos and Souza, Paulo Sergio},
title = {Contributions for the structural testing of multithreaded programs: coverage criteria, testing tool, and experimental evaluation},
year = {2018},
issue_date = {September 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9376-4},
doi = {10.1007/s11219-017-9376-4},
abstract = {Concurrent software testing is a challenging activity due to factors that are not present in sequential programs, such as communication, synchronization, and non-determinism, and that directly affect the testing process. When we consider multithreaded programs, new challenges for the testing activity are imposed. In the context of structural testing, an important problem raised is how to deal with the coverage of shared variables in order to establish the association between def-use of shared variables. This paper presents results related to the structural testing of multithreaded programs, including testing criteria for coverage testing, a supporting tool, called ValiPthread testing tool and results of an experimental study. This study was conducted to evaluate the cost, effectiveness, and strength of the testing criteria. Also, the study evaluates the contribution of these testing criteria to test specific aspects of multithreaded programs. The experimental results show evidence that the testing criteria present lower cost and higher effectiveness when revealing some kinds of defects, such as deadlock and critical region block. Also, compared to sequential testing criteria, the proposed criteria show that it is important to establish specific coverage testing for multithreaded programs.},
journal = {Software Quality Journal},
month = sep,
pages = {921–959},
numpages = {39},
keywords = {Coverage criteria, Experimental evaluation, Multithreaded programs, PThreads, Shared memory, Structural testing}
}

@article{10.5555/1839514.1839518,
author = {Twala, Bhekisipho and Cartwright, Michelle},
title = {Ensemble missing data techniques for software effort prediction},
year = {2010},
issue_date = {August 2010},
publisher = {IOS Press},
address = {NLD},
volume = {14},
number = {3},
issn = {1088-467X},
abstract = {Constructing an accurate effort prediction model is a challenge in software engineering. The development and validation of models that are used for prediction tasks require good quality data. Unfortunately, software engineering datasets tend to suffer from the incompleteness which could result to inaccurate decision making and project management and implementation. Recently, the use of machine learning algorithms has proven to be of great practical value in solving a variety of software engineering problems including software prediction, including the use of ensemble (combining) classifiers. Research indicates that ensemble individual classifiers lead to a significant improvement in classification performance by having them vote for the most popular class. This paper proposes a method for improving software effort prediction accuracy produced by a decision tree learning algorithm and by generating the ensemble using two imputation methods as elements. Benchmarking results on ten industrial datasets show that the proposed ensemble strategy has the potential to improve prediction accuracy compared to an individual imputation method, especially if multiple imputation is a component of the ensemble.},
journal = {Intell. Data Anal.},
month = aug,
pages = {299–331},
numpages = {33},
keywords = {Machine learning, decision tree, ensemble, imputation, incomplete data, missing data techniques, software prediction, supervised learning}
}

@article{10.1016/j.jksuci.2018.05.008,
author = {Bhatt, Arpita Jadhav and Gupta, Chetna and Mittal, Sangeeta},
title = {iABC-AL: Active learning-based privacy leaks threat detection for iOS applications},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {33},
number = {7},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2018.05.008},
doi = {10.1016/j.jksuci.2018.05.008},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = sep,
pages = {769–786},
numpages = {18},
keywords = {iOS applications, Information security, Static analysis, Permission extraction, Active learning}
}

@article{10.1145/3483424,
author = {Notaro, Paolo and Cardoso, Jorge and Gerndt, Michael},
title = {A Survey of AIOps Methods for Failure Management},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {6},
issn = {2157-6904},
url = {https://doi.org/10.1145/3483424},
doi = {10.1145/3483424},
abstract = {Modern society is increasingly moving toward complex and distributed computing systems. The increase in scale and complexity of these systems challenges O&amp;M teams that perform daily monitoring and repair operations, in contrast with the increasing demand for reliability and scalability of modern applications. For this reason, the study of automated and intelligent monitoring systems has recently sparked much interest across applied IT industry and academia. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to Machine Learning, AI, and Big Data. However, AIOps as a research topic is still largely unstructured and unexplored, due to missing conventions in categorizing contributions for their data requirements, target goals, and components. In this work, we focus on AIOps for Failure Management (FM), characterizing and describing 5 different categories and 14 subcategories of contributions, based on their time intervention window and the target problem being solved. We review 100 FM solutions, focusing on applicability requirements and the quantitative results achieved, to facilitate an effective application of AIOps solutions. Finally, we discuss current development problems in the areas covered by AIOps and delineate possible future trends for AI-based failure management.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = nov,
articleno = {81},
numpages = {45},
keywords = {AIOps, IT operations and maintenance, failure management, artificial intelligence}
}

@inproceedings{10.1145/2915970.2916004,
author = {Pfahl, Dietmar and Karus, Siim and Stavnycha, Myroslava},
title = {Improving expert prediction of issue resolution time},
year = {2016},
isbn = {9781450336918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2915970.2916004},
doi = {10.1145/2915970.2916004},
abstract = {Predicting the resolution times of issue reports in software development is important, because it helps allocate resources adequately. However, issue resolution time (IRT) prediction is difficult and prediction quality is limited. A common approach in industry is to base predictions on expert knowledge. While this manual approach requires the availability and effort of experts, automated approaches using data mining and machine learning techniques require a small upfront investment for setting up the data collection and analysis infrastructure as well as the availability of sufficient past data for model building. Several approaches for automated IRT prediction have been proposed and evaluated. The aim of our study was (1) to compare the prediction quality of expert-based IRT prediction in a software company located in Estonia with that of various fully automated IRT prediction approaches proposed and used by other researchers, including k-means clustering, k-nearest neighbor classification, Na\"{\i}ve Bayes classification, decision trees, random forest (RF) and ordered logistic regression (OLR), and (2) to improve the current IRT prediction quality in the company at hand. For our study, we analyzed issue reports collected by the company in the period from April 2011 to January 2015. Regarding our first goal, we found that experts in the case company were able to predict IRTs approximately 50% of the time within the range of ±10% of the actual IRTs. In addition, 67% of the experts' predictions have an absolute error that is less or equal 0.5 hours. When applying the automated approaches used by other researchers to the company's data, we observed lower predictive quality as compared to IRT predictions made by the company's experts, even for the best-performing approaches RF and OLR. Regarding our second goal, after unsuccessfully experimenting with improvements to the RF and OLR based approaches, we managed to develop models based on text analysis that achieved a prediction quality at par or better than that achieved by company experts.},
booktitle = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {42},
numpages = {6},
keywords = {expert prediction, issue report, k-means, k-nearest neighbors, latent semantic analysis, machine learning, na\"{\i}ve bayes classifier, ordered logistic regression, random forest, resolution time},
location = {Limerick, Ireland},
series = {EASE '16}
}

@inproceedings{10.5555/2664446.2664480,
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
title = {Can we predict types of code changes? an empirical analysis},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {There exist many approaches that help in pointing developers to the change-prone parts of a software system. Although beneficial, they mostly fall short in providing details of these changes. Fine-grained source code changes (SCC) capture such detailed code changes and their semantics on the statement level. These SCC can be condition changes, interface modifications, inserts or deletions of methods and attributes, or other kinds of statement changes. In this paper, we explore prediction models for whether a source file will be affected by a certain type of SCC. These predictions are computed on the static source code dependency graph and use social network centrality measures and object-oriented metrics. For that, we use change data of the Eclipse platform and the Azureus 3 project. The results show that Neural Network models can predict categories of SCC types. Furthermore, our models can output a list of the potentially change-prone files ranked according to their change-proneness, overall and per change type category.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {217–226},
numpages = {10},
keywords = {machine learning, software maintenance, software quality},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@inproceedings{10.1145/2970276.2970353,
author = {Yang, Yibiao and Harman, Mark and Krinke, Jens and Islam, Syed and Binkley, David and Zhou, Yuming and Xu, Baowen},
title = {An empirical study on dependence clusters for effort-aware fault-proneness prediction},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970353},
doi = {10.1145/2970276.2970353},
abstract = {A dependence cluster is a set of mutually inter-dependent program elements. Prior studies have found that large dependence clusters are prevalent in software systems. It has been suggested that dependence clusters have potentially harmful effects on software quality. However, little empirical evidence has been provided to support this claim. The study presented in this paper investigates the relationship between dependence clusters and software quality at the function-level with a focus on effort-aware fault-proneness prediction. The investigation first analyzes whether or not larger dependence clusters tend to be more fault-prone. Second, it investigates whether the proportion of faulty functions inside dependence clusters is significantly different from the proportion of faulty functions outside dependence clusters. Third, it examines whether or not functions inside dependence clusters playing a more important role than others are more fault-prone. Finally, based on two groups of functions (i.e., functions inside and outside dependence clusters), the investigation considers a segmented fault-proneness prediction model. Our experimental results, based on five well-known open-source systems, show that (1) larger dependence clusters tend to be more fault-prone; (2) the proportion of faulty functions inside dependence clusters is significantly larger than the proportion of faulty functions outside dependence clusters; (3) functions inside dependence clusters that play more important roles are more fault-prone; (4) our segmented prediction model can significantly improve the effectiveness of effort-aware fault-proneness prediction in both ranking and classification scenarios. These findings help us better understand how dependence clusters influence software quality.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {296–307},
numpages = {12},
keywords = {Dependence clusters, fault prediction, fault-proneness, network analysis},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1007/s11219-017-9359-5,
author = {Ufuktepe, Ekincan and Tuglular, Tugkan},
title = {Estimating software robustness in relation to input validation vulnerabilities using Bayesian networks},
year = {2018},
issue_date = {June      2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-017-9359-5},
doi = {10.1007/s11219-017-9359-5},
abstract = {Estimating the robustness of software in the presence of invalid inputs has long been a challenging task owing to the fact that developers usually fail to take the necessary action to validate inputs during the design and implementation of software. We propose a method for estimating the robustness of software in relation to input validation vulnerabilities using Bayesian networks. The proposed method runs on all program functions and/or methods. It calculates a robustness value using information on the existence of input validation code in the functions and utilizing common weakness scores of known input validation vulnerabilities. In the case study, ten well-known software libraries implemented in the JavaScript language, which are chosen because of their increasing popularity among software developers, are evaluated. Using our method, software development teams can track changes made to software to deal with invalid inputs.},
journal = {Software Quality Journal},
month = jun,
pages = {455–489},
numpages = {35},
keywords = {Bayesian networks, Input validation vulnerabilities, Robustness}
}

@inproceedings{10.1145/3460319.3464834,
author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
title = {Empirically evaluating readily available information for regression test optimization in continuous integration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464834},
doi = {10.1145/3460319.3464834},
abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–504},
numpages = {14},
keywords = {machine learning, regression test optimization, software testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3475716.3475789,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Wang, Dandan and Wang, Qing},
title = {Characterizing and Predicting Good First Issues},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475789},
doi = {10.1145/3475716.3475789},
abstract = {Background. Where to start contributing to a project is a critical challenge for newcomers of open source projects. To support newcomers, GitHub utilizes the Good First Issue (GFI) label, with which project members can manually tag issues in an open source project that are suitable for the newcomers. However, manually labeling GFIs is time- and effort-consuming given the large number of candidate issues. In addition, project members need to have a close understanding of the project to label GFIs accurately.Aims. This paper aims at providing a thorough understanding of the characteristics of GFIs and an automatic approach in GFIs prediction, to reduce the burden of project members and help newcomers easily onboard.Method. We first define 79 features to characterize the GFIs and further analyze the correlation between each feature and GFIs. We then build machine learning models to predict GFIs with the proposed features.Results. Experiments are conducted with 74,780 issues from 10 open source projects from GitHub. Results show that features related to the semantics, readability, and text richness of issues can be used to effectively characterize GFIs. Our prediction model achieves a median AUC of 0.88. Results from our user study further prove its potential practical value.Conclusions. This paper provides new insights and practical guidelines to facilitate the understanding of GFIs and the automation of GFIs labeling.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {13},
numpages = {12},
keywords = {Issue Report, Machine Learning, Newcomers, Open Source Software},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/1294948.1294954,
author = {Aversano, Lerina and Cerulo, Luigi and Del Grosso, Concettina},
title = {Learning from bug-introducing changes to prevent fault prone code},
year = {2007},
isbn = {9781595937223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294948.1294954},
doi = {10.1145/1294948.1294954},
abstract = {A version control system, such as CVS/SVN, can provide the history of software changes performed during the evolution of a software project. Among all the changes performed there are some which cause the introduction of bugs, often resolved later with other changes.In this paper we use a technique to identify bug-introducing changes to train a model that can be used to predict if a new change may introduces or not a bug. We represent software changes as elements of a n-dimensional vector space of terms coordinates extracted from source code snapshots.The evaluation of various learning algorithms on a set of open source projects looks very promising, in particular for KNN (K-Nearest Neighbor algorithm) where a significant tradeoff between precision and recall has been obtained.},
booktitle = {Ninth International Workshop on Principles of Software Evolution: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {19–26},
numpages = {8},
keywords = {bug prediction, mining software repositories, software evolution},
location = {Dubrovnik, Croatia},
series = {IWPSE '07}
}

@inproceedings{10.1145/3427921.3450243,
author = {Samoaa, Hazem and Leitner, Philipp},
title = {An Exploratory Study of the Impact of Parameterization on JMH Measurement Results in Open-Source Projects},
year = {2021},
isbn = {9781450381949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427921.3450243},
doi = {10.1145/3427921.3450243},
abstract = {The Java Microbenchmarking Harness (JMH) is a widely used tool for testing performance-critical code on a low level. One of the key features of JMH is the support for user-defined parameters, which allows executing the same benchmark with different workloads. However, a benchmark configured with n parameters with m different values each requires JMH to execute the benchmark mn times (once for each combination of configured parameter values). Consequently, even fairly modest parameterization leads to a combinatorial explosion of benchmarks that have to be executed, hence dramatically increasing execution time. However, so far no research has investigated how this type of parameterization is used in practice, and how important different parameters are to benchmarking results. In this paper, we statistically study how strongly different user parameters impact benchmark measurements for 126 JMH benchmarks from five well-known open source projects. We show that 40% of the studied metric parameters have no correlation with the resulting measurement, i.e., testing with different values in these parameters does not lead to any insights. If there is a correlation, it is often strongly predictable following a power law, linear, or step function curve. Our results provide a first understanding of practical usage of user-defined JMH parameters, and how they correlate with the measurements produced by benchmarks. We further show that a machine learning model based on Random Forest ensembles can be used to predict the measured performance of an untested metric parameter value with an accuracy of 93% or higher for all but one benchmark class, demonstrating that given sufficient training data JMH performance test results for different parameterizations are highly predictable.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {213–224},
numpages = {12},
keywords = {benchmark measurements, benchmark parametrization, java microbenchmarking harness (JMH), machine learning},
location = {Virtual Event, France},
series = {ICPE '21}
}

@article{10.1504/ijcse.2021.115645,
author = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
title = {Multi-label software bug categorisation based on fuzzy similarity},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {24},
number = {3},
issn = {1742-7185},
url = {https://doi.org/10.1504/ijcse.2021.115645},
doi = {10.1504/ijcse.2021.115645},
abstract = {The efficiency of the software depends on timely detection of bugs. For better quality and low-cost development bug fixing time should be minimised. Categorisation of software bugs helps to understand the root cause of software bugs and to improve triaging. As the software development approach is modular and multi-skilled, it is possible that one software bug can affect multiple modules, and multiple developers can fix newly reported bugs. Hence, a multi-label categorisation of software bugs is needed. Fuzzy similarity techniques can be helpful in understanding the belongingness of software bugs in multiple categories. In this paper a multi-label fuzzy similarity based categorisation technique is presented for effective categorisation of software bugs. Fuzzy similarity between a pair of bugs is computed and, based on a user defined threshold value, the bugs are categorised. Experiments are performed on software bug data sets, and the performance of the proposed classifier is evaluated.},
journal = {Int. J. Comput. Sci. Eng.},
month = jan,
pages = {244–258},
numpages = {14},
keywords = {software bug mining, software bug classification, fuzzy similarity, multi-label classification, MLC, software bug repository}
}

@article{10.1016/j.procs.2020.03.274,
author = {Taneja, Divya and Singh, Rajvir and Singh, Ajmer and Malik, Himanshu},
title = {A Novel technique for test case minimization in object oriented testing},
year = {2020},
issue_date = {2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {167},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/j.procs.2020.03.274},
doi = {10.1016/j.procs.2020.03.274},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {2221–2228},
numpages = {8},
keywords = {Object oriented metrics, Test Case Minimization, machine learning, object oriented testing}
}

@inproceedings{10.1145/2851613.2851973,
author = {Hussain, Shahid and Khan, Arif Ali and Bennin, Kwabena Ebo},
title = {Empirical investigation of fault predictors in context of class membership probability estimation},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851973},
doi = {10.1145/2851613.2851973},
abstract = {In the domain of software fault prediction, class membership probability of a selected classifier and the factors related to its estimation can be considered as necessary information for tester to take informed decisions about software quality issues. The objective of this study is to empirically investigate the class membership probability estimation capability of 15 classifiers/fault predictors on 12 datasets of open source projects retrieved from PROMISE repository. We empirically validate the effect of dataset characteristics and set of metrics on the performance of classifiers in estimating the class membership probability. We used Receiver Operating Characteristics-Area under Curve (ROC-AUC) value and overall accuracy as benchmarks to evaluate and compare the performance of classifiers. We apply Friedman's, post-hoc Nemenyi and Analysis of Means (ANOM) test to compare the significant performance of classifiers. We conclude that ADTree and RandomForest outperform, while ZeroR classifier cannot show significant performance for estimation of class membership probability.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {1550–1553},
numpages = {4},
keywords = {accuracy, chidamber and kemerer metrics, classifiers, estimation, performance, probability, weka},
location = {Pisa, Italy},
series = {SAC '16}
}

@article{10.1016/j.infsof.2017.08.004,
title = {MULTI},
year = {2018},
issue_date = {January 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {93},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.08.004},
doi = {10.1016/j.infsof.2017.08.004},
abstract = {Context: Just-in-time software defect prediction (JIT-SDP) aims to conduct defect prediction on code changes, which have finer granularity. A recent study by Yang etal. has shown that there exist some unsupervised methods, which are comparative to supervised methods in effort-aware JIT-SDP.Objective: However, we still believe that supervised methods should have better prediction performance since they effectively utilize the gathered defect prediction datasets. Therefore we want to design a new supervised method for JIT-SDP with better performance.Method: In this article, we propose a multi-objective optimization based supervised method MULTI to build JIT-SDP models. In particular, we formalize JIT-SDP as a multi-objective optimization problem. One objective is designed to maximize the number of identified buggy changes and another object is designed to minimize the efforts in software quality assurance activities. There exists an obvious conflict between these two objectives. MULTI uses logistic regression to build the models and uses NSGA-II to generate a set of non-dominated solutions, which each solution denotes the coefficient vector for the logistic regression.Results: We design and conduct a large-scale empirical studies to compare MULTI with 43 state-of-the-art supervised and unsupervised methods under the three commonly used performance evaluation scenarios: cross-validation, cross-project-validation, and timewise-cross-validation. Based on six open-source projects with 227,417 changes in total, our experimental results show that MULTI can perform significantly better than all of the state-of-the-art methods when considering ACC and POPT performance metrics.Conclusion: By using multi-objective optimization, MULTI can perform significantly better than the state-of-the-art supervised and unsupervised methods in the three performance evaluation scenarios. The results confirm that supervised methods are still promising in effort-aware JIT-SDP.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {1–13},
numpages = {13}
}

@article{10.4018/IJOSSP.2016040103,
author = {Sureka, Ashish and Lal, Sangeeta and Sardana, Neetu},
title = {Improving Logging Prediction on Imbalanced Datasets: A Case Study on Open Source Java Projects},
year = {2016},
issue_date = {April 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016040103},
doi = {10.4018/IJOSSP.2016040103},
abstract = {Logging is an important yet tough decision for OSS developers. Machine-learning models are useful in improving several steps of OSS development, including logging. Several recent studies propose machine-learning models to predict logged code construct. The prediction performances of these models are limited due to the class-imbalance problem since the number of logged code constructs is small as compared to non-logged code constructs. No previous study analyzes the class-imbalance problem for logged code construct prediction. The authors first analyze the performances of J48, RF, and SVM classifiers for catch-blocks and if-blocks logged code constructs prediction on imbalanced datasets. Second, the authors propose LogIm, an ensemble and threshold-based machine-learning model. Third, the authors evaluate the performance of LogIm on three open-source projects. On average, LogIm model improves the performance of baseline classifiers, J48, RF, and SVM, by 7.38%, 9.24%, and 4.6% for catch-blocks, and 12.11%, 14.95%, and 19.13% for if-blocks logging prediction.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {43–71},
numpages = {29},
keywords = {Data Sampling, Debugging, Ensemble Methods, Imbalanced Data, Logging, Machine Learning, Open Source, Tracing}
}

@inproceedings{10.5555/2664446.2664476,
author = {Chen, Tse-Hsun and Thomas, Stephen W. and Nagappan, Meiyappan and Hassan, Ahmed E.},
title = {Explaining software defects using topic models},
year = {2012},
isbn = {9781467317610},
publisher = {IEEE Press},
abstract = {Researchers have proposed various metrics based on measurable aspects of the source code entities (e.g., methods, classes, files, or modules) and the social structure of a software project in an effort to explain the relationships between software development and software defects. However, these metrics largely ignore the actual functionality, i.e., the conceptual concerns, of a software system, which are the main technical concepts that reflect the business logic or domain of the system. For instance, while lines of code may be a good general measure for defects, a large entity responsible for simple I/O tasks is likely to have fewer defects than a small entity responsible for complicated compiler implementation details. In this paper, we study the effect of conceptual concerns on code quality. We use a statistical topic modeling technique to approximate software concerns as topics; we then propose various metrics on these topics to help explain the defect-proneness (i.e., quality) of the entities. Paramount to our proposed metrics is that they take into account the defect history of each topic. Case studies on multiple versions of Mozilla Firefox, Eclipse, and Mylyn show that (i) some topics are much more defect-prone than others, (ii) defect-prone topics tend to remain so over time, and (iii) defect-prone topics provide additional explanatory power for code quality over existing structural and historical metrics.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
pages = {189–198},
numpages = {10},
keywords = {code quality, software concerns, topic modeling},
location = {Zurich, Switzerland},
series = {MSR '12}
}

@article{10.1145/3057269,
author = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
title = {Effective Regression Test Case Selection: A Systematic Literature Review},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057269},
doi = {10.1145/3057269},
abstract = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {29},
numpages = {32},
keywords = {SLR, Software testing, cost effectiveness, coverage, fault detection ability}
}

@inproceedings{10.1145/3379597.3387457,
author = {Pecorelli, Fabiano and Palomba, Fabio and Khomh, Foutse and De Lucia, Andrea},
title = {Developer-Driven Code Smell Prioritization},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387457},
doi = {10.1145/3379597.3387457},
abstract = {Code smells are symptoms of poor implementation choices applied during software evolution. While previous research has devoted effort in the definition of automated solutions to detect them, still little is known on how to support developers when prioritizing them. Some works attempted to deliver solutions that can rank smell instances based on their severity, computed on the basis of software metrics. However, this may not be enough since it has been shown that the recommendations provided by current approaches do not take the developer's perception of design issues into account. In this paper, we perform a first step toward the concept of developer-driven code smell prioritization and propose an approach based on machine learning able to rank code smells according to the perceived criticality that developers assign to them. We evaluate our technique in an empirical study to investigate its accuracy and the features that are more relevant for classifying the developer's perception. Finally, we compare our approach with a state-of-the-art technique. Key findings show that the our solution has an F-Measure up to 85% and outperforms the baseline approach.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {220–231},
numpages = {12},
keywords = {Code smells, Empirical Software Engineering, Machine Learning for Software Engineering},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1016/j.cag.2021.04.034,
author = {Tian, Zonglin and Zhai, Xiaorui and van Driel, Daan and van Steenpaal, Gijs and Espadoto, Mateus and Telea, Alexandru},
title = {Using multiple attribute-based explanations of multidimensional projections to explore high-dimensional data},
year = {2021},
issue_date = {Aug 2021},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {98},
number = {C},
issn = {0097-8493},
url = {https://doi.org/10.1016/j.cag.2021.04.034},
doi = {10.1016/j.cag.2021.04.034},
journal = {Comput. Graph.},
month = aug,
pages = {93–104},
numpages = {12},
keywords = {Dimensionality reduction, Explanatory techniques, High-dimensional data analysis}
}

@article{10.1016/j.jss.2017.01.026,
author = {Arvanitou, Elvira Maria and Ampatzoglou, Apostolos and Chatzigeorgiou, Alexander and Galster, Matthias and Avgeriou, Paris},
title = {A mapping study on design-time quality attributes and metrics},
year = {2017},
issue_date = {May 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {127},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.01.026},
doi = {10.1016/j.jss.2017.01.026},
abstract = {Support to the quality attribute (QA) &amp; metric selection process.Maintainability is the most studied QA for most domains and development phases.Quality attributes are usually assessed through a correlation to a single metric.Metrics are validated in empirical settings and may lack theoretical validity. Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.},
journal = {J. Syst. Softw.},
month = may,
pages = {52–77},
numpages = {26},
keywords = {Design-time quality attributes, Mapping study, Measurement, Software quality}
}

@article{10.1007/s10515-014-0162-2,
author = {Xia, Xin and Lo, David and Shihab, Emad and Wang, Xinyu and Zhou, Bo},
title = {Automatic, high accuracy prediction of reopened bugs},
year = {2015},
issue_date = {March     2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {1},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0162-2},
doi = {10.1007/s10515-014-0162-2},
abstract = {Bug fixing is one of the most time-consuming and costly activities of the software development life cycle. In general, bugs are reported in a bug tracking system, validated by a triage team, assigned for someone to fix, and finally verified and closed. However, in some cases bugs have to be reopened. Reopened bugs increase software maintenance cost, cause rework for already busy developers and in some cases even delay the future delivery of a software release. Therefore, a few recent studies focused on studying reopened bugs. However, these prior studies did not achieve high performance (in terms of precision and recall), required manual intervention, and used very simplistic techniques when dealing with this textual data, which leads us to believe that further improvements are possible. In this paper, we propose ReopenPredictor, which is an automatic, high accuracy predictor of reopened bugs. ReopenPredictor uses a number of features, including textual features, to achieve high accuracy prediction of reopened bugs. As part of ReopenPredictor, we propose two algorithms that are used to automatically estimate various thresholds to maximize the prediction performance. To examine the benefits of ReopenPredictor, we perform experiments on three large open source projects--namely Eclipse, Apache HTTP and OpenOffice. Our results show that ReopenPredictor outperforms prior work, achieving a reopened F-measure of 0.744, 0.770, and 0.860 for Eclipse, Apache HTTP and OpenOffice, respectively. These results correspond to an improvement in the reopened F-measure of the method proposed in the prior work by Shihab et al. by 33.33, 12.57 and 3.12 % for Eclipse, Apache HTTP and OpenOffice, respectively.},
journal = {Automated Software Engg.},
month = mar,
pages = {75–109},
numpages = {35},
keywords = {Imbalanced feature selection, Imbalanced learning, Reopened bugs}
}

@article{10.1016/j.eswa.2015.12.027,
author = {Kang, Pilsung and Kim, Dongil and Cho, Sungzoon},
title = {Semi-supervised support vector regression based on self-training with label uncertainty},
year = {2016},
issue_date = {June 2016},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {51},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2015.12.027},
doi = {10.1016/j.eswa.2015.12.027},
abstract = {A new semi-supervised support vector regression method is proposed.Label distribution is estimated by probabilistic local reconstruction algorithm.Different oversampling rate is used based on uncertainty information.Expected margin based pattern selection is used to reduce the training complexity.The proposed method improves the prediction performance with lower time complexity. Dataset size continues to increase and data are being collected from numerous applications. Because collecting labeled data is expensive and time consuming, the amount of unlabeled data is increasing. Semi-supervised learning (SSL) has been proposed to improve conventional supervised learning methods by training from both unlabeled and labeled data. In contrast to classification problems, the estimation of labels for unlabeled data presents added uncertainty for regression problems. In this paper, a semi-supervised support vector regression (SS-SVR) method based on self-training is proposed. The proposed method addresses the uncertainty of the estimated labels for unlabeled data. To measure labeling uncertainty, the label distribution of the unlabeled data is estimated with two probabilistic local reconstruction (PLR) models. Then, the training data are generated by oversampling from the unlabeled data and their estimated label distribution. The sampling rate is different based on uncertainty. Finally, expected margin-based pattern selection (EMPS) is employed to reduce training complexity. We verify the proposed method with 30 regression datasets and a real-world problem: virtual metrology (VM) in semiconductor manufacturing. The experiment results show that the proposed method improves the accuracy by 8% compared with conventional supervised SVR, and the training time for the proposed method is 20% shorter than that of the benchmark methods.},
journal = {Expert Syst. Appl.},
month = jun,
pages = {85–106},
numpages = {22},
keywords = {Data generation, Probabilistic local reconstruction, Semi-supervised learning, Semiconductor manufacturing, Support vector regression, Virtual metrology}
}

@inproceedings{10.1145/3427228.3427269,
author = {Das, Sanjeev and James, Kedrian and Werner, Jan and Antonakakis, Manos and Polychronakis, Michalis and Monrose, Fabian},
title = {A Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427269},
doi = {10.1145/3427228.3427269},
abstract = {Among various fuzzing approaches, coverage-guided grey-box fuzzing is perhaps the most prominent, due to its ease of use and effectiveness. Using this approach, the selection of inputs focuses on maximizing program coverage, e.g., in terms of the different branches that have been traversed. In this work, we begin with the observation that selecting any input that explores a new path, and giving equal weight to all paths, can lead to severe inefficiencies. For instance, although seemingly “new” crashes involving previously unexplored paths may be discovered, these often have the same root cause and actually correspond to the same bug. To address these inefficiencies, we introduce a framework that incorporates a tighter feedback loop to guide the fuzzing process in exploring truly diverse code paths. Our framework employs (i) a vulnerability-aware selection of coverage metrics for enhancing the effectiveness of code exploration, (ii) crash deduplication information for early feedback, and (iii) a configurable input culling strategy that interleaves multiple strategies to achieve comprehensiveness. A novel aspect of our work is the use of hardware performance counters to derive coverage metrics. We present an approach for assessing and selecting the hardware events that can be used as a meaningful coverage metric for a target program. The results of our empirical evaluation using real-world programs demonstrate the effectiveness of our approach: in some cases, we explore fewer than 50% of the paths compared to a base fuzzer (AFL, MOpt, and Fairfuzz), yet on average, we improve new bug discovery by 31%, and find the same bugs (as the base) 3.3 times faster. Moreover, although we specifically chose applications that have been subject to recent fuzzing campaigns, we still discovered 9 new vulnerabilities.},
booktitle = {Proceedings of the 36th Annual Computer Security Applications Conference},
pages = {345–359},
numpages = {15},
keywords = {Fuzzing, Hardware Performance Counters, Machine Learning},
location = {Austin, USA},
series = {ACSAC '20}
}

@article{10.1007/s11219-018-9419-5,
author = {Garousi, Vahid and Tarhan, Ay\c{c}a and Pfahl, Dietmar and Co\c{s}kun\c{c}ay, Ahmet and Demir\"{o}rs, Onur},
title = {Correlation of critical success factors with success of software projects: an empirical investigation},
year = {2019},
issue_date = {March     2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9419-5},
doi = {10.1007/s11219-018-9419-5},
abstract = {Software engineering researchers have, over the years, proposed different critical success factors (CSFs) which are believed to be critically correlated with the success of software projects. To conduct an empirical investigation into the correlation of CSFs with success of software projects, we adapt and extend in this work an existing contingency fit model of CSFs. To archive the above objective, we designed an online survey and gathered CSF-related data for 101 software projects in the Turkish software industry. Among our findings is that the top three CSFs having the most significant associations with project success were: (1) team experience with the software development methodologies, (2) team's expertise with the task, and (3) project monitoring and controlling. A comprehensive correlation analysis between the CSFs and project success indicates positive associations between the majority of the factors and variables, however, in most of the cases at non-significant levels. By adding to the body of evidence in this field, the results of the study will be useful for a wide audience. Software managers can use the results to prioritize the improvement opportunities in their organizations w.r.t. the discussed CSFs. Software engineers might use the results to improve their skills in different dimensions, and researchers might use the results to prioritize and conduct follow-up in-depth studies on those factors.},
journal = {Software Quality Journal},
month = mar,
pages = {429–493},
numpages = {65},
keywords = {Critical success factors, Empirical studies, Project management, Software engineering, Software projects, Success and failure}
}

@inproceedings{10.1145/3379597.3387489,
author = {Zhang, Xunhui and Rastogi, Ayushi and Yu, Yue},
title = {On the Shoulders of Giants: A New Dataset for Pull-based Development Research},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387489},
doi = {10.1145/3379597.3387489},
abstract = {Pull-based development is a widely adopted paradigm for collaboration in distributed software development, attracting eyeballs from both academic and industry. To better study pull-based development model, this paper presents a new dataset containing 96 features collected from 11,230 projects and 3,347,937 pull requests. We describe the creation process and explain the features in details. To the best of our knowledge, our dataset is the most comprehensive and largest one toward a complete picture for pull-based development research.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {543–547},
numpages = {5},
keywords = {distributed software development, pull request, pull-based development},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/2970276.2970339,
author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
title = {Too much automation? the bellwether effect and its implications for transfer learning},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970339},
doi = {10.1145/2970276.2970339},
abstract = {Transfer learning: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple bellwether transfer learner. Given N data sets, we find which one produce the best predictions on all the others. This bellwether data set is then used for all subsequent predictions (or, until such time as its predictions start failing-- at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) bellwethers are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {122–131},
numpages = {10},
keywords = {Data Mining, Defect Prediction, Transfer learning},
location = {Singapore, Singapore},
series = {ASE '16}
}

@article{10.1007/s10462-018-9667-6,
author = {Fazel Zarandi, Mohammad Hossein and Sadat Asl, Ali Akbar and Sotudian, Shahabeddin and Castillo, Oscar},
title = {A state of the art review of intelligent scheduling},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {53},
number = {1},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-018-9667-6},
doi = {10.1007/s10462-018-9667-6},
abstract = {Intelligent scheduling covers various tools and techniques for successfully and efficiently solving the scheduling problems. In this paper, we provide a survey of intelligent scheduling systems by categorizing them into five major techniques containing fuzzy logic, expert systems, machine learning, stochastic local search optimization algorithms and constraint programming. We also review the application case studies of these techniques.},
journal = {Artif. Intell. Rev.},
month = jan,
pages = {501–593},
numpages = {93},
keywords = {Intelligent scheduling, Fuzzy logic, Expert system, Machine learning, Stochastic local search optimization algorithms, Constraint programming}
}

@inproceedings{10.1145/3379597.3387482,
author = {Pinto, Gustavo and Miranda, Breno and Dissanayake, Supun and d'Amorim, Marcelo and Treude, Christoph and Bertolino, Antonia},
title = {What is the Vocabulary of Flaky Tests?},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387482},
doi = {10.1145/3379597.3387482},
abstract = {Flaky tests are tests whose outcomes are non-deterministic. Despite the recent research activity on this topic, no effort has been made on understanding the vocabulary of flaky tests. This work proposes to automatically classify tests as flaky or not based on their vocabulary. Static classification of flaky tests is important, for example, to detect the introduction of flaky tests and to search for flaky tests after they are introduced in regression test suites.We evaluated performance of various machine learning algorithms to solve this problem. We constructed a data set of flaky and non-flaky tests by running every test case, in a set of 64k tests, 100 times (6.4 million test executions). We then used machine learning techniques on the resulting data set to predict which tests are flaky from their source code. Based on features, such as counting stemmed tokens extracted from source code identifiers, we achieved an F-measure of 0.95 for the identification of flaky tests. The best prediction performance was obtained when using Random Forest and Support Vector Machines. In terms of the code identifiers that are most strongly associated with test flakiness, we noted that job, action, and services are commonly associated with flaky tests. Overall, our results provides initial yet strong evidence that static detection of flaky tests is effective.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {492–502},
numpages = {11},
keywords = {Regression testing, Test flakiness, Text classification},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1016/j.infsof.2021.106686,
author = {Liu, Shiran and Guo, Zhaoqiang and Li, Yanhui and Lu, Hongmin and Chen, Lin and Xu, Lei and Zhou, Yuming and Xu, Baowen},
title = {Prioritizing code documentation effort: Can we do it simpler but better?},
year = {2021},
issue_date = {Dec 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {140},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106686},
doi = {10.1016/j.infsof.2021.106686},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {16},
keywords = {Code documentation, Program comprehension, PageRank, Metrics}
}

@article{10.1016/j.ins.2011.09.034,
author = {Yu, Lean},
title = {An evolutionary programming based asymmetric weighted least squares support vector machine ensemble learning methodology for software repository mining},
year = {2012},
issue_date = {May, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {191},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2011.09.034},
doi = {10.1016/j.ins.2011.09.034},
abstract = {In this paper, a novel evolutionary programming (EP) based asymmetric weighted least squares support vector machine (LSSVM) ensemble learning methodology is proposed for software repository mining. In this methodology, an asymmetric weighted LSSVM model is first proposed. Then the process of building the EP-based asymmetric weighted LSSVM ensemble learning methodology is described in detail. Two publicly available software defect datasets are finally used for illustration and verification of the effectiveness of the proposed EP-based asymmetric weighted LSSVM ensemble learning methodology. Experimental results reveal that the proposed EP-based asymmetric weighted LSSVM ensemble learning methodology can produce promising classification accuracy in software repository mining, relative to other classification methods listed in this study.},
journal = {Inf. Sci.},
month = may,
pages = {31–46},
numpages = {16},
keywords = {Asymmetric weighted least squares support vector machine, Ensemble learning algorithm, Evolutionary programming, Software repository mining}
}

@inproceedings{10.1007/978-3-030-23281-8_20,
author = {Nnamoko, Nonso and Cabrera-Diego, Luis Adri\'{a}n and Campbell, Daniel and Korkontzelos, Yannis},
title = {Bug Severity Prediction Using a Hierarchical One-vs.-Remainder Approach},
year = {2019},
isbn = {978-3-030-23280-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23281-8_20},
doi = {10.1007/978-3-030-23281-8_20},
abstract = {Assigning severity level to reported bugs is a critical part of software maintenance to ensure an efficient resolution process. In many bug trackers, e.g.&nbsp;Bugzilla, this is a time consuming process, because bug reporters must manually assign one of seven severity levels to each bug. In addition, some bug types may be reported more often than others, leading to a disproportionate distribution of severity labels. Machine learning techniques can be used to predict the label of a newly reported bug automatically. However, learning from imbalanced data in a multi-class task remains one of the major difficulties for machine learning classifiers. In this paper, we propose a hierarchical classification approach that exploits class imbalance in the training data, to reduce classification bias. Specifically, we designed a classification tree that consists of multiple binary classifiers organised hierarchically, such that instances from the most dominant class are trained against the remaining classes but are not used for training the next level of the classification tree. We used FastText classifier to test and compare between the hierarchical and standard classification approaches. Based on 93,051 bug reports from 38 Eclipse open-source products, the hierarchical approach was shown to perform relatively well with 65% Micro F-Score and 45% Macro F-Score.},
booktitle = {Natural Language Processing and Information Systems: 24th International Conference on Applications of Natural Language to Information Systems, NLDB 2019, Salford, UK, June 26–28, 2019, Proceedings},
pages = {247–260},
numpages = {14},
keywords = {Bug severity, Imbalanced data, Text mining, Machine learning, Multi-class classification, FastText},
location = {Salford, United Kingdom}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {24},
numpages = {35},
keywords = {Affected version, SZZ, defect origin, developing defects repository}
}

@article{10.1007/s10664-020-09802-1,
author = {Guo, Zhaoqiang and Li, Yanhui and Ma, Wanwangying and Zhou, Yuming and Lu, Hongmin and Chen, Lin and Xu, Baowen},
title = {Boosting crash-inducing change localization with rank-performance-based feature subset selection},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09802-1},
doi = {10.1007/s10664-020-09802-1},
abstract = {Given a bucket of crash reports, it would be helpful for developers to find and fix the corresponding defects if the crash-inducing software changes can be automatically located. Recently, an approach called ChangeLocator was proposed, which used ten change-level features to train a supervised model based on the data from the historical fixed crashes. It was reported that ChangeLocator achieved a good performance in terms of Recall@1, MAP, and MRR, when all the ten features were combined together. However, in ChangeLocator, the redundancy between features are neglected, which may degrade the localization effectiveness. In this paper, we propose an improved approach ChangeRanker with a rank-performance-based feature selection technology (Rfs) to boost the effectiveness of crash-inducing change localization. Our experimental results on NetBeans show that ChangeRanker can achieve an improvement of 35.9%, 17.4%, and 15.3% over ChangeLocator in terms of Recall@1, MRR, and MAP, respectively. Furthermore, compared with three popular feature selection approaches, Rfs is able to select more informative features to boost localization effectiveness. In order to assess the real generalization capability of the proposed extension, we adapt ChangeRanker and ChangeLocator to locate bug-inducing changes on three additional data sets. Again, we observe that, on average, ChangeRanker achieves an improvement of 115.3%, 37.6%, and 41.2% in terms of Recall@1, MRR, and MAP, respectively. This indicates that our proposed rank-performance-based feature selection method has a good generalization capability. In summary, our work provides an easy-to-use approach to boosting the performance of the state-of-the-art crash-inducing change localization approach.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {1905–1950},
numpages = {46},
keywords = {Crash-inducing change, Software crash, Bug localization, Feature subset selection, Crash stack}
}

@inproceedings{10.1145/2810146.2810149,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
title = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
year = {2015},
isbn = {9781450337151},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810146.2810149},
doi = {10.1145/2810146.2810149},
abstract = {BACKGROUND -- During the last 10 years hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall.OBJECTIVE -- We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers.METHOD -- We perform a sensitivity analysis to compare the performance of Random Forest, Na\"{\i}ve Bayes, RPart and SVM classifiers when predicting defects in 12 NASA data sets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty is compared against different classifiers.RESULTS -- Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others.CONCLUSIONS -- Our results confirm that a unique sub-set of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Classifier ensembles with decision making strategies not based on majority voting are likely to perform best.},
booktitle = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {3},
numpages = {10},
location = {Beijing, China},
series = {PROMISE '15}
}

@inproceedings{10.1145/3275219.3275239,
author = {Qin, Hanmin and Sun, Xin},
title = {Classifying Bug Reports into Bugs and Non-bugs Using LSTM},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275239},
doi = {10.1145/3275219.3275239},
abstract = {Studies have found that significant amount of bug reports are misclassified between bugs and non-bugs, which inevitably affects relevant studies, e.g., bug prediction. Manually classifying bug reports helps reduce the noise but is often time-consuming. To ease the problem, we propose a bug classification method based on Long Short-Term Memory (LSTM), a typical recurrent neural network which is widely used in text classification tasks. Our method outperforms existing topic-based method and n-gram IDF-based method on four datasets from three popular JAVA open source projects. We believe our work can assist developers and researches to classify bug reports and identify misclassified bug reports. Datasets and scripts used in this work are provided on GitHub1 for others to reproduce and further improve our study.},
booktitle = {Proceedings of the 10th Asia-Pacific Symposium on Internetware},
articleno = {20},
numpages = {4},
keywords = {LSTM, bug classification},
location = {Beijing, China},
series = {Internetware '18}
}

@article{10.1016/j.cl.2016.10.001,
author = {Bansal, Ankita},
title = {Empirical analysis of search based algorithms to identify change prone classes of open source software},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {P2},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.10.001},
doi = {10.1016/j.cl.2016.10.001},
abstract = {There are numerous reasons leading to change in software such as changing requirements, changing technology, increasing customer demands, fixing of defects etc. Thus, identifying and analyzing the change-prone classes of the software during software evolution is gaining wide importance in the field of software engineering. This would help software developers to judiciously allocate the resources used for testing and maintenance. Software metrics can be used for constructing various classification models which can be used for timely identification of change prone classes. Search based algorithms which form a subset of machine learning algorithms can be utilized for constructing prediction models to identify change prone classes of software. Search based algorithms use a fitness function to find the best optimal solution among all the possible solutions. In this work, we analyze the effectiveness of hybridized search based algorithms for change prediction. In other words, the aim of this work is to find whether search based algorithms are capable for accurate model construction to predict change prone classes. We have also constructed models using machine learning techniques and compared the performance of these models with the models constructed using Search Based Algorithms. The validation is carried out on two open source Apache projects, Rave and Commons Math. The results prove the effectiveness of hybridized search based algorithms in predicting change prone classes of software. Thus, they can be utilized by the software developers to produce an efficient and better developed software. Used hybridized search based algorithms to identify change prone classes.For empirical validation, two open source projects (Apache Rave,Commons Math) used.Assessed performance of search based algorithms using g-mean and accuracy.Machine learning models constructed and performance compared with hybridised models.Results showed that hybridised models outperformed machine learning models.},
journal = {Comput. Lang. Syst. Struct.},
month = jan,
pages = {211–231},
numpages = {21},
keywords = {Change proneness, Empirical validation, Metrics, Object oriented paradigm, Search based algorithms, Software quality}
}

@article{10.1007/s11219-018-9432-8,
author = {Aichernig, Bernhard K. and Bauerst\"{a}tter, Priska and J\"{o}bstl, Elisabeth and Kann, Severin and Koro\v{s}ec, Robert and Krenn, Willibald and Mateis, Cristinel and Schlick, Rupert and Schumi, Richard},
title = {Learning and statistical model checking of system response times},
year = {2019},
issue_date = {June      2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {27},
number = {2},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9432-8},
doi = {10.1007/s11219-018-9432-8},
abstract = {Since computers have become increasingly more powerful, users are less willing to accept slow responses of systems. Hence, performance testing is important for interactive systems. However, it is still challenging to test if a system provides acceptable performance or can satisfy certain response-time limits, especially for different usage scenarios. On the one hand, there are performance-testing techniques that require numerous costly tests of the system. On the other hand, model-based performance analysis methods have a doubtful model quality. Hence, we propose a combined method to mitigate these issues. We learn response-time distributions from test data in order to augment existing behavioral models with timing aspects. Then, we perform statistical model checking with the resulting model for a performance prediction. Finally, we test the accuracy of our prediction with hypotheses testing of the real system. Our method is implemented with a property-based testing tool with integrated statistical model checking algorithms. We demonstrate the feasibility of our techniques in an industrial case study with a web-service application.},
journal = {Software Quality Journal},
month = jun,
pages = {757–795},
numpages = {39},
keywords = {Cost learning, FsCheck, Model-based testing, Performance testing, Property-based testing, Response time, Statistical model checking, User profiles}
}

@inproceedings{10.1109/AST.2017.7,
author = {Al-Hajjaji, Mustafa and Kr\"{u}ger, Jacob and Schulze, Sandro and Leich, Thomas and Saake, Gunter},
title = {Efficient product-line testing using cluster-based product prioritization},
year = {2017},
isbn = {9781538615485},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AST.2017.7},
doi = {10.1109/AST.2017.7},
abstract = {A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.},
booktitle = {Proceedings of the 12th International Workshop on Automation of Software Testing},
pages = {16–22},
numpages = {7},
location = {Buenos Aires, Argentina},
series = {AST '17}
}

@article{10.1007/s10664-018-9656-z,
author = {Blincoe, Kelly and Dehghan, Ali and Salaou, Abdoul-Djawadou and Neal, Adam and Linaker, Johan and Damian, Daniela},
title = {High-level software requirements and iteration changes: a predictive model},
year = {2019},
issue_date = {Jun 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9656-z},
doi = {10.1007/s10664-018-9656-z},
abstract = {Knowing whether a software feature will be completed in its planned iteration can help with release planning decisions. However, existing research has focused on predictions of only low-level software tasks, like bug fixes. In this paper, we describe a mixed-method empirical study on three large IBM projects. We investigated the types of iteration changes that occur. We show that up to 54% of high-level requirements do not make their planned iteration. Requirements are most often pushed out to the next iteration, but high-level requirements are also commonly moved to the next minor or major release or returned to the product or release backlog. We developed and evaluated a model that uses machine learning to predict if a high-level requirement will be completed within its planned iteration. The model includes 29 features that were engineered based on prior work, interviews with IBM developers, and domain knowledge. Predictions were made at four different stages of the requirement lifetime. Our model is able to achieve up to 100% precision. We ranked the importance of our model features and found that some features are highly dependent on project and prediction stage. However, some features (e.g., the time remaining in the iteration and creator of the requirement) emerge as important across all projects and stages. We conclude with a discussion on future research directions.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1610–1648},
numpages = {39},
keywords = {Completion prediction, Machine learning, Mining software repositories, Release planning, Software requirements}
}

@article{10.1016/j.infsof.2019.07.009,
author = {Gomes, Luiz Alberto Ferreira and Torres, Ricardo da Silva and C\^{o}rtes, Mario L\'{u}cio},
title = {Bug report severity level prediction in open source software: A survey and research opportunities},
year = {2019},
issue_date = {Nov 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {115},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.07.009},
doi = {10.1016/j.infsof.2019.07.009},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {58–78},
numpages = {21},
keywords = {Software maintenance, Bug tracking systems, Bug reports, Severity level prediction, Software repositories, Systematic mapping, Machine learning}
}

@article{10.1016/j.jss.2019.02.056,
author = {Sierra, Giancarlo and Shihab, Emad and Kamei, Yasutaka},
title = {A survey of self-admitted technical debt},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {152},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.02.056},
doi = {10.1016/j.jss.2019.02.056},
journal = {J. Syst. Softw.},
month = jun,
pages = {70–82},
numpages = {13},
keywords = {Self admitted technical debt, Software maintenance, Literature survey, Source code comments}
}

@article{10.1007/s11219-009-9081-z,
author = {Bak\i{}r, Ay\c{s}e and Turhan, Burak and Bener, Ay\c{s}e B.},
title = {A new perspective on data homogeneity in software cost estimation: a study in the embedded systems domain},
year = {2010},
issue_date = {March     2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-009-9081-z},
doi = {10.1007/s11219-009-9081-z},
abstract = {Cost estimation and effort allocation are the key challenges for successful project planning and management in software development. Therefore, both industry and the research community have been working on various models and techniques to accurately predict the cost of projects. Recently, researchers have started debating whether the prediction performance depends on the structure of data rather than the models used. In this article, we focus on a new aspect of data homogeneity, "cross- versus within-application domain", and investigate what kind of training data should be used for software cost estimation in the embedded systems domain. In addition, we try to find out the effect of training dataset size on the prediction performance. Based on our empirical results, we conclude that it is better to use cross-domain data for embedded software cost estimation and the optimum training data size depends on the method used.},
journal = {Software Quality Journal},
month = mar,
pages = {57–80},
numpages = {24},
keywords = {Application domain, Cost estimation, Data homogeneity, Embedded software, Machine learning}
}

@article{10.1016/j.compind.2020.103247,
author = {S\'{a}nchez, M. and Exposito, E. and Aguilar, J.},
title = {Implementing self-* autonomic properties in self-coordinated manufacturing processes for the Industry 4.0 context},
year = {2020},
issue_date = {Oct 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {121},
number = {C},
issn = {0166-3615},
url = {https://doi.org/10.1016/j.compind.2020.103247},
doi = {10.1016/j.compind.2020.103247},
journal = {Comput. Ind.},
month = oct,
numpages = {17},
keywords = {Industry 4.0, Self-supervising, Autonomic computing, Process mining, Self-coordination}
}

@article{10.1007/s11704-020-9281-z,
author = {Yu, Dongjin and Wang, Lin and Chen, Xin and Chen, Jie},
title = {Using BiLSTM with attention mechanism to automatically detect self-admitted technical debt},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {15},
number = {4},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-020-9281-z},
doi = {10.1007/s11704-020-9281-z},
abstract = {Technical debt is a metaphor for seeking short-term gains at expense of long-term code quality. Previous studies have shown that self-admitted technical debt, which is introduced intentionally, has strong negative impacts on software development and incurs high maintenance overheads. To help developers identify self-admitted technical debt, researchers have proposed many state-of-the-art methods. However, there is still room for improvement about the effectiveness of the current methods, as self-admitted technical debt comments have the characteristics of length variability, low proportion and style diversity. Therefore, in this paper, we propose a novel approach based on the bidirectional long short-term memory (BiLSTM) networks with the attention mechanism to automatically detect self-admitted technical debt by leveraging source code comments. In BiLSTM, we utilize a balanced cross entropy loss function to overcome the class unbalance problem. We experimentally investigate the performance of our approach on a public dataset including 62, 566 code comments from ten open source projects. Experimental results show that our approach achieves 81.75% in terms of precision, 72.24% in terms of recall and 75.86% in terms of F1-score on average and outperforms the state-of-the-art text mining-based method by 8.14%, 5.49% and 6.64%, respectively.},
journal = {Front. Comput. Sci.},
month = aug,
numpages = {12},
keywords = {technical debt, self-admitted technical debt, long short-term memory, attention mechanism, natural language processing}
}

@article{10.1007/s11219-018-9417-7,
author = {Harrison, Rachel},
title = {In this issue},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-018-9417-7},
doi = {10.1007/s11219-018-9417-7},
journal = {Software Quality Journal},
month = dec,
pages = {1185–1186},
numpages = {2}
}

@article{10.5555/3546258.3546523,
author = {Wang, Feicheng and Janson, Lucas},
title = {Exact asymptotics for linear quadratic adaptive control},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Recent progress in reinforcement learning has led to remarkable performance in a range of applications, but its deployment in high-stakes settings remains quite rare. One reason is a limited understanding of the behavior of reinforcement algorithms, both in terms of their regret and their ability to learn the underlying system dynamics--existing work is focused almost exclusively on characterizing rates, with little attention paid to the constants multiplying those rates that can be critically important in practice. To start to address this challenge, we study perhaps the simplest non-bandit reinforcement learning problem: linear quadratic adaptive control (LQAC) . By carefully combining recent finite-sample performance bounds for the LQAC problem with a particular (less-recent) martingale central limit theorem, we are able to derive asymptotically-exact expressions for the regret, estimation error, and prediction error of a rate-optimal stepwise-updating LQAC algorithm. In simulations on both stable and unstable systems, we find that our asymptotic theory also describes the algorithm's finite-sample behavior remarkably well.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {265},
numpages = {112},
keywords = {reinforcement learning, adaptive control, linear dynamical system, system identification, safety, uncertainty quantification, exact asymptotics}
}

@article{10.1016/j.jss.2019.110456,
author = {Wu, Xiaoxue and Zheng, Wei and Chen, Xiang and Wang, Fang and Mu, Dejun},
title = {CVE-assisted large-scale security bug report dataset construction method},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {160},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110456},
doi = {10.1016/j.jss.2019.110456},
journal = {J. Syst. Softw.},
month = feb,
numpages = {14},
keywords = {Security bug report prediction, Voting classification, Dataset construction, Common vulnerabilities and exposures}
}

@article{10.1016/j.infsof.2020.106392,
author = {Saidani, Islem and Ouni, Ali and Chouchen, Moataz and Mkaouer, Mohamed Wiem},
title = {Predicting continuous integration build failures using evolutionary search},
year = {2020},
issue_date = {Dec 2020},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {128},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2020.106392},
doi = {10.1016/j.infsof.2020.106392},
journal = {Inf. Softw. Technol.},
month = dec,
numpages = {16},
keywords = {Continuous integration, Build prediction, Multi-Objective optimization, Search-Based software engineering, Machine learning}
}

@article{10.1007/s10458-021-09497-8,
author = {Cuccu, Giuseppe and Togelius, Julian and Cudr\'{e}-Mauroux, Philippe},
title = {Playing Atari with few neurons: Improving the efficacy of reinforcement learning by decoupling feature extraction and decision making},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {35},
number = {2},
issn = {1387-2532},
url = {https://doi.org/10.1007/s10458-021-09497-8},
doi = {10.1007/s10458-021-09497-8},
abstract = {We propose a new method for learning compact state representations and policies separately but simultaneously for policy approximation in vision-based applications such as Atari games. Approaches based on deep reinforcement learning typically map pixels directly to actions to enable end-to-end training. Internally, however, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it, two objectives which can be addressed independently. Separating the image processing from the action selection allows for a better understanding of either task individually, as well as potentially finding smaller policy representations which is inherently interesting. Our approach learns state representations using a compact encoder based on two novel algorithms: (i) Increasing Dictionary Vector Quantization builds a dictionary of state representations which grows in size over time, allowing our method to address new observations as they appear in an open-ended online-learning context; and (ii) Direct Residuals Sparse Coding encodes observations in function of the dictionary, aiming for highest information inclusion by disregarding reconstruction error and maximizing code sparsity. As the dictionary size increases, however, the encoder produces increasingly larger inputs for the neural network; this issue is addressed with a new variant of the Exponential Natural Evolution Strategies algorithm which adapts the dimensionality of its probability distribution along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on each game’s controls). These are still capable of achieving results that are not much worse, and occasionally superior, to the state-of-the-art in direct policy search which uses two orders of magnitude more neurons.},
journal = {Autonomous Agents and Multi-Agent Systems},
month = oct,
numpages = {23},
keywords = {Game playing, Neuroevolution, Evolutionary algorithms, Learning agent capabilities}
}

@article{10.1016/j.advengsoft.2011.03.010,
author = {Alsmadi, Izzat and Najadat, Hassan},
title = {Evaluating the change of software fault behavior with dataset attributes based on categorical correlation},
year = {2011},
issue_date = {August, 2011},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {42},
number = {8},
issn = {0965-9978},
url = {https://doi.org/10.1016/j.advengsoft.2011.03.010},
doi = {10.1016/j.advengsoft.2011.03.010},
abstract = {Utilization of data mining in software engineering has been the subject of several research papers. Majority of subjects of those paper were in making use of historical data for decision making activities such as cost estimation and product or project attributes prediction and estimation. The ability to predict software fault modules and the ability to correlate relations between faulty modules and product attributes using statistics is the subject of this paper. Correlations and relations between the attributes and the categorical variable or the class are studied through generating a pool of records from each dataset and then select two samples every time from the dataset and compare them. The correlation between the two selected records is studied in terms of changing from faulty to non-faulty or the opposite for the module defect attribute and the value change between the two records in each evaluated attribute (e.g. equal, larger or smaller). The goal was to study if there are certain attributes that are consistently affecting changing the state of the module from faulty to none, or the opposite. Results indicated that such technique can be very useful in studying the correlations between each attribute and the defect status attribute. Another prediction algorithm is developed based on statistics of the module and the overall dataset. The algorithm gave each attribute true class and faulty class predictions. We found that dividing prediction capability for each attribute into those two (i.e. correct and faulty module prediction) facilitate understanding the impact of attribute values on the class and hence improve the overall prediction relative to previous studies and data mining algorithms. Results were evaluated and compared with other algorithms and previous studies. ROC metrics were used to evaluate the performance of the developed metrics. Results from those metrics showed that accuracy or prediction performance calculated traditionally using accurately predicted records divided by the total number of records in the dataset does not necessarily give the best indicator of a good metric or algorithm predictability. Those predictions may give wrong implication if other metrics are not considered with them. The ROC metrics were able to show some other important aspects of performance or accuracy.},
journal = {Adv. Eng. Softw.},
month = aug,
pages = {535–546},
numpages = {12},
keywords = {Clustering, Correlation, Data mining, Fault prone modules, Prediction algorithms, Software mining, Software quality}
}

@article{10.1007/s11219-010-9110-y,
author = {Hsu, Chao-Jung and Huang, Chin-Yu},
title = {Comparison of weighted grey relational analysis for software effort estimation},
year = {2011},
issue_date = {March     2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9110-y},
doi = {10.1007/s11219-010-9110-y},
abstract = {In recent years, grey relational analysis (GRA), a similarity-based method, has been proposed and used in many applications. However, we found that most traditional GRA methods only consider nonweighted similarity for predicting software development effort. In fact, nonweighted similarity may cause biased predictions, because each feature of a project may have a different degree of relevance to the development effort. Therefore, this paper proposes six weighted methods, including nonweighted, distance-based, correlative, linear, nonlinear, and maximal weights, to be integrated into GRA for software effort estimation. Numerical examples and sensitivity analyses based on four public datasets are used to show the performance of the proposed methods. The experimental results indicate that the weighted GRA can improve estimation accuracy and reliability from the nonweighted GRA. The results also demonstrate that the weighted GRA performs better than other estimation techniques and published results. In summary, we can conclude that weighted GRA can be a viable and alternative method for predicting software development effort.},
journal = {Software Quality Journal},
month = mar,
pages = {165–200},
numpages = {36},
keywords = {Grey relational analysis (GRA), Software cost, Software development effort, Software effort estimation, Weighted assignment}
}

@article{10.1016/j.eswa.2017.05.069,
author = {Mrquez-Chamorro, Alfonso E. and Resinas, Manuel and Ruiz-Corts, Antonio and Toro, Miguel},
title = {Run-time prediction of business process indicators using evolutionary decision rules},
year = {2017},
issue_date = {November 2017},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {87},
number = {C},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2017.05.069},
doi = {10.1016/j.eswa.2017.05.069},
abstract = {An evolutionary rule-based system for the prediction of BP indicators is proposed.Generated decision rules can be easily interpreted by users.A software stack to support the stages of a predictive monitoring system is presented. Predictive monitoring of business processes is a challenging topic of process mining which is concerned with the prediction of process indicators of running process instances. The main value of predictive monitoring is to provide information in order to take proactive and corrective actions to improve process performance and mitigate risks in real time. In this paper, we present an approach for predictive monitoring based on the use of evolutionary algorithms. Our method provides a novel event window-based encoding and generates a set of decision rules for the run-time prediction of process indicators according to event log properties. These rules can be interpreted by users to extract further insight of the business processes while keeping a high level of accuracy. Furthermore, a full software stack consisting of a tool to support the training phase and a framework that enables the integration of run-time predictions with business process management systems, has been developed. Obtained results show the validity of our proposal for two large real-life datasets: BPI Challenge 2013 and IT Department of Andalusian Health Service (SAS).},
journal = {Expert Syst. Appl.},
month = nov,
pages = {1–14},
numpages = {14},
keywords = {Business process indicator, Business process management, Evolutionary algorithm, Predictive monitoring, Process mining}
}

@article{10.1016/j.jss.2007.07.040,
author = {Elish, Karim O. and Elish, Mahmoud O.},
title = {Predicting defect-prone software modules using support vector machines},
year = {2008},
issue_date = {May, 2008},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {81},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2007.07.040},
doi = {10.1016/j.jss.2007.07.040},
abstract = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.},
journal = {J. Syst. Softw.},
month = may,
pages = {649–660},
numpages = {12},
keywords = {Defect-prone modules, Predictive models, Software metrics, Support vector machines}
}

@article{10.1016/j.infsof.2019.03.014,
author = {Yadav, Asmita and Singh, Sandeep Kumar and Suri, Jasjit S.},
title = {Ranking of software developers based on expertise score for bug triaging},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.014},
doi = {10.1016/j.infsof.2019.03.014},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1–17},
numpages = {17},
keywords = {Bug repository, Bug triaging, Developer expertise, Bug assignment, Bug reports, Bug tossing, Developer contribution assessment, Open source software (OSS), Software process, Software metrics, SUMa, DOBb, ASc, CLSd, Pe, NBf, SVMg, COMh, REPi, EMj, RULk, DNl, WBFSm, SRn, PRo, PTp, TSq, PRD r, VD s, TD t, BNu, Cmv}
}

@article{10.1016/j.infsof.2019.03.001,
author = {Zhang, Wen and Li, Ziqiang and Wang, Qing and Li, Juan},
title = {FineLocator: A novel approach to method-level fine-grained bug localization by query expansion},
year = {2019},
issue_date = {Jun 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {110},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.03.001},
doi = {10.1016/j.infsof.2019.03.001},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {121–135},
numpages = {15},
keywords = {Method-level bug localization, Query expansion, Semantic similarity, Temporal proximity, Call dependency}
}

@inproceedings{10.5555/257572.257592,
author = {Briand, Lionel C. and Thomas, William M. and Hetmanski, Christopher J.},
title = {Modeling and managing risk early in software development},
year = {1993},
isbn = {0897915887},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the 15th International Conference on Software Engineering},
pages = {55–65},
numpages = {11},
location = {Baltimore, Maryland, USA},
series = {ICSE '93}
}

@inproceedings{10.1145/2989238.2989242,
author = {Dehghan, Ali and Blincoe, Kelly and Damian, Daniela},
title = {A hybrid model for task completion effort estimation},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989242},
doi = {10.1145/2989238.2989242},
abstract = {Predicting time and effort of software task completion has been an active area of research for a long time. Previous studies have proposed predictive models based on either text data or metadata of software tasks to estimate either completion time or completion effort of software tasks, but there is a lack of focus in the literature on integrating all sets of attributes together to achieve better performing models. We first apply the previously proposed models on the datasets of two IBM commercial projects called RQM and RTC to find the best performing model in predicting task completion effort on each set of attributes. Then we propose an approach to create a hybrid model based on selected individual predictors to achieve more accurate and stable results in early prediction of task completion effort and to make sure the model is not bounded to some attributes and consequently is adoptable to a larger number of tasks. Categorizing task completion effort values into Low and High labels based on their measured median value, we show that our hybrid model provides 3-8% more accuracy in early prediction of task completion effort compared to the best individual predictors.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {22–28},
numpages = {7},
keywords = {Mining software repositories, effort estimation, ensemble learning, machine learning, task completion effort},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@article{10.1145/2856821,
author = {Capraro, Maximilian and Riehle, Dirk},
title = {Inner Source Definition, Benefits, and Challenges},
year = {2016},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2856821},
doi = {10.1145/2856821},
abstract = {Inner Source (IS) is the use of open source software development practices and the establishment of an open source-like culture within organizations. The organization may still develop proprietary software but internally opens up its development. A steady stream of scientific literature and practitioner reports indicates the interest in this research area. However, the research area lacks a systematic assessment of known research work: No model exists that defines IS thoroughly. Various case studies provide insights into IS programs in the context of specific organizations but only few publications apply a broader perspective. To resolve this, we performed an extensive literature survey and analyzed 43 IS related publications plus additional background literature. Using qualitative data analysis methods, we developed a model of the elements that constitute IS. We present a classification framework for IS programs and projects and apply it to lay out a map of known IS endeavors. Further, we present qualitative models summarizing the benefits and challenges of IS adoption. The survey provides the first broad review of IS literature and systematic arrangement of IS research results.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {67},
numpages = {36},
keywords = {Inner source, internal open source, open collaboration, software development efficiency, software development methods, software development productivity, software engineering, taxonomy}
}

@article{10.1016/j.cose.2017.02.007,
author = {Stergiopoulos, George and Katsaros, Panayiotis and Gritzalis, Dimitris},
title = {Program analysis with risk-based classification of dynamic invariants for logical error detection},
year = {2017},
issue_date = {November 2017},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {71},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2017.02.007},
doi = {10.1016/j.cose.2017.02.007},
abstract = {The logical errors in programs causing deviations from the intended functionality cannot be detected by automated source code analysis, which mainly focuses on known defects and code vulnerabilities. To this end, we introduce a combination of analysis techniques implemented in a proof-of-concept prototype called PLATO. First, a set of dynamic invariants is inferred from the source code that represents the program's logic. The code is instrumented with assertions from the invariants, which are subsequently valuated through the program's symbolic execution. The findings are ranked using a fuzzy logic system with two scales characterizing their impact: (i) a Severity scale for the execution paths' characteristics and their Information Gain, (ii) a Reliability scale based on the measured Computational Density. Real, as well as synthetic applications with at least four different types of logical errors were analyzed. The method's effectiveness was assessed based on a dataset from 25 experiments. Albeit not without restrictions, the proposed automated analysis seems able to detect a wide variety of logical errors, while it filters out the false positives.},
journal = {Comput. Secur.},
month = nov,
pages = {36–50},
numpages = {15},
keywords = {Computational density, Dynamic invariants, Fuzzy logic, Information gain, Logical errors, Symbolic execution}
}

@article{10.1007/s10664-021-10026-0,
author = {Silva, Camila Costa and Galster, Matthias and Gilson, Fabian},
title = {Topic modeling in software engineering research},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10026-0},
doi = {10.1007/s10664-021-10026-0},
abstract = {Topic modeling using models such as Latent Dirichlet Allocation (LDA) is a text mining technique to extract human-readable semantic “topics” (i.e., word clusters) from a corpus of textual documents. In software engineering, topic modeling has been used to analyze textual data in empirical studies (e.g., to find out what developers talk about online), but also to build new techniques to support software engineering tasks (e.g., to support source code comprehension). Topic modeling needs to be applied carefully (e.g., depending on the type of textual data analyzed and modeling parameters). Our study aims at describing how topic modeling has been applied in software engineering research with a focus on four aspects: (1) which topic models and modeling techniques have been applied, (2) which textual inputs have been used for topic modeling, (3) how textual data was “prepared” (i.e., pre-processed) for topic modeling, and (4) how generated topics (i.e., word clusters) were named to give them a human-understandable meaning. We analyzed topic modeling as applied in 111 papers from ten highly-ranked software engineering venues (five journals and five conferences) published between 2009 and 2020. We found that (1) LDA and LDA-based techniques are the most frequent topic modeling techniques, (2) developer communication and bug reports have been modelled most, (3) data pre-processing and modeling parameters vary quite a bit and are often vaguely reported, and (4) manual topic naming (such as deducting names based on frequent words in a topic) is common.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {62},
keywords = {Topic modeling, Text mining, Natural language processing, Literature analysis}
}

@inproceedings{10.1145/2901739.2901740,
author = {Guo, Jin and Rahimi, Mona and Cleland-Huang, Jane and Rasin, Alexander and Hayes, Jane Huffman and Vierhauser, Michael},
title = {Cold-start software analytics},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901740},
doi = {10.1145/2901739.2901740},
abstract = {Software project artifacts such as source code, requirements, and change logs represent a gold-mine of actionable information. As a result, software analytic solutions have been developed to mine repositories and answer questions such as "who is the expert?," "which classes are fault prone?," or even "who are the domain experts for these fault-prone classes?" Analytics often require training and configuring in order to maximize performance within the context of each project. A cold-start problem exists when a function is applied within a project context without first configuring the analytic functions on project-specific data. This scenario exists because of the non-trivial effort necessary to instrument a project environment with candidate tools and algorithms and to empirically evaluate alternate configurations. We address the cold-start problem by comparatively evaluating 'best-of-breed' and 'profile-driven' solutions, both of which reuse known configurations in new project contexts. We describe and evaluate our approach against 20 project datasets for the three analytic areas of artifact connectivity, fault-prediction, and finding the expert, and show that the best-of-breed approach outperformed the profile-driven approach in all three areas; however, while it delivered acceptable results for artifact connectivity and find the expert, both techniques underperformed for cold-start fault prediction.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {142–153},
numpages = {12},
keywords = {cold-start, configuration, software analytics},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.5555/1565098.1565108,
author = {Yazid, Houria and Lounis, Hakim},
title = {Exploring an Open Source Data Mining Environment for Software Product Quality Decision Making},
year = {2006},
isbn = {1586036408},
publisher = {IOS Press},
address = {NLD},
abstract = {Software metrics play a major role in predicting software quality; they help project managers in decision-making. Indeed, software metrics provide a quantitative approach allowing the control and the improvement of the development process including the maintenance. The ISO/IEC international standard (14598) on software product quality states, “Internal metrics are of little value unless there is evidence that they are related to external quality”. Many empirical prediction models are presented in the literature; their goal is to investigate the relationship between internal metrics and external qualities, in order to assess software quality. In this paper, we explore different machine-learning (ML) algorithms provided by an open source data-mining environment. We analyse their capacities to produce accurate and usable predictive models.},
booktitle = {Proceedings of the 2006 Conference on Knowledge-Based Software Engineering: Proceedings of the Seventh Joint Conference on Knowledge-Based Software Engineering},
pages = {73–82},
numpages = {10},
keywords = {Corrective Maintenance, Fault-Proneness, Machine-Learning, Metrics, Reusability, Software Product Quality}
}

@inproceedings{10.5555/3291656.3291668,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: predicting which node will fail when on supercomputers},
year = {2018},
publisher = {IEEE Press},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {HPC, failure analysis, machine learning},
location = {Dallas, Texas},
series = {SC '18}
}

@article{10.3233/JIFS-210246,
author = {Alagarsamy, Ramachandran and Arunpraksh, R. and Ganapathy, Sannasi and Rajagopal, Aghila and Kavitha, R.J.},
title = {A fuzzy content recommendation system using similarity analysis, content ranking and clustering},
year = {2021},
issue_date = {2021},
publisher = {IOS Press},
address = {NLD},
volume = {41},
number = {6},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-210246},
doi = {10.3233/JIFS-210246},
abstract = {Recently, the e-learners are drastically increased from the last two decades. Everything is learnt through internet without help of the tutor as well. For this purpose, the e-learners are required more e-learning applications that are able to supply optimal and satisfied data based on their capability. No content recommendation system is available for recommending suitable contents to the learners. For this purpose, this paper proposes a new semantic and fuzzy aware content recommendation system for retrieving the suitable content for the users. In this content recommendation system, we propose two content pre-processing algorithms namely Target Keyword based Data Pre-processing Algorithm (TKDPA) and Intelligent Anova-T Residual Algorithm (IAATRA) for selecting the more relevant features from the document. Moreover, a new Fuzzy rule based Similarity Matching algorithm (FRSMA) is proposed and used in this system for finding the similarity between the two terms and also rank them by using the newly proposed Similarity and Temporal aware Weighted Document Ranking Algorithm (STWDRA). In addition, a content clustering process is also incorporated for gathering relevant content. Finally, a new Fuzzy, Target Keyword and Similarity Score based Content Recommendation Algorithm (FTKSCRA) is also proposed for recommending the more relevant content to the learners accurately. The experiments have been conducted for evaluating the proposed content recommendation system and proved as better than the existing recommendation systems in terms of precision, recall, f-measure and prediction accuracy.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {6429–6441},
numpages = {13},
keywords = {Fuzzy logic, content ranking, clustering, content recommendation, semantic analysis, fuzzy rules and annova-T}
}

@inproceedings{10.1109/SC.2018.00012,
author = {Das, Anwesha and Mueller, Frank and Hargrove, Paul and Roman, Eric and Baden, Scott},
title = {Doomsday: predicting which node will fail when on supercomputers},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00012},
doi = {10.1109/SC.2018.00012},
abstract = {Predicting which node will fail and how soon remains a challenge for HPC resilience, yet may pave the way to exploiting proactive remedies before jobs fail. Not only for increasing scalability up to exascale systems but even for contemporary supercomputer architectures does it require substantial efforts to distill anomalous events from noisy raw logs. To this end, we propose a novel phrase extraction mechanism called TBP (time-based phrases) to pin-point node failures, which is unprecedented. Our study, based on real system data and statistical machine learning, demonstrates the feasibility to predict which specific node will fail in Cray systems. TBP achieves no less than 83% recall rates with lead times as high as 2 minutes. This opens up the door for enhancing prediction lead times for supercomputing systems in general, thereby facilitating efficient usage of both computing capacity and power in large scale production systems.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {9},
numpages = {14},
keywords = {HPC, failure analysis, machine learning},
location = {Dallas, Texas},
series = {SC '18}
}

@article{10.1016/j.infsof.2018.02.005,
author = {Agrawal, Amritanshu and Fu, Wei and Menzies, Tim},
title = {What is wrong with topic modeling? And how to fix it using search-based software engineering},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.02.005},
doi = {10.1016/j.infsof.2018.02.005},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {74–88},
numpages = {15},
keywords = {Topic modeling, Stability, LDA, Tuning, Differential evolution}
}

@article{10.1007/s11219-013-9199-x,
author = {Parsa, Saeed and Vahidi-Asl, Mojtaba and Asadi-Aghbolaghi, Maryam},
title = {Hierarchy-Debug: a scalable statistical technique for fault localization},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9199-x},
doi = {10.1007/s11219-013-9199-x},
abstract = {Considering the fact that faults may be revealed as undesired mutual effect of program predicates on each other, a new approach for localizing latent bugs, namely Hierarchy-Debug, is presented in this paper. To analyze the vertical effect of predicates on each other and on program termination status, the predicates are fitted into a logistic lasso model. To support scalability, a hierarchical clustering algorithm is applied to cluster the predicates according to their presence in different executions. Considering each cluster as a pseudo-predicate, a distinct lasso model is built for intermediate levels of the hierarchy. Then, we apply a majority voting technique to score the predicates according to their lasso coefficients at different levels of the hierarchy. The predicates with relatively higher scores are ranked as fault relevant predicates. To provide the context of failure, faulty sub-paths are identified as sequences of fault relevant predicates. The grouping effect of Hierarchy-Debug helps programmers to detect multiple bugs. Four case studies have been designed to evaluate the proposed approach on three well-known test suites, Space, Siemens, and Bash. The evaluations show that Hierarchy-Debug produces more precise results compared with prior fault localization techniques on the subject programs.},
journal = {Software Quality Journal},
month = sep,
pages = {427–466},
numpages = {40},
keywords = {Fault relevant predicates, Faulty sub-paths, Hierarchical clustering, Lasso method, Majority voting, Multiple bugs, Statistical bug localization}
}

@article{10.1007/s11390-019-1953-5,
author = {Xi, Sheng-Qu and Yao, Yuan and Xiao, Xu-Sheng and Xu, Feng and Lv, Jian},
title = {Bug Triaging Based on Tossing Sequence Modeling},
year = {2019},
issue_date = {Sep 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {34},
number = {5},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-019-1953-5},
doi = {10.1007/s11390-019-1953-5},
abstract = {Bug triaging, which routes the bug reports to potential fixers, is an integral step in software development and maintenance. To make bug triaging more efficient, many researchers propose to adopt machine learning and information retrieval techniques to identify some suitable fixers for a given bug report. However, none of the existing proposals simultaneously take into account the following three aspects that matter for the efficiency of bug triaging: 1) the textual content in the bug reports, 2) the metadata in the bug reports, and 3) the tossing sequence of the bug reports. To simultaneously make use of the above three aspects, we propose iTriage which first adopts a sequence-to-sequence model to jointly learn the features of textual content and tossing sequence, and then uses a classification model to integrate the features from textual content, metadata, and tossing sequence. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches.},
journal = {J. Comput. Sci. Technol.},
month = sep,
pages = {942–956},
numpages = {15},
keywords = {bug triaging, tossing sequence, software repository minings}
}

@inproceedings{10.1145/3092703.3098237,
author = {Katz, Deborah S.},
title = {Understanding intended behavior using models of low-level signals},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098237},
doi = {10.1145/3092703.3098237},
abstract = {As software systems increase in complexity and operate with less human supervision, it becomes more difficult to use traditional techniques to detect when software is not behaving as intended. Furthermore, many systems operating today are nondeterministic and operate in unpredictable environments, making it difficult to even define what constitutes correct behavior. I propose a family of novel techniques to model the behavior of executing programs using low-level signals collected during executions. The models provide a basis for predicting whether an execution of the program or program unit under test represents intended behavior. I have demonstrated success with these techniques for detecting faulty and unexpected behavior on small programs. I propose to extend the work to smaller units of large, complex programs.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {424–427},
numpages = {4},
keywords = {Oracle problem, Software quality, Software testing},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@article{10.1007/s11390-020-0526-y,
author = {Xu, Yi-Sen and Jia, Xiang-Yang and Wu, Fan and Li, Lingbo and Xuan, Ji-Feng},
title = {Automatically Identifying Calling-Prone Higher-Order Functions of Scala Programs to Assist Testers},
year = {2020},
issue_date = {Nov 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {35},
number = {6},
issn = {1000-9000},
url = {https://doi.org/10.1007/s11390-020-0526-y},
doi = {10.1007/s11390-020-0526-y},
abstract = {For the rapid development of internetware, functional programming languages, such as Haskell and Scala, can be used to implement complex domain-specific applications. In functional programming languages, a higher-order function is a function that takes functions as parameters or returns a function. Using higher-order functions in programs can increase the generality and reduce the redundancy of source code. To test a higher-order function, a tester needs to check the requirements and write another function as the test input. However, due to the complex structure of higher-order functions, testing higher-order functions is a time-consuming and labor-intensive task. Testers have to spend an amount of manual effort in testing all higher-order functions. Such testing is infeasible if the time budget is limited, such as a period before a project release. In practice, not every higher-order function is actually called. We refer to higher-order functions that are about to be called as calling-prone ones. Calling-prone higher-order functions should be tested first. In this paper, we propose an automatic approach, namely Phof, which predicts whether a higher-order function of Scala programs will be called in the future, i.e., identifying calling-prone higher-order functions. Our approach can assist testers to reduce the number of higher-order functions of Scala programs under test. In Phof, we extracted 24 features from source code and logs to train a predictive model based on known higher-order function calls. We empirically evaluated our approach on 4 832 higher-order functions from 27 real-world Scala projects. Experimental results show that Phof based on the random forest algorithm and the Synthetic Minority Oversampling Technique Processing strategy (SMOTE) performs well in the prediction of calls of higher-order functions. Our work can be used to support the scheduling of limited test resources.},
journal = {J. Comput. Sci. Technol.},
month = nov,
pages = {1278–1294},
numpages = {17},
keywords = {higher-order function, testing tool, test management, Scala program, internetware}
}

@article{10.1007/s10845-019-01511-x,
author = {Goyal, Deepam and Choudhary, Anurag and Pabla, B. S. and Dhami, S. S.},
title = {Support vector machines based non-contact fault diagnosis system for bearings},
year = {2020},
issue_date = {Jun 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {31},
number = {5},
issn = {0956-5515},
url = {https://doi.org/10.1007/s10845-019-01511-x},
doi = {10.1007/s10845-019-01511-x},
abstract = {Bearing defects have been accepted as one of the major causes of failure in rotating machinery. It is important to identify and diagnose the failure behavior of bearings for the reliable operation of equipment. In this paper, a low-cost non-contact vibration sensor has been developed for detecting the faults in bearings. The supervised learning method, support vector machine (SVM), has been employed as a tool to validate the effectiveness of the developed sensor. Experimental vibration data collected for different bearing defects under various loading and running conditions have been analyzed to develop a system for diagnosing the faults for machine health monitoring. Fault diagnosis has been accomplished using discrete wavelet transform for denoising the signal. Mahalanobis distance criteria has been employed for selecting the strongest feature on the extracted relevant features. Finally, these selected features have been passed to the SVM classifier for identifying and classifying the various bearing defects. The results reveal that the vibration signatures obtained from developed non-contact sensor compare well with the accelerometer data obtained under the same conditions. A developed sensor is a promising tool for detecting the bearing damage and identifying its class. SVM results have established the effectiveness of the developed non-contact sensor as a vibration measuring instrument which makes the developed sensor a cost-effective tool for the condition monitoring of rotating machines.},
journal = {J. Intell. Manuf.},
month = jun,
pages = {1275–1289},
numpages = {15},
keywords = {Support vector machines (SVM), Vibration signatures, Bearings, Discrete wavelet transform (DWT), Non-contact fault diagnosis}
}

@inproceedings{10.5555/2820518.2820552,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Characterization and prediction of issue-related risks in software projects},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Identifying risks relevant to a software project and planning measures to deal with them are critical to the success of the project. Current practices in risk assessment mostly rely on high-level, generic guidance or the subjective judgements of experts. In this paper, we propose a novel approach to risk assessment using historical data associated with a software project. Specifically, our approach identifies patterns of past events that caused project delays, and uses this knowledge to identify risks in the current state of the project. A set of risk factors characterizing "risky" software tasks (in the form of issues) were extracted from five open source projects: Apache, Duraspace, JBoss, Moodle, and Spring. In addition, we performed feature selection using a sparse logistic regression model to select risk factors with good discriminative power. Based on these risk factors, we built predictive models to predict if an issue will cause a project delay. Our predictive models are able to predict both the risk impact (i.e. the extend of the delay) and the likelihood of a risk occurring. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 48%--81% precision, 23%--90% recall, 29%--71% F-measure, and 70%--92% Area Under the ROC Curve. Our predictive models also have low error rates: 0.39--0.75 for Macro-averaged Mean Cost-Error and 0.7--1.2 for Macro-averaged Mean Absolute Error.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {280–291},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1016/j.infsof.2008.03.004,
author = {Chen, Li-Wei and Huang, Sun-Jen},
title = {Accuracy and efficiency comparisons of single- and multi-cycled software classification models},
year = {2009},
issue_date = {January, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.03.004},
doi = {10.1016/j.infsof.2008.03.004},
abstract = {Software classification models have been regarded as an essential support tool in performing measurement and analysis processes. Most of the established models are single-cycled in the model usage stage, and thus require the measurement data of all the model's variables to be simultaneously collected and utilized for classifying an unseen case within only a single decision cycle. Conversely, the multi-cycled model allows the measurement data of all the model's variables to be gradually collected and utilized for such a classification within more than one decision cycle, and thus intuitively seems to have better classification efficiency but poorer classification accuracy. Software project managers often have difficulties in choosing an appropriate classification model that is better suited to their specific environments and needs. However, this important topic is not adequately explored in software measurement and analysis literature. By using an industrial software measurement dataset of NASA KC2, this paper explores the quantitative performance comparisons of the classification accuracy and efficiency of the discriminant analysis (DA)- and logistic regression (LR)-based single-cycled models and the decision tree (DT)-based (C4.5 and ECHAID algorithms) multi-cycled models. The experimental results suggest that the re-appraisal cost of the Type I MR, the software failure cost of Type II MR and the data collection cost of software measurements should be considered simultaneously when choosing an appropriate classification model.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {173–181},
numpages = {9},
keywords = {Classification accuracy and efficiency, Multi-cycle, Single-cycle, Software classification model, Software measurement and analysis}
}

@article{10.1016/j.infsof.2019.04.009,
author = {Mohan, M. and Greer, D.},
title = {Using a many-objective approach to investigate automated refactoring},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.009},
doi = {10.1016/j.infsof.2019.04.009},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {83–101},
numpages = {19},
keywords = {Search-based software engineering, Maintenance, Automated refactoring, Refactoring tools, Software quality, Many-objective optimisation, Genetic algorithms}
}

@article{10.1016/j.eswa.2011.08.063,
author = {Bdiri, Taoufik and Bouguila, Nizar},
title = {Positive vectors clustering using inverted Dirichlet finite mixture models},
year = {2012},
issue_date = {February, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {2},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.08.063},
doi = {10.1016/j.eswa.2011.08.063},
abstract = {In this work we present an unsupervised algorithm for learning finite mixture models from multivariate positive data. Indeed, this kind of data appears naturally in many applications, yet it has not been adequately addressed in the past. This mixture model is based on the inverted Dirichlet distribution, which offers a good representation and modeling of positive non-Gaussian data. The proposed approach for estimating the parameters of an inverted Dirichlet mixture is based on the maximum likelihood (ML) using Newton Raphson method. We also develop an approach, based on the minimum message length (MML) criterion, to select the optimal number of clusters to represent the data using such a mixture. Experimental results are presented using artificial histograms and real data sets. The challenging problem of software modules classification is investigated within the proposed statistical framework, also.},
journal = {Expert Syst. Appl.},
month = feb,
pages = {1869–1882},
numpages = {14},
keywords = {Data clustering, Inverted Dirichlet distribution, MML, Maximum likelihood, Mixture models, Unsupervised learning}
}

@inproceedings{10.1145/3267323.3268951,
author = {McKnight, Christopher and Goldberg, Ian},
title = {Style Counsel: Seeing the (Random) Forest for the Trees in Adversarial Code Stylometry},
year = {2018},
isbn = {9781450359894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267323.3268951},
doi = {10.1145/3267323.3268951},
abstract = {The results of recent experiments have suggested that code stylometry can successfully identify the author of short programs from among hundreds of candidates with up to 98% precision. This potential ability to discern the programmer of a code sample from a large group of possible authors could have concerning consequences for the open-source community at large, particularly those contributors that may wish to remain anonymous. Recent international events have suggested the developers of certain anti-censorship and anti-surveillance tools are being targeted by their governments and forced to delete their repositories or face prosecution. In light of this threat to the freedom and privacy of individual programmers around the world, we devised a tool, Style Counsel, to aid programmers in obfuscating their inherent style and imitating another, overt, author's style in order to protect their anonymity from this forensic technique. Our system utilizes the implicit rules encoded in the decision points of a random forest ensemble in order to derive a set of recommendations to present to the user detailing how to achieve this obfuscation and mimicry attack.},
booktitle = {Proceedings of the 2018 Workshop on Privacy in the Electronic Society},
pages = {138–142},
numpages = {5},
keywords = {adversarial machine learning, programmer privacy, software authorship attribution, source code stylometry},
location = {Toronto, Canada},
series = {WPES'18}
}

@article{10.1016/j.eswa.2013.05.044,
author = {L\'{o}Pez-Chau, Asdr\'{u}Bal and Cervantes, Jair and L\'{o}Pez-Garc\'{\i}A, Lourdes and Lamont, Farid Garc\'{\i}A},
title = {Fisher's decision tree},
year = {2013},
issue_date = {November, 2013},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {40},
number = {16},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2013.05.044},
doi = {10.1016/j.eswa.2013.05.044},
abstract = {Univariate decision trees are classifiers currently used in many data mining applications. This classifier discovers partitions in the input space via hyperplanes that are orthogonal to the axes of attributes, producing a model that can be understood by human experts. One disadvantage of univariate decision trees is that they produce complex and inaccurate models when decision boundaries are not orthogonal to axes. In this paper we introduce the Fisher's Tree, it is a classifier that takes advantage of dimensionality reduction of Fisher's linear discriminant and uses the decomposition strategy of decision trees, to come up with an oblique decision tree. Our proposal generates an artificial attribute that is used to split the data in a recursive way. The Fisher's decision tree induces oblique trees whose accuracy, size, number of leaves and training time are competitive with respect to other decision trees reported in the literature. We use more than ten public available data sets to demonstrate the effectiveness of our method.},
journal = {Expert Syst. Appl.},
month = nov,
pages = {6283–6291},
numpages = {9},
keywords = {C4.5, Fisher's linear discriminant, Oblique decision tree}
}

@inproceedings{10.1109/IPSN.2018.00049,
author = {Alippi, Cesare and Disabato, Simone and Roveri, Manuel},
title = {Moving convolutional neural networks to embedded systems: the alexnet and VGG-16 case},
year = {2018},
isbn = {9781538652985},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/IPSN.2018.00049},
doi = {10.1109/IPSN.2018.00049},
abstract = {Execution of deep learning solutions is mostly restricted to high performing computing platforms, e.g., those endowed with GPUs or FPGAs, due to the high demand on computation and memory such solutions require. Despite the fact that dedicated hardware is nowadays subject of research and effective solutions exist, we envision a future where deep learning solutions -here Convolutional Neural Networks (CNNs)- are mostly executed by low-cost off-the shelf embedded platforms already available in the market.This paper moves in this direction and aims at filling the gap between CNNs and embedded systems by introducing a methodology for the design and porting of CNNs to limited in resources embedded systems. In order to achieve this goal we employ approximate computing techniques to reduce the computational load and memory occupation of the deep learning architecture by compromising accuracy with memory and computation.The proposed methodology has been validated on two well-know CNNs, i.e., AlexNet and VGG-16, applied to an image-recognition application and ported to two relevant off-the-shelf embedded platforms.},
booktitle = {Proceedings of the 17th ACM/IEEE International Conference on Information Processing in Sensor Networks},
pages = {212–223},
numpages = {12},
keywords = {approximate computing, convolutional neural networks, deep learning, embedded systems},
location = {Porto, Portugal},
series = {IPSN '18}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00020,
author = {Zheng, Yunhui and Pujar, Saurabh and Lewis, Burn and Buratti, Luca and Epstein, Edward and Yang, Bo and Laredo, Jim and Morari, Alessandro and Su, Zhong},
title = {D2A: a dataset built for AI-based vulnerability detection methods using differential analysis},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00020},
doi = {10.1109/ICSE-SEIP52600.2021.00020},
abstract = {Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {111–120},
numpages = {10},
keywords = {auto-labeler, dataset, vulnerability detection},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1007/s11219-011-9152-9,
author = {Siegmund, Norbert and Rosenm\"{u}ller, Marko and Kuhlemann, Martin and K\"{a}stner, Christian and Apel, Sven and Saake, Gunter},
title = {SPL Conqueror: Toward optimization of non-functional properties in software product lines},
year = {2012},
issue_date = {September 2012},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3–4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-011-9152-9},
doi = {10.1007/s11219-011-9152-9},
abstract = {A software product line (SPL) is a family of related programs of a domain. The programs of an SPL are distinguished in terms of features, which are end-user visible characteristics of programs. Based on a selection of features, stakeholders can derive tailor-made programs that satisfy functional requirements. Besides functional requirements, different application scenarios raise the need for optimizing non-functional properties of a variant. The diversity of application scenarios leads to heterogeneous optimization goals with respect to non-functional properties (e.g., performance vs. footprint vs. energy optimized variants). Hence, an SPL has to satisfy different and sometimes contradicting requirements regarding non-functional properties. Usually, the actually required non-functional properties are not known before product derivation and can vary for each application scenario and customer. Allowing stakeholders to derive optimized variants requires us to measure non-functional properties after the SPL is developed. Unfortunately, the high variability provided by SPLs complicates measurement and optimization of non-functional properties due to a large variant space. With SPL Conqueror, we provide a holistic approach to optimize non-functional properties in SPL engineering. We show how non-functional properties can be qualitatively specified and quantitatively measured in the context of SPLs. Furthermore, we discuss the variant-derivation process in SPL Conqueror that reduces the effort of computing an optimal variant. We demonstrate the applicability of our approach by means of nine case studies of a broad range of application domains (e.g., database management and operating systems). Moreover, we show that SPL Conqueror is implementation and language independent by using SPLs that are implemented with different mechanisms, such as conditional compilation and feature-oriented programming.},
journal = {Software Quality Journal},
month = sep,
pages = {487–517},
numpages = {31},
keywords = {Feature-oriented software development, Measurement and optimization, Non-functional properties, SPL Conqueror, Software product lines}
}

@article{10.5555/2699306.2699308,
author = {Brumen, Bo\v{s}tjan and H\"{o}lbl, Marko and Harej Pulko, Katja and Welzer, Tatjana and Heri\v{c}ko, Marjan and Juri\v{c}, Matja\v{z} B. and Jaakkola, Hannu},
title = {Learning Process Termination Criteria},
year = {2012},
issue_date = {October 2012},
publisher = {IOS Press},
address = {NLD},
volume = {23},
number = {4},
issn = {0868-4952},
abstract = {In a supervised learning, the relationship between the available data and the performance (what is learnt) is not well understood. How much data to use, or when to stop the learning process, are the key questions.In the paper, we present an approach for an early assessment of the extracted knowledge (classification models) in the terms of performance (accuracy). The key questions are answered by detecting the point of convergence, i.e., where the classification model's performance does not improve any more even when adding more data items to the learning set. For the learning process termination criteria we developed a set of equations for detection of the convergence that follow the basic principles of the learning curve. The developed solution was evaluated on real datasets. The results of the experiment prove that the solution is well-designed: the learning process stopping criteria are not subjected to local variance and the convergence is detected where it actually has occurred.},
journal = {Informatica},
month = oct,
pages = {521–536},
numpages = {16},
keywords = {Accuracy, Assessment, Classification, Data Mining, Learning Curve, Learning Process}
}

@inproceedings{10.1145/1370788.1370801,
author = {Menzies, Tim and Turhan, Burak and Bener, Ayse and Gay, Gregory and Cukic, Bojan and Jiang, Yue},
title = {Implications of ceiling effects in defect predictors},
year = {2008},
isbn = {9781605580364},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370788.1370801},
doi = {10.1145/1370788.1370801},
abstract = {Context: There are many methods that input static code features and output a predictor for faulty code modules. These data mining methods have hit a "performance ceiling"; i.e., some inherent upper bound on the amount of information offered by, say, static code features when identifying modules which contain faults. Objective: We seek an explanation for this ceiling effect. Perhaps static code features have "limited information content"; i.e. their information can be quickly and completely discovered by even simple learners. Method:An initial literature review documents the ceiling effect in other work. Next, using three sub-sampling techniques (under-, over-, and micro-sampling), we look for the lower useful bound on the number of training instances. Results: Using micro-sampling, we find that as few as 50 instances yield as much information as larger training sets. Conclusions: We have found much evidence for the limited information hypothesis. Further progress in learning defect predictors may not come from better algorithms. Rather, we need to be improving the information content of the training data, perhaps with case-based reasoning methods.},
booktitle = {Proceedings of the 4th International Workshop on Predictor Models in Software Engineering},
pages = {47–54},
numpages = {8},
keywords = {defect prediction, naive bayes, over-sampling, under-sampling},
location = {Leipzig, Germany},
series = {PROMISE '08}
}

@inproceedings{10.1145/3377811.3380344,
author = {Brindescu, Caius and Ahmed, Iftekhar and Leano, Rafael and Sarma, Anita},
title = {Planning for untangling: predicting the difficulty of merge conflicts},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380344},
doi = {10.1145/3377811.3380344},
abstract = {Merge conflicts are inevitable in collaborative software development and are disruptive. When they occur, developers have to stop their current work, understand the conflict and the surrounding code, and plan an appropriate resolution. However, not all conflicts are equally problematic---some can be easily fixed, while others might be complicated enough to need multiple people. Currently, there is not much support to help developers plan their conflict resolution. In this work, we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution. The ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts. In this work, we investigate the characteristics of difficult merge conflicts, and automatically classify them. We analyzed 6,380 conflicts across 128 java projects and found that merge conflict difficulty can be accurately predicted (AUC of 0.76) through machine learning algorithms, such as bagging.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {801–811},
numpages = {11},
keywords = {empirical analysis, merge conflict difficulty prediction, merge conflict resolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2786805.2786845,
author = {Siegmund, Norbert and Grebhahn, Alexander and Apel, Sven and K\"{a}stner, Christian},
title = {Performance-influence models for highly configurable systems},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786845},
doi = {10.1145/2786805.2786845},
abstract = {Almost every complex software system today is configurable. While configurability has many benefits, it challenges performance prediction, optimization, and debugging. Often, the influences of individual configuration options on performance are unknown. Worse, configuration options may interact, giving rise to a configuration space of possibly exponential size. Addressing this challenge, we propose an approach that derives a performance-influence model for a given configurable system, describing all relevant influences of configuration options and their interactions. Our approach combines machine-learning and sampling heuristics in a novel way. It improves over standard techniques in that it (1) represents influences of options and their interactions explicitly (which eases debugging), (2) smoothly integrates binary and numeric configuration options for the first time, (3) incorporates domain knowledge, if available (which eases learning and increases accuracy), (4) considers complex constraints among options, and (5) systematically reduces the solution space to a tractable size. A series of experiments demonstrates the feasibility of our approach in terms of the accuracy of the models learned as well as the accuracy of the performance predictions one can make with them.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {284–294},
numpages = {11},
keywords = {Performance-influence models, machine learning, sampling},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1145/3462478,
author = {Chauhan, Uttam and Shah, Apurva},
title = {Topic Modeling Using Latent Dirichlet allocation: A Survey},
year = {2021},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3462478},
doi = {10.1145/3462478},
abstract = {We are not able to deal with a mammoth text corpus without summarizing them into a relatively small subset. A computational tool is extremely needed to understand such a gigantic pool of text. Probabilistic Topic Modeling discovers and explains the enormous collection of documents by reducing them in a topical subspace. In this work, we study the background and advancement of topic modeling techniques. We first introduce the preliminaries of the topic modeling techniques and review its extensions and variations, such as topic modeling over various domains, hierarchical topic modeling, word embedded topic models, and topic models in multilingual perspectives. Besides, the research work for topic modeling in a distributed environment, topic visualization approaches also have been explored. We also covered the implementation and evaluation techniques for topic models in brief. Comparison matrices have been shown over the experimental results of the various categories of topic modeling. Diverse technical challenges and future directions have been discussed.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {145},
numpages = {35},
keywords = {Topic modeling, gibbs sampling, latent dirichlet allocation, probabilistic model, statistical inference}
}

@inproceedings{10.1145/3273934.3273944,
author = {Palma, Francis and Abdou, Tamer and Bener, Ayse and Maidens, John and Liu, Stella},
title = {An Improvement to Test Case Failure Prediction in the Context of Test Case Prioritization},
year = {2018},
isbn = {9781450365932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3273934.3273944},
doi = {10.1145/3273934.3273944},
abstract = {Aim: In this study, we aim to re-evaluate research questions on the ability of a logistic regression model proposed in a previous work to predict and prioritize the failing test cases based on some test quality metrics. Background: The process of prioritizing test cases aims to come up with a ranked test suite where test cases meeting certain criteria are prioritized. One criterion may be the ability of test cases to find faults that can be predicted a priori. Ranking test cases and executing the top-ranked test cases is particularly beneficial when projects have tight schedules and budgets. Method: We performed the comparison by first rebuilding the predictive models using the features from the original study and then we extended the original work to improve the predictive models using new features by combining with the existing ones. Results: The results of our study, using a dataset of five open-source systems, confirm that the findings from the original study hold and that our predictive models with new features outperform the original models in predicting and prioritizing the failing test cases. Conclusions: We plan to apply this method to a large-scale dataset from a large commercial enterprise project, to better demonstrate the improvement that our modified features provide and to explore the model's performance at scale.},
booktitle = {Proceedings of the 14th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {80–89},
numpages = {10},
keywords = {Logistic Regression Model, Machine Learning, Prediction, Test Case Prioritization, Test Quality Metrics},
location = {Oulu, Finland},
series = {PROMISE'18}
}

@inproceedings{10.1145/3395363.3397354,
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
title = {Reinforcement learning based curiosity-driven testing of Android applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397354},
doi = {10.1145/3395363.3397354},
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {Android app testing, functional scenario division, reinforcement learning},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3474624.3474627,
author = {Barros, Daniel and Horita, Flavio and Wiese, Igor and Silva, Kanan},
title = {A Mining Software Repository Extended Cookbook: Lessons learned from a literature review},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3474627},
doi = {10.1145/3474624.3474627},
abstract = {The main purpose of Mining Software Repositories (MSR) is to discover the latest enhancements and provide an insight into how to make improvements in a software project. In light of it, this paper updates the MSR findings of the original MSR Cookbook, by first conducting a systematic mapping study to elicit and analyze the state-of-the-art, and then proposing an extended version of the Cookbook. This extended Cookbook was built on four high-level themes, which were derived from the analysis of a list of 112 selected studies. Hence, it was used to consolidate the extended Cookbook as a contribution to practice and research in the following areas by: 1) including studies published in all available and relevant publication venues; 2) including and updating recommendations in all four high-level themes, with an increase of 84% in comments in this study when compared with the original MSR Cookbook; 3) summarizing the tools employed for each high-level theme; and 4) providing lessons learned for future studies. Thus, the extended Cookbook examined in this work can support new research projects, as upgraded recommendations and the lessons learned are available with the aid of samples and tools.},
booktitle = {Proceedings of the XXXV Brazilian Symposium on Software Engineering},
pages = {1–10},
numpages = {10},
keywords = {Artificial Intelligence, Data Mining, Mining Software Repositories, Software Repositories},
location = {Joinville, Brazil},
series = {SBES '21}
}

@article{10.1007/s10664-016-9488-7,
author = {Malhotra, Ruchika and Khanna, Megha},
title = {An empirical study for software change prediction using imbalanced data},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9488-7},
doi = {10.1007/s10664-016-9488-7},
abstract = {Software change prediction is crucial in order to efficiently plan resource allocation during testing and maintenance phases of a software. Moreover, correct identification of change-prone classes in the early phases of software development life cycle helps in developing cost-effective, good quality and maintainable software. An effective software change prediction model should equally recognize change-prone and not change-prone classes with high accuracy. However, this is not the case as software practitioners often have to deal with imbalanced data sets where instances of one type of class is much higher than the other type. In such a scenario, the minority classes are not predicted with much accuracy leading to strategic losses. This study evaluates a number of techniques for handling imbalanced data sets using various data sampling methods and MetaCost learners on six open-source data sets. The results of the study advocate the use of resample with replacement sampling method for effective imbalanced learning.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2806–2851},
numpages = {46},
keywords = {Change proneness, Data sampling, Empirical validation, Imbalanced learning, MetaCost learners, Object-oriented metrics}
}

@article{10.1007/s10664-020-09837-4,
author = {Krutauz, Andrey and Dey, Tapajit and Rigby, Peter C. and Mockus, Audris},
title = {Do code review measures explain the incidence of post-release defects? Case study replications and bayesian networks},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09837-4},
doi = {10.1007/s10664-020-09837-4},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3323–3356},
numpages = {34},
keywords = {Code review measures, Statistical models, Bayesian networks}
}

@article{10.1007/s11334-021-00393-8,
author = {Majumdar, Srijoni and Chatterjee, Nachiketa and Das, Partha Pratim and Chakrabarti, Amlan},
title = {A mathematical framework for design discovery from multi-threaded applications using neural sequence solvers},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {3},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-021-00393-8},
doi = {10.1007/s11334-021-00393-8},
abstract = {Comprehending existing multi-threaded applications effectively is a challenge without proper assistance. Research has been proposed to mine programs to extract aspects of high-level design but not much to reverse-engineer the concurrent design from multi-threaded applications. To address the same, we develop a generic mathematical model to interpret run-time non-deterministic events and encode functional as well as thread-specific behaviour in form of quantifiable features, which can be fitted into a standard solver for automated inference of design aspects from multi-threaded applications. We build a tool Dcube based on the mathematical model and use various classifiers of a machine learning framework to infer design aspects related to concurrency and resource management. We collect a dataset of 480 projects from Github, CodeProject and Stack Overflow and 3 benchmark suites—CDAC Pthreads, Open POSIX Test Suites and PARSEC 3.0 and achieve an accuracy score of around 93.71% for all the design choices.},
journal = {Innov. Syst. Softw. Eng.},
month = sep,
pages = {289–307},
numpages = {19},
keywords = {Dynamic instrumentation, Design models, Recurrent neural networks, Software maintenance, Multi-threading}
}

@inproceedings{10.1145/2597073.2597122,
author = {Gousios, Georgios and Zaidman, Andy},
title = {A dataset for pull-based development research},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597122},
doi = {10.1145/2597073.2597122},
abstract = {Pull requests form a new method for collaborating in distributed software development. To study the pull request distributed development model, we constructed a dataset of almost 900 projects and 350,000 pull requests, including some of the largest users of pull requests on Github. In this paper, we describe how the project selection was done, we analyze the selected features and present a machine learning tool set for the R statistics environment.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {368–371},
numpages = {4},
keywords = {distributed software development, empirical software engineering, pull request, pull-based development},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/3236024.3236060,
author = {Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei},
title = {Predicting Node failure in cloud service systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236060},
doi = {10.1145/3236024.3236060},
abstract = {In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {480–490},
numpages = {11},
keywords = {Failure prediction, cloud service systems, maintenance, node failure, service availability},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1007/s10664-021-09989-x,
author = {Aleti, Aldeida and Martinez, Matias},
title = {E-APR: Mapping the effectiveness of automated program repair techniques},
year = {2021},
issue_date = {Sep 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-09989-x},
doi = {10.1007/s10664-021-09989-x},
abstract = {Automated Program Repair (APR) is a fast growing area with numerous new techniques being developed to tackle one of the most challenging software engineering problems. APR techniques have shown promising results, giving us hope that one day it will be possible for software to repair itself. In this paper, we focus on the problem of objective performance evaluation of APR techniques. We introduce a new approach, Explaining Automated Program Repair (E-APR), which identifies features of buggy programs that explain why a particular instance is difficult for an APR technique. E-APR is used to examine the diversity and quality of the buggy programs used by most researchers, and analyse the strengths and weaknesses of existing APR techniques. E-APR visualises an instance space of buggy programs, with each buggy program represented as a point in the space. The instance space is constructed to reveal areas of hard and easy buggy programs, and enables the strengths and weaknesses of APR techniques to be identified.},
journal = {Empirical Softw. Engg.},
month = sep,
numpages = {30},
keywords = {Automated program repair, Software features}
}

@inproceedings{10.5555/2034161.2034166,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {A novel framework for locating software faults using latent divergences},
year = {2011},
isbn = {9783642238079},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Fault localization, i.e., identifying erroneous lines of code in a buggy program, is a tedious process, which often requires considerable manual effort and is costly. Recent years have seen much progress in techniques for automated fault localization, specifically using program spectra - executions of failed and passed test runs provide a basis for isolating the faults. Despite the progress, fault localization in large programs remains a challenging problem, because even inspecting a small fraction of the lines of code in a large problem can require substantial manual effort. This paper presents a novel framework for fault localization based on latent divergences - an effective method for feature selection in machine learning. Our insight is that the problem of fault localization can be reduced to the problem of feature selection, where lines of code correspond to features. We also present an experimental evaluation of our framework using the Siemens suite of subject programs, which are a standard benchmark for studying fault localization techniques in software engineering. The results show that our framework enables more accurate fault localization than existing techniques.},
booktitle = {Proceedings of the 2011 European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part III},
pages = {49–64},
numpages = {16},
location = {Athens, Greece},
series = {ECML PKDD'11}
}

@inproceedings{10.1145/3196398.3196444,
author = {M\"{a}ntyl\"{a}, Mika V. and Calefato, Fabio and Claes, Maelick},
title = {Natural language or not (NLON): a package for software engineering text analysis pipeline},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196444},
doi = {10.1145/3196398.3196444},
abstract = {The use of natural language processing (NLP) is gaining popularity in software engineering. In order to correctly perform NLP, we must pre-process the textual information to separate natural language from other information, such as log messages, that are often part of the communication in software engineering. We present a simple approach for classifying whether some textual input is natural language or not. Although our NLoN package relies on only 11 language features and character tri-grams, we are able to achieve an area under the ROC curve performances between 0.976-0.987 on three different data sources, with Lasso regression from Glmnet as our learner and two human raters for providing ground truth. Cross-source prediction performance is lower and has more fluctuation with top ROC performances from 0.913 to 0.980. Compared with prior work, our approach offers similar performance but is considerably more lightweight, making it easier to apply in software engineering text mining pipelines. Our source code and data are provided as an R-package for further improvements.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {387–391},
numpages = {5},
keywords = {character n-grams, filtering, glmnet, lasso, logistic regression, machine learning, natural language processing, preprocessing, regular expressions},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@article{10.5555/1756006.1953016,
author = {Bouckaert, Remco R. and Frank, Eibe and Hall, Mark A. and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
title = {WEKA---Experiences with a Java Open-Source Project},
year = {2010},
issue_date = {3/1/2010},
publisher = {JMLR.org},
volume = {11},
issn = {1532-4435},
abstract = {WEKA is a popular machine learning workbench with a development life of nearly two decades. This article provides an overview of the factors that we believe to be important to its success. Rather than focussing on the software's functionality, we review aspects of project management and historical development decisions that likely had an impact on the uptake of the project.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {2533–2541},
numpages = {9}
}

@article{10.1016/j.jss.2019.04.026,
author = {Gacit\'{u}a, Ricardo and Sep\'{u}lveda, Samuel and Mazo, Ra\'{u}l},
title = {FM-CF: A framework for classifying feature model building approaches},
year = {2019},
issue_date = {Aug 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {154},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.04.026},
doi = {10.1016/j.jss.2019.04.026},
journal = {J. Syst. Softw.},
month = aug,
pages = {1–21},
numpages = {21},
keywords = {Feature model, Software product lines, Framework, Classification, Models}
}

@article{10.1016/j.eswa.2006.09.042,
author = {Chen, Jr-Shian and Cheng, Ching-Hsue},
title = {Extracting classification rule of software diagnosis using modified MEPA},
year = {2008},
issue_date = {January, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {1},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2006.09.042},
doi = {10.1016/j.eswa.2006.09.042},
abstract = {Defective software modules cause software failures, increase development and maintenance costs, and reduce customer satisfaction. Effective defect prediction models can help developers focus quality assurance activities on defect-prone modules and thus improve software quality by using resources more efficiently. In real-world databases are highly susceptible to noisy, missing, and inconsistent data. Noise is a random error or variance in a measured variable [Han, J., &amp; Kamber, M. (2001). Data Mining: Concepts and Techniques, San Francisco: Morgan Kaufmann Publishers]. When decision trees are built, many of the branches may reflect noisy or outlier data. Therefore, data preprocessing steps are very important. There are many methods for data preprocessing. Concept hierarchies are a form of data discretization that can use for data preprocessing. Data discretization has many advantages, such as data can be reduced and simplified. Using discrete features are usually more compact, shorter and more accurate than using continuous ones [Liu, H., Hussain, F., Tan, C.L., &amp; Dash, M. (2002). Discretization: An enabling technique. Data Mining and Knowledge Discovery, 6(4), 393-423]. In this paper, we propose a modified minimize entropy principle approach and develop a modified MEPA system to partition the data, and then build the classification tree model. For verification, two NASA software projects KC2 and JM1 are applied to illustrate our proposed method. We establish a prototype system to discrete data from these projects. The error rate and number of rules show that the proposed approach is both better than other methods.},
journal = {Expert Syst. Appl.},
month = jan,
pages = {411–418},
numpages = {8},
keywords = {C4.5, Data discretization, Minimize entropy, Software diagnosis}
}

@inproceedings{10.1145/2597073.2597099,
author = {Valdivia Garcia, Harold and Shihab, Emad},
title = {Characterizing and predicting blocking bugs in open source projects},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597099},
doi = {10.1145/2597073.2597099},
abstract = {As software becomes increasingly important, its quality becomes an increasingly important issue. Therefore, prior work focused on software quality and proposed many prediction models to identify the location of software bugs, to estimate their fixing-time, etc. However, one special type of severe bugs is blocking bugs. Blocking bugs are software bugs that prevent other bugs from being fixed. These blocking bugs may increase maintenance costs, reduce overall quality and delay the release of the software systems.  In this paper, we study blocking-bugs in six open source projects and propose a model to predict them. Our goal is to help developers identify these blocking bugs early on. We collect the bug reports from the bug tracking systems of the projects, then we obtain 14 different factors related to, for example, the textual description of the bug, the location the bug is found in and the people involved with the bug. Based on these factors we build decision trees for each project to predict whether a bug will be a blocking bug or not. Then, we analyze these decision trees in order to determine which factors best indicate these blocking bugs. Our results show that our prediction models achieve F-measures of 15-42%, which is a two- to four-fold improvement over the baseline random predictors. We also find that the most important factors in determining blocking bugs are the comment text, comment size, the number of developers in the CC list of the bug report and the reporter's experience. Our analysis shows that our models reduce the median time to identify a blocking bug by 3-18 days.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {72–81},
numpages = {10},
keywords = {Code Metrics, Post-release Defects, Process Metrics},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/3328778.3366863,
author = {Edwards, John and Leinonen, Juho and Hellas, Arto},
title = {A Study of Keystroke Data in Two Contexts: Written Language and Programming Language Influence Predictability of Learning Outcomes},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366863},
doi = {10.1145/3328778.3366863},
abstract = {We study programming process data from two introductory programming courses. Between the course contexts, the programming languages differ, the teaching approaches differ, and the spoken languages differ. In both courses, students' keystroke data -- timestamps and the pressed keys -- are recorded as students work on programming assignments. We study how the keystroke data differs between the contexts, and whether research on predicting course outcomes using keystroke latencies generalizes to other contexts. Our results show that there are differences between the contexts in terms of frequently used keys, which can be partially explained by the differences between the spoken languages and the programming languages. Further, our results suggest that programming process data that can be collected non-intrusive in-situ can be used for predicting course outcomes in multiple contexts. The predictive power, however, varies between contexts possibly because the frequently used keys differ between programming languages and spoken languages. Thus, context-specific fine-tuning of predictive models may be needed.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {413–419},
numpages = {7},
keywords = {digraphs, educational data mining, keystroke analysis, keystroke data, predicting performance, programming process data},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@article{10.1016/j.infsof.2019.04.007,
author = {Malhotra, Ruchika and Khanna, Megha},
title = {Dynamic selection of fitness function for software change prediction using Particle Swarm Optimization},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.007},
doi = {10.1016/j.infsof.2019.04.007},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {51–67},
numpages = {17},
keywords = {Change proneness, Empirical validation, Fitness function, Particle Swarm Optimization}
}

@article{10.1007/s11219-008-9054-7,
author = {Khoshgoftaar, Taghi M. and Hulse, Jason},
title = {Imputation techniques for multivariate missingness in software measurement data},
year = {2008},
issue_date = {December  2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {4},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-008-9054-7},
doi = {10.1007/s11219-008-9054-7},
abstract = {The problem of missing values in software measurement data used in empirical analysis has led to the proposal of numerous potential solutions. Imputation procedures, for example, have been proposed to `fill-in' the missing values with plausible alternatives. We present a comprehensive study of imputation techniques using real-world software measurement datasets. Two different datasets with dramatically different properties were utilized in this study, with the injection of missing values according to three different missingness mechanisms (MCAR, MAR, and NI). We consider the occurrence of missing values in multiple attributes, and compare three procedures, Bayesian multiple imputation, k Nearest Neighbor imputation, and Mean imputation. We also examine the relationship between noise in the dataset and the performance of the imputation techniques, which has not been addressed previously. Our comprehensive experiments demonstrate conclusively that Bayesian multiple imputation is an extremely effective imputation technique.},
journal = {Software Quality Journal},
month = dec,
pages = {563–600},
numpages = {38},
keywords = {Bayesian multiple imputation, Data quality, Imputation, Missing data, Software quality}
}

@inproceedings{10.1145/2597073.2597080,
author = {Tulsian, Varun and Kanade, Aditya and Kumar, Rahul and Lal, Akash and Nori, Aditya V.},
title = {MUX: algorithm selection for software model checkers},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597080},
doi = {10.1145/2597073.2597080},
abstract = {With the growing complexity of modern day software, software model checking has become a critical technology for ensuring correctness of software. As is true with any promising technology, there are a number of tools for software model checking. However, their respective performance trade-offs are difficult to characterize accurately – making it difficult for practitioners to select a suitable tool for the task at hand. This paper proposes a technique called MUX that addresses the problem of selecting the most suitable software model checker for a given input instance. MUX performs machine learning on a repository of software verification instances. The algorithm selector, synthesized through machine learning, uses structural features from an input instance, comprising a program-property pair, at runtime and determines which tool to use.  We have implemented MUX for Windows device drivers and evaluated it on a number of drivers and model checkers. Our results are promising in that the algorithm selector not only avoids a significant number of timeouts but also improves the total runtime by a large margin, compared to any individual model checker. It also outperforms a portfolio-based algorithm selector being used in Microsoft at present. Besides, MUX identifies structural features of programs that are key factors in determining performance of model checkers.},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {132–141},
numpages = {10},
keywords = {Algorithm selection, machine learning, software model checking},
location = {Hyderabad, India},
series = {MSR 2014}
}

@article{10.3233/JIFS-169468,
author = {Malhotra, Ruchika and Khanna, Megha and Thampi, Sabu M. and El-Alfy, El-Sayed M. and Mitra, Sushmita and Trajkovic, Ljiljana},
title = {Prediction of change prone classes using evolution-based and object-oriented metrics},
year = {2018},
issue_date = {2018},
publisher = {IOS Press},
address = {NLD},
volume = {34},
number = {3},
issn = {1064-1246},
url = {https://doi.org/10.3233/JIFS-169468},
doi = {10.3233/JIFS-169468},
abstract = {Determination of change prone classes is crucial in providing guidance to software practitioners for efficient allocation of limited resources and to develop favorable quality software products with optimum costs. Previous literature studies have proposed successful use of design metrics to predict classes which are more prone to change in an Object-Oriented (OO) software. However, the use of evolution-based metrics suite, which quantifies history of changes in a software, release by release should also be evaluated for effective prediction of change prone classes. Evolution-based metrics are representative of evolution characteristics of a class over all its previous releases and are important in order to understand progression and change-prone nature of a class. This study evaluates the use of evolution-based metrics when used in conjunction with OO metrics for prediction of classes which are change prone in nature. In order to empirically validate the results, the study uses two application packages of the Android software namely Contacts and Gallery2. The results indicate that evolution based metrics when used in conjunction with OO metrics are the best predictors of change prone classes. Furthermore, the study statistically evaluates the superiority of this combined metric suite for change proneness prediction.},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {1755–1766},
numpages = {12},
keywords = {Change proneness, evolution-based metrics, empirical validation, machine learning techniques}
}

@article{10.1016/j.datak.2019.101781,
author = {Adam, Chlo\'{e} and Aliotti, Antoine and Malliaros, Fragkiskos D. and Courn\`{e}de, Paul-Henry},
title = {Dynamic monitoring of software use with recurrent neural networks},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {125},
number = {C},
issn = {0169-023X},
url = {https://doi.org/10.1016/j.datak.2019.101781},
doi = {10.1016/j.datak.2019.101781},
journal = {Data Knowl. Eng.},
month = jan,
numpages = {14},
keywords = {Recurrent neural networks, LSTM, Action embeddings, Action representation, Next action prediction, Crash monitoring, Medical imaging software}
}

@inproceedings{10.1145/3457913.3457932,
author = {Yang, Huaiwei and Liu, Shuang and Gui, Lin and Zhao, Yongxin and Sun, Jun and Chen, Junjie},
title = {What Makes Open Source Software Projects Impactful: A Data-Driven Approach},
year = {2021},
isbn = {9781450388191},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3457913.3457932},
doi = {10.1145/3457913.3457932},
abstract = {With the wide adoption and acceptance of open source version control and hosting systems, more and more companies, including Google, Microsoft, Apple and Facebook, are putting their projects on such platforms, e.g., GitHub. It is very important for open source projects to be impactful, i.e., to attract attentions from the open source development community, so as to gain support on development, testing as well as maintenance from the community. However, the question of what factors affect open source project impact, remains largely open. Given the numerous confounding factors and the complex correlations among the factors, it is a challenge to answer the question. In this study, we gather a large dataset from GitHub and provide empirical insights on this question base on a data-driven approach. We randomly collect 146,286 projects from GitHub and then adopt data analysis techniques to automatically analyze the correlations of different features with the software project impact. We also provide suggestions on how to potentially make open source projects impactful base on our analysis results.},
booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
pages = {126–135},
numpages = {10},
location = {Singapore, Singapore},
series = {Internetware '20}
}

@article{10.1504/ijaip.2021.112899,
author = {Agrawal, Arun Prakash and Kaur, Arvinder},
title = {Performance comparison of Bat search and Cuckoo search using software artefact infrastructure repository and regression testing},
year = {2021},
issue_date = {2021},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {18},
number = {2},
issn = {1755-0386},
url = {https://doi.org/10.1504/ijaip.2021.112899},
doi = {10.1504/ijaip.2021.112899},
abstract = {Regression testing is conducted to ensure that no new errors have been introduced into the software as a result of the maintenance activity performed. Re-executing all the existing test cases is however a highly expensive and time consuming approach to gain this confidence. Previous research revealed that nature inspired algorithms have vast application in this area. In this paper, Bat search algorithm is tested against Cuckoo search algorithm to solve regression test case selection problem. Two factors: number of faults covered and computational time are considered for the comparison. Extensive experiments have been conducted over the objects adopted from benchmarked software artifact infrastructure repository. Rigorous statistical tests are conducted to draw a conclusion that Cuckoo search is marginally advantageous over Bat search algorithm with respect to performance parameters. We believe that the results reported in this paper will enable researchers to develop more powerful algorithm for testing in near future.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {99–118},
numpages = {19},
keywords = {regression testing, test effort optimisation, metaheuristics, Bat search algorithm, Cuckoo search optimisation}
}

@article{10.1007/s11704-018-7008-1,
author = {Wu, Tianyong and Deng, Xi and Yan, Jun and Zhang, Jian},
title = {Analyses for specific defects in android applications: a survey},
year = {2019},
issue_date = {December  2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {13},
number = {6},
issn = {2095-2228},
url = {https://doi.org/10.1007/s11704-018-7008-1},
doi = {10.1007/s11704-018-7008-1},
abstract = {Android applications (APPS) are in widespread use and have enriched our life. To ensure the quality and security of the apps, many approaches have been proposed in recent years for detecting bugs and defects in the apps, of which program analysis is a major one. This paper mainly makes an investigation of existing works on the analysis of Android apps. We summarize the purposes and proposed techniques of existing approaches, and make a taxonomy of these works, based on which we point out the trends and challenges of research in this field. From our survey, we sum up four main findings: (1) program analysis in Android security field has gained particular attention in the past years, the fields of functionality and performance should also gain proper attention; the infrastructure that supports detection of various defects should be enriched to meet the industry's need; (2) many kinds of defects result from developers' misunderstanding or misuse of the characteristics and mechanisms in Android system, thus the works that can systematically collect and formalize Android recommendations are in demand; (3) various program analysis approaches with techniques in other fields are applied in analyzing Android apps; however, they can be improved with more precise techniques to be more applicable; (4) The fragmentation and evolution of Android system blocks the usability of existing tools, which should be taken into consideration when developing new approaches.},
journal = {Front. Comput. Sci.},
month = dec,
pages = {1210–1227},
numpages = {18},
keywords = {Android apps, functionality, performance, program analysis, security}
}

@article{10.1007/s10664-016-9484-y,
author = {Le, Tien-Duy B. and Thung, Ferdian and Lo, David},
title = {Will this localization tool be effective for this bug? Mitigating the impact of unreliability of information retrieval based bug localization tools},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9484-y},
doi = {10.1007/s10664-016-9484-y},
abstract = {Information retrieval (IR) based bug localization approaches process a textual bug report and a collection of source code files to find buggy files. They output a ranked list of files sorted by their likelihood to contain the bug. Recently, several IR-based bug localization tools have been proposed. However, there are no perfect tools that can successfully localize faults within a few number of most suspicious program elements for every single input bug report. Therefore, it is difficult for developers to decide which tool would be effective for a given bug report. Furthermore, for some bug reports, no bug localization tools would be useful. Even a state-of-the-art bug localization tool outputs many ranked lists where buggy files appear very low in the lists. This potentially causes developers to distrust bug localization tools. In this work, we build an oracle that can automatically predict whether a ranked list produced by an IR-based bug localization tool is likely to be effective or not. We consider a ranked list to be effective if a buggy file appears in the top-N position of the list. If a ranked list is unlikely to be effective, developers do not need to waste time in checking the recommended files one by one. In such cases, it is better for developers to use traditional debugging methods or request for further information to localize bugs. To build this oracle, our approach extracts features that can be divided into four categories: score features, textual features, topic model features, and metadata features. We build a separate prediction model for each category, and combine them to create a composite prediction model which is used as the oracle. We name this solution APRILE, which stands for Automated PRediction of IR-based Bug Localization's Effectiveness. We further integrate APRILE with two other components that are learned using our bagging-based ensemble classification (BEC) method. We refer to the extension of APRILE as APRILE +. We have evaluated APRILE + to predict the effectiveness of three state-of-the-art IR-based bug localization tools on more than three thousands bug reports from AspectJ, Eclipse, SWT, and Tomcat. APRILE + can achieve an average precision, recall, and F-measure of 77.61 %, 88.94 %, and 82.09 %, respectively. Furthermore, APRILE + outperforms a baseline approach by Le and Lo and APRILE by up to a 17.43 % and 10.51 % increase in F-measure respectively.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2237–2279},
numpages = {43},
keywords = {Bug localization, Bug reports, Effectiveness prediction, Information retrieval, Text classification}
}

@article{10.1016/j.jss.2016.05.016,
author = {Idri, Ali and Hosni, Mohamed and Abran, Alain},
title = {Systematic literature review of ensemble effort estimation},
year = {2016},
issue_date = {August 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {118},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.05.016},
doi = {10.1016/j.jss.2016.05.016},
abstract = {Systematic review of 24 selected studies on Ensemble Effort Estimation.Studies analyzing based on six review questions.EEE techniques usually yield acceptable estimation accuracy than single models.There is no EEE technique performing better than others in all situations. The need to overcome the weaknesses of single estimation techniques for prediction tasks has given rise to ensemble methods in software development effort estimation (SDEE). An ensemble effort estimation (EEE) technique combines several of the single/classical models found in the SDEE literature. However, to the best of our knowledge, no systematic review has yet been performed with a focus on the use of EEE techniques in SDEE. The purpose of this review is to analyze EEE techniques from six viewpoints: single models used to construct ensembles, ensemble estimation accuracy, rules used to combine single estimates, accuracy comparison of EEE techniques with single models, accuracy comparison between EEE techniques and methodologies used to construct ensemble methods. We performed a systematic review of EEE studies published between 2000 and 2016, and we selected 24 of them to address the questions raised in this review. We found that EEE techniques may be separated into two types: homogeneous and heterogeneous, and that the machine learning single models are the most frequently employed in constructing EEE techniques. We also found that EEE techniques usually yield acceptable estimation accuracy, and in fact are more accurate than single models.},
journal = {J. Syst. Softw.},
month = aug,
pages = {151–175},
numpages = {25},
keywords = {Ensemble effort estimation, Software development effort estimation, Systematic literature review}
}

@article{10.1016/j.infsof.2021.106668,
author = {Tong, Yao and Zhang, Xiaofang},
title = {Crowdsourced test report prioritization considering bug severity},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106668},
doi = {10.1016/j.infsof.2021.106668},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {15},
keywords = {Crowdsourced testing, Test report processing, Prioritization, Bug severity, Textual description}
}

@article{10.1016/j.eswa.2011.03.041,
author = {Afzal, Wasif and Torkar, Richard},
title = {Review: On the application of genetic programming for software engineering predictive modeling: A systematic review},
year = {2011},
issue_date = {September, 2011},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {38},
number = {9},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.03.041},
doi = {10.1016/j.eswa.2011.03.041},
abstract = {The objective of this paper is to investigate the evidence for symbolic regression using genetic programming (GP) being an effective method for prediction and estimation in software engineering, when compared with regression/machine learning models and other comparison groups (including comparisons with different improvements over the standard GP algorithm). We performed a systematic review of literature that compared genetic programming models with comparative techniques based on different independent project variables. A total of 23 primary studies were obtained after searching different information sources in the time span 1995-2008. The results of the review show that symbolic regression using genetic programming has been applied in three domains within software engineering predictive modeling: (i) Software quality classification (eight primary studies). (ii) Software cost/effort/size estimation (seven primary studies). (iii) Software fault prediction/software reliability growth modeling (eight primary studies). While there is evidence in support of using genetic programming for software quality classification, software fault prediction and software reliability growth modeling; the results are inconclusive for software cost/effort/size estimation.},
journal = {Expert Syst. Appl.},
month = sep,
pages = {11984–11997},
numpages = {14},
keywords = {Genetic programming, Modeling, Symbolic regression, Systematic review}
}

@article{10.1007/s10664-018-9640-7,
author = {Chowdhury, Shaiful and Borle, Stephanie and Romansky, Stephen and Hindle, Abram},
title = {GreenScaler: training software energy models with automatic test generation},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9640-7},
doi = {10.1007/s10664-018-9640-7},
abstract = {Software energy consumption is a performance related non-functional requirement that complicates building software on mobile devices today. Energy hogging applications (apps) are a liability to both the end-user and software developer. Measuring software energy consumption is non-trivial, requiring both equipment and expertise, yet researchers have found that software energy consumption can be modelled. Prior works have hinted that with more energy measurement data we can make more accurate energy models. This data, however, was expensive to extract because it required energy measurement of running test cases (rare) or time consuming manually written tests. In this paper, we show that automatic random test generation with resource-utilization heuristics can be used successfully to build accurate software energy consumption models. Code coverage, although well-known as a heuristic for generating and selecting tests in traditional software testing, performs poorly at selecting energy hungry tests. We propose an accurate software energy model, GreenScaler, that is built on random tests with CPU-utilization as the test selection heuristic. GreenScaler not only accurately estimates energy consumption for randomly generated tests, but also for meaningful developer written tests. Also, the produced models are very accurate in detecting energy regressions between versions of the same app. This is directly helpful for the app developers who want to know if a change in the source code, for example, is harmful for the total energy consumption. We also show that developers can use GreenScaler to select the most energy efficient API when multiple APIs are available for solving the same problem. Researchers can also use our test generation methodology to further study how to build more accurate software energy models.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1649–1692},
numpages = {44},
keywords = {Automatic software testing, Energy modeling, Energy optimization, Machine learning, Mining software repositories, Software energy consumption, Software energy efficiency, Test generation}
}

@article{10.1145/3444944,
author = {Yao, Liuyi and Chu, Zhixuan and Li, Sheng and Li, Yaliang and Gao, Jing and Zhang, Aidong},
title = {A Survey on Causal Inference},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {1556-4681},
url = {https://doi.org/10.1145/3444944},
doi = {10.1145/3444944},
abstract = {Causal inference is a critical research topic across many domains, such as statistics, computer science, education, public policy, and economics, for decades. Nowadays, estimating causal effect from observational data has become an appealing research direction owing to the large amount of available data and low budget requirement, compared with randomized controlled trials. Embraced with the rapidly developed machine learning area, various causal effect estimation methods for observational data have sprung up. In this survey, we provide a comprehensive review of causal inference methods under the potential outcome framework, one of the well-known causal inference frameworks. The methods are divided into two categories depending on whether they require all three assumptions of the potential outcome framework or not. For each category, both the traditional statistical methods and the recent machine learning enhanced methods are discussed and compared. The plausible applications of these methods are also presented, including the applications in advertising, recommendation, medicine, and so on. Moreover, the commonly used benchmark datasets as well as the open-source codes are also summarized, which facilitate researchers and practitioners to explore, evaluate and apply the causal inference methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = may,
articleno = {74},
numpages = {46},
keywords = {Treatment effect estimation; Representation learning}
}

@article{10.1016/j.cogsys.2018.06.001,
author = {Geng, Wang},
title = {RETRACTED: Cognitive Deep Neural Networks prediction method for software fault tendency module based on Bound Particle Swarm Optimization},
year = {2018},
issue_date = {Dec 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {52},
number = {C},
issn = {1389-0417},
url = {https://doi.org/10.1016/j.cogsys.2018.06.001},
doi = {10.1016/j.cogsys.2018.06.001},
journal = {Cogn. Syst. Res.},
month = dec,
pages = {12–20},
numpages = {9}
}

@article{10.1016/j.csi.2019.103389,
author = {Bukhsh, Faiza Allah and Bukhsh, Zaharah Allah and Daneva, Maya},
title = {A systematic literature review on requirement prioritization techniques and their empirical evaluation},
year = {2020},
issue_date = {Mar 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {69},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2019.103389},
doi = {10.1016/j.csi.2019.103389},
journal = {Comput. Stand. Interfaces},
month = mar,
numpages = {18},
keywords = {Requirements engineering, Requirements prioritization, Empirical study, Empirical research method, Systematic literature review}
}

@inproceedings{10.5555/2663370.2663378,
author = {Kanewala, Upulee and Bieman, James M.},
title = {Techniques for testing scientific programs without an Oracle},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {The existence of an oracle is often assumed in software testing. But in many situations, especially for scientific programs, oracles do not exist or they are too hard to implement. This paper examines three techniques that are used to test programs without oracles: (1) Metamorphic testing, (2) Run-time Assertions and (3) Developing test oracles using machine learning. We examine these methods in terms of their (1) fault finding ability, (2) automation, and (3) required domain knowledge. Several case studies apply these three techniques to effectively test scientific programs that do not have oracles. Certain techniques have reported a better fault finding ability than the others when testing specific programs. Finally, there is potential to increase the level of automation of these techniques, thereby reducing the required level of domain knowledge. Techniques that can potentially be automated include (1) detection of likely metamorphic relations, (2) static analyses to eliminate spurious invariants and (3) structural analyses to develop machine learning generated oracles.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {48–57},
numpages = {10},
keywords = {assertion checking, machine learning, metamorphic relation, metamorphic testing, mutation analysis, scientific software testing, test oracles},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@article{10.1016/j.eswa.2011.09.071,
author = {Kim, Jungeun and Choi, Keunho and Kim, Gunwoo and Suh, Yongmoo},
title = {Classification cost: An empirical comparison among traditional classifier, Cost-Sensitive Classifier, and MetaCost},
year = {2012},
issue_date = {March, 2012},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {39},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2011.09.071},
doi = {10.1016/j.eswa.2011.09.071},
abstract = {Loan fraud is a critical factor in the insolvency of financial institutions, so companies make an effort to reduce the loss from fraud by building a model for proactive fraud prediction. However, there are still two critical problems to be resolved for the fraud detection: (1) the lack of cost sensitivity between type I error and type II error in most prediction models, and (2) highly skewed distribution of class in the dataset used for fraud detection because of sparse fraud-related data. The objective of this paper is to examine whether classification cost is affected both by the cost-sensitive approach and by skewed distribution of class. To that end, we compare the classification cost incurred by a traditional cost-insensitive classification approach and two cost-sensitive classification approaches, Cost-Sensitive Classifier (CSC) and MetaCost. Experiments were conducted with a credit loan dataset from a major financial institution in Korea, while varying the distribution of class in the dataset and the number of input variables. The experiments showed that the lowest classification cost was incurred when the MetaCost approach was used and when non-fraud data and fraud data were balanced. In addition, the dataset that includes all delinquency variables was shown to be most effective on reducing the classification cost.},
journal = {Expert Syst. Appl.},
month = mar,
pages = {4013–4019},
numpages = {7},
keywords = {Cost-Sensitive Classifier, Cost-sensitive learning, Fraud detection, MetaCost}
}

@inproceedings{10.1145/3324884.3416583,
author = {Wang, Xin and Liu, Jin and Li, Li and Chen, Xiao and Liu, Xiao and Wu, Hao},
title = {Detecting and explaining self-admitted technical debts with attention-based neural networks},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416583},
doi = {10.1145/3324884.3416583},
abstract = {Self-Admitted Technical Debt (SATD) is a sub-type of technical debt. It is introduced to represent such technical debts that are intentionally introduced by developers in the process of software development. While being able to gain short-term benefits, the introduction of SATDs often requires to be paid back later with a higher cost, e.g., introducing bugs to the software or increasing the complexity of the software.To cope with these issues, our community has proposed various machine learning-based approaches to detect SATDs. These approaches, however, are either not generic that usually require manual feature engineering efforts or do not provide promising means to explain the predicted outcomes. To that end, we propose to the community a novel approach, namely HATD (Hybrid Attention-based method for self-admitted Technical Debt detection), to detect and explain SATDs using attention-based neural networks. Through extensive experiments on 445,365 comments in 20 projects, we show that HATD is effective in detecting SATDs on both in-the-lab and in-the-wild datasets under both within-project and cross-project settings. HATD also outperforms the state-of-the-art approaches in detecting and explaining SATDs.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {871–882},
numpages = {12},
keywords = {SATD, attention-based neural networks, self-admitted technical debt, word embedding},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1007/s10664-020-09885-w,
author = {Herbold, Steffen and Trautsch, Alexander and Trautsch, Fabian},
title = {On the feasibility of automated prediction of bug and non-bug issues},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09885-w},
doi = {10.1007/s10664-020-09885-w},
journal = {Empirical Softw. Engg.},
month = nov,
pages = {5333–5369},
numpages = {37},
keywords = {Issue type prediction, Mislabeled issues, Issue tracking}
}

@article{10.1007/s42979-020-00408-4,
author = {Tsoukalas, Dimitrios and Mathioudaki, Maria and Siavvas, Miltiadis and Kehagias, Dionysios and Chatzigeorgiou, Alexander},
title = {A Clustering Approach Towards Cross-Project Technical Debt Forecasting},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {1},
url = {https://doi.org/10.1007/s42979-020-00408-4},
doi = {10.1007/s42979-020-00408-4},
abstract = {Technical debt (TD) describes quality compromises that can yield short-term benefits but may negatively affect the quality of software products in the long run. A wide range of tools and techniques have been introduced over the years in order for the developers to be able to determine and manage TD. However, being able to also predict its future evolution is of equal importance to avoid its accumulation, and, in turn, the unlikely event of making the project unmaintainable. Although recent research endeavors have showcased the feasibility of building accurate project-specific TD forecasting models, there is a gap in the field regarding cross-project TD forecasting. Cross-project TD forecasting is of practical importance, since it would enable the application of pre-existing forecasting models on previously unknown software projects, especially new projects that do not exhibit sufficient commit history to enable the construction of project-specific models. To this end, in the present paper, we focus on cross-project TD forecasting, and we examine whether the consideration of similarities between software projects could be the key for more accurate forecasting. More specifically, we propose an approach based on data clustering. In fact, a relatively large repository of software projects is divided into clusters of similar projects with respect to their TD aspects, and specific TD forecasting models are built for each cluster, using regression algorithms. According to our approach, previously unknown software projects are assigned to one of the defined clusters and the cluster-specific TD forecasting model is applied to predict future TD values. The approach was evaluated through several experiments based on real-world applications. The results of the analysis suggest that the proposed approach comprises a promising solution for accurate cross-project TD forecasting.},
journal = {SN Comput. Sci.},
month = jan,
numpages = {30},
keywords = {Technical debt, Technical debt forecasting, Cross-project prediction, Data clustering}
}

@inproceedings{10.5555/2818754.2818807,
author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
title = {Learning to log: helping developers make informed logging decisions},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {415–425},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1007/s10664-016-9472-2,
author = {Menzies, Tim and Yang, Ye and Mathew, George and Boehm, Barry and Hihn, Jairus},
title = {Negative results for software effort estimation},
year = {2017},
issue_date = {October   2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9472-2},
doi = {10.1007/s10664-016-9472-2},
abstract = {More than half the literature on software effort estimation (SEE) focuses on comparisons of new estimation methods. Surprisingly, there are no studies comparing state of the art latest methods with decades-old approaches. Accordingly, this paper takes five steps to check if new SEE methods generated better estimates than older methods. Firstly, collect effort estimation methods ranging from "classical" COCOMO (parametric estimation over a pre-determined set of attributes) to "modern" (reasoning via analogy using spectral-based clustering plus instance and feature selection, and a recent "baseline method" proposed in ACM Transactions on Software Engineering). Secondly, catalog the list of objections that lead to the development of post-COCOMO estimation methods. Thirdly, characterize each of those objections as a comparison between newer and older estimation methods. Fourthly, using four COCOMO-style data sets (from 1991, 2000, 2005, 2010) and run those comparisons experiments. Fifthly, compare the performance of the different estimators using a Scott-Knott procedure using (i) the A12 effect size to rule out "small" differences and (ii) a 99 % confident bootstrap procedure to check for statistically different groupings of treatments. The major negative result of this paper is that for the COCOMO data sets, nothing we studied did any better than Boehms original procedure. Hence, we conclude that when COCOMO-style attributes are available, we strongly recommend (i) using that data and (ii) use COCOMO to generate predictions. We say this since the experiments of this paper show that, at least for effort estimation, how data is collected is more important than what learner is applied to that data.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2658–2683},
numpages = {26},
keywords = {A12, Bootstrap sampling, CART, COCOMO, Clustering, Effect size, Effort estimation, Feature selection, Nearest neighbor, Prototype generation}
}

@article{10.1016/j.infsof.2015.09.001,
author = {Ma, Wanwangying and Chen, Lin and Yang, Yibiao and Zhou, Yuming and Xu, Baowen},
title = {Empirical analysis of network measures for effort-aware fault-proneness prediction},
year = {2016},
issue_date = {January 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {69},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.09.001},
doi = {10.1016/j.infsof.2015.09.001},
abstract = {ContextRecently, network measures have been proposed to predict fault-prone modules. Leveraging the dependency relationships between software entities, network measures describe the structural features of software systems. However, there is no consensus about their effectiveness for fault-proneness prediction. Specifically, the predictive ability of network measures in effort-aware context has not been addressed. ObjectiveWe aim to provide a comprehensive evaluation on the predictive effectiveness of network measures with the effort needed to inspect the code taken into consideration. MethodWe first constructed software source code networks of 11 open-source projects by extracting the data and call dependencies between modules. We then employed univariate logistic regression to investigate how each single network measure was correlated with fault-proneness. Finally, we built multivariate prediction models to examine the usefulness of network measures under three prediction settings: cross-validation, across-release, and inter-project predictions. In particular, we used the effort-aware performance indicators to compare their predictive ability against the commonly used code metrics in both ranking and classification scenarios. ResultsBased on the 11 open-source software systems, our results show that: (1) most network measures are significantly positively related to fault-proneness; (2) the performance of network measures varies under different prediction settings; (3) network measures have inconsistent effects on various projects. ConclusionNetwork measures are of practical value in the context of effort-aware fault-proneness prediction, but researchers and practitioners should be careful of choosing whether and when to use network measures in practice.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {50–70},
numpages = {21},
keywords = {Dependency relationships, Effort-aware, Fault-proneness prediction, Network measures}
}

@inproceedings{10.1109/ICSE43902.2021.00140,
author = {Alshammari, Abdulrahman and Morris, Christopher and Hilton, Michael and Bell, Jonathan},
title = {FlakeFlagger: Predicting Flakiness Without Rerunning Tests},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00140},
doi = {10.1109/ICSE43902.2021.00140},
abstract = {When developers make changes to their code, they typically run regression tests to detect if their recent changes (re)introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1572–1584},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.5555/3489212.3489277,
author = {Dai, Jiarun and Zhang, Yuan and Jiang, Zheyue and Zhou, Yingtian and Chen, Junyan and Xing, Xinyu and Zhang, Xiaohan and Tan, Xin and Yang, Min and Yang, Zhemin},
title = {BScout: direct whole patch presence test for Java executables},
year = {2020},
isbn = {978-1-939133-17-5},
publisher = {USENIX Association},
address = {USA},
abstract = {To protect end-users and software from known vulnerabilities, it is crucial to apply security patches to affected executables timely. To this end, patch presence tests are proposed with the capability of independently investigating patch application status on a target without source code. Existing work on patch presence testing adopts a signature-based approach. To make a trade-off between the uniqueness and the stability of the signature, existing work is limited to use a small and localized patch snippet (instead of the whole patch) for signature generation, so they are inherently unreliable.In light of this, we present BSCOUT, which directly checks the presence of a whole patch in Java executables without generating signatures. BSCOUT features several new techniques to bridge the semantic gap between source code and bytecode instructions during the testing, and accurately checks the fine-grained patch semantics in the whole target executable. We evaluate BScout with 194 CVEs from the Android framework and third-party libraries. The results show that it achieves remarkable accuracy with and without line number information (i.e., debug information) presented in a target executable. We further apply BSCOUT to perform a large-scale patch application practice study with 2,506 Android system images from 7 vendors. Our study reveals many findings that have not yet been reported.},
booktitle = {Proceedings of the 29th USENIX Conference on Security Symposium},
articleno = {65},
numpages = {18},
series = {SEC'20}
}

@article{10.1016/j.infsof.2014.04.002,
author = {Machado, Ivan Do Carmo and Mcgregor, John D. and Cavalcanti, Yguarat\~{a} Cerqueira and De Almeida, Eduardo Santana},
title = {On strategies for testing software product lines: A systematic literature review},
year = {2014},
issue_date = {October, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.04.002},
doi = {10.1016/j.infsof.2014.04.002},
abstract = {Context: Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed. Objective: The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort. Method: We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies. Results: The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn. Conclusion: This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1183–1199},
numpages = {17},
keywords = {Software product lines, Software quality, Software testing, Systematic literature review}
}

@inproceedings{10.1145/3127005.3127006,
author = {Noor, Tanzeem Bin and Hemmati, Hadi},
title = {Studying Test Case Failure Prediction for Test Case Prioritization},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127006},
doi = {10.1145/3127005.3127006},
abstract = {Background: Test case prioritization refers to the process of ranking test cases within a test suite for execution. The goal is ranking fault revealing test cases higher so that in case of limited budget one only executes the top ranked tests and still detects as many bugs as possible. Since the actual fault detection ability of test cases is unknown before execution, heuristics such as "code coverage" of the test cases are used for ranking test cases. Other test quality metrics such as "coverage of the changed parts of the code" and "number of fails in the past"' have also been studied in the literature. Aims: In this paper, we propose using a logistic regression model to predict the failing test cases in the current release based on a set of test quality metrics. Method: We have studied the effect of including our newly proposed quality metric ("similarity-based" metric) into this model for tests prioritization. Results: The results of our experiments on five open source systems show that none of the individual quality metrics of our study outperforms the others in all the projects. Conclusions: However, the ranks given by the regression model are more consistent in prioritizing fault revealing test cases in the current release.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Regression Model, Test Case Prioritization, Test Case Quality Metrics},
location = {Toronto, Canada},
series = {PROMISE}
}

@article{10.1016/j.patcog.2014.08.024,
author = {Khan, Salman H. and Ali Akbar, M. and Shahzad, Farrukh and Farooq, Mudassar and Khan, Zeashan},
title = {Secure biometric template generation for multi-factor authentication},
year = {2015},
issue_date = {February 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {48},
number = {2},
issn = {0031-3203},
url = {https://doi.org/10.1016/j.patcog.2014.08.024},
doi = {10.1016/j.patcog.2014.08.024},
abstract = {In the light of recent security incidents, leading to compromise of services using single factor authentication mechanisms, industry and academia researchers are actively investigating novel multi-factor authentication schemes. Moreover, exposure of unprotected authentication data is a high risk threat for organizations with online presence. The challenge is how to ensure security of multi-factor authentication data without deteriorating the performance of an identity verification system? To solve this problem, we present a novel framework that applies random projections to biometric data (inherence factor), using secure keys derived from passwords (knowledge factor), to generate inherently secure, efficient and revocable/renewable biometric templates for users' verification. We evaluate the security strength of the framework against possible attacks by adversaries. We also undertake a case study of deploying the proposed framework in a two-factor authentication setup that uses users' passwords and dynamic handwritten signatures. Our system preserves the important biometric information even when the user specific password is compromised - a highly desirable feature but not existent in the state-of-the-art transformation techniques. We have evaluated the performance of the framework on three publicly available signature datasets. The results prove that the proposed framework does not undermine the discriminating features of genuine and forged signatures and the verification performance is comparable to that of the state-of-the-art benchmark results. HighlightsA novel two-factor authentication framework involving user biometrics and passwords.A hybrid scheme for template security using subspace mapping and arithmetic hashing.Detailed security analysis of the proposed framework under different attack scenarios.Application of the proposed approach to protect dynamic signature templates.To show that proposed technique does not undermine the verification performance.},
journal = {Pattern Recogn.},
month = feb,
pages = {458–472},
numpages = {15},
keywords = {Biohashing, Biometric template protection, Distance matching, Dynamic signature verification, Feature transformation, Random projections, Two factor authentication}
}

@article{10.1007/s42979-020-00416-4,
author = {Lim, Sachiko and Henriksson, Aron and Zdravkovic, Jelena},
title = {Data-Driven Requirements Elicitation: A Systematic Literature Review},
year = {2021},
issue_date = {Feb 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {2},
number = {1},
url = {https://doi.org/10.1007/s42979-020-00416-4},
doi = {10.1007/s42979-020-00416-4},
abstract = {Requirements engineering has traditionally been stakeholder-driven. In addition to domain knowledge, widespread digitalization has led to the generation of vast amounts of data (Big Data) from heterogeneous digital sources such as the Internet of Things (IoT), mobile devices, and social networks. The digital transformation has spawned new opportunities to consider such data as potentially valuable sources of requirements, although they are not intentionally created for requirements elicitation. A challenge to data-driven requirements engineering concerns the lack of methods to facilitate seamless and autonomous requirements elicitation from such dynamic and unintended digital sources. There are numerous challenges in processing the data effectively to be fully exploited in organizations. This article, thus, reviews the current state-of-the-art approaches to data-driven requirements elicitation from dynamic data sources and identifies research gaps. We obtained 1848 hits when searching six electronic databases. Through a two-level screening and a complementary forward and backward reference search, 68 papers were selected for final analysis. The results reveal that the existing automated requirements elicitation primarily focuses on utilizing human-sourced data, especially online reviews, as requirements sources, and supervised machine learning for data processing. The outcomes of automated requirements elicitation often result in mere identification and classification of requirements-related information or identification of features, without eliciting requirements in a ready-to-use form. This article highlights the need for developing methods to leverage process-mediated and machine-generated data for requirements elicitation and addressing the issues related to variety, velocity, and volume of Big Data for the efficient and effective software development and evolution.},
journal = {SN Comput. Sci.},
month = jan,
numpages = {35},
keywords = {Requirements engineering, Requirements elicitation, Big Data, Automation}
}

@inproceedings{10.1145/2070821.2070824,
author = {Menzies, Tim and Bird, Christian and Zimmermann, Thomas and Schulte, Wolfram and Kocaganeli, Ekrem},
title = {The inductive software engineering manifesto: principles for industrial data mining},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070824},
doi = {10.1145/2070821.2070824},
abstract = {The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {19–26},
numpages = {8},
keywords = {inductive engineering, industry},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1016/j.jss.2017.11.002,
author = {Feyzi, Farid and Parsa, Saeed},
title = {FPA-FL},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {136},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.11.002},
doi = {10.1016/j.jss.2017.11.002},
abstract = {Considering the static structure and the fault-proneness associated with different portions of the code in fault localization.Considering the complex interactions among program elements by using an Elastic-Net regression model.Effective multiple-fault localization based on grouping effect of the proposed Elastic-Net model. Despite the proven applicability of the statistical methods in automatic fault localization, these approaches are biased by data collected from different executions of the program. This biasness could result in unstable statistical models which may vary dependent on test data provided for trial executions of the program. To resolve the difficulty, in this article a new fault-proneness-aware statistical approach based on Elastic-Net regression, namely FPA-FL is proposed. The main idea behind FPA-FL is to consider the static structure and the fault-proneness of the program statements in addition to their dynamic correlations with the program termination state. The grouping effect of FPA-FL is helpful for finding multiple faults and supporting scalability. To provide the context of failure, cause-effect chains of program faults are discovered. FPA-FL is evaluated from different viewpoints on well-known test suites. The results reveal high fault localization performance of our approach, compared with similar techniques in the literature.},
journal = {J. Syst. Softw.},
month = feb,
pages = {39–58},
numpages = {20},
keywords = {Backward dynamic slice, Coincidental correctness, Elastic-net regression, Fault localization, Fault-proneness, Statistical debugging}
}

@article{10.1007/s10664-019-09739-0,
author = {Catolino, Gemma and Palomba, Fabio and Fontana, Francesca Arcelli and De Lucia, Andrea and Zaidman, Andy and Ferrucci, Filomena},
title = {Improving change prediction models with code smell-related information},
year = {2020},
issue_date = {Jan 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09739-0},
doi = {10.1007/s10664-019-09739-0},
abstract = {Code smells are sub-optimal implementation choices applied by developers that have the effect of negatively impacting, among others, the change-proneness of the affected classes. Based on this consideration, in this paper we conjecture that code smell-related information can be effectively exploited to improve the performance of change prediction models, i.e., models having the goal of indicating which classes are more likely to change in the future. We exploit the so-called intensity index—a previously defined metric that captures the severity of a code smell—and evaluate its contribution when added as additional feature in the context of three state of the art change prediction models based on product, process, and developer-based features. We also compare the performance achieved by the proposed model with a model based on previously defined antipattern metrics, a set of indicators computed considering the history of code smells in files. Our results report that (i) the prediction performance of the intensity-including models is statistically better than the baselines and, (ii) the intensity is a better predictor than antipattern metrics. We observed some orthogonality between the set of change-prone and non-change-prone classes correctly classified by the models relying on intensity and antipattern metrics: for this reason, we also devise and evaluate a smell-aware combined change prediction model including product, process, developer-based, and smell-related features. We show that the F-Measure of this model is notably higher than other models.},
journal = {Empirical Softw. Engg.},
month = jan,
pages = {49–95},
numpages = {47},
keywords = {Change prediction, Code smells, Empirical study}
}

@article{10.1016/j.infsof.2019.04.005,
author = {Menzies, Tim and Shepperd, Martin},
title = {“Bad smells” in software analytics papers},
year = {2019},
issue_date = {Aug 2019},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {112},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2019.04.005},
doi = {10.1016/j.infsof.2019.04.005},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {35–47},
numpages = {13}
}

@article{10.1007/s00766-016-0251-9,
author = {Maalej, Walid and Kurtanovi\'{c}, Zijad and Nabil, Hadeer and Stanik, Christoph},
title = {On the automatic classification of app reviews},
year = {2016},
issue_date = {September 2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {21},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-016-0251-9},
doi = {10.1007/s00766-016-0251-9},
abstract = {App stores like Google Play and Apple AppStore have over 3 million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. The majority of the reviews, however, is rather non-informative just praising the app and repeating to the star ratings in words. This paper introduces several probabilistic techniques to classify app reviews into four types: bug reports, feature requests, user experiences, and text ratings. For this, we use review metadata such as the star rating and the tense, as well as, text classification, natural language processing, and sentiment analysis techniques. We conducted a series of experiments to compare the accuracy of the techniques and compared them with simple string matching. We found that metadata alone results in a poor classification accuracy. When combined with simple text classification and natural language preprocessing of the text--particularly with bigrams and lemmatization--the classification precision for all review types got up to 88---92 % and the recall up to 90---99 %. Multiple binary classifiers outperformed single multiclass classifiers. Our results inspired the design of a review analytics tool, which should help app vendors and developers deal with the large amount of reviews, filter critical reviews, and assign them to the appropriate stakeholders. We describe the tool main features and summarize nine interviews with practitioners on how review analytics tools including ours could be used in practice.},
journal = {Requir. Eng.},
month = sep,
pages = {311–331},
numpages = {21},
keywords = {Data-driven requirements engineering, Machine learning, Natural language processing, Review analytics, Software analytics, User feedback}
}

@article{10.5555/2938656.2938664,
author = {Kakar, Adarsh Kumar},
title = {Separating the Wheat from the Chaff: Extracting Business Value from Feature Requests Posted in User Forums},
year = {2016},
issue_date = {April 2016},
publisher = {IGI Global},
address = {USA},
volume = {28},
number = {2},
issn = {1546-2234},
abstract = {When making a decision to add features to an existing software product in response to feature requests posted in user forums, it is important to pursue only those changes that deliver value to both the user and the producer. But selecting critical user requirements expressed as features requests is a challenging task. While excluding a high value requirement may mean losing customers to a competing product, including a requirement that is unneeded increases time to market and introduces unnecessary costs and complexity in the product. Keeping these issues in focus, promising methods of feature selection were first identified from a review of requirements engineering, product development and quality literatures. An empirical study was then conducted to investigate the efficacy of methods in separating the vital few user feature requests from the trivial many posted in user forums without adversely impacting user satisfaction. The result of the empirical study demonstrates that the Kano survey method has potential in separating the wheat from the chaff. The reasons for this finding is empirically investigated and discussed.},
journal = {J. Organ. End User Comput.},
month = apr,
pages = {124–141},
numpages = {18},
keywords = {Business Value, Feature Requests, Feature Selection, Kano Survey Method, User Forums}
}

@inproceedings{10.1145/2393596.2393670,
author = {Shihab, Emad and Hassan, Ahmed E. and Adams, Bram and Jiang, Zhen Ming},
title = {An industrial study on the risk of software changes},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393670},
doi = {10.1145/2393596.2393670},
abstract = {Modelling and understanding bugs has been the focus of much of the Software Engineering research today. However, organizations are interested in more than just bugs. In particular, they are more concerned about managing risk, i.e., the likelihood that a code or design change will cause a negative impact on their products and processes, regardless of whether or not it introduces a bug. In this paper, we conduct a year-long study involving more than 450 developers of a large enterprise, spanning more than 60 teams, to better understand risky changes, i.e., changes for which developers believe that additional attention is needed in the form of careful code or design reviewing and/or more testing. Our findings show that different developers and different teams have their own criteria for determining risky changes. Using factors extracted from the changes and the history of the files modified by the changes, we are able to accurately identify risky changes with a recall of more than 67%, and a precision improvement of 87% (using developer specific models) and 37% (using team specific models), over a random model. We find that the number of lines and chunks of code added by the change, the bugginess of the files being changed, the number of bug reports linked to a change and the developer experience are the best indicators of change risk. In addition, we find that when a change has many related changes, the reliability of developers in marking risky changes is negatively affected. Our findings and models are being used today in practice to manage the risk of software projects.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {62},
numpages = {11},
keywords = {bug inducing changes, change metrics, change risk, code metrics},
location = {Cary, North Carolina},
series = {FSE '12}
}

@article{10.1016/j.infsof.2018.01.016,
author = {Soares, Larissa Rocha and Schobbens, Pierre-Yves and do Carmo Machado, Ivan and de Almeida, Eduardo Santana},
title = {Feature interaction in software product line engineering: A systematic mapping study},
year = {2018},
issue_date = {Jun 2018},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {98},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2018.01.016},
doi = {10.1016/j.infsof.2018.01.016},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {44–58},
numpages = {15},
keywords = {Feature interaction, Software product lines, Systematic mapping}
}

@article{10.1016/j.sysarc.2010.06.003,
author = {Chowdhury, Istehad and Zulkernine, Mohammad},
title = {Using complexity, coupling, and cohesion metrics as early indicators of vulnerabilities},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {57},
number = {3},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2010.06.003},
doi = {10.1016/j.sysarc.2010.06.003},
abstract = {Software security failures are common and the problem is growing. A vulnerability is a weakness in the software that, when exploited, causes a security failure. It is difficult to detect vulnerabilities until they manifest themselves as security failures in the operational stage of software, because security concerns are often not addressed or known sufficiently early during the software development life cycle. Numerous studies have shown that complexity, coupling, and cohesion (CCC) related structural metrics are important indicators of the quality of software architecture, and software architecture is one of the most important and early design decisions that influences the final quality of the software system. Although these metrics have been successfully employed to indicate software faults in general, there are no systematic guidelines on how to use these metrics to predict vulnerabilities in software. If CCC metrics can be used to indicate vulnerabilities, these metrics could aid in the conception of more secured architecture, leading to more secured design and code and eventually better software. In this paper, we present a framework to automatically predict vulnerabilities based on CCC metrics. To empirically validate the framework and prediction accuracy, we conduct a large empirical study on fifty-two releases of Mozilla Firefox developed over a period of four years. To build vulnerability predictors, we consider four alternative data mining and statistical techniques - C4.5 Decision Tree, Random Forests, Logistic Regression, and Naive-Bayes - and compare their prediction performances. We are able to correctly predict majority of the vulnerability-prone files in Mozilla Firefox, with tolerable false positive rates. Moreover, the predictors built from the past releases can reliably predict the likelihood of having vulnerabilities in the future releases. The experimental results indicate that structural information from the non-security realm such as complexity, coupling, and cohesion are useful in vulnerability prediction.},
journal = {J. Syst. Archit.},
month = mar,
pages = {294–313},
numpages = {20},
keywords = {Cohesion, Complexity, Coupling, Software metrics, Vulnerability prediction}
}

@article{10.1007/s10515-016-0203-0,
author = {Malhotra, Ruchika and Khanna, Megha},
title = {An exploratory study for software change prediction in object-oriented systems using hybridized techniques},
year = {2017},
issue_date = {September 2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {3},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-016-0203-0},
doi = {10.1007/s10515-016-0203-0},
abstract = {Variation in software requirements, technological upgrade and occurrence of defects necessitate change in software for its effective use. Early detection of those classes of a software which are prone to change is critical for software developers and project managers as it can aid in efficient resource allocation of limited resources. Moreover, change prone classes should be efficiently restructured and designed to prevent introduction of defects. Recently, use of search based techniques and their hybridized counter-parts have been advocated in the field of software engineering predictive modeling as these techniques help in identification of optimal solutions for a specific problem by testing the goodness of a number of possible solutions. In this paper, we propose a novel approach for change prediction using search-based techniques and hybridized techniques. Further, we address the following issues: (i) low repeatability of empirical studies, (ii) less use of statistical tests for comparing the effectiveness of models, and (iii) non-assessment of trade-off between runtime and predictive performance of various techniques. This paper presents an empirical validation of search-based techniques and their hybridized versions, which yields unbiased, accurate and repeatable results. The study analyzes and compares the predictive performance of five search-based, five hybridized techniques and four widely used machine learning techniques and a statistical technique for predicting change prone classes in six application packages of a popular operating system for mobile--Android. The results of the study advocate the use of hybridized techniques for developing models to identify change prone classes.},
journal = {Automated Software Engg.},
month = sep,
pages = {673–717},
numpages = {45},
keywords = {Change proneness, Empirical validation, Hybridized techniques, Object-oriented metrics, Predictive modeling, Search-based techniques}
}

@article{10.5555/3269687.3269696,
author = {Cao, Zherui and Tian, Yuan and Le, Tien-Duy B. and Lo, David},
title = {Rule-based specification mining leveraging learning to rank},
year = {2018},
issue_date = {September 2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {0928-8910},
abstract = {Software systems are often released without formal specifications. To deal with the problem of lack of and outdated specifications, rule-based specification mining approaches have been proposed. These approaches analyze execution traces of a system to infer the rules that characterize the protocols, typically of a library, that its clients must obey. Rule-based specification mining approaches work by exploring the search space of all possible rules and use interestingness measures to differentiate specifications from false positives. Previous rule-based specification mining approaches often rely on one or two interestingness measures, while the potential benefit of combining multiple available interestingness measures is not yet investigated. In this work, we propose a learning to rank based approach that automatically learns a good combination of 38 interestingness measures. Our experiments show that the learning to rank based approach outperforms the best performing approach leveraging single interestingness measure by up to 66%.},
journal = {Automated Software Engg.},
month = sep,
pages = {501–530},
numpages = {30},
keywords = {Automated software development, Learning to rank, Software maintenance and evolution, Specification mining}
}

@article{10.1145/3460345,
author = {He, Shilin and He, Pinjia and Chen, Zhuangbin and Yang, Tianyi and Su, Yuxin and Lyu, Michael R.},
title = {A Survey on Automated Log Analysis for Reliability Engineering},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3460345},
doi = {10.1145/3460345},
abstract = {Logs are semi-structured text generated by logging statements in software source code. In recent decades, software logs have become imperative in the reliability assurance mechanism of many software systems, because they are often the only data available that record software runtime information. As modern software is evolving into a large scale, the volume of logs has increased rapidly. To enable effective and efficient usage of modern software logs in reliability engineering, a number of studies have been conducted on automated log analysis. This survey presents a detailed overview of automated log analysis research, including how to automate and assist the writing of logging statements, how to compress logs, how to parse logs into structured event templates, and how to employ logs to detect anomalies, predict failures, and facilitate diagnosis. Additionally, we survey work that releases open-source toolkits and datasets. Based on the discussion of the recent advances, we present several promising future directions toward real-world and next-generation automated log analysis.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {130},
numpages = {37},
keywords = {Log, log analysis, log compression, log mining, log parsing, logging}
}

@article{10.1504/IJIEI.2017.082554,
title = {Towards distributed layered intrusion detection system for large scale wireless sensor networks},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {5},
number = {1},
issn = {1758-8715},
url = {https://doi.org/10.1504/IJIEI.2017.082554},
doi = {10.1504/IJIEI.2017.082554},
abstract = {In the current era of computer and communication rapid development, network security has become one of the most important factors to consider. Security considerations in wireless sensor networks WSNs have been an interesting point in research especially with the fast spread of WSNs. In this paper, an efficient two-layer and three-layer intrusion detection models are introduced. The two-layer model represents the levels of the sensor and sink nodes. The three-layer model represents the levels of the sensor, sink and base station. The models are elaborated and examined through a set of experiments. A supervised learning algorithm is introduced to be used in the sensor node layer and an unsupervised learning algorithm is introduced to be used in the other layers. The learning algorithms used only 10% of the data for training and gave a high detection accuracy on the used dataset, using lesser number of features compared to other approaches.},
journal = {Int. J. Intell. Eng. Inform.},
month = jan,
pages = {1–28},
numpages = {28}
}

@inproceedings{10.1007/978-3-030-39306-9_3,
author = {Capizzi, Antonio and Distefano, Salvatore and Ara\'{u}jo, Luiz J. P. and Mazzara, Manuel and Ahmad, Muhammad and Bobrov, Evgeny},
title = {Anomaly Detection in DevOps Toolchain},
year = {2019},
isbn = {978-3-030-39305-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-39306-9_3},
doi = {10.1007/978-3-030-39306-9_3},
abstract = {The tools employed in the DevOps Toolchain generates a large quantity of data that is typically ignored or inspected only on particular occasions, at most. However, the analysis of such data could enable the extraction of useful information about the status and evolution of the project. For example, metrics like the “lines of code added since the last release” or “failures detected in the staging environment” are good indicators for predicting potential risks in the incoming release. In order to prevent problems appearing in later stages of production, an anomaly detection system can operate in the staging environment to compare the current incoming release with previous ones according to predefined metrics. The analysis is conducted before going into production to identify anomalies which should be addressed by human operators that address false-positive and negatives that can appear. In this paper, we describe a prototypical implementation of the aforementioned idea in the form of a “proof of concept”. The current study effectively demonstrates the feasibility of the approach for a set of implemented functionalities.},
booktitle = {Software Engineering Aspects of Continuous Development and New Paradigms of Software Production and Deployment: Second International Workshop, DEVOPS 2019, Ch\^{a}teau de Villebrumier, France, May 6–8, 2019, Revised Selected Papers},
pages = {37–51},
numpages = {15},
location = {Villebrumier, France}
}

@article{10.4018/jiit.2011070104,
author = {Purvis, Martin and Purvis, Maryam and Deng, Jeremiah D.},
title = {Software Effort Estimation: Harmonizing Algorithms and Domain Knowledge in an Integrated Data Mining Approach},
year = {2011},
issue_date = {July 2011},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {3},
issn = {1548-3657},
url = {https://doi.org/10.4018/jiit.2011070104},
doi = {10.4018/jiit.2011070104},
abstract = {Software development effort estimation is important for quality management in the software development industry, yet its automation still remains a challenging issue. Applying machine learning algorithms alone often cannot achieve satisfactory results. This paper presents an integrated data mining framework that incorporates domain knowledge into a series of data analysis and modeling processes, including visualization, feature selection, and model validation. An empirical study on the software effort estimation problem using a benchmark dataset shows the necessity and effectiveness of the proposed approach.},
journal = {Int. J. Intell. Inf. Technol.},
month = jul,
pages = {41–53},
numpages = {13},
keywords = {Data Analysis, Data Mining, Domain Knowledge, Modelling, Software Effort Estimation}
}

@article{10.1007/s10664-020-09808-9,
author = {Agrawal, Amritanshu and Menzies, Tim and Minku, Leandro L. and Wagner, Markus and Yu, Zhe},
title = {Better software analytics via “DUO”: Data mining algorithms using/used-by optimizers},
year = {2020},
issue_date = {May 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09808-9},
doi = {10.1007/s10664-020-09808-9},
abstract = {This paper claims that a new field of empirical software engineering research and practice is emerging: data mining using/used-by optimizers for empirical studies, or DUO. For example, data miners can generate models that are explored by optimizers. Also, optimizers can advise how to best adjust the control parameters of a data miner. This combined approach acts like an agent leaning over the shoulder of an analyst that advises “ask this question next” or “ignore that problem, it is not relevant to your goals”. Further, those agents can help us build “better” predictive models, where “better” can be either greater predictive accuracy or faster modeling time (which, in turn, enables the exploration of a wider range of options). We also caution that the era of papers that just use data miners is coming to an end. Results obtained from an unoptimized data miner can be quickly refuted, just by applying an optimizer to produce a different (and better performing) model. Our conclusion, hence, is that for software analytics it is possible, useful and necessary to combine data mining and optimization using DUO.},
journal = {Empirical Softw. Engg.},
month = may,
pages = {2099–2136},
numpages = {38},
keywords = {Software analytics, Data mining, Optimization, Evolutionary algorithms}
}

@inproceedings{10.1145/3358960.3379126,
author = {Cortellessa, Vittorio and Traini, Luca},
title = {Detecting Latency Degradation Patterns in Service-based Systems},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379126},
doi = {10.1145/3358960.3379126},
abstract = {Performance in heterogeneous service-based systems shows non-determistic trends. Even for the same request type, latency may vary from one request to another. These variations can occur due to several reasons on different levels of the software stack: operating system, network, software libraries, application code or others. Furthermore, a request may involve several Remote Procedure Calls (RPC), where each call can be subject to performance variation. Performance analysts inspect distributed traces and seek for recurrent patterns in trace attributes, such as RPCs execution time, in order to cluster traces in which variations may be induced by the same cause. Clustering "similar" traces is a prerequisite for effective performance debugging. Given the scale of the problem, such activity can be tedious and expensive. In this paper, we present an automated approach that detects relevant RPCs execution time patterns associated to request latency degradation, i.e. latency degradation patterns. The presented approach is based on a genetic search algorithm driven by an information retrieval relevance metric and an optimized fitness evaluation. Each latency degradation pattern identifies a cluster of requests subject to latency degradation with similar patterns in RPCs execution time. We show on a microservice-based application case study that the proposed approach can effectively detect clusters identified by artificially injected latency degradation patterns. Experimental results show that our approach outperforms in terms of F-score a state-of-art approach for latency profile analysis and widely popular machine learning clustering algorithms. We also show how our approach can be easily extended to trace attributes other than RPC execution time (e.g. HTTP headers, execution node, etc.).},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {161–172},
numpages = {12},
keywords = {distributed systems, performance debugging, search-based software engineering, software performance, traces analysis},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@article{10.1145/2030365.2030367,
author = {Kulesza, Todd and Stumpf, Simone and Wong, Weng-Keen and Burnett, Margaret M. and Perona, Stephen and Ko, Amy J. and Oberst, Ian},
title = {Why-oriented end-user debugging of naive Bayes text classification},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/2030365.2030367},
doi = {10.1145/2030365.2030367},
abstract = {Machine learning techniques are increasingly used in intelligent assistants, that is, software targeted at and continuously adapting to assist end users with email, shopping, and other tasks. Examples include desktop SPAM filters, recommender systems, and handwriting recognition. Fixing such intelligent assistants when they learn incorrect behavior, however, has received only limited attention. To directly support end-user “debugging” of assistant behaviors learned via statistical machine learning, we present a Why-oriented approach which allows users to ask questions about how the assistant made its predictions, provides answers to these “why” questions, and allows users to interactively change these answers to debug the assistant's current and future predictions. To understand the strengths and weaknesses of this approach, we then conducted an exploratory study to investigate barriers that participants could encounter when debugging an intelligent assistant using our approach, and the information those participants requested to overcome these barriers. To help ensure the inclusiveness of our approach, we also explored how gender differences played a role in understanding barriers and information needs. We then used these results to consider opportunities for Why-oriented approaches to address user barriers and information needs.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = oct,
articleno = {2},
numpages = {31},
keywords = {Debugging, end-user programming, machine learning}
}

@inproceedings{10.1109/ICSE.2019.00055,
author = {Cruciani, Emilio and Miranda, Breno and Verdecchia, Roberto and Bertolino, Antonia},
title = {Scalable approaches for test suite reduction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00055},
doi = {10.1109/ICSE.2019.00055},
abstract = {Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similarity-based test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input). We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {419–429},
numpages = {11},
keywords = {clustering, random projection, similarity-based testing, software testing, test suite reduction},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/781131.781148,
author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
title = {Bug isolation via remote program sampling},
year = {2003},
isbn = {1581136625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/781131.781148},
doi = {10.1145/781131.781148},
abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
booktitle = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation},
pages = {141–154},
numpages = {14},
keywords = {assertions, bug isolation, feature selection, logistic regression, random sampling, statistical debugging},
location = {San Diego, California, USA},
series = {PLDI '03}
}

@article{10.1007/s10664-020-09854-3,
author = {Maipradit, Rungroj and Treude, Christoph and Hata, Hideaki and Matsumoto, Kenichi},
title = {Wait for it: identifying “On-Hold” self-admitted technical debt},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09854-3},
doi = {10.1007/s10664-020-09854-3},
abstract = {Self-admitted technical debt refers to situations where a software developer knows that their current implementation is not optimal and indicates this using a source code comment. In this work, we hypothesize that it is possible to develop automated techniques to understand a subset of these comments in more detail, and to propose tool support that can help developers manage self-admitted technical debt more effectively. Based on a qualitative study of 333 comments indicating self-admitted technical debt, we first identify one particular class of debt amenable to automated management: on-hold self-admitted technical debt (on-hold SATD), i.e., debt which contains a condition to indicate that a developer is waiting for a certain event or an updated functionality having been implemented elsewhere. We then design and evaluate an automated classifier which can identify these on-hold instances with an area under the receiver operating characteristic curve (AUC) of 0.98 as well as detect the specific conditions that developers are waiting for. Our work presents a first step towards automated tool support that is able to indicate when certain instances of self-admitted technical debt are ready to be addressed.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3770–3798},
numpages = {29},
keywords = {Self-admitted technical debt, Qualitative study, Classification}
}

@article{10.1007/s10515-016-0198-6,
author = {Zhao, Yangyang and Yang, Yibiao and Lu, Hongmin and Liu, Jinping and Leung, Hareton and Wu, Yansong and Zhou, Yuming and Xu, Baowen},
title = {Understanding the value of considering client usage context in package cohesion for fault-proneness prediction},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-016-0198-6},
doi = {10.1007/s10515-016-0198-6},
abstract = {By far, many package cohesion metrics have been proposed from internal structure view and external usage view. Based on whether client usage context (i.e., the way packages are used by their clients) is exploited, we group these metrics into two categories: non-context-based and context-based. Currently, there is no comprehensive empirical research devoted to understanding the actual value of client usage context for fault-proneness prediction. In this study, we conduct a thorough empirical study to investigate the value of considering client usage context in package cohesion for fault-proneness prediction. First, we use principal component analysis to examine the relationships between context-based and non-context-based cohesion metrics. Second, we employ univariate logistic regression analysis to investigate the correlation between context-based cohesion metrics and fault-proneness. Then, we build multivariate prediction models to analyze the ability of context-based cohesion metrics for fault-proneness prediction when used alone or used together with non-context-based cohesion metrics. To obtain comprehensive evaluations, we evaluate the effectiveness of these multivariate models in the ranking and classification scenarios from both cross-validation and across-version perspectives. The experimental results show that (1) context-based cohesion metrics are complementary to non-context-based cohesion metrics; (2) most of context-based cohesion metrics have a significantly negative association with fault-proneness; (3) when used alone or used together with non-context-based cohesion metrics, context-based cohesion metrics can substantially improve the effectiveness of fault-proneness prediction in most studied systems under both cross-validation and across-version evaluation. Client usage context has an important value in package cohesion for fault-proneness prediction.},
journal = {Automated Software Engg.},
month = jun,
pages = {393–453},
numpages = {61},
keywords = {Cohesion, Context, Fault-proneness, Metrics, Package, Prediction}
}

@inproceedings{10.1007/978-3-030-03192-3_21,
author = {Tran, Ha Manh and Van Nguyen, Sinh and Ha, Synh Viet Uyen and Le, Thanh Quoc},
title = {An Analysis of Software Bug Reports Using Random Forest},
year = {2018},
isbn = {978-3-030-03191-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-03192-3_21},
doi = {10.1007/978-3-030-03192-3_21},
abstract = {Bug tracking systems manage bug reports for assuring the quality of software products. A bug report also referred as trouble, problem, ticket or defect contains several features for problem management and resolution purposes. Severity and priority are two essential features of a bug report that define the effect level and fixing order of the bug. Determining these features is challenging and depends heavily on human being, e.g., software developers or system operators, especially for assessing a large number of error and warning events occurring on software products or network services. This study proposes an approach of using random forest for assessing severity and priority for software bug reports automatically. This approach aims at constructing multiple decision trees based on the subsets of the existing bug dataset and features, and then selecting the best decision trees to assess the severity and priority of new bugs. The approach can be applied for detecting and forecasting faults in large, complex communication networks and distributed systems today. We have presented the applicability of random forest for bug report analysis and performed several experiments on software bug datasets obtained from open source bug tracking systems. Random forest yields an average accuracy score of 0.75 that can be sufficient for assisting system operators in determining these features. We have provided some analysis of the experimental results.},
booktitle = {Future Data and Security Engineering: 5th International Conference, FDSE 2018, Ho Chi Minh City, Vietnam, November 28–30, 2018, Proceedings},
pages = {273–285},
numpages = {13},
keywords = {Random forest, Decision tree, Software bug report, Network fault detection, Fault management},
location = {Ho Chi Minh City, Vietnam}
}

@article{10.1007/s11219-013-9197-z,
author = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
title = {Quality attribute modeling and quality aware product configuration in software product lines},
year = {2014},
issue_date = {September 2014},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-013-9197-z},
doi = {10.1007/s11219-013-9197-z},
abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study.},
journal = {Software Quality Journal},
month = sep,
pages = {365–401},
numpages = {37},
keywords = {Analytic hierarchical process (AHP), Feature model, Non-functional requirement (NFR) framework, Product configuration, Quality attributes assessment, Software product line}
}

@inproceedings{10.1145/1065010.1065014,
author = {Liblit, Ben and Naik, Mayur and Zheng, Alice X. and Aiken, Alex and Jordan, Michael I.},
title = {Scalable statistical bug isolation},
year = {2005},
isbn = {1595930566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1065010.1065014},
doi = {10.1145/1065010.1065014},
abstract = {We present a statistical debugging algorithm that isolates bugs in programs containing multiple undiagnosed bugs. Earlier statistical algorithms that focus solely on identifying predictors that correlate with program failure perform poorly when there are multiple bugs. Our new technique separates the effects of different bugs and identifies predictors that are associated with individual bugs. These predictors reveal both the circumstances under which bugs occur as well as the frequencies of failure modes, making it easier to prioritize debugging efforts. Our algorithm is validated using several case studies, including examples in which the algorithm identified previously unknown, significant crashing bugs in widely used systems.},
booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {15–26},
numpages = {12},
keywords = {bug isolation, feature selection, invariants, random sampling, statistical debugging},
location = {Chicago, IL, USA},
series = {PLDI '05}
}

@article{10.1007/s10664-015-9368-6,
author = {Cheung, Wai Ting and Ryu, Sukyoung and Kim, Sunghun},
title = {Development nature matters: An empirical study of code clones in JavaScript applications},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9368-6},
doi = {10.1007/s10664-015-9368-6},
abstract = {Code cloning is one of the active research areas in the software engineering community. Specifically, researchers have conducted numerous empirical studies on code cloning and reported that 7 % to 23 % of the code in a typical software system has been cloned. However, there was less awareness of code clones in dynamically-typed languages and most studies are limited to statically-typed languages such as Java, C, and C++. In addition, most previous studies did not consider different application domains such as standalone projects or web applications. As a result, very little is known about clones in dynamically-typed languages, such as JavaScript, in different application domains. In this paper, we report a large-scale clone detection experiment in a dynamically-typed programming language, JavaScript, for different application domains: web pages and standalone projects. Our experimental results showed that unlike JavaScript standalone projects, JavaScript web applications have 95 % of inter-file clones and 91---97 % of widely scattered clones. We observed that web application developers created clones intentionally and such clones may not be as risky as claimed in previous studies. Understanding the risks of cloning in web applications requires further studies, as cloning may be due to either good or bad intentions. Also, we identified unique development practices such as including browser-dependent or device-specific code in code clones of JavaScript web applications. This indicates that features of programming languages and technologies affect how developers duplicate code.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {517–564},
numpages = {48},
keywords = {Clone properties, Cloning patterns, Code clones, JavaScript, Software metrics, Web applications}
}

@inproceedings{10.1145/2491411.2491449,
author = {Sun, Chengnian and Khoo, Siau-Cheng},
title = {Mining succinct predicated bug signatures},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491449},
doi = {10.1145/2491411.2491449},
abstract = {A bug signature is a set of program elements highlighting the cause or effect of a bug, and provides contextual information for debugging. In order to mine a signature for a buggy program, two sets of execution profiles of the program, one capturing the correct execution and the other capturing the faulty, are examined to identify the program elements contrasting faulty from correct. Signatures solely consisting of control flow transitions have been investigated via discriminative sequence and graph mining algorithms. These signatures might be handicapped in cases where the effect of a bug is not manifested by any deviation in control flow transitions. In this paper, we introduce the notion of predicated bug signature/ that aims to enhance the predictive power of bug signatures by utilizing both data predicates and control-flow information. We introduce a novel ``discriminative itemset generator'' mining technique to generate succinct/ signatures which do not contain redundant or irrelevant program elements. Our case studies demonstrate that predicated signatures can hint at more scenarios of bugs where traditional control-flow signatures fail.},
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {576–586},
numpages = {11},
keywords = {bug signature, feature selection, statistical debugging},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@article{10.1007/s10664-016-9460-6,
author = {Falessi, Davide and Di Penta, Massimiliano and Canfora, Gerardo and Cantone, Giovanni},
title = {Estimating the number of remaining links in traceability recovery},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9460-6},
doi = {10.1007/s10664-016-9460-6},
abstract = {Although very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different kinds of software artifacts including requirements, use cases, design documents, source code, and test cases. Results from our study indicate that: (i) specific estimation models are able to provide accurate estimates of the number of remaining positive links; (ii) the estimation accuracy depends on the choice of the NLP technique, and (iii) univariate estimation models outperform multivariate ones.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {996–1027},
numpages = {32},
keywords = {Information retrieval, Metrics and measurement, Traceability link recovery}
}

@article{10.1016/j.jss.2018.07.055,
author = {Bi, Tingting and Liang, Peng and Tang, Antony and Yang, Chen},
title = {A systematic mapping study on text analysis techniques in software architecture},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.055},
doi = {10.1016/j.jss.2018.07.055},
journal = {J. Syst. Softw.},
month = oct,
pages = {533–558},
numpages = {26},
keywords = {Software architecture, Text analysis technique, Systematic mapping study}
}

@article{10.1016/j.ijinfomgt.2018.08.015,
author = {Amin, Adnan and Shah, Babar and Khattak, Asad Masood and Lopes Moreira, Fernando Joaquim and Ali, Gohar and Rocha, Alvaro and Anwar, Sajid},
title = {Cross-company customer churn prediction in telecommunication: A comparison of data transformation methods},
year = {2019},
issue_date = {Jun 2019},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {0268-4012},
url = {https://doi.org/10.1016/j.ijinfomgt.2018.08.015},
doi = {10.1016/j.ijinfomgt.2018.08.015},
journal = {Int. J. Inf. Manag.},
month = jun,
pages = {304–319},
numpages = {16},
keywords = {Churn prediction, Cross-company, Data transformation, Box-cox, Rank, Log, Z-Score}
}

@article{10.1007/s10772-020-09783-y,
author = {Rahul, Kumar and Banyal, Rohitash Kumar},
title = {RETRACTED ARTICLE: Firefly algorithm: an optimization solution in big data processing for the healthcare and engineering sector},
year = {2021},
issue_date = {Sep 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {3},
issn = {1381-2416},
url = {https://doi.org/10.1007/s10772-020-09783-y},
doi = {10.1007/s10772-020-09783-y},
abstract = {The firefly algorithm is nature-inspired, and it belongs to swarm intelligence category optimization. Firefly algorithm is the latest algorithm developed under the nature-inspired algorithm (NIA), which is fruitful for business and engineering optimization. So, for business optimization, healthcare industries are affected through big data where big data affects in different ways such as patients understanding and care, improved personalized care, analysis trends, predict health outcomes, etc. Big data analysis is required to optimize a broad set of data generated by the different mediums in the healthcare and engineering sectors. A firefly algorithm can be used to optimize data analysis and results. Therefore, the firefly algorithm becomes essential to optimize the healthcare sector application process and provides optimized solutions. This paper discussed firefly algorithm implementations, optimization solutions for an engineering problem, fireflies' algorithm (FA) performance through MATLAB 2019b, formulation, optimization problems in healthcare, firefly algorithms in other applications, and conclusions. This paper focuses on how the firefly algorithm (FA) can be used, modified, and merged with other optimization algorithms to solve engineering problems.},
journal = {Int. J. Speech Technol.},
month = sep,
pages = {581–592},
numpages = {12},
keywords = {Nature-inspired algorithm, Fireflies, Optimization, Particle swarm optimization, Healthcare}
}

@inproceedings{10.5555/2820282.2820290,
author = {Le, Tien-Duy B. and Linares-V\'{a}squez, Mario and Lo, David and Poshyvanyk, Denys},
title = {RCLinker: automated linking of issue reports and commits leveraging rich contextual information},
year = {2015},
publisher = {IEEE Press},
abstract = {Links between issue reports and their corresponding commits in version control systems are often missing. However, these links are important for measuring the quality of a software system, predicting defects, and many other tasks. Several approaches have been designed to solve this problem by automatically linking bug reports to source code commits via comparison of textual information in commit messages and bug reports. Yet, the effectiveness of these techniques is oftentimes suboptimal when commit messages are empty or contain minimum information; this particular problem makes the process of recovering traceability links between commits and bug reports particularly challenging. In this work, we aim at improving the effectiveness of existing bug linking techniques by utilizing rich contextual information. We rely on a recently proposed approach, namely ChangeScribe, which generates commit messages containing rich contextual information by using code summarization techniques. Our approach then extracts features from these automatically generated commit messages and bug reports, and inputs them into a classification technique that creates a discriminative model used to predict if a link exists between a commit message and a bug report. We compared our approach, coined as RCLinker (Rich Context Linker), to MLink, which is an existing state-of-the-art bug linking approach. Our experiment results on bug reports from six software projects show that RCLinker outperforms MLink in terms of F-measure by 138.66%.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {36–47},
numpages = {12},
location = {Florence, Italy},
series = {ICPC '15}
}

@article{10.1016/j.jss.2006.10.049,
author = {Zhou, Yuming and Leung, Hareton},
title = {Predicting object-oriented software maintainability using multivariate adaptive regression splines},
year = {2007},
issue_date = {August, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.10.049},
doi = {10.1016/j.jss.2006.10.049},
abstract = {Accurate software metrics-based maintainability prediction can not only enable developers to better identify the determinants of software quality and thus help them improve design or coding, it can also provide managers with useful information to help them plan the use of valuable resources. In this paper, we employ a novel exploratory modeling technique, multiple adaptive regression splines (MARS), to build software maintainability prediction models using the metric data collected from two different object-oriented systems. The prediction accuracy of the MARS models are evaluated and compared using multivariate linear regression models, artificial neural network models, regression tree models, and support vector models. The results suggest that for one system MARS can predict maintainability more accurately than the other four typical modeling techniques, and that for the other system MARS is as accurate as the best modeling technique.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1349–1361},
numpages = {13},
keywords = {Maintainability, Multiple adaptive regression splines, Object-oriented, Prediction}
}

@inproceedings{10.1145/2896995.2896997,
author = {Didar Al Alam, S. M. and Karim, Muhammad Rezaul and Pfahl, Dietmar and Ruhe, G\"{u}nther},
title = {Comparative analysis of predictive techniques for release readiness classification},
year = {2016},
isbn = {9781450341653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896995.2896997},
doi = {10.1145/2896995.2896997},
abstract = {Context: A software release is the deployment of a version of an evolving software product. Product managers are typically responsible for deciding the release content, time frame, price, and quality of the release. Due to all the dynamic changes in the project and process parameters, the decision is highly complex and of high impact.Objective: This paper has two objectives: i) Comparative analysis of predictive techniques in classifying an ongoing release in terms of its expected release readiness., and ii) Comparative analysis between regular and ensemble classifiers to classify an ongoing release in terms of its expected release readiness.Methodology: We use machine learning classifiers to predict release readiness. We analyzed three OSS projects under Apache Software Foundation from JIRA issue repository. As a retrospective study, we covered a period of 70 months, 85 releases and 1696 issues. We monitored eight established variables to train classifiers in order to predict whether releases will be ready versus non-ready. Predictive performance of different classifiers was compared by measuring precision, recall, F-measure, balanced accuracy, and area under the ROC curve (AUC).Results: Comparative analysis among nine classifiers revealed that ensemble classifiers significantly outperform regular classifiers. Balancing precision and recall, Random Forrest and BaggedADABoost were the two best performers in total, while Na\"{\i}ve Bayes performed best among just the regular classifiers.},
booktitle = {Proceedings of the 5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {15–21},
numpages = {7},
keywords = {classification, comparative analysis, empirical analysis, predictive techniques, release readiness},
location = {Austin, Texas},
series = {RAISE '16}
}

@article{10.1007/s10462-020-09942-2,
author = {B\'{e}cue, Adrien and Pra\c{c}a, Isabel and Gama, Jo\~{a}o},
title = {Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities},
year = {2021},
issue_date = {Jun 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {54},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-020-09942-2},
doi = {10.1007/s10462-020-09942-2},
abstract = {This survey paper discusses opportunities and threats of using artificial intelligence (AI) technology in the manufacturing sector with consideration for offensive and defensive uses of such technology. It starts with an introduction of Industry 4.0 concept and an understanding of AI use in this context. Then provides elements of security principles and detection techniques applied to operational technology (OT) which forms the main attack surface of manufacturing systems. As some intrusion detection systems (IDS) already involve some AI-based techniques, we focus on existing machine-learning and data-mining based techniques in use for intrusion detection. This article presents the major strengths and weaknesses of the main techniques in use. We also discuss an assessment of their relevance for application to OT, from the manufacturer point of view. Another part of the paper introduces the essential drivers and principles of Industry 4.0, providing insights on the advent of AI in manufacturing systems as well as an understanding of the new set of challenges it implies. AI-based techniques for production monitoring, optimisation and control are proposed with insights on several application cases. The related technical, operational and security challenges are discussed and an understanding of the impact of such transition on current security practices is then provided in more details. The final part of the report further develops a vision of security challenges for Industry 4.0. It addresses aspects of orchestration of distributed detection techniques, introduces an approach to adversarial/robust AI development and concludes with human–machine behaviour monitoring requirements.},
journal = {Artif. Intell. Rev.},
month = jun,
pages = {3849–3886},
numpages = {38},
keywords = {Intrusion detection systems, Security, Industry 4.0, Artificial intelligence}
}

@inproceedings{10.1145/3324884.3416616,
author = {Chen, Bihuan and Chen, Linlin and Zhang, Chen and Peng, Xin},
title = {BuildFast: history-aware build outcome prediction for fast feedback and reduced cost in continuous integration},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416616},
doi = {10.1145/3324884.3416616},
abstract = {Long build times in continuous integration (CI) can greatly increase the cost in human and computing resources, and thus become a common barrier faced by software organizations adopting CI. Build outcome prediction has been proposed as one of the remedies to reduce such cost. However, the state-of-the-art approaches have a poor prediction performance for failed builds, and are not designed for practical usage scenarios. To address the problems, we first conduct an empirical study on 2,590,917 builds to characterize build times in real-world projects, and a survey with 75 developers to understand their perceptions about build outcome prediction. Then, motivated by our study and survey results, we propose a new history-aware approach, named BuildFast, to predict CI build outcomes cost-efficiently and practically. We develop multiple failure-specific features from closely related historical builds via analyzing build logs and changed files, and propose an adaptive prediction model to switch between two models based on the build outcome of the previous build. We investigate a practical online usage scenario of BuildFast, where builds are predicted in chronological order, and measure the benefit from correct predictions and the cost from incorrect predictions. Our experiments on 20 projects have shown that BuildFast improved the state-of-the-art by 47.5% in F1-score for failed builds.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {42–53},
numpages = {12},
keywords = {build failures, continuous integration, failure prediction},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1016/j.jss.2010.12.036,
author = {Reyes, Francisco and Cerpa, Narciso and Candia-V\'{e}jar, Alfredo and Bardeen, Matthew},
title = {The optimization of success probability for software projects using genetic algorithms},
year = {2011},
issue_date = {May, 2011},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {84},
number = {5},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2010.12.036},
doi = {10.1016/j.jss.2010.12.036},
abstract = {The software development process is usually affected by many risk factors that may cause the loss of control and failure, thus which need to be identified and mitigated by project managers. Software development companies are currently improving their process by adopting internationally accepted practices, with the aim of avoiding risks and demonstrating the quality of their work. This paper aims to develop a method to identify which risk factors are more influential in determining project outcome. This method must also propose a cost effective investment of project resources to improve the probability of project success. To achieve these aims, we use the probability of success relative to cost to calculate the efficiency of the probable project outcome. The definition of efficiency used in this paper was proposed by researchers in the field of education. We then use this efficiency as the fitness function in an optimization technique based on genetic algorithms. This method maximizes the success probability output of a prediction model relative to cost. The optimization method was tested with several software risk prediction models that have been developed based on the literature and using data from a survey which collected information from in-house and outsourced software development projects in the Chilean software industry. These models predict the probability of success of a project based on the activities undertaken by the project manager and development team. The results show that the proposed method is very useful to identify those activities needing greater allocation of resources, and which of these will have a higher impact on the projects success probability. Therefore using the measure of efficiency has allowed a modular approach to identify those activities in software development on which to focus the project's limited resources to improve its probability of success. The genetic algorithm and the measure of efficiency presented in this paper permit model independence, in both prediction of success and cost evaluation.},
journal = {J. Syst. Softw.},
month = may,
pages = {775–785},
numpages = {11},
keywords = {Efficiency, Genetic algorithm, Optimization, Prediction model, Software project outcome, Software project success}
}

@article{10.1007/s10922-013-9289-x,
author = {Bashar, Abul and Parr, Gerard and Mcclean, Sally and Scotney, Bryan and Nauck, Detlef},
title = {Application of Bayesian Networks for Autonomic Network Management},
year = {2014},
issue_date = {April     2014},
publisher = {Plenum Press},
address = {USA},
volume = {22},
number = {2},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-013-9289-x},
doi = {10.1007/s10922-013-9289-x},
abstract = {The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework.},
journal = {J. Netw. Syst. Manage.},
month = apr,
pages = {174–207},
numpages = {34},
keywords = {Artificial Intelligence, Bayesian Networks, Call Admission Control, Data Mining, Intelligent Traffic Engineering, Machine Learning, Network Management, Next Generation Networks}
}

@article{10.1016/j.infsof.2016.05.007,
author = {Bao, Tie and Liu, Shufen},
title = {Quality evaluation and analysis for domain software},
year = {2016},
issue_date = {October 2016},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {78},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.05.007},
doi = {10.1016/j.infsof.2016.05.007},
abstract = {Establishing attribute model to illustrate attributes to be looked into.Establishing evaluation model to illustrate computation logic of evaluation.Establishing classifies model to illustrate quality levels and mapping conditions.Analyzing quality bottleneck problems based on evaluation model data. Context: Domain software plays an important role in the management of many enterprises; therefore, its quality is critical to these enterprises and impacts its selection, maintenance, and service quality evaluation to a great extent. Consequently, how to evaluate the quality of domain software has become an area that deserves more research. The existing research pays much attention to software for specific purposes and fields, or to specific stage of the software, which is not enough.Objective: This paper proposes a systematic quality evaluation method, which fully considers the generality, domain features, and application environment of domain software, and also supports subsequent analysis.Method: The quality evaluation method proposed in this paper defines all aspects of the domain software to be inspected by establishing an attribute model, obtains basic data on evaluation by analyzing and measuring evidence, tailor-makes the computing logic of quality evaluation score by establishing an evaluation model, and classifies domain software quality by establishing level model. This research verifies the applicability of evaluation by applying this method to management information system (MIS) in the field of a power plant.Results: The case example not only indicates that the proposed method can be applied to the quality evaluation and analysis of the power plant's MIS, but also its ease of usability.Conclusion: The method is applicable and can be easily used for the quality evaluation and analysis of the MIS in a power plant. Based on mechanisms, such as model customization and weight determination, this method also supports the quality evaluation and analysis of software applied in different fields. In the future, there will be a need to improve the objectivity and automation degree of the evaluation, and to apply and verify the method according to the different types of software in various fields.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {53–65},
numpages = {13},
keywords = {Domain software, Evaluation model, Evidence metrics, Level model, Software quality}
}

@inproceedings{10.1145/2639490.2639505,
author = {Wiese, Igor Scaliante and C\^{o}go, Filipe Roseiro and R\'{e}, Reginaldo and Steinmacher, Igor and Gerosa, Marco Aur\'{e}lio},
title = {Social metrics included in prediction models on software engineering: a mapping study},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639505},
doi = {10.1145/2639490.2639505},
abstract = {Context: Previous work that used prediction models on Software Engineering included few social metrics as predictors, even though many researchers argue that Software Engineering is a social activity. Even when social metrics were considered, they were classified as part of other dimensions, such as process, history, or change. Moreover, few papers report the individual effects of social metrics. Thus, it is not clear yet which social metrics are used in prediction models and what are the results of their use in different contexts. Objective: To identify, characterize, and classify social metrics included in prediction models reported in the literature. Method: We conducted a mapping study (MS) using a snowballing citation analysis. We built an initial seed list adapting strings of two previous systematic reviews on software prediction models. After that, we conducted backward and forward citation analysis using the initial seed list. Finally, we visited the profile of each distinct author identified in the previous steps and contacted each author that published more than 2 papers to ask for additional candidate studies. Results: We identified 48 primary studies and 51 social metrics. We organized the metrics into nine categories, which were divided into three groups - communication, project, and commit-related. We also mapped the applications of each group of metrics, indicating their positive or negative effects. Conclusions: This mapping may support researchers and practitioners to build their prediction models considering more social metrics.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {72–81},
numpages = {10},
keywords = {mapping study, prediction models, social metrics, social network analysis},
location = {Turin, Italy},
series = {PROMISE '14}
}

@article{10.1007/s11219-010-9127-2,
author = {Bagheri, Ebrahim and Gasevic, Dragan},
title = {Assessing the maintainability of software product line feature models using structural metrics},
year = {2011},
issue_date = {September 2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {19},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-010-9127-2},
doi = {10.1007/s11219-010-9127-2},
abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models.},
journal = {Software Quality Journal},
month = sep,
pages = {579–612},
numpages = {34},
keywords = {Controlled experimentation, Feature model, Maintainability, Quality attributes, Software prediction model, Software product line, Structural complexity}
}

@article{10.1016/j.jss.2008.07.019,
author = {Chan, W. K. and Cheung, S. C. and Ho, Jeffrey C. F. and Tse, T. H.},
title = {PAT: A pattern classification approach to automatic reference oracles for the testing of mesh simplification programs},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {3},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2008.07.019},
doi = {10.1016/j.jss.2008.07.019},
abstract = {Graphics applications often need to manipulate numerous graphical objects stored as polygonal models. Mesh simplification is an approach to vary the levels of visual details as appropriate, thereby improving on the overall performance of the applications. Different mesh simplification algorithms may cater for different needs, producing diversified types of simplified polygonal model as a result. Testing mesh simplification implementations is essential to assure the quality of the graphics applications. However, it is very difficult to determine the oracles (or expected outcomes) of mesh simplification for the verification of test results. A reference model is an implementation closely related to the program under test. Is it possible to use such reference models as pseudo-oracles for testing mesh simplification programs? If so, how effective are they? This paper presents a fault-based pattern classification methodology called PAT, to address the questions. In PAT, we train the C4.5 classifier using black-box features of samples from a reference model and its fault-based versions, in order to test samples from the subject program. We evaluate PAT using four implementations of mesh simplification algorithms as reference models applied to 44 open-source three-dimensional polygonal models. Empirical results reveal that the use of a reference model as a pseudo-oracle is effective for testing the implementations of resembling mesh simplification algorithms. However, the results also show a tradeoff: When compared with a simple reference model, the use of a resembling but sophisticated reference model is more effective and accurate but less robust.},
journal = {J. Syst. Softw.},
month = mar,
pages = {422–434},
numpages = {13},
keywords = {Graphics rendering, Mesh simplification, Pattern classification reference models, Software testing, Test oracles}
}

@inproceedings{10.1145/3293882.3338984,
author = {Grano, Giovanni},
title = {A new dimension of test quality: assessing and generating higher quality unit test cases},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338984},
doi = {10.1145/3293882.3338984},
abstract = {Unit tests form the first defensive line against the introduction of bugs in software systems. Therefore, their quality is of a paramount importance to produce robust and reliable software. To assess test quality, many organizations relies on metrics like code and mutation coverage. However, they are not always optimal to fulfill such a purpose. In my research, I want to make mutation testing scalable by devising a lightweight approach to estimate test effectiveness. Moreover, I plan to introduce a new metric measuring test focus—as a proxy for the effort needed by developers to understand and maintain a test— that both complements code coverage to assess test quality and can be used to drive automated test case generation of higher quality tests.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {419–423},
numpages = {5},
keywords = {Automated Testing, Software Testing, Test Quality},
location = {Beijing, China},
series = {ISSTA 2019}
}

@article{10.1007/s10515-014-0160-4,
author = {Devine, Thomas and Goseva-Popstojanova, Katerina and Krishnan, Sandeep and Lutz, Robyn R.},
title = {Assessment and cross-product prediction of software product line quality: accounting for reuse across products, over multiple releases},
year = {2016},
issue_date = {June      2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-014-0160-4},
doi = {10.1007/s10515-014-0160-4},
abstract = {The goals of cross-product reuse in a software product line (SPL) are to mitigate production costs and improve the quality. In addition to reuse across products, due to the evolutionary development process, a SPL also exhibits reuse across releases. In this paper, we empirically explore how the two types of reuse--reuse across products and reuse across releases--affect the quality of a SPL and our ability to accurately predict fault proneness. We measure the quality in terms of post-release faults and consider different levels of reuse across products (i.e., common, high-reuse variation, low-reuse variation, and single-use packages), over multiple releases. Assessment results showed that quality improved for common, low-reuse variation, and single-use packages as they evolved across releases. Surprisingly, within each release, among preexisting (`old') packages, the cross-product reuse did not affect the change and fault proneness. Cross-product predictions based on pre-release data accurately ranked the packages according to their post-release faults and predicted the 20 % most faulty packages. The predictions benefited from data available for other products in the product line, with models producing better results (1) when making predictions on smaller products (consisting mostly of common packages) rather than on larger products and (2) when trained on larger products rather than on smaller products.},
journal = {Automated Software Engg.},
month = jun,
pages = {253–302},
numpages = {50},
keywords = {Assessment, Cross-product prediction, Cross-product reuse, Cross-release reuse, Fault proneness prediction, Longitudinal study, Software product lines}
}

@inproceedings{10.1109/ASE.2019.00096,
author = {Gerostathopoulos, Ilias and Kugele, Stefan and Segler, Christoph and Bures, Tomas and Knoll, Alois},
title = {Automated trainability evaluation for smart software functions},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00096},
doi = {10.1109/ASE.2019.00096},
abstract = {More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)---which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {998–1001},
numpages = {4},
keywords = {continuous deployment, smart software functions, trainability},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.knosys.2011.12.009,
author = {Casamayor, Agustin and Godoy, Daniela and Campo, Marcelo},
title = {Functional grouping of natural language requirements for assistance in architectural software design},
year = {2012},
issue_date = {June, 2012},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {30},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2011.12.009},
doi = {10.1016/j.knosys.2011.12.009},
abstract = {Modern software systems are becoming larger and more complex every day. One of the most challenging steps for designing a good architecture for a certain piece of software is the analysis of requirements, usually written in natural language by engineers not familiar with specific design formalisms. The main problem related to this task is the conceptual gap existing between low-level requirements and higher views of the system decomposing its functionality. In this paper, we introduce an approach for mining and grouping functionality from textual descriptions of requirements using text mining techniques aiming at helping software designers with this complex and time-consuming task. The knowledge discovered starting from informally written requirements using a combination of natural language processing (NLP) and text clustering algorithms can be then easily mapped into design concerns of a possible architecture for the system. Experimental validation in three case studies suggests a great potential of the proposed approach for providing assistance to software designers during early stages of the software development process.},
journal = {Know.-Based Syst.},
month = jun,
pages = {78–86},
numpages = {9},
keywords = {Functional grouping of requirements, Natural language processing, Requirements engineering, Software design, Text mining}
}

@article{10.1007/s10664-015-9402-8,
author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
title = {A survey on the use of topic models when mining software repositories},
year = {2016},
issue_date = {October   2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9402-8},
doi = {10.1007/s10664-015-9402-8},
abstract = {Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {1843–1919},
numpages = {77},
keywords = {LDA, LSI, Survey, Topic modeling}
}

@article{10.1007/s00766-021-00353-5,
author = {Robinson, Matthew and Sarkani, Shahram and Mazzuchi, Thomas},
title = {Network structure and requirements crowdsourcing for OSS projects},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {4},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-021-00353-5},
doi = {10.1007/s00766-021-00353-5},
abstract = {Crowdsourcing system requirements enables project managers to elicit feedback from a broader range of stakeholders. The advantages of crowdsourcing include a higher volume of requirements reflecting a more comprehensive array of use cases and a more engaged and committed user base. Researchers cite the inability of project teams to effectively manage an increasing volume of system requirements as a possible drawback. This paper analyzes a data set consisting of project management artifacts from 562 open-source software (OSS) projects to determine how OSS project performance varies as the share of crowdsourced requirements increases using six measures of effectiveness: requirement close-out time, requirement response time, average comment activity, the average number of requirements per crowd member, the average retention time for crowd members, and the total volume of requirements. Additionally, the models measure how the impact of increasing the share of crowdsourced requirements changes with stakeholder network structure. The analysis shows that stakeholder network structure impacts OSS performance outcomes and that the effect changes with the share of crowdsourced requirements. OSS projects with more concentrated stakeholder networks perform the best. The results indicate that requirements crowdsourcing faces diminishing marginal returns. OSS projects that crowdsource more than 70% of their requirements benefit more from implementing processes to organize and prioritize existing requirements than from incentivizing the crowd to generate additional requirements. Analysis in the paper also suggests that OSS projects could benefit from employing CrowdRE techniques and assigning dedicated community managers to more effectively channel input from the crowd.},
journal = {Requir. Eng.},
month = dec,
pages = {509–534},
numpages = {26},
keywords = {CrowdRE, Crowdsourcing, Collaborative requirements elicitation, Stakeholder networks, Stakeholder analysis}
}

@inproceedings{10.1109/ICSE.2017.70,
author = {Chen, Junjie and Bai, Yanwei and Hao, Dan and Xiong, Yingfei and Zhang, Hongyu and Xie, Bing},
title = {Learning to prioritize test programs for compiler testing},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.70},
doi = {10.1109/ICSE.2017.70},
abstract = {Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {700–711},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3447247,
author = {Guo, Zhaoqiang and Liu, Shiran and Liu, Jinping and Li, Yanhui and Chen, Lin and Lu, Hongmin and Zhou, Yuming},
title = {How Far Have We Progressed in Identifying Self-admitted Technical Debts? A Comprehensive Empirical Study},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3447247},
doi = {10.1145/3447247},
abstract = {Background. Self-admitted technical debt (SATD) is a special kind of technical debt that is intentionally introduced and remarked by code comments. Those technical debts reduce the quality of software and increase the cost of subsequent software maintenance. Therefore, it is necessary to find out and resolve these debts in time. Recently, many automatic approaches have been proposed to identify SATD. Problem. Popular IDEs support a number of predefined task annotation tags for indicating SATD in comments, which have been used in many projects. However, such clear prior knowledge is neglected by existing SATD identification approaches when identifying SATD. Objective. We aim to investigate how far we have really progressed in the field of SATD identification by comparing existing approaches with a simple approach that leverages the predefined task tags to identify SATD. Method. We first propose a simple heuristic approach that fuzzily Matches task Annotation Tags (MAT) in comments to identify SATD. In nature, MAT is an unsupervised approach, which does not need any data to train a prediction model and has a good understandability. Then, we examine the real progress in SATD identification by comparing MAT against existing approaches. Result. The experimental results reveal that: (1) MAT has a similar or even superior performance for SATD identification compared with existing approaches, regardless of whether non-effort-aware or effort-aware evaluation indicators are considered; (2) the SATDs (or non-SATDs) correctly identified by existing approaches are highly overlapped with those identified by MAT; and (3) supervised approaches misclassify many SATDs marked with task tags as non-SATDs, which can be easily corrected by their combinations with MAT. Conclusion. It appears that the problem of SATD identification has been (unintentionally) complicated by our community, i.e., the real progress in SATD comments identification is not being achieved as it might have been envisaged. We hence suggest that, when many task tags are used in the comments of a target project, future SATD identification studies should use MAT as an easy-to-implement baseline to demonstrate the usefulness of any newly proposed approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {45},
numpages = {56},
keywords = {Self-admitted technical debt, baseline, code comment, match, task annotation tag}
}

@inproceedings{10.1007/978-3-030-88494-9_16,
author = {Shin, Donghwan and Khan, Zanis Ali and Bianculli, Domenico and Briand, Lionel},
title = {A Theoretical Framework for Understanding the Relationship Between Log Parsing and Anomaly Detection},
year = {2021},
isbn = {978-3-030-88493-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-88494-9_16},
doi = {10.1007/978-3-030-88494-9_16},
abstract = {Log-based anomaly detection identifies systems’ anomalous behaviors by analyzing system runtime information recorded in logs. While many approaches have been proposed, all of them have in common an essential pre-processing step called log parsing. This step is needed because automated log analysis requires structured input logs, whereas original logs contain semi-structured text printed by logging statements. Log parsing bridges this gap by converting the original logs into structured input logs fit for anomaly detection.Despite the intrinsic dependency between log parsing and anomaly detection, no existing work has investigated the impact of the “quality” of log parsing results on anomaly detection. In particular, the concept of “ideal” log parsing results with respect to anomaly detection has not been formalized yet. This makes it difficult to determine, upon obtaining inaccurate results from anomaly detection, if (and why) the root cause for such results lies in the log parsing step.In this short paper, we lay the theoretical foundations for defining the concept of “ideal” log parsing results for anomaly detection. Based on these foundations, we discuss practical implications regarding the identification and localization of root causes, when dealing with inaccurate anomaly detection, and the identification of irrelevant log messages.},
booktitle = {Runtime Verification: 21st International Conference, RV 2021, Virtual Event, October 11–14, 2021, Proceedings},
pages = {277–287},
numpages = {11},
keywords = {Log parsing, Log analysis, Anomaly detection}
}

@inproceedings{10.1145/3464432.3464438,
author = {Meier, Dominik and Mattis, Toni and Hirschfeld, Robert},
title = {Toward Exploratory Understanding of Software using Test Suites},
year = {2021},
isbn = {9781450389860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464432.3464438},
doi = {10.1145/3464432.3464438},
abstract = {Changing software without correctly understanding it often leads to confusion, as developers do not understand how the change corresponds to the new observed behaviour of the system. Today, many software systems are equipped with a test suite. Test suites document code and give feedback on changed program behaviour. We explored ways to use test suites for software comprehension and implemented a tool that provides additional visualisation and gives immediate feedback on software changes. Information about changes in the software and their implications to the test suite are collected using mutation testing. The tool uses this information to present relevant test cases for developers, and additionally prioritise test executions for immediate feedback. Our research indicates that entropy metrics can find test cases that are relevant for a specific context in the source code. Additionally, simple test case prioritisation strategies can already lead to a significant decrease in feedback time. Based on our case study we argue that test suites are not only useful for regression testing but can be used to generate meaningful information for software comprehension activities.},
booktitle = {Companion Proceedings of the 5th International Conference on the Art, Science, and Engineering of Programming},
pages = {60–67},
numpages = {8},
keywords = {immediate feedback, mutation testing, program comprehension, test prioritisation},
location = {Cambridge, United Kingdom},
series = {Programming '21}
}

@inproceedings{10.1145/3395027.3419586,
author = {Shatnawi, Ahmed S. and Munson, Ethan V.},
title = {Interactive and Scalable visualization framework for Version-aware XML documents},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419586},
doi = {10.1145/3395027.3419586},
abstract = {The Extensible Markup Language (XML) is widely used to store, retrieve, and share digital documents. Recently, a form of Version Control System has been applied to the language, resulting in Version-Aware XML and allowing for enhanced portability and scalability. While Version Control Systems are able to keep track of changes made to documents, we think that there is untapped potential in the technology. In this paper, we define a set of requirements for visualization tools in a modern version control system. We also present an interactive and scalable visualization framework to represent Version-Aware-related data that helps users visualize and understand version control data, delete specific revisions of a document, and access a comprehensive overview of the entire versioning history.We evaluated our interface prototype by conducting semi-structured usability tests and questionnaires to obtain both qualitative and quantitative feedback from volunteers with a general technological background.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {11},
numpages = {4},
keywords = {Document Content Analysis, Document engineering, Version control},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@article{10.1016/j.jss.2017.08.045,
author = {Zhang, Fanlong and Khoo, Siau-cheng and Su, Xiaohong},
title = {Predicting change consistency in a clone group},
year = {2017},
issue_date = {December 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {134},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.08.045},
doi = {10.1016/j.jss.2017.08.045},
abstract = {Developed a Bayesian network for predicting clone consistency-requirement.Introduced code, context and clone evolution attributes to quantify clone change.Performed experiments on 4 open source repositories to demonstrate its effectiveness.Provided a plug-in prototype in Eclipse to avoid clone consistency-defect. Code cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as clone consistency-defect, which can adversely impact software reusability.In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability.},
journal = {J. Syst. Softw.},
month = dec,
pages = {105–119},
numpages = {15},
keywords = {Bayesian network, Clone attributes, Clone maintenance, Code clones, Consistency-requirement prediction, Software reuse}
}

@article{10.1007/s11334-021-00384-9,
author = {Bajaj, Anu and Sangwan, Om Prakash},
title = {Tri-level regression testing using nature-inspired algorithms},
year = {2021},
issue_date = {Mar 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {17},
number = {1},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-021-00384-9},
doi = {10.1007/s11334-021-00384-9},
abstract = {A software needs to be updated to survive in the customers’ ever-changing demands and the competitive market. The modifications may produce undesirable changes that require retesting, known as regression testing, before releasing it in the public domain. This retesting cost increases with the growth of the software test suite. Thus, regression testing is divided into three techniques: test case prioritization, selection, and minimization to reduce costs and efforts. The efficiency and effectiveness of these techniques are further enhanced with the help of optimization techniques. Therefore, we present the regression testing using well-known algorithms, genetic algorithm, particle swarm optimization, a relatively new nature-inspired approach, gravitational search algorithm, and its hybrid with particle swarm optimization algorithm. Furthermore, we propose a tri-level regression testing, i.e., it performs all the three methods in succession. Nature-inspired algorithms prioritize the test cases on code coverage criteria. It is followed by selecting the modification-revealing test cases based on the proposed adaptive test case selection approach. The last step consists of the removal of redundant test cases. The hybrid algorithm performed well for the average percentage of statement coverage, and the efficiency of genetic algorithm and particle swarm optimization is better comparatively. The proposed test case selection method can select at least 75% modification-revealing test cases using nature-inspired algorithms. Additionally, it minimizes the test suite with full statement coverage and almost negligible fault coverage loss. Overall, the simulation results show that the proposed hybrid technique outperformed the other algorithms.},
journal = {Innov. Syst. Softw. Eng.},
month = mar,
pages = {1–16},
numpages = {16},
keywords = {Regression testing, Test case prioritization, Test case selection, Test case minimization, Search-based software testing, Nature-inspired algorithms, Genetic algorithm, Particle swarm optimization, Gravitational search algorithm}
}

@article{10.1007/s10462-016-9478-6,
author = {Uddin, Jamal and Ghazali, Rozaida and Deris, Mustafa Mat and Naseem, Rashid and Shah, Habib},
title = {A survey on bug prioritization},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {47},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-016-9478-6},
doi = {10.1007/s10462-016-9478-6},
abstract = {Daily large number of bug reports are received in large open and close source bug tracking systems. Dealing with these reports manually utilizes time and resources which leads to delaying the resolution of important bugs. As an important process in software maintenance, bug triaging process carefully analyze these bug reports to determine, for example, whether the bugs are duplicate or unique, important or unimportant, and who will resolve them. Assigning bug reports based on their priority or importance may play an important role in enhancing the bug triaging process. The accurate and timely prioritization and hence resolution of these bug reports not only improves the quality of software maintenance task but also provides the basis to keep particular software alive. In the past decade, various studies have been conducted to prioritize bug reports using data mining techniques like classification, information retrieval and clustering that can overcome incorrect prioritization. Due to their popularity and importance, we survey the automated bug prioritization processes in a systematic way. In particular, this paper gives a small theoretical study for bug reports to motivate the necessity for work on bug prioritization. The existing work on bug prioritization and some possible problems in working with bug prioritization are summarized.},
journal = {Artif. Intell. Rev.},
month = feb,
pages = {145–180},
numpages = {36},
keywords = {Bug prioritization, Bug report, Bug triaging, Classification, Clustering, Survey}
}

@inproceedings{10.1007/978-3-030-27541-9_13,
author = {Yu, Jiahui and Gao, Hongwei and Ju, Zhaojie},
title = {KPCA-Based Visual Fault Diagnosis for Nonlinear Industrial Process},
year = {2019},
isbn = {978-3-030-27540-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-27541-9_13},
doi = {10.1007/978-3-030-27541-9_13},
abstract = {With the increasingly large-scale, continuous, and complicated chemical process, it is particularly important to ensure the stability and safety of the production process. However, in past studies, the accuracy of fault diagnosis and the degree of system visualization are still insufficient. Here, in order to solve these problems, a visual fault diagnosis system based on LabVIEW and Matlab is designed. First, the system uses LabVIEW interface design, applying Matlab to compile the algorithm program, which makes the system has a powerful data calculation and processing functions, as well as a clear visual interface, the system design also optimizes the communication interface. Second, the typical chemical production process TE (Tennessee Eastman) process is the subject of systematic testing. Additionally, because most of the industrial processes are non-linear, the fault diagnosis method based on Kernel Principal Component Analysis (KPCA) is used in the system design, and the implementation process of this method is elaborated. Finally, the system achieves the functions of TE process data acquisition, data preprocessing, and fault diagnosis lamps. A large number of simulation results verify the effectiveness of the proposed method. The system has entered the stage of laboratory application and provides a good application platform for the research of fault diagnosis of complex systems such as chemical process control.},
booktitle = {Intelligent Robotics and Applications: 12th International Conference, ICIRA 2019, Shenyang, China, August 8–11, 2019, Proceedings, Part V},
pages = {145–154},
numpages = {10},
keywords = {Fault diagnosis, TE process, KPCA, Visualization system},
location = {Shenyang, China}
}

@inproceedings{10.1007/978-3-319-46307-0_11,
author = {Ardimento, Pasquale and Bilancia, Massimo and Monopoli, Stefano},
title = {Predicting Bug-Fix Time: Using Standard Versus Topic-Based Text Categorization Techniques},
year = {2016},
isbn = {978-3-319-46306-3},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-46307-0_11},
doi = {10.1007/978-3-319-46307-0_11},
abstract = {In modern software development, finding and fixing bugs is a vital part of software development and quality assurance. Once a bug is reported, it is typically recorded in the Bug Tracking System, and is assigned to a developer to resolve (bug triage). Current practice of bug triage is largely a manual collaborative process, which is often time-consuming and error-prone. Predicting on the basis of past data the time to fix a newly-reported bug has been shown to be an important target to support the whole triage process. Many researchers have, therefore, proposed methods for automated bug-fix time prediction, largely based on statistical prediction models exploiting the attributes of bug reports. However, existing algorithms often fail to validate on multiple large projects widely-used in bug studies, mostly as a consequence of inappropriate attribute selection [2]. In this paper, instead of focusing on attribute subset selection, we explore an alternative promising approach consisting of using all available textual information. The problem of bug-fix time estimation is then mapped to a text categorization problem. We consider a multi-topic Supervised Latent Dirichlet Allocation (SLDA) model, which adds to Latent Dirichlet Allocation a response variable consisting of an unordered binary target variable, denoting time to resolution discretized into FAST (negative class) and SLOW (positive class) labels. We have evaluated SLDA on four large-scale open source projects. We show that the proposed model greatly improves recall, when compared to standard single topic algorithms.},
booktitle = {Discovery Science: 19th International Conference, DS 2016, Bari, Italy, October 19–21, 2016, Proceedings},
pages = {167–182},
numpages = {16},
keywords = {Bug triage, Bug-fix time prediction, Text categorization, Supervised topic models, Supervised Latent Dirichlet Allocation (SLDA)},
location = {Bari, Italy}
}

@inproceedings{10.1007/978-3-030-51253-8_11,
author = {Ko, In-Young and Baek, KyeongDeok and Kwon, Jung-Hyun and Lira, Hernan and Moon, HyeongCehol},
title = {Environment-Aware and Human-Centric Software Testing Framework for Cyber-Physical Systems},
year = {2019},
isbn = {978-3-030-51252-1},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-51253-8_11},
doi = {10.1007/978-3-030-51253-8_11},
abstract = {The functionalities, actuations and effects that are produced by an application of a cyber physical system (CPS) are usually consumed by users while they perform their daily activities. Therefore, it is critical to ensure that they do not interfere with human activities and do not harm the people who are involved in the CPS. In this paper, we propose a framework to test and verify the reliability and safety of CPS applications in the perspectives of CPS environments and users. The framework provides an environment-aware testing method by which the efficiency of testing CPS applications can be improved by prioritizing CPS environments, and by applying machine learning techniques. The framework also includes a metric and an algorithm by which we can test and choose the most effective services that can deliver effects from their associated physical devices to users. In addition, the framework provides a computational model to test whether a CPS application may cause a cognitive depletion or contention problems for users.},
booktitle = {Current Trends in Web Engineering: ICWE 2019 International Workshops, DSKG, KDWEB, MATWEP, Daejeon, South Korea, June 11, 2019, Proceedings},
pages = {104–115},
numpages = {12},
keywords = {Environment-aware testing, Service effects, Human cognitive resources, Service-oriented CPS software},
location = {Daejeon, Korea (Republic of)}
}

@inproceedings{10.1145/1985441.1985472,
author = {Bhattacharya, Pamela and Neamtiu, Iulian},
title = {Bug-fix time prediction models: can we do better?},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985472},
doi = {10.1145/1985441.1985472},
abstract = {Predicting bug-fix time is useful in several areas of software evolution, such as predicting software quality or coordinating development effort during bug triaging. Prior work has proposed bug-fix time prediction models that use various bug report attributes (e.g., number of developers who participated in fixing the bug, bug severity, number of patches, bug-opener's reputation) for estimating the time it will take to fix a newly-reported bug. In this paper we take a step towards constructing more accurate and more general bug-fix time prediction models by showing how existing models fail to validate on large projects widely-used in bug studies. In particular, we used multivariate and univariate regression testing to test the prediction significance of existing models on 512,474 bug reports from five open source projects: Eclipse, Chrome and three products from the Mozilla project (Firefox, Seamonkey and Thunderbird). The results of our regression testing indicate that the predictive power of existing models is between 30% and 49% and that there is a need for more independent variables (attributes) when constructing a prediction model. Additionally, we found that, unlike in prior recent studies on commercial software, in the projects we examined there is no correlation between bug-fix likelihood, bug-opener's reputation and the time it takes to fix a bug. These findings indicate three open research problems: (1) assessing whether prioritizing bugs using bug-opener's reputation is beneficial, (2) identifying attributes which are effective in predicting bug-fix time, and (3) constructing bug-fix time prediction models which can be validated on multiple projects.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {207–210},
numpages = {4},
keywords = {bug report triage, bug-fix time, issue tracking, statistical model},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@article{10.1177/1094342018760614,
author = {Cecilia, Jos\'{e} M and Chen, Daobi and Yuan, Liang and Zhang, Yunquan and Yan, Jingfu and Kahaner, David},
title = {HPC software capability landscape in China},
year = {2020},
issue_date = {Jan 2020},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {34},
number = {1},
issn = {1094-3420},
url = {https://doi.org/10.1177/1094342018760614},
doi = {10.1177/1094342018760614},
abstract = {Hardware, applications, and software are equally important to the comprehensive strength of a country in the high-performance computing (HPC) arena. China has made significant progress in developing HPC systems in recent years. The nation’s first win of the Gordon Bell Prize at Supercomputing 2016 (SC16) also represents an accomplishment in HPC applications. China’s subsequent win in 2017 shows that the 2016 accomplishment was no accident. However, lacking adequately reliable and scalable application software remains the biggest challenge for China. Scientists and engineers who can develop algorithms and software to effectively use supercomputers are in short supply. The present report describes the current HPC software development landscape in China, including government projects and leading universities/research organizations/companies in terms of developing application software and programming frameworks (middleware), as well as examples of self-developed software in the application areas of energy and physics, aerospace, manufacturing, weather and climate, biotechnology (biotech), material science, artificial intelligence (AI), and data analytics. In addition, China’s demand and supply of HPC experts are analyzed. Data for this report were generated during the first half of 2017. Some modifications to the text have been added to account for new information through the end of 2017.},
journal = {Int. J. High Perform. Comput. Appl.},
month = jan,
pages = {115–153},
numpages = {39},
keywords = {Advanced materials, artificial intelligence/AI, biotechnology/biotech, earth sciences, education, energy/power/natural resources, engineering, environmental/pollution, government S&amp;T policy/funding, high-performance computing (HPC)/hw/sw/apps, information technology/IT, manufacturing, mathematics, meteorology, modeling/simulation, physics, space/satellite/aerospace}
}

@article{10.1504/IJBIDM.2012.048729,
author = {Altidor, Wilker and Khoshgoftaar, Taghi M. and Napolitano, Amri},
title = {Measuring stability of feature ranking techniques: a noise-based approach},
year = {2012},
issue_date = {August 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {7},
number = {1/2},
issn = {1743-8195},
url = {https://doi.org/10.1504/IJBIDM.2012.048729},
doi = {10.1504/IJBIDM.2012.048729},
abstract = {One very common criterion used to evaluate feature selection methods is the performance of a chosen classifier trained with the selected features. Another important evaluation criterion that has, until recently, been neglected is the stability of these feature selection methods. While other studies have shown interest in measuring the degree of agreement between the outputs of a technique trained on randomly selected subsets from the same input data, this study presents the importance of evaluating stability in the presence of noise. Experiments are conducted with 17 filters (six standard filter-based ranking techniques and 11 threshold-based feature selection techniques) on nine different real-world datasets. This paper identifies the techniques that are inherently more sensitive to class noise and demonstrates how certain characteristics (sample size and class imbalance) of the data can affect the stability performance of some feature selection methods.},
journal = {Int. J. Bus. Intell. Data Min.},
month = aug,
pages = {80–115},
numpages = {36}
}

@article{10.1016/j.jss.2012.07.050,
author = {Nassif, Ali Bou and Ho, Danny and Capretz, Luiz Fernando},
title = {Towards an early software estimation using log-linear regression and a multilayer perceptron model},
year = {2013},
issue_date = {January, 2013},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {86},
number = {1},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2012.07.050},
doi = {10.1016/j.jss.2012.07.050},
abstract = {Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects.},
journal = {J. Syst. Softw.},
month = jan,
pages = {144–160},
numpages = {17},
keywords = {Log-linear regression model, Multilayer perceptron, Software effort estimation, Use case points}
}

@inproceedings{10.1007/978-3-030-68446-4_10,
author = {Amadini, Roberto and Gange, Graeme and Schachte, Peter and S\o{}ndergaard, Harald and Stuckey, Peter J.},
title = {Algorithm Selection for Dynamic Symbolic Execution: A Preliminary Study},
year = {2020},
isbn = {978-3-030-68445-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-68446-4_10},
doi = {10.1007/978-3-030-68446-4_10},
abstract = {Given a portfolio of algorithms, the goal of Algorithm Selection (AS) is to select the best algorithm(s) for a new, unseen problem instance. Dynamic Symbolic Execution (DSE) brings together concrete and symbolic execution to maximise the program coverage. DSE uses a constraint solver to solve the path conditions and generate new inputs to explore. In this paper we join these lines of research by introducing a model that combines DSE and AS approaches. The proposed AS/DSE model is a generic and flexible framework enabling the DSE engine to solve the path conditions it collects with a portfolio of different solvers, by exploiting and extending the well-known AS techniques that have been developed over the last decade. In this way, one can increase the coverage and sometimes even outperform the aggregate coverage achievable by running simultaneously all the solvers of the portfolio.},
booktitle = {Logic-Based Program Synthesis and Transformation: 30th International Symposium, LOPSTR 2020, Bologna, Italy, September 7–9, 2020, Proceedings},
pages = {192–209},
numpages = {18},
keywords = {Software verification, Dynamic symbolic execution, Algorithm selection, Constraint solving, Portfolio solving},
location = {Bologna, Italy}
}

@inproceedings{10.1145/3183440.3183478,
author = {Liu, Zhongxin and Huang, Qiao and Xia, Xin and Shihab, Emad and Lo, David and Li, Shanping},
title = {SATD detector: a text-mining-based self-admitted technical debt detection tool},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183478},
doi = {10.1145/3183440.3183478},
abstract = {In software projects, technical debt metaphor is used to describe the situation where developers and managers have to accept compromises in long-term software quality to achieve short-term goals. There are many types of technical debt, and self-admitted technical debt (SATD) was proposed recently to consider debt that is introduced intentionally (e.g., through temporaryfi x) and admitted by developers themselves. Previous work has shown that SATD can be successfully detected using source code comments. However, most current state-of-the-art approaches identify SATD comments through pattern matching, which achieve high precision but very low recall. That means they may miss many SATD comments and are not practical enough. In this paper, we propose SATD Detector, a tool that is able to (i) automatically detect SATD comments using text mining and (ii) highlight, list and manage detected comments in an integrated development environment (IDE). This tool consists of a Java library and an Eclipse plug-in. The Java library is the back-end, which provides command-line interfaces and Java APIs to re-train the text mining model using users' data and automatically detect SATD comments using either the build-in model or a user-specified model. The Eclipse plug-in, which is the front-end, first leverages our pre-trained composite classifier to detect SATD comments, and then highlights and marks these detected comments in the source code editor of Eclipse. In addition, the Eclipse plug-in provides a view in IDE which collects all detected comments for management.Demo URL: https://youtu.be/sn4gU2qhGm0Java library download: https://git.io/vNdnYEclipse plug-in download: https://goo.gl/ZzjBzp},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {9–12},
numpages = {4},
keywords = {SATD detection, eclipse plug-in, self-admitted technical debt},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1109/ESEM.2017.23,
author = {Hassan, Foyzul and Wang, Xiaoyin},
title = {Change-aware build prediction model for stall avoidance in continuous integration},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.23},
doi = {10.1109/ESEM.2017.23},
abstract = {Continuous Integration(CI) is a widely used development practice where developers integrate their work after submitting code changes at central repository. CI servers usually monitor central repository for code change submission and automatically build software with changed code, perform unit testing, integration testing and provide test summary report. If build or test fails developers fix those issues and submit the code changes. Continuous submission of code modification by developers and build latency time creates stalls at CI server build pipeline and hence developers have to wait long time to get build outcome. In this paper, we proposed build prediction model that uses TravisTorrent data set with build error log clustering and AST level code change modification data to predict whether a build will be successful or not without attempting actual build so that developer can get early build outcome result. With the proposed model we can predict build outcome with an average F-Measure over 87% on all three build systems (Ant, Maven, Gradle) under the cross-project prediction scenario.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {157–162},
numpages = {6},
keywords = {continuous integration, software build outcome prediction},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@book{10.5555/2886235,
author = {Bird, Christian and Menzies, Tim and Zimmermann, Thomas},
title = {The Art and Science of Analyzing Software Data},
year = {2015},
isbn = {0124115195},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.Presents best practices, hints, and tips to analyze data and apply tools in data science projectsPresents research methods and case studies that have emerged over the past few years to further understanding of software dataShares stories from the trenches of successful data science initiatives in industry}
}

@inproceedings{10.5555/3291291.3291310,
author = {Barrak, Amine and Laverdi\`{e}re, Marc-Andr\'{e} and Khomh, Foutse and An, Le and Merlo, Ettore},
title = {Just-in-time detection of protection-impacting changes on WordPress and MediaWiki},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Access control mechanisms based on roles and privileges restrict the access of users to security sensitive resources in a multi-user software system. Unintentional privilege protection changes may occur during the evolution of a system, which may introduce security vulnerabilities; threatening user's confidential data, and causing other severe problems. In this paper, we use the Pattern Traversal Flow Analysis technique to identify definite protection differences in WordPress and MediaWiki systems. We analyse the evolution of privilege protections across 211 and 193 releases from respectively WordPress and Mediawiki, and observe that around 60% of commits affect privileges protections in both projects. We refer to these commits as protection-impacting change (PIC) commits. To help developers identify PIC commits just-in-time, we extract a series of metrics from commit logs and source code, and build statistical models. The evaluation of these models revealed that they can achieve a precision up to 73.8% and a recall up to 98.8% in WordPress and for MediaWiki, a precision up to 77.2% and recall up to 97.8%. Among the metrics examined, commit churn, bug fixing, author experiences and code complexity between two releases are the most important predictors in the models. We performed a qualitative analysis of false positives and false negatives and observe that PIC commits detectors should ignore documentation-only commits and process code changes without the comments.Software organizations can use our proposed approach and models, to identify unintentional privilege protection changes as soon as they are introduced, in order to prevent the introduction of vulnerabilities in their systems.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {178–188},
numpages = {11},
keywords = {privilege protection changes, protection impacting changes, reliability, security vulnerabilities},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@article{10.1007/s10664-016-9476-y,
author = {Zogaan, Waleed and Mujhid, Ibrahim and S. Santos, Joanna C. and Gonzalez, Danielle and Mirakhorli, Mehdi},
title = {Automated training-set creation for software architecture traceability problem},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9476-y},
doi = {10.1007/s10664-016-9476-y},
abstract = {Automated trace retrieval methods based on machine-learning algorithms can significantly reduce the cost and effort needed to create and maintain traceability links between requirements, architecture and source code. However, there is always an upfront cost to train such algorithms to detect relevant architectural information for each quality attribute in the code. In practice, training supervised or semi-supervised algorithms requires the expert to collect several files of architectural tactics that implement a quality requirement and train a learning method. Establishing such a training set can take weeks to months to complete. Furthermore, the effectiveness of this approach is largely dependent upon the knowledge of the expert. In this paper, we present three baseline approaches for the creation of training data. These approaches are (i) Manual Expert-Based, (ii) Automated Web-Mining, which generates training sets by automatically mining tactic's APIs from technical programming websites, and lastly (iii) Automated Big-Data Analysis, which mines ultra-large scale code repositories to generate training sets. We compare the trace-link creation accuracy achieved using each of these three baseline approaches and discuss the costs and benefits associated with them. Additionally, in a separate study, we investigate the impact of training set size on the accuracy of recovering trace links. The results indicate that automated techniques can create a reliable training set for the problem of tracing architectural tactics.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1028–1062},
numpages = {35},
keywords = {Architecturally significant requirements, Architecture traceability, Automation, Dataset generation}
}

@inproceedings{10.1145/3092703.3092731,
author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
title = {Boosting spectrum-based fault localization using PageRank},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092731},
doi = {10.1145/3092703.3092731},
abstract = {Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–272},
numpages = {12},
keywords = {PageRank, Software testing, Spectrum-based fault localization},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@article{10.1016/j.dss.2015.07.002,
author = {Zaidan, A.A. and Zaidan, B.B. and Hussain, Muzammil and Haiqi, Ahmed and Mat Kiah, M.L. and Abdulnabi, Mohamed},
title = {Multi-criteria analysis for OS-EMR software selection problem},
year = {2015},
issue_date = {October 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {78},
number = {C},
issn = {0167-9236},
url = {https://doi.org/10.1016/j.dss.2015.07.002},
doi = {10.1016/j.dss.2015.07.002},
abstract = {Various software packages offer a large number of customizable features to meet the specific needs of organizations. Improper selection of a software package may result in incorrect strategic decisions and subsequent economic loss of organizations. This paper presents a comparative study that aims to evaluate and select open-source electronic medical record (OS-EMR) software based on multiple-criteria decision-making (MCDM) techniques. A hands-on study is performed, and a set of OS-EMR software are implemented locally in separate virtual machines to closely examine the systems. Several measures as evaluation bases are specified, and systems are selected based on a set of metric outcomes by using AHP integrated with different MCDM techniques, namely, WPM, WSM, SAW, HAW, and TOPSIS. Paired sample t-test is then utilized to measure the correlations among different techniques on ranking scores and orders. Findings are as follows. (1) Significant differences exist among MCDM techniques on the basis of different integrations on ranking scores, whereas no significant differences exist among them when representing the ranking scores to the ranking orders in place of the technique scale. (2) The software GNUmed, OpenEMR, OpenMRS, and ZEPRS do not differ in ranking scores/orders of experiments for all MCDM techniques presented. On the contrary, discrepancies among the ranking scores/orders are more noticeable in other software. (3) GNUmed, OpenEMR, and OpenMRS software are the most promising candidates for providing a good basis on ranking scores/orders, whereas ZEPRS is not recommended because it records the worst ranking score/order in comparison with other OS-EMR software. Display Omitted Significant differences exist among MCDM techniques based on different integration's on ranking scores.There is no significant differences among MCDM techniques when the ranking scores are represented to the ranking orders. The software GNUmed, OpenEMR, OpenMRS, and ZEPRS do not differ in ranking scores/orders for all MCDM techniques presentedGNUmed, OpenEMR, and OpenMRS software are the most promising candidates for providing a good basis ranking scores/orders. ZEPRS is not recommended because it records the worst ranking score/order in comparison with other OS-EMR software.},
journal = {Decis. Support Syst.},
month = oct,
pages = {15–27},
numpages = {13},
keywords = {Multi-criteria analysis, Multiple-criteria decision-making, Open-source electronic medical record software}
}

@inproceedings{10.1145/1414004.1414049,
author = {Vivanco, Rodrigo and Jin, Dean},
title = {Enhancing predictive models using principal component analysis and search based metric selection: a comparative study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414049},
doi = {10.1145/1414004.1414049},
abstract = {Predictive models are used for the detection of potentially problematic component that decrease product quality. Source code metrics can be used as input features in predictive models; however, there exist numerous structural measures that capture different aspects of size, coupling, cohesion, inheritance and complexity. An important question to answer is which metrics should be used with a predictor. A comparative analysis of metric selection strategies (principal component analysis, a genetic algorithm and the CK metrics set) has been carried out. Initial results indicate that search-based metric selection gives the best predictive performance in identifying Java classes with high cognitive complexity that degrades product maintenance.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {273–275},
numpages = {3},
keywords = {genetic algorithm, metric selection, pca, predictive models},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1145/3239235.3239523,
author = {Wang, Junjie and Wang, Song and Wang, Qing},
title = {Is there a "golden" feature set for static warning identification? an experimental evaluation},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239523},
doi = {10.1145/3239235.3239523},
abstract = {Background: The most important challenge regarding the use of static analysis tools (e.g., FindBugs) is that there are a large number of warnings that are not acted on by developers. Many features have been proposed to build classification models for the automatic identification of actionable warnings. Through analyzing these features and related studies, we observe several limitations that make the users lack practical guides to apply these features.Aims: This work aims at conducting a systematic experimental evaluation of all the public available features, and exploring whether there is a golden feature set for actionable warning identification.Method: We first conduct a systematic literature review to collect all public available features for warning identification. We employ 12 projects with totally 60 revisions as our subject projects. We then implement a tool to extract the values of all features for each project revision to prepare the experimental data.Results: Experimental evaluation on 116 collected features demonstrates that there is a common set of features (23 features) which take effect in warning identification for most project revisions. These features can achieve satisfied performance with far less time cost for warning identification.Conclusions: These commonly-selected features can be treated as the golden feature set for identifying actionable warnings. This finding can serve as a practical guideline for facilitating real-world warning identification.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {17},
numpages = {10},
keywords = {actionable warning identification, experimental evaluation, static analysis},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1016/S1568-4946(10)00181-X,
title = {Subject Index},
year = {2010},
issue_date = {September, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {10},
number = {4},
issn = {1568-4946},
url = {https://doi.org/10.1016/S1568-4946(10)00181-X},
doi = {10.1016/S1568-4946(10)00181-X},
journal = {Appl. Soft Comput.},
month = sep,
pages = {1304–1316},
numpages = {13}
}

@article{10.1007/s10664-018-9602-0,
author = {Fan, Yuanrui and Xia, Xin and Lo, David and Li, Shanping},
title = {Early prediction of merged code changes to prioritize reviewing tasks},
year = {2018},
issue_date = {December  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9602-0},
doi = {10.1007/s10664-018-9602-0},
abstract = {Modern Code Review (MCR) has been widely used by open source and proprietary software projects. Inspecting code changes consumes reviewers much time and effort since they need to comprehend patches, and many reviewers are often assigned to review many code changes. Note that a code change might be eventually abandoned, which causes waste of time and effort. Thus, a tool that predicts early on whether a code change will be merged can help developers prioritize changes to inspect, accomplish more things given tight schedule, and not waste reviewing effort on low quality changes. In this paper, motivated by the above needs, we build a merged code change prediction tool. Our approach first extracts 34 features from code changes, which are grouped into 5 dimensions: code, file history, owner experience, collaboration network, and text. And then we leverage machine learning techniques such as random forest to build a prediction model. To evaluate the performance of our approach, we conduct experiments on three open source projects (i.e., Eclipse, LibreOffice, and OpenStack), containing a total of 166,215 code changes. Across three datasets, our approach statistically significantly improves random guess classifiers and two prediction models proposed by Jeong et al. (2009) and Gousios et al. (2014) in terms of several evaluation metrics. Besides, we also study the important features which distinguish merged code changes from abandoned ones.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {3346–3393},
numpages = {48},
keywords = {Code review, Features, Predictive model}
}

@article{10.1007/s10664-006-7552-4,
author = {Li, Jingzhou and Ruhe, Guenther and Al-Emran, Ahmed and Richter, Michael M.},
title = {A flexible method for software effort estimation by analogy},
year = {2007},
issue_date = {February  2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {12},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-006-7552-4},
doi = {10.1007/s10664-006-7552-4},
abstract = {Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods__ __ First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number  N  of analogies (similar objects) taken for adaptation and the threshold  T  for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of  N  and  T  likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {65–106},
numpages = {42},
keywords = {Analogy-based effort estimation, Comparative analysis, Learning, Missing values, Non-quantitative attributes, Software development effort}
}

@article{10.1016/j.micpro.2020.103029,
author = {Kasiviswanathan, Somasundaram and Ramalingam, Dillibabu},
title = {Development and application of user review quality model for embedded system},
year = {2020},
issue_date = {Apr 2020},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {74},
number = {C},
issn = {0141-9331},
url = {https://doi.org/10.1016/j.micpro.2020.103029},
doi = {10.1016/j.micpro.2020.103029},
journal = {Microprocess. Microsyst.},
month = apr,
numpages = {15}
}

@inproceedings{10.1145/1368088.1368135,
author = {Ruthruff, Joseph R. and Penix, John and Morgenthaler, J. David and Elbaum, Sebastian and Rothermel, Gregg},
title = {Predicting accurate and actionable static analysis warnings: an experimental approach},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368135},
doi = {10.1145/1368088.1368135},
abstract = {Static analysis tools report software defects that may or may not be detected by other verification methods. Two challenges complicating the adoption of these tools are spurious false positive warnings and legitimate warnings that are not acted on. This paper reports automated support to help address these challenges using logistic regression models that predict the foregoing types of warnings from signals in the warnings and implicated code. Because examining many potential signaling factors in large software development settings can be expensive, we use a screening methodology to quickly discard factors with low predictive power and cost-effectively build predictive models. Our empirical evaluation indicates that these models can achieve high accuracy in predicting accurate and actionable static analysis warnings, and suggests that the models are competitive with alternative models built without screening.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {341–350},
numpages = {10},
keywords = {experimental program analysis, logistic regression analysis, screening, software quality, static analysis tools},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@article{10.1145/2744200,
author = {Proksch, Sebastian and Lerch, Johannes and Mezini, Mira},
title = {Intelligent Code Completion with Bayesian Networks},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2744200},
doi = {10.1145/2744200},
abstract = {Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items.This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed.Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {3},
numpages = {31},
keywords = {Content assist, code completion, code recommender, evaluation, integrated development environments, machine learning, productivity}
}

@inproceedings{10.1145/3183519.3183547,
author = {Tantithamthavorn, Chakkrit and Hassan, Ahmed E.},
title = {An experience report on defect modelling in practice: pitfalls and challenges},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183547},
doi = {10.1145/3183519.3183547},
abstract = {Over the past decade with the rise of the Mining Software Repositories (MSR)field, the modelling of defects for large and long-lived systems has become one of the most common applications of MSR. The findings and approaches of such studies have attracted the attention of many of our industrial collaborators (and other practitioners worldwide). At the core of many of these studies is the development and use of analytical models for defects. In this paper, we discuss common pitfalls and challenges that we observed as practitioners attempt to develop such models or reason about the findings of such studies. The key goal of this paper is to document such pitfalls and challenges so practitioners can avoid them in future efforts. We also hope that other academics will be mindful of such pitfalls and challenges in their own work and industrial engagements.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {286–295},
numpages = {10},
keywords = {defect modelling, empirical software engineering, experimental design, mining software repositories, software analytics},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/3468264.3468563,
author = {Yin, Likang and Chen, Zhuangzhi and Xuan, Qi and Filkov, Vladimir},
title = {Sustainability forecasting for Apache incubator projects},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468563},
doi = {10.1145/3468264.3468563},
abstract = {Although OSS development is very popular, ultimately more than 80% of OSS projects fail. Identifying the factors associated with OSS success can help in devising interventions when a project takes a downturn. OSS success has been studied from a variety of angles, more recently in empirical studies of large numbers of diverse projects, using proxies for sustainability, e.g., internal metrics related to productivity and external ones, related to community popularity. The internal socio-technical structure of projects has also been shown important, especially their dynamics. This points to another angle on evaluating software success, from the perspective of self-sustaining and self-governing communities. To uncover the dynamics of how a project at a nascent development stage gradually evolves into a sustainable one, here we apply a socio-technical network modeling perspective to a dataset of Apache Software Foundation Incubator (ASFI), sustainability-labeled projects. To identify and validate the determinants of sustainability, we undertake a mix of quantitative and qualitative studies of ASFI projects’ socio-technical network trajectories. We develop interpretable models which can forecast a project becoming sustainable with 93+% accuracy, within 8 months of incubation start. Based on the interpretable models we describe a strategy for real-time monitoring and suggesting actions, which can be used by projects to correct their sustainability trajectories.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1056–1067},
numpages = {12},
keywords = {Apache Incubator, OSS Sustainability, Sociotechnical System},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/1007512.1007539,
author = {Bowring, James F. and Rehg, James M. and Harrold, Mary Jean},
title = {Active learning for automatic classification of software behavior},
year = {2004},
isbn = {1581138202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007512.1007539},
doi = {10.1145/1007512.1007539},
abstract = {A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning approach. In contrast, we explore an active-learning paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.},
booktitle = {Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {195–205},
numpages = {11},
keywords = {Markov models, machine learning, software behavior, software testing},
location = {Boston, Massachusetts, USA},
series = {ISSTA '04}
}

@article{10.1145/2516955.2516959,
author = {Choi, Keunho and Kim, Gunwoo and Suh, Yongmoo},
title = {Classification model for detecting and managing credit loan fraud based on individual-level utility concept},
year = {2013},
issue_date = {August 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/2516955.2516959},
doi = {10.1145/2516955.2516959},
abstract = {As credit loan products significantly increase in most financial institutions, the number of fraudulent transactions is also growing rapidly. Therefore, to manage the financial risks successfully, the financial institutions should reinforce the qualifications for a loan and augment the ability to detect and manage a credit loan fraud proactively. In the process of building a classification model to detect credit loan frauds, utility from classification results (i.e., benefits from correct prediction and costs from incorrect prediction) is more important than the accuracy rate of classification. The objective of this paper is two-fold: (1) to propose a new approach to building a classification model for detecting credit loan fraud based on an individual-level utility, and (2) to suggest customized interest rate for each customer - from both opportunity utility and cash flow perspectives. Experimental results show that our proposed model comes up with higher utility than the fraud detection models which do not take into account the individual-level utility concept. Also, it is shown that the individual-level utility from our model is more accurate than the mean-level utility used in previous researches, from both opportunity utility and cash flow perspectives. Implications of the experimental results from both perspectives are provided.},
journal = {SIGMIS Database},
month = aug,
pages = {49–67},
numpages = {19},
keywords = {credit loan fraud, fraud detection, individual-level utility, utility-sensitive classification}
}

@article{10.1016/j.infsof.2014.01.003,
author = {Fern\'{a}ndez-Diego, Marta and Gonz\'{a}lez-Ladr\'{o}n-De-Guevara, Fernando},
title = {Potential and limitations of the ISBSG dataset in enhancing software engineering research: A mapping review},
year = {2014},
issue_date = {June, 2014},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {56},
number = {6},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2014.01.003},
doi = {10.1016/j.infsof.2014.01.003},
abstract = {Context: The International Software Benchmarking Standards Group (ISBSG) maintains a software development repository with over 6000 software projects. This dataset makes it possible to estimate a project's size, effort, duration, and cost. Objective: The aim of this study was to determine how and to what extent, ISBSG has been used by researchers from 2000, when the first papers were published, until June of 2012. Method: A systematic mapping review was used as the research method, which was applied to over 129 papers obtained after the filtering process. Results: The papers were published in 19 journals and 40 conferences. Thirty-five percent of the papers published between years 2000 and 2011 have received at least one citation in journals and only five papers have received six or more citations. Effort variable is the focus of 70.5% of the papers, 22.5% center their research in a variable different from effort and 7% do not consider any target variable. Additionally, in as many as 70.5% of papers, effort estimation is the research topic, followed by dataset properties (36.4%). The more frequent methods are Regression (61.2%), Machine Learning (35.7%), and Estimation by Analogy (22.5%). ISBSG is used as the only support in 55% of the papers while the remaining papers use complementary datasets. The ISBSG release 10 is used most frequently with 32 references. Finally, some benefits and drawbacks of the usage of ISBSG have been highlighted. Conclusion: This work presents a snapshot of the existing usage of ISBSG in software development research. ISBSG offers a wealth of information regarding practices from a wide range of organizations, applications, and development types, which constitutes its main potential. However, a data preparation process is required before any analysis. Lastly, the potential of ISBSG to develop new research is also outlined.},
journal = {Inf. Softw. Technol.},
month = jun,
pages = {527–544},
numpages = {18},
keywords = {ISBSG, Research methods, Software cost prediction, Software effort estimation, Software engineering, Systematic mapping study}
}

@inproceedings{10.5555/3291291.3291293,
author = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Is it a bug or an enhancement? a text-based approach to classify change requests},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as "bug" for lack of a better classification support or of knowledge about the possible kinds.This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities.We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and JBoss indicate that issues can be classified with between 77% and 82% of correct decisions.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {2–16},
numpages = {15},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3196321.3196365,
author = {Pantiuchina, Jevgenija and Bavota, Gabriele and Tufano, Michele and Poshyvanyk, Denys},
title = {Towards just-in-time refactoring recommenders},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196365},
doi = {10.1145/3196321.3196365},
abstract = {Empirical studies have provided ample evidence that low code quality is generally associated with lower maintainability. For this reason, tools have been developed to automatically detect design flaws (e.g., code smells). However, these tools are not able to prevent the introduction of design flaws. This means that the code has to experience a quality decay (with a consequent increase of maintenance/evolution costs) before state-of-the-art tools can be applied to identify and refactor the design flaws.Our goal is to develop a new generation of refactoring recommenders aimed at preventing, via refactoring operations, the introduction of design flaws rather than fixing them once they already affect the system. We refer to such a novel perspective on software refactoring as just-in-time refactoring. In this paper, we make a first step towards this direction, presenting an approach able to predict which classes will be affected in the future by code smells.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {312–315},
numpages = {4},
keywords = {code smells, refactoring},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@article{10.1007/s11219-007-9040-5,
author = {Kojima, Tsutomu and Hasegawa, Toru and Misumi, Munechika and Nakamura, Tsuyoshi},
title = {Risk analysis of software process measurements},
year = {2008},
issue_date = {September 2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {3},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-007-9040-5},
doi = {10.1007/s11219-007-9040-5},
abstract = {Quantitative process management (QPM) and causal analysis and resolution (CAR) are requirements of capability maturity model (CMM) levels 4 and 5, respectively. They indicate the necessity of process improvement based on objective evidence obtained from statistical analysis of metrics. However, it is difficult to achieve these requirements in practice, and only a few companies have done so successfully. Evidence-based risk-management methods have been proposed for the control of software processes, but are not fully appreciated, compared to clinical practice in medicine. Furthermore, there is no convincing answer as to why these methods are difficult to incorporate in software processes, despite the fact that they are well established in some business enterprises and industries. In this article, we challenge this issue, point out a problem peculiar to software processes, and develop a generally applicable method for identifying the risk of failure for a project in its early stages. The proposed method is based on statistical analyses of process measurements collected continuously throughout a project by a risk assessment and tracking system (RATS). Although this method may be directly applicable to only a limited number of process types, the fundamental idea might be useful for a broader range of applications.},
journal = {Software Quality Journal},
month = sep,
pages = {361–376},
numpages = {16},
keywords = {Logistic model, Risk assessment, Software process, Statistical analysis, Temodori, Yore}
}

@article{10.1504/IJCAT.2012.048204,
author = {Singh, Yogesh and Saha, Anju},
title = {Prediction of testability using the design metrics for object-oriented software},
year = {2012},
issue_date = {July 2012},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {44},
number = {1},
issn = {0952-8091},
url = {https://doi.org/10.1504/IJCAT.2012.048204},
doi = {10.1504/IJCAT.2012.048204},
abstract = {One of the cost effective methods to monitor the testing effort is to assess the testability of the software from the early phases of the software development. Software metrics based testability assessment can enable testers to focus more on the testing of the less testable classes. This paper presents a study based on the experimental analysis that uses fourteen design metrics as the independent variables and two JUnit based test metrics as the dependent variables. The results of the study indicate a number of promising effects of design metrics on testability of a class in object oriented software.},
journal = {Int. J. Comput. Appl. Technol.},
month = jul,
pages = {12–22},
numpages = {11}
}

@inproceedings{10.1145/2896995.2896999,
author = {Akbarinasaji, Shirin and Bener, Ayse Basar and Erdem, Atakan},
title = {Measuring the principal of defect debt},
year = {2016},
isbn = {9781450341653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896995.2896999},
doi = {10.1145/2896995.2896999},
abstract = {Identifying and fixing of defects is part of software maintenance activities. However, due to tight budget and schedule, software development teams may not resolve all the existing bugs in the issue tracking systems. The trade-off between the short-term benefit of postponing bug fixing activities and long-term consequence of delaying those activities is interpreted as defect debt. The accumulation of defect debt in the issue tracking system might cause system bankruptcy. Therefore, there is a necessity for software project managers to measure and monitor defect debts. In this study, we categorized the bugs into regular bugs and debt prone bugs and employed the historical data from regular bugs to train a prediction model for estimating the principal for debt prone bugs. The principal for the regular bug is equivalent to a standard amount of time to fix them. There are studies in the literature that predict bug fixing time as a classification. We proposed KNN-regression to predict the standard time for bug fixing time (principal). We performed an empirical study on both commercial and open source projects to investigate the feasibility of our model. The results showed that KNN-regression outperformed the simple linear regression with the predictive power (R2) ranges between 74% to 85 %.},
booktitle = {Proceedings of the 5th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {1–7},
numpages = {7},
keywords = {KNN-regression, bug fixing time, defect debt, measuring the technical debt},
location = {Austin, Texas},
series = {RAISE '16}
}

@article{10.1007/s11192-016-2006-2,
author = {Ginde, Gouri and Saha, Snehanshu and Mathur, Archana and Venkatagiri, Sukrit and Vadakkepat, Sujith and Narasimhamurthy, Anand and Daya Sagar, B. S.},
title = {ScientoBASE: a framework and model for computing scholastic indicators of non-local influence of journals via native data acquisition algorithms},
year = {2016},
issue_date = {September 2016},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {108},
number = {3},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-016-2006-2},
doi = {10.1007/s11192-016-2006-2},
abstract = {Defining and measuring internationality as a function of influence diffusion of scientific journals is an open problem. There exists no metric to rank journals based on the extent or scale of internationality. Measuring internationality is qualitative, vague, open to interpretation and is limited by vested interests. With the tremendous increase in the number of journals in various fields and the unflinching desire of academics across the globe to publish in "international" journals, it has become an absolute necessity to evaluate, rank and categorize journals based on internationality. Authors, in the current work have defined internationality as a measure of influence that transcends across geographic boundaries. There are concerns raised by the authors about unethical practices reflected in the process of journal publication whereby scholarly influence of a select few are artificially boosted, primarily by resorting to editorial maneuvers. To counter the impact of such tactics, authors have come up with a new method that defines and measures internationality by eliminating such local effects when computing the influence of journals. A new metric, Non-Local Influence Quotient is proposed as one such parameter for internationality computation along with another novel metric, Other-Citation Quotient as the complement of the ratio of self-citation and total citation. In addition, SNIP and international collaboration ratio are used as two other parameters. As these journal parameters are not readily available in one place, algorithms to scrape these metrics are written and documented as a part of the current manuscript. Cobb---Douglas production function is utilized as a model to compute Journal Internationality Modeling Index. Current work elucidates the metric acquisition algorithms while delivering arguments in favor of the suitability of the proposed model. Acquired data is corroborated by different supervised learning techniques. As part of future work, the authors present a bigger picture, Reputation and Global Influence Score, that will be computed to facilitate the formation of clusters of journals of high, moderate and low internationality.},
journal = {Scientometrics},
month = sep,
pages = {1479–1529},
numpages = {51},
keywords = {Cobb---Douglas production function, Convex optimization, Feature extraction, Journal Influence Score, Journal Internationality Modeling Index (JIMI), Non-Local Influence Quotient (NLIQ), Other Citation Quotient (OCQ), Source-Normalized Impact per Paper (SNIP), Supervised learning, Web scraping}
}

@inproceedings{10.1145/3341105.3373984,
author = {Sharma, Shipra and Sodhi, Balwinder},
title = {FACT-from actual to conceptual tie-ins: a multi-level knowledge graph structured on context and semantics of software artefacts},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373984},
doi = {10.1145/3341105.3373984},
abstract = {Knowledge Graphs have increasingly become the preferred approach to complex problems that involve low-level business data. It is well known that Google, LinkedIn, Facebook, Twitter - all have a knowledge graph at their core.In this paper, we propose: i) A knowledge graph, called FACT, which is designed for applications in software engineering problems. ii) A novel system which can automatically populate FACT. Example applications of FACT include verifying design decisions, recommending software elements to reify design decisions, and so on. Vertices of FACT represent items such as a software design concept, a concrete software element which reifies a concept. An edge represents the relationship that may exist between the vertices.The design of FACT as well as the system used to populate it has been validated a) at micro-level by verifying or proving the correctness of the individual components of FACT, and b) at macro-level by experimentally ascertaining the correctness of scenario-based inferences derived from the knowledge contained in FACT.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1604–1613},
numpages = {10},
keywords = {architecture-centric software engineering, engineering-centric graph-database, knowledge graph, reference architecture, semantics based text processing, semantics-based-software engineering, software automation},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.5555/3225643.3225836,
author = {Hulse, Jason D. and Khoshgoftaar, Taghi M. and Huang, Haiying},
title = {The pairwise attribute noise detection algorithm},
year = {2007},
issue_date = {February  2007},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {2},
issn = {0219-1377},
abstract = {Analyzing the quality of data prior to constructing data mining models is emerging as an important issue. Algorithms for identifying noise in a given data set can provide a good measure of data quality. Considerable attention has been devoted to detecting class noise or labeling errors. In contrast, limited research work has been devoted to detecting instances with attribute noise, in part due to the difficulty of the problem. We present a novel approach for detecting instances with attribute noise and demonstrate its usefulness with case studies using two different real-world software measurement data sets. Our approach, called Pairwise Attribute Noise Detection Algorithm (PANDA), is compared with a nearest neighbor, distance-based outlier detection technique (denoted DM) investigated in related literature. Since what constitutes noise is domain specific, our case studies uses a software engineering expert to inspect the instances identified by the two approaches to determine whether they actually contain noise. It is shown that PANDA provides better noise detection performance than the DM algorithm.},
journal = {Knowl. Inf. Syst.},
month = feb,
pages = {171–190},
numpages = {20},
keywords = {Data cleaning, Data quality, Noise detection, PANDA}
}

@inproceedings{10.5555/2666527.2666538,
author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel},
title = {On software engineering repositories and their open problems},
year = {2012},
isbn = {9781467317535},
publisher = {IEEE Press},
abstract = {In the last decade, a large number of software repositories have been created for different purposes. In this paper we present a survey of the publicly available repositories and classify the most common ones as well as discussing the problems faced by researchers when applying machine learning or statistical techniques to them.},
booktitle = {Proceedings of the First International Workshop on Realizing AI Synergies in Software Engineering},
pages = {52–56},
numpages = {5},
keywords = {data quality, preprocessing software engineering data, quality, software engineering repositories},
location = {Zurich, Switzerland},
series = {RAISE '12}
}

@article{10.1016/S1877-0509(21)02038-X,
title = {Contents},
year = {2021},
issue_date = {2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {192},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(21)02038-X},
doi = {10.1016/S1877-0509(21)02038-X},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–xxxii},
numpages = {30}
}

@article{10.4018/ijkss.2014100103,
author = {Bashari, Mahdi and Noorian, Mahdi and Bagheri, Ebrahim},
title = {Product Line Stakeholder Preference Elicitation via Decision Processes},
year = {2014},
issue_date = {October 2014},
publisher = {IGI Global},
address = {USA},
volume = {5},
number = {4},
issn = {1947-8208},
url = {https://doi.org/10.4018/ijkss.2014100103},
doi = {10.4018/ijkss.2014100103},
abstract = {In the software product line configuration process, certain features are selected based on the stakeholders' needs and preferences regarding the available functional and quality properties. This book chapter presents how a product configuration can be modeled as a decision process and how an optimal strategy representing the stakeholders' desirable configuration can be found. In the decision process model of product configuration, the product is configured by making decisions at a number of decision points. The decisions at each of these decision points contribute to functional and quality attributes of the final product. In order to find an optimal strategy for the decision process, a utility-based approach can be adopted, through which, the strategy with the highest utility is selected as the optimal strategy. In order to define utility for each strategy, a multi-attribute utility function is defined over functional and quality properties of a configured product and a utility elicitation process is then introduced for finding this utility function. The utility elicitation process works based on asking gamble queries over functional and quality requirement from the stakeholder. Using this utility function, the optimal strategy and therefore optimal product configuration is determined.},
journal = {Int. J. Knowl. Syst. Sci.},
month = oct,
pages = {35–51},
numpages = {17},
keywords = {Configuration Process, Decision Process, Economic Value, Software Product Line, Utility Elicitation}
}

@inproceedings{10.5555/978-3-030-29551-6_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29550-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part I},
pages = {i–xxii},
location = {Athens, Greece}
}

@inproceedings{10.5555/3172077.3172318,
author = {Wu, Gengshen and Liu, Li and Guo, Yuchen and Ding, Guiguang and Han, Jungong and Shen, Jialie and Shao, Ling},
title = {Unsupervised deep video hashing with balanced rotation},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {Recently, hashing video contents for fast retrieval has received increasing attention due to the enormous growth of online videos. As the extension of image hashing techniques, traditional video hashing methods mainly focus on seeking the appropriate video features but pay little attention to how the video-specific features can be leveraged to achieve optimal binarization. In this paper, an end-to-end hashing framework, namely Unsupervised Deep Video Hashing (UDVH), is proposed, where feature extraction, balanced code learning and hash function learning are integrated and optimized in a self-taught manner. Particularly, distinguished from previous work, our framework enjoys two novelties: 1) an unsupervised hashing method that integrates the feature clustering and feature binarization, enabling the neighborhood structure to be preserved in the binary space; 2) a smart rotation applied to the video-specific features that are widely spread in the low-dimensional space such that the variance of dimensions can be balanced, thus generating more effective hash codes. Extensive experiments have been performed on two real-world datasets and the results demonstrate its superiority, compared to the state-of-the-art video hashing methods. To bootstrap further developments, the source code will be made publically available.},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {3076–3082},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/3183399.3183424,
author = {Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Explainable software analytics},
year = {2018},
isbn = {9781450356626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183399.3183424},
doi = {10.1145/3183399.3183424},
abstract = {Software analytics has been the subject of considerable recent attention but is yet to receive significant industry traction. One of the key reasons is that software practitioners are reluctant to trust predictions produced by the analytics machinery without understanding the rationale for those predictions. While complex models such as deep learning and ensemble methods improve predictive performance, they have limited explainability. In this paper, we argue that making software analytics models explainable to software practitioners is as important as achieving accurate predictions. Explainability should therefore be a key measure for evaluating software analytics models. We envision that explainability will be a key driver for developing software analytics models that are useful in practice. We outline a research roadmap for this space, building on social science, explainable artificial intelligence and software engineering.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {53–56},
numpages = {4},
keywords = {mining software repositories, software analytics, software engineering},
location = {Gothenburg, Sweden},
series = {ICSE-NIER '18}
}

@inproceedings{10.1145/3092703.3098241,
author = {Oliveira, Carlos},
title = {Mapping hardness of automated software testing},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098241},
doi = {10.1145/3092703.3098241},
abstract = {Automated Test Case Generation (ATCG) is an important topic in Software Testing, with a wide range of techniques and tools being used in academia and industry. While their usefulness is widely recognized, due to the labor-intensive nature of the task, the effectiveness of the different techniques in automatically generating test cases for different software systems is not thoroughly understood. Despite many studies introducing various ATCG techniques, much remains to be learned, however, about what makes a particular technique work well (or not) for a specific software system. Therefore, we propose a new methodology to evaluate and select the most effective ATCG technique using structure-based complexity measures. Empirical tests are going to be performed using two different techniques: Search-based Software Testing (SBST) and Random Testing (RT).},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {440–443},
numpages = {4},
keywords = {Automated Software Testing, META Framework, RT, SBST},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@article{10.1016/j.jnca.2020.102537,
author = {Aldhaheri, Sahar and Alghazzawi, Daniyal and Cheng, Li and Barnawi, Ahmed and Alzahrani, Bandar A.},
title = {Artificial Immune Systems approaches to secure the internet of things: A systematic review of the literature and recommendations for future research},
year = {2020},
issue_date = {May 2020},
publisher = {Academic Press Ltd.},
address = {GBR},
volume = {157},
number = {C},
issn = {1084-8045},
url = {https://doi.org/10.1016/j.jnca.2020.102537},
doi = {10.1016/j.jnca.2020.102537},
journal = {J. Netw. Comput. Appl.},
month = may,
numpages = {24},
keywords = {Artificial intelligence, Artificial immune system, Artificial immune networks, Clonal selection, Cyber security, Danger theory, Dendritic cell, Internet of things, IoT, Negative selection, Network security}
}

@inproceedings{10.1145/1321631.1321660,
author = {Jiang, Lingxiao and Su, Zhendong},
title = {Context-aware statistical debugging: from bug predictors to faulty control flow paths},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321660},
doi = {10.1145/1321631.1321660},
abstract = {Effective bug localization is important for realizing automated debugging. One attractive approach is to apply statistical techniques on a collection of evaluation profiles of program properties to help localize bugs. Previous research has proposed various specialized techniques to isolate certain program predicates as bug predictors. However, because many bugs may not be directly associated with these predicates, these techniques are often ineffective in localizing bugs. Relevant control flow paths that may contain bug locations are more informative than stand-alone predicates for discovering and understanding bugs. In this paper, we propose an approach to automatically generate such faulty control flow paths that link many bug predictors together for revealing bugs. Our approach combines feature selection (to accurately select failure-related predicates as bug predictors), clustering (to group correlated predicates), and control flow graph traversal in a novel way to help generate the paths. We have evaluated our approach on code including the Siemens test suite and rhythmbox (a large music management application for GNOME). Our experiments show that the faulty control flow paths are accurate, useful for localizing many bugs, and helped to discover previously unknown errors in rhythmbox},
booktitle = {Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {bug localization, control flow analysis, machine learning, statistical debugging},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@article{10.1007/s10664-016-9462-4,
author = {Assun\c{c}\~{a}o, Wesley K. and Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Vergilio, Silvia R. and Egyed, Alexander},
title = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
year = {2017},
issue_date = {August    2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9462-4},
doi = {10.1007/s10664-016-9462-4},
abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {1763–1794},
numpages = {32},
keywords = {Empirical evaluation, Feature models, Multi-objective evolutionary algorithms, Reverse engineering}
}

@article{10.1007/s10664-014-9343-7,
author = {Kechagia, Maria and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {Charting the API minefield using software telemetry data},
year = {2015},
issue_date = {December  2015},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-014-9343-7},
doi = {10.1007/s10664-014-9343-7},
abstract = {Programs draw significant parts of their functionality through the use of Application Programming Interfaces (APIs). Apart from the way developers incorporate APIs in their software, the stability of these programs depends on the design and implementation of the APIs. In this work, we report how we used software telemetry data to analyze the causes of API failures in Android applications. Specifically, we got 4.9 gb worth of crash data that thousands of applications sent to a centralized crash report management service. We processed that data to extract approximately a million stack traces, stitching together parts of chained exceptions, and established heuristic rules to draw the border between applications and the API calls. We examined a set of more than a half million stack traces associated with risky API calls to map the space of the most common application failure reasons. Our findings show that the top ones can be attributed to memory exhaustion, race conditions or deadlocks, and missing or corrupt resources. Given the classes of the crash causes we identified, we recommend API design and implementation choices, such as specific exceptions, default resources, and non-blocking algorithms, that can eliminate common failures. In addition, we argue that development tools like memory analyzers, thread debuggers, and static analyzers can prevent crashes through early code testing and analysis. Finally, some execution platform and framework designs for process and memory management can also eliminate some application crashes.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {1785–1830},
numpages = {46},
keywords = {Application programming interfaces, Mobile applications, Reliability, Stack traces}
}

@inproceedings{10.1109/ICSE-NIER.2019.00018,
author = {Couceiro, Ricardo and Duarte, Gon\c{c}alo and Dur\~{a}es, Jo\~{a}o and Castelhano, Jo\~{a}o and Duarte, Catarina and Teixeira, C\'{e}sar and Branco, Miguel Castelo and de Carvalho, Paulo and Madeira, Henrique},
title = {Biofeedback augmented software engineering: monitoring of programmers' mental effort},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00018},
doi = {10.1109/ICSE-NIER.2019.00018},
abstract = {This paper presents emergent experimental results showing that mental effort of programmers in code understanding tasks can be monitored through HRV (heart rate variability) using non-intrusive wearable devices. Results suggest that HRV is a good predictor for cognitive load when analyzing code and HRV results are consistent with the mental effort perceived by programmers using NASA-TLX. Furthermore, code complexity metrics do not correlate entirely with mental effort and do not seem a good indicator of the subjective perception of complexity felt by programmers. These first results are presented in the context of the project BASE-Biofeedback Augmented Software Engineering, which is briefly sketched, and proposes a radical neuroscience enabled approach to introduce biofeedback in software development.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {37–40},
numpages = {4},
keywords = {HRV, biofeedback, mental effort, software faults},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@article{10.1007/s11219-007-9030-7,
author = {Serrano, Manuel Angel and Calero, Coral and Sahraoui, Houari A. and Piattini, Mario},
title = {Empirical studies to assess the understandability of data warehouse schemas using structural metrics},
year = {2008},
issue_date = {March     2008},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {1},
issn = {0963-9314},
url = {https://doi.org/10.1007/s11219-007-9030-7},
doi = {10.1007/s11219-007-9030-7},
abstract = {Data warehouses are powerful tools for making better and faster decisions in organizations where information is an asset of primary importance. Due to the complexity of data warehouses, metrics and procedures are required to continuously assure their quality. This article describes an empirical study and a replication aimed at investigating the use of structural metrics as indicators of the understandability, and by extension, the cognitive complexity of data warehouse schemas. More specifically, a four-step analysis is conducted: (1) check if individually and collectively, the considered metrics can be correlated with schema understandability using classical statistical techniques, (2) evaluate whether understandability can be predicted by case similarity using the case-based reasoning technique, (3) determine, for each level of understandability, the subsets of metrics that are important by means of a classification technique, and assess, by means of a probabilistic technique, the degree of participation of each metric in the understandability prediction. The results obtained show that although a linear model is a good approximation of the relation between structure and understandability, the associated coefficients are not significant enough. Additionally, classification analyses reveal respectively that prediction can be achieved by considering structure similarity, that extracted classification rules can be used to estimate the magnitude of understandability, and that some metrics such as the number of fact tables have more impact than others.},
journal = {Software Quality Journal},
month = mar,
pages = {79–106},
numpages = {28},
keywords = {Data warehouse, Empirical studies, Metrics, Quality}
}

@inproceedings{10.5555/978-3-030-29563-9_fm,
title = {Front Matter},
year = {2019},
isbn = {978-3-030-29562-2},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Knowledge Science, Engineering and Management: 12th International Conference, KSEM 2019, Athens, Greece, August 28–30, 2019, Proceedings, Part II},
pages = {I–XXI},
location = {Athens, Greece}
}

@article{10.1007/s10664-016-9494-9,
author = {Li, Xuelin and Wong, W. Eric and Gao, Ruizhi and Hu, Linghuan and Hosono, Shigeru},
title = {Genetic Algorithm-based Test Generation for Software Product Line with the Integration of Fault Localization Techniques},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9494-9},
doi = {10.1007/s10664-016-9494-9},
abstract = {In response to the highly competitive market and the pressure to cost-effectively release good-quality software, companies have adopted the concept of software product line to reduce development cost. However, testing and debugging of each product, even from the same family, is still done independently. This can be very expensive. To solve this problem, we need to explore how test cases generated for one product can be used for another product. We propose a genetic algorithm-based framework which integrates software fault localization techniques and focuses on reusing test specifications and input values whenever feasible. Case studies using four software product lines and eight fault localization techniques were conducted to demonstrate the effectiveness of our framework. Discussions on factors that may affect the effectiveness of the proposed framework is also presented. Our results indicate that test cases generated in such a way can be easily reused (with appropriate conversion) between different products of the same family and help reduce the overall testing and debugging cost.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {1–51},
numpages = {51},
keywords = {Coverage, Debugging/fault localization, EXAM score, Genetic algorithm, Software product line, Test generation}
}

@article{10.1007/s11235-019-00575-7,
author = {Talal, Mohammed and Zaidan, A. A. and Zaidan, B. B. and Albahri, O. S. and Alsalem, M. A. and Albahri, A. S. and Alamoodi, A. H. and Kiah, M. L. M. and Jumaah, F. M. and Alaa, Mussab},
title = {Comprehensive review and analysis of anti-malware apps for smartphones},
year = {2019},
issue_date = {Oct 2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {72},
number = {2},
issn = {1018-4864},
url = {https://doi.org/10.1007/s11235-019-00575-7},
doi = {10.1007/s11235-019-00575-7},
abstract = {The new and disruptive technologies for ensuring smartphone security are very limited and largely scattered. The available options and gaps in this research area must be analysed to provide valuable insights about the present technological environment. This work illustrates the research landscape by mapping the existing literature to a comprehensive taxonomy with four categories. The first category includes review and survey articles related to smartphone security. The second category includes papers on smartphone security solutions. The third category includes smartphone malware studies that examine the security aspects of smartphones and the threats posed by malware. The fourth category includes ranking, clustering and classification studies that classify malware based on their families or security risk levels. Several smartphone security apps have also been analysed and compared based on their mechanisms to identify their contents and distinguishing features by using several evaluation metrics and parameters. Two malware detection techniques, namely, machine-learning-based and non-machine-learning-based malware detection, are drawn from the review. The basic characteristics of this emerging field of research are discussed in the following aspects: (1) motivation behind the development of security measures for different smartphone operating system (Oss), (2) open challenges that compromise the usability and personal information of users and (3) recommendations for enhancing smartphone security. This work also reviews the functionalities and services of several anti-malware companies to fully reveal their security mechanisms, features and strategies. This work also highlights the open challenges and issues related to the evaluation and benchmarking of malware detection techniques to identify the best malware detection apps for smartphones.},
journal = {Telecommun. Syst.},
month = oct,
pages = {285–337},
numpages = {53},
keywords = {Smartphone security, Malware, Malicious software, Smartphone OS}
}

@article{10.5555/3041299.3168985,
author = {Goyal, Deepam and Pabla, B. S. and Dhami, S. S. and Lachhwani, Kailash},
title = {Optimization of condition-based maintenance using soft computing},
year = {2017},
issue_date = {January   2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {28},
number = {1},
issn = {0941-0643},
abstract = {Due to high costs associated with conventional maintenance strategies, application of soft computing in monitoring the condition of equipment to predict the health of various components of machine tools in manufacturing processes has attracted the attention of researchers. Soft computing is a better alternate to predict and optimize the manufacturing processes related to physics-based models as these processes are complex and precarious. The theories of artificial neural systems, fuzzy logic, genetic algorithms, ant colony optimization, simulated annealing and particle swarm optimization are utilized by soft computing techniques to handle real-world issues that cannot be palatably handled utilizing conventional computing methods. This paper presents a state-of-the-art review on the recent developments in the use of soft computing in condition-based maintenance in manufacturing.},
journal = {Neural Comput. Appl.},
month = jan,
pages = {829–844},
numpages = {16},
keywords = {Condition-based maintenance, Manufacturing processes, Optimization, Soft computing techniques}
}

@inproceedings{10.1145/1273463.1273467,
author = {Arumuga Nainar, Piramanayagam and Chen, Ting and Rosin, Jake and Liblit, Ben},
title = {Statistical debugging using compound boolean predicates},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273467},
doi = {10.1145/1273463.1273467},
abstract = {Statistical debugging uses dynamic instrumentation and machine learning to identify predicates on program state that are strongly predictive of program failure. Prior approaches have only considered simple, atomic predicates such as the directions of branches or the return values of function calls. We enrich the predicate vocabulary by adding complex Boolean formulae derived from these simple predicates. We draw upon three-valued logic, static program structure, and statistical estimation techniques to efficiently sift through large numbers of candidate Boolean predicate formulae. We present qualitative and quantitative evidence that complex predicates are practical, precise, and informative. Furthermore, we demonstrate that our approach is robust in the face of incomplete data provided by the sparse random sampling that typifies postdeployment statistical debugging.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {5–15},
numpages = {11},
keywords = {debugging effort metrics, dynamic feedback analysis, statistical bug isolation, three-valued logic},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@article{10.1016/j.cosrev.2010.06.001,
author = {R\"{a}Ih\"{a}, Outi},
title = {Survey: A survey on search-based software design},
year = {2010},
issue_date = {November, 2010},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {4},
number = {4},
issn = {1574-0137},
url = {https://doi.org/10.1016/j.cosrev.2010.06.001},
doi = {10.1016/j.cosrev.2010.06.001},
abstract = {This survey investigates search-based approaches to software design. The basics of the most popular meta-heuristic algorithms are presented as background to the search-based viewpoint. Software design is considered from a wide viewpoint, including topics that can also be categorized as software maintenance or re-engineering. Search-based approaches have been used in research from the high architecture design level to software clustering and finally software refactoring. Enhancing and predicting software quality with search-based methods is also taken into account as a part of the design process. The background for the underlying software engineering problems is discussed, after which search-based approaches are presented. Summarizing remarks and tables collecting the fundamental issues of approaches for each type of problem are given. The choices regarding critical decisions, such as representation and fitness function, when used in meta-heuristic search algorithms, are emphasized and discussed in detail. Ideas for future research directions are also given.},
journal = {Comput. Sci. Rev.},
month = nov,
pages = {203–249},
numpages = {47},
keywords = {Search algorithms, Search-based software engineering, Software design, Software quality}
}

@inproceedings{10.1145/3368308.3415388,
author = {Luo, Jiwen and Lu, Feng and Wang, Tao},
title = {A Multi-Dimensional Assessment Model and Its Application in E-learning Courses of Computer Science},
year = {2020},
isbn = {9781450370455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368308.3415388},
doi = {10.1145/3368308.3415388},
abstract = {Computer science is a practical discipline. It is always a great challenge to evaluate students' computer practice using computer-aided means for large scale students. We always need to address problems such as suspected plagiarism and deviation of the overall difficulty factor. In this paper, a multi-dimensional assessment model is designed for CS courses based on the detailed practice processing data in an E-learning system. The model comprehensively evaluates the students' learning process and results in three aspects of correctness, originality, and quality detection. Besides, the teacher can easily participate in the assessment according to their needs. The correctness is an essential requirement, and the originality is based on the clustering results of students' behaviors after clone detection to curb homework plagiarism. SonarQube is used to detect code quality and put forward higher requirements for codes. Manual participation intelligence has improved the flexibility and applicability of the model to a certain extent. We applied this model on the EduCoder online education platform and carried out a comprehensive analysis of 485 students in the Parallel Programming Principles and Practice Class of Huazhong University of Science and Technology. Experiment results confirm the distinction, rationality, and fairness of the model in assessing student performance. It not only gives students a credible, comprehensive score in large-scale online practical programming courses but also gives teachers and students corresponding suggestions based on the evaluation results. Furthermore, the model can be extended to other online education platforms.},
booktitle = {Proceedings of the 21st Annual Conference on Information Technology Education},
pages = {187–193},
numpages = {7},
keywords = {massive online open practice, multi-dimensional intelligent scoring model, student assessment, student behavior analysis},
location = {Virtual Event, USA},
series = {SIGITE '20}
}

@inproceedings{10.5555/2487085.2487089,
author = {Shokripour, Ramin and Anvik, John and Kasirun, Zarinah M. and Zamani, Sima},
title = {Why so complicated? simple term filtering and weighting for location-based bug report assignment recommendation},
year = {2013},
isbn = {9781467329361},
publisher = {IEEE Press},
abstract = {Large software development projects receive many bug reports and each of these reports needs to be triaged. An important step in the triage process is the assignment of the report to a developer. Most previous efforts towards improving bug report assignment have focused on using an activity-based approach. We address some of the limitations of activity-based approaches by proposing a two-phased location-based approach where bug report assignment recommendations are based on the predicted location of the bug. The proposed approach utilizes a noun extraction process on several information sources to determine bug location information and a simple term weighting scheme to provide a bug report assignment recommendation. We found that by using a location-based approach, we achieved an accuracy of 89.41% and 59.76% when recommending five developers for the Eclipse and Mozilla projects, respectively.},
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
pages = {2–11},
numpages = {10},
location = {San Francisco, CA, USA},
series = {MSR '13}
}

@inproceedings{10.1109/ASE.2019.00101,
author = {Wang, Min and Lin, Zeqi and Zou, Yanzhen and Xie, Bing},
title = {CoRA: decomposing and describing tangled code changes for reviewer},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00101},
doi = {10.1109/ASE.2019.00101},
abstract = {Code review is an important mechanism for code quality assurance both in open source software and industrial software. Reviewers usually suffer from numerous, tangled and loosely related code changes that are bundled in a single commit, which makes code review very difficult. In this paper, we propose CoRA (&lt;u&gt;Co&lt;/u&gt;de &lt;u&gt;R&lt;/u&gt;eview &lt;u&gt;A&lt;/u&gt;ssistant), an automatic approach to decompose a commit into different parts and generate concise descriptions for reviewers. More specifically, CoRA can decompose a commit into independent parts (e.g., bug fixing, new feature adding, or refactoring) by code dependency analysis and tree-based similar-code detection, then identify the most important code changes in each part based on the PageRank algorithm and heuristic rules. As a result, CoRA can generate a concise description for each part of the commit. We evaluate our approach in seven open source software projects and 50 code commits. The results indicate that CoRA can improve the accuracy of decomposing code changes by 6.3% over the state-of-art practice. At the same time, CoRA can identify the important part from the fine-grained code changes with a mean average precision (MAP) of 87.7%. We also conduct a human study with eight participants to evaluate the performance and usefulness of CoRA, the user feedback indicates that CoRA can effectively help reviewers.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1050–1061},
numpages = {12},
keywords = {code changes decomposition, code changes description, code review, program comprehension},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1016/j.jpdc.2011.03.006,
author = {Liu, Xu and Zhan, Jianfeng and Zhan, Kunlin and Shi, Weisong and Yuan, Lin and Meng, Dan and Wang, Lei},
title = {Automatic performance debugging of SPMD-style parallel programs},
year = {2011},
issue_date = {July, 2011},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {71},
number = {7},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2011.03.006},
doi = {10.1016/j.jpdc.2011.03.006},
abstract = {Automatic performance debugging of parallel applications includes two main steps: locating performance bottlenecks and uncovering their root causes for performance optimization. Previous work fails to resolve this challenging issue in two ways: first, several previous efforts automate locating bottlenecks, but present results in a confined way that only identifies performance problems with a priori knowledge; second, several tools take exploratory or confirmatory data analysis to automatically discover relevant performance data relationships, but these efforts do not focus on locating performance bottlenecks or uncovering their root causes. The simple program and multiple data (SPMD) programming model is widely used for both high performance computing and Cloud computing. In this paper, we design and implement an innovative system, AutoAnalyzer, that automates the process of debugging performance problems of SPMD-style parallel programs, including data collection, performance behavior analysis, locating bottlenecks, and uncovering their root causes. AutoAnalyzer is unique in terms of two features: first, without any prior knowledge, it automatically locates bottlenecks and uncovers their root causes for performance optimization; second, it is lightweight in terms of the size of performance data to be collected and analyzed. Our contributions are three-fold: first, we propose two effective clustering algorithms to investigate the existence of performance bottlenecks that cause process behavior dissimilarity or code region behavior disparity, respectively; meanwhile, we present two searching algorithms to locate bottlenecks; second, on the basis of the rough set theory, we propose an innovative approach to automatically uncover root causes of bottlenecks; third, on the cluster systems with two different configurations, we use two production applications, written in Fortran 77, and one open source code-MPIBZIP2 (http://compression.ca/mpibzip2/), written in C++, to verify the effectiveness and correctness of our methods. For three applications, we also propose an experimental approach to investigating the effects of different metrics on locating bottlenecks.},
journal = {J. Parallel Distrib. Comput.},
month = jul,
pages = {925–937},
numpages = {13},
keywords = {Automatic performance debugging, Performance bottleneck, Performance optimization, Root cause analysis, SPMD parallel programs}
}

@article{10.5555/2010978.2010987,
author = {Halkidi, M. and Spinellis, D. and Tsatsaronis, G. and Vazirgiannis, M.},
title = {Data mining in software engineering},
year = {2011},
issue_date = {August 2011},
publisher = {IOS Press},
address = {NLD},
volume = {15},
number = {3},
issn = {1088-467X},
abstract = {The increased availability of data created as part of the software development process allows us to apply novel analysis techniques on the data and use the results to guide the process's optimization. In this paper we describe various data sources and discuss the principles and techniques of data mining as applied on software engineering data. Data that can be mined is generated by most parts of the development process: requirements elicitation, development analysis, testing, debugging, and maintenance. Based on this classification we survey the mining approaches that have been used and categorize them according to the corresponding parts of the development process and the task they assist. Thus the survey provides researchers with a concise overview of data mining techniques applied to software engineering data, and aids practitioners on the selection of appropriate data mining techniques for their work.},
journal = {Intell. Data Anal.},
month = aug,
pages = {413–441},
numpages = {29},
keywords = {Data mining techniques, KDD methods, mining software engineering data}
}

@article{10.1016/j.jss.2016.07.016,
author = {Wiese, Igor Scaliante and R, Reginaldo and Steinmacher, Igor and Kuroda, Rodrigo Takashi and Oliva, Gustavo Ansaldi and Treude, Christoph and Gerosa, Marco Aurlio},
title = {Using contextual information to predict co-changes},
year = {2017},
issue_date = {June 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {128},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2016.07.016},
doi = {10.1016/j.jss.2016.07.016},
abstract = {Contextual information can improve the co-change prediction, especially the precision.The proposed models outperform the association rules used as baseline model.More than one dimension was frequently selected by our classifier. Background: Co-change prediction makes developers aware of which artifacts will change together with the artifact they are working on. In the past, researchers relied on structural analysis to build prediction models. More recently, hybrid approaches relying on historical information and textual analysis have been proposed. Despite the advances in the area, software developers still do not use these approaches widely, presumably because of the number of false recommendations. We conjecture that the contextual information of software changes collected from issues, developers communication, and commit metadata captures the change patterns of software artifacts and can improve the prediction models. Objective: Our goal is to develop more accurate co-change prediction models by using contextual information from software changes. Method: We selected pairs of files based on relevant association rules and built a prediction model for each pair relying on their associated contextual information. We evaluated our approach on two open source projects, namely Apache CXF and Derby. Besides calculating model accuracy metrics, we also performed a feature selection analysis to identify the best predictors when characterizing co-changes and to reduce overfitting. Results: Our models presented low rates of false negatives (8% average rate) and false positives (11% average rate). We obtained prediction models with AUC values ranging from 0.89 to 1.00 and our models outperformed association rules, our baseline model, when we compared their precision values. Commit-related metrics were the most frequently selected ones for both projects. On average, 6 out of 23 metrics were necessary to build the classifiers. Conclusions: Prediction models based on contextual information from software changes are accurate and, consequently, they can be used to support software maintenance and evolution, warning developers when they miss relevant artifacts while performing a software change.},
journal = {J. Syst. Softw.},
month = jun,
pages = {220–235},
numpages = {16},
keywords = {Change coupling, Change impact analysis, Change propagation, Co-change prediction, Contextual information, Software change context}
}

@article{10.1016/j.csi.2016.03.003,
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
title = {Intelligent software product line configurations},
year = {2016},
issue_date = {November 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {48},
number = {C},
issn = {0920-5489},
url = {https://doi.org/10.1016/j.csi.2016.03.003},
doi = {10.1016/j.csi.2016.03.003},
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product. Literature review of AI applications to SPL configuration issuesDevelop a taxonomy based on eight different problem domainsThis review shows use of logic, constraint satisfaction, reasoning, ontology and optimization.Several important future research directions are proposed.We justify advanced analytics and swarm intelligence as better future applications.},
journal = {Comput. Stand. Interfaces},
month = nov,
pages = {30–48},
numpages = {19},
keywords = {Artificial intelligence, Automated feature selection, Inconsistencies, Industrial SPL tools, Literature review, Predictive analytics, Software product line}
}

@inproceedings{10.1145/2970276.2970343,
author = {Ceccato, Mariano and Nguyen, Cu D. and Appelt, Dennis and Briand, Lionel C.},
title = {SOFIA: an automated security oracle for black-box testing of SQL-injection vulnerabilities},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970343},
doi = {10.1145/2970276.2970343},
abstract = {Security testing is a pivotal activity in engineering secure software. It consists of two phases: generating attack inputs to test the system, and assessing whether test executions expose any vulnerabilities. The latter phase is known as the security oracle problem.  In this work, we present SOFIA, a Security Oracle for SQL-Injection Vulnerabilities. SOFIA is programming-language and source-code independent, and can be used with various attack generation tools. Moreover, because it does not rely on known attacks for learning, SOFIA is meant to also detect types of SQLi attacks that might be unknown at learning time. The oracle challenge is recast as a one-class classification problem where we learn to characterise legitimate SQL statements to accurately distinguish them from SQLi attack statements.  We have carried out an experimental validation on six applications, among which two are large and widely-used. SOFIA was used to detect real SQLi vulnerabilities with inputs generated by three attack generation tools. The obtained results show that SOFIA is computationally fast and achieves a recall rate of 100% (i.e., missing no attacks) with a low false positive rate (0.6%).},
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {167–177},
numpages = {11},
keywords = {SQL-injection, Security oracle, Security testing},
location = {Singapore, Singapore},
series = {ASE '16}
}

@inproceedings{10.1145/2786805.2786838,
author = {Daka, Ermira and Campos, Jos\'{e} and Fraser, Gordon and Dorn, Jonathan and Weimer, Westley},
title = {Modeling readability to improve unit tests},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786838},
doi = {10.1145/2786805.2786838},
abstract = {Writing good unit tests can be tedious and error prone, but even once they are written, the job is not done: Developers need to reason about unit tests throughout software development and evolution, in order to diagnose test failures, maintain the tests, and to understand code written by other developers. Unreadable tests are more difficult to maintain and lose some of their value to developers. To overcome this problem, we propose a domain-specific model of unit test readability based on human judgements, and use this model to augment automated unit test generation. The resulting approach can automatically generate test suites with both high coverage and also improved readability. In human studies users prefer our improved tests and are able to answer maintenance questions about them 14% more quickly at the same level of accuracy.},
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {107–118},
numpages = {12},
keywords = {Readability, automated test generation, unit testing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2642937.2642991,
author = {Milani Fard, Amin and Mirzaaghaei, Mehdi and Mesbah, Ali},
title = {Leveraging existing tests in automated test generation for web applications},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642991},
doi = {10.1145/2642937.2642991},
abstract = {To test web applications, developers currently write test cases in frameworks such as Selenium. On the other hand, most web test generation techniques rely on a crawler to explore the dynamic states of the application. The first approach requires much manual effort, but benefits from the domain knowledge of the developer writing the test cases. The second one is automated and systematic, but lacks the domain knowledge required to be as effective. We believe combining the two can be advantageous. In this paper, we propose to (1) mine the human knowledge present in the form of input values, event sequences, and assertions, in the human-written test suites, (2) combine that inferred knowledge with the power of automated crawling, and (3) extend the test suite for uncovered/unchecked portions of the web application under test. Our approach is implemented in a tool called Testilizer. An evaluation of our approach indicates that Testilizer (1) outperforms a random test generator, and (2) on average, can generate test suites with improvements of up to 150% in fault detection rate and up to 30% in code coverage, compared to the original test suite.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {67–78},
numpages = {12},
keywords = {automated test generation, test reuse, web applications},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.5555/978-3-030-58802-1_fm,
title = {Front Matter},
year = {2020},
isbn = {978-3-030-58801-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2020: 20th International Conference, Cagliari, Italy, July 1–4, 2020, Proceedings, Part II},
pages = {i–xlii},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3239235.3240504,
author = {Rastogi, Ayushi and Nagappan, Nachiappan and Gousios, Georgios and van der Hoek, Andr\'{e}},
title = {Relationship between geographical location and evaluation of developer contributions in github},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3240504},
doi = {10.1145/3239235.3240504},
abstract = {Background Open source software projects show gender bias suggesting that other demographic characteristics of developers, like geographical location, can negatively influence evaluation of contributions too. Aim This study contributes to this emerging body of knowledge in software development by presenting a quantitative analysis of the relationship between the geographical location of developers and evaluation of their contributions on GitHub. Method We present an analysis of 70,000+ pull requests selected from 17 most actively participating countries to model the relationship between the geographical location of developers and pull request acceptance decision. Results and Conclusion We observed structural differences in pull request acceptance rates across 17 countries. Countries with no apparent similarities such as Switzerland and Japan had one of the highest pull request acceptance rates while countries like China and Germany had one of the lowest pull request acceptance rates. Notably, higher acceptance rates were observed for all but one country when pull requests were evaluated by developers from the same country.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {22},
numpages = {8},
keywords = {geographical location, github, open source, pull requests},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.1145/1670679.1670680,
author = {Salfner, Felix and Lenk, Maren and Malek, Miroslaw},
title = {A survey of online failure prediction methods},
year = {2010},
issue_date = {March 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/1670679.1670680},
doi = {10.1145/1670679.1670680},
abstract = {With the ever-growing complexity and dynamicity of computer systems, proactive fault management is an effective approach to enhancing availability. Online failure prediction is the key to such techniques. In contrast to classical reliability methods, online failure prediction is based on runtime monitoring and a variety of models and methods that use the current state of a system and, frequently, the past experience as well. This survey describes these methods. To capture the wide spectrum of approaches concerning this area, a taxonomy has been developed, whose different approaches are explained and major concepts are described in detail.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {10},
numpages = {42},
keywords = {Error, failure prediction, fault, prediction metrics, runtime monitoring}
}

@inproceedings{10.1007/978-3-030-23502-4_21,
author = {Yao, Jia and Maleki Shoja, Babak and Tabrizi, Nasseh},
title = {An Overview of Cloud Computing Testing Research},
year = {2019},
isbn = {978-3-030-23501-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-23502-4_21},
doi = {10.1007/978-3-030-23502-4_21},
abstract = {With the rapid growth in information technology, there is a significant increase in research activities in the field of cloud computing. Cloud testing can be interpreted as (i) testing of cloud applications, which involves continuous monitoring of cloud application status to verify Service Level Agreements, and (ii) testing as a cloud service which involves using the cloud as a testing middleware to execute a large-scale simulation of real-time user interactions. This study aims to examine the methodologies and tools used in cloud testing and the current research trends in cloud computing testing.},
booktitle = {Cloud Computing – CLOUD 2019: 12th International Conference, Held as Part of the Services Conference Federation, SCF 2019, San Diego, CA, USA, June 25–30, 2019, Proceedings},
pages = {303–313},
numpages = {11},
keywords = {Cloud testing, Cloud computing, Cloud services, Review},
location = {San Diego, CA, USA}
}

@article{10.4018/IJOSSP.2016040102,
author = {Goyal, Anjali and Sardana, Neetu},
title = {Analytical Study on Bug Triaging Practices},
year = {2016},
issue_date = {April 2016},
publisher = {IGI Global},
address = {USA},
volume = {7},
number = {2},
issn = {1942-3926},
url = {https://doi.org/10.4018/IJOSSP.2016040102},
doi = {10.4018/IJOSSP.2016040102},
abstract = {Software bugs are inevitable and fixing these bugs is a difficult and time consuming task. Bug report assignment is the activity of designating a developer who makes source code changes in order to fix the bug. Many bug assignment techniques have been proposed in the existing studies. These studies use different datasets, varied input and evaluation parameters to validate their work. This diversification in bug triaging results in perplexity among researchers. Hence, this paper organizes the work performed in bug triaging in a structured manner. This paper aims to present current state of the art to provide a structured consolidation of bug triaging approaches. The paper has identified six research questions under five dimensions to address the various aspects of bug triaging. 60 articles from 36 venues have been reviewed and categorized in order to organize and substructure existing work in the field of bug report assignment. This study will help researchers to wisely decide the weapons for bug triaging. Also, it will act as a ready reference for the bug triaging practitioners.},
journal = {Int. J. Open Source Softw. Process.},
month = apr,
pages = {20–42},
numpages = {23},
keywords = {Bug Management, Bug Report Assignment, Bug Tracking Systems, Bug Triaging, OSS, Open Source Software, Software Bugs}
}

@article{10.1145/3034827,
author = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
title = {CASE Tool Support for Variability Management in Software Product Lines},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3034827},
doi = {10.1145/3034827},
abstract = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools’ characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {14},
numpages = {45},
keywords = {Software engineering, computer-aided software engineering, software variability}
}

@article{10.1016/j.infsof.2013.04.002,
author = {Shar, Lwin Khin and Tan, Hee Beng Kuan},
title = {Predicting SQL injection and cross site scripting vulnerabilities through mining input sanitization patterns},
year = {2013},
issue_date = {October, 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {10},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2013.04.002},
doi = {10.1016/j.infsof.2013.04.002},
abstract = {Context: SQL injection (SQLI) and cross site scripting (XSS) are the two most common and serious web application vulnerabilities for the past decade. To mitigate these two security threats, many vulnerability detection approaches based on static and dynamic taint analysis techniques have been proposed. Alternatively, there are also vulnerability prediction approaches based on machine learning techniques, which showed that static code attributes such as code complexity measures are cheap and useful predictors. However, current prediction approaches target general vulnerabilities. And most of these approaches locate vulnerable code only at software component or file levels. Some approaches also involve process attributes that are often difficult to measure. Objective: This paper aims to provide an alternative or complementary solution to existing taint analyzers by proposing static code attributes that can be used to predict specific program statements, rather than software components, which are likely to be vulnerable to SQLI or XSS. Method: From the observations of input sanitization code that are commonly implemented in web applications to avoid SQLI and XSS vulnerabilities, in this paper, we propose a set of static code attributes that characterize such code patterns. We then build vulnerability prediction models from the historical information that reflect proposed static attributes and known vulnerability data to predict SQLI and XSS vulnerabilities. Results: We developed a prototype tool called PhpMinerI for data collection and used it to evaluate our models on eight open source web applications. Our best model achieved an averaged result of 93% recall and 11% false alarm rate in predicting SQLI vulnerabilities, and 78% recall and 6% false alarm rate in predicting XSS vulnerabilities. Conclusion: The experiment results show that our proposed vulnerability predictors are useful and effective at predicting SQLI and XSS vulnerabilities.},
journal = {Inf. Softw. Technol.},
month = oct,
pages = {1767–1780},
numpages = {14},
keywords = {Data mining, Empirical study, Input sanitization, Static code attributes, Vulnerability prediction, Web application vulnerability}
}

@article{10.1007/s11334-014-0231-5,
author = {Barb, Adrian S. and Neill, Colin J. and Sangwan, Raghvinder S. and Piovoso, Michael J.},
title = {A statistical study of the relevance of lines of code measures in software projects},
year = {2014},
issue_date = {December  2014},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {10},
number = {4},
issn = {1614-5046},
url = {https://doi.org/10.1007/s11334-014-0231-5},
doi = {10.1007/s11334-014-0231-5},
abstract = {Lines of code metrics are routinely used as measures of software system complexity, programmer productivity, and defect density, and are used to predict both effort and cost. The guidelines for using a direct metric, such as lines of code, as a proxy for a quality factor such as complexity or defect density, or in derived metrics such as cost and effort are clear. Amongst other criteria, the direct metric must be linearly related to, and accurately predict, the quality factor and these must be validated through statistical analysis following a rigorous validation methodology. In this paper, we conduct such an analysis to determine the validity and utility of lines of code as a measure using the ISBGS-10 data set. We find that it fails to meet the specified validity tests and, therefore, has limited utility in derived measures.},
journal = {Innov. Syst. Softw. Eng.},
month = dec,
pages = {243–260},
numpages = {18},
keywords = {Data mining, ISBSG-10, Linear models, Lines of code, Software estimation}
}

@inproceedings{10.5555/2643634.2643677,
author = {Tartler, Reinhard and Dietrich, Christian and Sincero, Julio and Schr\"{o}der-Preikschat, Wolfgang and Lohmann, Daniel},
title = {Static analysis of variability in system software: the 90,000 #ifdefs issue},
year = {2014},
isbn = {9781931971102},
publisher = {USENIX Association},
address = {USA},
abstract = {System software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. The Linux v3.2 kernel, for instance, provides more than 12,000 configurable features, which control the configuration-dependent inclusion of 31,000 source files with 89,000 #ifdef blocks.Tools for static analyses can greatly assist with ensuring the quality of code-bases of this size. Unfortunately, static configurability limits the success of automated software testing and bug hunting. For proper type checking, the tools need to be invoked on a concrete configuration, so programmers have to manually derive many configurations to ensure that the configuration-conditional parts of their code are checked. This tedious and error-prone process leaves many easy to find bugs undetected.We propose an approach and tooling to systematically increase the configuration coverage (CC) in compile-time configurable system software. Our VAMPYR tool derives the required configurations and can be combined with existing static checkers to improve their results. With GCC as static checker, we thereby have found hundreds of issues in Linux v3.2, BUSYBOX, and L4/FIASCO, many of which went unnoticed for several years and have to be classified as serious bugs. Our resulting patches were accepted by the respective upstream developers.},
booktitle = {Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference},
pages = {421–432},
numpages = {12},
location = {Philadelphia, PA},
series = {USENIX ATC'14}
}

@article{10.1016/j.infsof.2012.11.008,
author = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R. and Goseva-Popstojanova, Katerina and Dorman, Karin S.},
title = {Predicting failure-proneness in an evolving software product line},
year = {2013},
issue_date = {August 2013},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {55},
number = {8},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2012.11.008},
doi = {10.1016/j.infsof.2012.11.008},
abstract = {ContextPrevious work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. ObjectiveThe work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. MethodThis investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. ResultsOur experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. ConclusionAs the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
journal = {Inf. Softw. Technol.},
month = aug,
pages = {1479–1495},
numpages = {17},
keywords = {Change metrics, Failure-prone files, Post-release defects, Prediction, Reuse, Software product lines}
}

@article{10.1016/j.infsof.2016.11.007,
author = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M. and Inoue, Katsuro},
title = {Search-based software library recommendation using multi-objective optimization},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.007},
doi = {10.1016/j.infsof.2016.11.007},
abstract = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers.Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find "useful" third-party libraries to the implementation of their software systems.Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries.Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5.Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {55–75},
numpages = {21},
keywords = {Multi-objective optimization, Search-based software engineering, Software library, Software reuse}
}

@inproceedings{10.1145/1294261.1294276,
author = {Tan, Lin and Yuan, Ding and Krishna, Gopal and Zhou, Yuanyuan},
title = {/*icomment: bugs or bad comments?*/},
year = {2007},
isbn = {9781595935915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294261.1294276},
doi = {10.1145/1294261.1294276},
abstract = {Commenting source code has long been a common practice in software development. Compared to source code, comments are more direct, descriptive and easy-to-understand. Comments and sourcecode provide relatively redundant and independent information regarding a program's semantic behavior. As software evolves, they can easily grow out-of-sync, indicating two problems: (1) bugs -the source code does not follow the assumptions and requirements specified by correct program comments; (2) bad comments - comments that are inconsistent with correct code, which can confuse and mislead programmers to introduce bugs in subsequent versions. Unfortunately, as most comments are written in natural language, no solution has been proposed to automatically analyze commentsand detect inconsistencies between comments and source code. This paper takes the first step in automatically analyzing commentswritten in natural language to extract implicit program rulesand use these rules to automatically detect inconsistencies between comments and source code, indicating either bugs or bad comments. Our solution, iComment, combines Natural Language Processing(NLP), Machine Learning, Statistics and Program Analysis techniques to achieve these goals. We evaluate iComment on four large code bases: Linux, Mozilla, Wine and Apache. Our experimental results show that iComment automatically extracts 1832 rules from comments with 90.8-100% accuracy and detects 60 comment-code inconsistencies, 33 newbugs and 27 bad comments, in the latest versions of the four programs. Nineteen of them (12 bugs and 7 bad comments) have already been confirmed by the corresponding developers while the others are currently being analyzed by the developers.},
booktitle = {Proceedings of Twenty-First ACM SIGOPS Symposium on Operating Systems Principles},
pages = {145–158},
numpages = {14},
keywords = {comment analysis, natural language processing for software engineering, programming rules and static analysis},
location = {Stevenson, Washington, USA},
series = {SOSP '07}
}

@article{10.1016/j.infsof.2021.106637,
author = {Wang, Dong and Kula, Raula Gaikovina and Ishio, Takashi and Matsumoto, Kenichi},
title = {Automatic patch linkage detection in code review using textual content and file location features},
year = {2021},
issue_date = {Nov 2021},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {139},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2021.106637},
doi = {10.1016/j.infsof.2021.106637},
journal = {Inf. Softw. Technol.},
month = nov,
numpages = {18},
keywords = {Modern code review, Mining software repositories, Patch linkage}
}

@article{10.1016/j.jss.2019.110434,
author = {Vrzakova, Hana and Begel, Andrew and Meht\"{a}talo, Lauri and Bednarik, Roman},
title = {Affect Recognition in Code Review: An In-situ Biometric Study of Reviewer’s Affect},
year = {2020},
issue_date = {Jan 2020},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {159},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110434},
doi = {10.1016/j.jss.2019.110434},
journal = {J. Syst. Softw.},
month = jan,
numpages = {12},
keywords = {Code Review, Affective Computing, Physiological Signals, CSCW}
}

@inproceedings{10.5555/2486788.2486929,
author = {Femmer, Henning and Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David},
title = {Detecting inconsistencies in wrappers: a case study},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Exchangeability between software components such as operating systems, middleware, databases, and hardware components is a common requirement in many software systems. One way to enable exchangeability is to promote indirect use through a common interface and an implementation for each component that wraps the original component. As developers use the interface instead of the underlying component, they assume that the software system will behave in a specific way independently of the actual component in use. However, differences in the implementations of the wrappers may lead to different behavior when one component is changed for another, which might lead to failures in the field. This work reports on a simple, yet effective approach to detect these differences. The approach is based on tool-supported reviews leveraging lightweight static analysis and machine learning. The approach is evaluated in a case study that analyzes NASAs Operating System Abstraction Layer (OSAL), which is used in various space missions. We detected 84 corner-case issues of which 57 turned out to be bugs that could have resulted in runtime failures.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1022–1031},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1007/s10015-021-00681-3,
author = {Polpinij, Jantima},
title = {A method of non-bug report identification from bug report repository},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {3},
issn = {1433-5298},
url = {https://doi.org/10.1007/s10015-021-00681-3},
doi = {10.1007/s10015-021-00681-3},
abstract = {One of the most common issues addressed by bug report studies is misclassification when identifying and then filtering non-bug reports from the bug report repository. Having to filter out unrelated reports wastes time in identifying actual bug reports, and this escalates costs as extra maintenance and effort are required to triage and fix bugs. Therefore, this issue has been seriously studied and is addressed here. To tackle this problem, this study proposes a method of automatically identifying non-bug reports in the bug report repository using classification techniques. Three points are considered here. First, the bug report features used are unigram and CamelCase, where CamelCase words are used for feature expansion. Second, five term weighting schemes are compared to determine an appropriate term weighting scheme for this task. Lastly, the support vector machine (SVM) family i.e. binary-class SVM, one class SVM based on Sch\"{o}lkopf methodology and support vector data description (SVDD) are used as the main mechanisms for modeling non-bug report identifiers. After testing by recall, precision, and F1, the results demonstrate the efficiency of identifying non-bug reports in the bug report repository. Our results may be acceptable after comparing to the previous well-known studies, and the performance of non-bug report identifiers with tf-igm and modified tf-icf weighting schemes for both Sc\"{o}lkopf methodology and SVDD methods yielded the best value when compared to others.},
journal = {Artif. Life Robot.},
month = aug,
pages = {318–328},
numpages = {11},
keywords = {Bug reports, Non-bug report identifier, Text classification, Support vector machine (SVM), Sch\"{o}lkopf methodology, Support vector data description (SVDD)}
}

@article{10.1007/s10664-006-6405-5,
author = {Natt Och Dag, Johan and Thelin, Thomas and Regnell, Bj\"{o}rn},
title = {An experiment on linguistic tool support for consolidation of requirements from multiple sources in market-driven product development},
year = {2006},
issue_date = {June      2006},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {11},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-006-6405-5},
doi = {10.1007/s10664-006-6405-5},
abstract = {This paper presents an experiment with a linguistic support tool for consolidation of requirements sets. The experiment is designed based on the requirements management process at a large market-driven software development company that develops generic solutions to satisfy many different customers. New requirements and requests for information are continuously issued, which must be analyzed and responded to. The new requirements should first be consolidated with the old to avoid reanalysis of previously elicited requirements and to complement existing requirements with new information. In the presented experiment, a new open-source tool is evaluated in a laboratory setting. The tool uses linguistic engineering techniques to calculate similarities between requirements and presents a ranked list of suggested similar requirements, between which links may be assigned. It is hypothesized that the proposed technique for finding and linking similar requirements makes the consolidation more efficient. The results show that subjects that are given the support provided by the tool are significantly more efficient and more correct in consolidating two requirements sets, than are subjects that do not get the support. The results suggest that the proposed techniques may give valuable support and save time in an industrial requirements consolidation process.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {303–329},
numpages = {27},
keywords = {Linguistic engineering, Natural language requirements, Requirements management, Software product development}
}

@inproceedings{10.1145/3321707.3321880,
author = {Oliveira, Carlos and Aleti, Aldeida and Li, Yuan-Fang and Abdelrazek, Mohamed},
title = {Footprints of fitness functions in search-based software testing},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321880},
doi = {10.1145/3321707.3321880},
abstract = {Testing is technically and economically crucial for ensuring software quality. One of the most challenging testing tasks is to create test suites that will reveal potential defects in software. However, as the size and complexity of software systems increase, the task becomes more labour-intensive and manual test data generation becomes infeasible. To address this issue, researchers have proposed different approaches to automate the process of generating test data using search techniques; an area that is known as Search-Based Software Testing (SBST).SBST methods require a fitness function to guide the search to promising areas of the solution space. Over the years, a plethora of fitness functions have been proposed. Some methods use control information, others focus on goals. Deciding on what fitness function to use is not easy, as it depends on the software system under test. This work investigates the impact of software features on the effectiveness of different fitness functions. We propose the Mapping the Effectiveness of Test Automation (META) Framework which analyses the footprint of different fitness functions and creates a decision tree that enables the selection of the appropriate function based on software features.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1399–1407},
numpages = {9},
keywords = {genetic algorithms, search based software engineering},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3452383.3453714,
author = {Kumar, Atul and Panda, Subhrakanta},
title = {A Report on Tutorials and Tech-Briefings co-located with ISEC 2021},
year = {2021},
isbn = {9781450390460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452383.3453714},
doi = {10.1145/3452383.3453714},
abstract = {This is a short report on the Tutorials and Tech Briefings track of the 14th Innovations in Software Engineering (ISEC 2021) conference held fully virtually during 25-27 February 2021. The tutorials are popular with the participants because they offer in depth treatment of tutorial topics including hands on sessions. While tech briefings offer a gentle and friendly introduction to cutting edge topics and research at the frontiers of the discipline of software engineering. This year's track selected one tutorial and seven tech briefings reflecting the current interests and directions in the field of software engineering.},
booktitle = {Proceedings of the 14th Innovations in Software Engineering Conference (Formerly Known as India Software Engineering Conference)},
articleno = {25},
numpages = {3},
keywords = {ACM proceedings, Software engineering},
location = {Bhubaneswar, Odisha, India},
series = {ISEC '21}
}

@inproceedings{10.1109/ESEM.2009.5314233,
author = {Riaz, Mehwish and Mendes, Emilia and Tempero, Ewan},
title = {A systematic review of software maintainability prediction and metrics},
year = {2009},
isbn = {9781424448425},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ESEM.2009.5314233},
doi = {10.1109/ESEM.2009.5314233},
abstract = {This paper presents the results of a systematic review conducted to collect evidence on software maintainability prediction and metrics. The study was targeted at the software quality attribute of maintainability as opposed to the process of software maintenance. The evidence was gathered from the selected studies against a set of meaningful and focused questions. 710 studies were initially retrieved; however of these only 15 studies were selected; their quality was assessed; data extraction was performed; and data was synthesized against the research questions. Our results suggest that there is little evidence on the effectiveness of software maintainability prediction techniques and models.},
booktitle = {Proceedings of the 2009 3rd International Symposium on Empirical Software Engineering and Measurement},
pages = {367–377},
numpages = {11},
series = {ESEM '09}
}

@inproceedings{10.5555/3291656.3291748,
author = {Kalra, Charu and Previlon, Fritz and Li, Xiangyu and Rubin, Norman and Kaeli, David},
title = {PRISM: predicting resilience of GPU applications using statistical methods},
year = {2018},
publisher = {IEEE Press},
abstract = {As Graphics Processing Units (GPUs) become more pervasive in High Performance Computing (HPC) and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults. Due to the random nature of these faults, predicting whether they will alter program output is a challenging problem. In this paper, we build a framework named PRISM which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors to drive our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault injection campaigns, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {69},
numpages = {14},
location = {Dallas, Texas},
series = {SC '18}
}

@inproceedings{10.1109/SC.2018.00072,
author = {Kalra, Charu and Previlon, Fritz and Li, Xiangyu and Rubin, Norman and Kaeli, David},
title = {PRISM: predicting resilience of GPU applications using statistical methods},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC.2018.00072},
doi = {10.1109/SC.2018.00072},
abstract = {As Graphics Processing Units (GPUs) become more pervasive in High Performance Computing (HPC) and safety-critical domains, ensuring that GPU applications can be protected from data corruption grows in importance. Despite prior efforts to mitigate errors, we still lack a clear understanding of how resilient these applications are in the presence of transient faults. Due to the random nature of these faults, predicting whether they will alter program output is a challenging problem. In this paper, we build a framework named PRISM which uses a systematic approach to predict failures in GPU programs. PRISM extracts micro-architecture agnostic features to characterize program resiliency, which serve as predictors to drive our statistical model. PRISM enables us to predict failures in applications without running exhaustive fault injection campaigns, thereby reducing the error estimation effort. PRISM can also be used to gain insight into potential architectural support required to improve the reliability of GPU applications.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {69},
numpages = {14},
location = {Dallas, Texas},
series = {SC '18}
}

@article{10.1016/j.eswa.2007.03.005,
author = {Arranz, Antonio and Cruz, Alberto and Sanz-Bobi, Miguel A. and Ru\'{\i}z, Pablo and Couti\~{n}o, Josu\'{e}},
title = {DADICC: Intelligent system for anomaly detection in a combined cycle gas turbine plant},
year = {2008},
issue_date = {May, 2008},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {34},
number = {4},
issn = {0957-4174},
url = {https://doi.org/10.1016/j.eswa.2007.03.005},
doi = {10.1016/j.eswa.2007.03.005},
abstract = {DADICC is the abbreviated name for an intelligent system able to detect on-line and diagnose anomalies as soon as possible in the dynamic evolution of the behaviour of a power plant based on a combined cycle gas turbine. In order to reach this objective, a modelling process is required for the characterization of the normal performance when any symptom of a possible fault is present. This will be the reference for early detection of possible anomalies. If a deviation in respect to the normal behaviour predicted is observed, an analysis of its causes is performed in order to diagnose the potential problem, and, if possible, its prevention. A multi-agent system supports the different roles required in DADICC. The detection of anomalies is based on agents that use models elaborated using mainly neural networks techniques. The diagnosis of the anomalies is prepared by agents based on an expert-system structure. This paper describes the main characteristics of DADICC and its operation.},
journal = {Expert Syst. Appl.},
month = may,
pages = {2267–2277},
numpages = {11},
keywords = {Anomaly detection, Diagnosis, Expert system, Multi-agent system, Neural network, Normal behaviour}
}

@inproceedings{10.1145/2351676.2351690,
author = {Haiduc, Sonia and Bavota, Gabriele and Oliveto, Rocco and De Lucia, Andrea and Marcus, Andrian},
title = {Automatic query performance assessment during the retrieval of software artifacts},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351690},
doi = {10.1145/2351676.2351690},
abstract = {Text-based search and retrieval is used by developers in the context of many SE tasks, such as, concept location, traceability link retrieval, reuse, impact analysis, etc. Solutions for software text search range from regular expression matching to complex techniques using text retrieval. In all cases, the results of a search depend on the query formulated by the developer. A developer needs to run a query and look at the results before realizing that it needs reformulating. Our aim is to automatically assess the performance of a query before it is executed. We introduce an automatic query performance assessment approach for software artifact retrieval, which uses 21 measures from the field of text retrieval. We evaluate the approach in the context of concept location in source code. The evaluation shows that our approach is able to predict the performance of queries with 79% accuracy, using very little training data.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Query performance, concept location, text retrieval},
location = {Essen, Germany},
series = {ASE '12}
}

@inproceedings{10.1145/1146238.1146246,
author = {Bell, Robert M. and Ostrand, Thomas J. and Weyuker, Elaine J.},
title = {Looking for bugs in all the right places},
year = {2006},
isbn = {1595932631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1146238.1146246},
doi = {10.1145/1146238.1146246},
abstract = {We continue investigating the use of a negative binomial regression model to predict which files in a large industrial software system are most likely to contain many faults in the next release. A new empirical study is described whose subject is an automated voice response system. Not only is this system's functionality substantially different from that of the earlier systems we studied (an inventory system and a service provisioning system), it also uses a significantly different software development process. Instead of having regularly scheduled releases as both of the earlier systems did, this system has what are referred to as "continuous releases." We explore the use of three versions of the negative binomial regression model, as well as a simple lines-of-code based model, to make predictions for this system and discuss the differences observed from the earlier studies. Despite the different development process, the best version of the prediction model was able to identify, over the lifetime of the project, 20% of the system's files that contained, on average, nearly three quarters of the faults that were detected in the system's next releases.},
booktitle = {Proceedings of the 2006 International Symposium on Software Testing and Analysis},
pages = {61–72},
numpages = {12},
keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
location = {Portland, Maine, USA},
series = {ISSTA '06}
}

@article{10.1145/3392051,
author = {Aftab, Muhammad and Chau, Sid Chi-Kin and Shenoy, Prashant},
title = {Efficient Online Classification and Tracking on Resource-constrained IoT Devices},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3392051},
doi = {10.1145/3392051},
abstract = {Timely processing has been increasingly required on smart IoT devices, which leads to directly implementing information processing tasks on an IoT device for bandwidth savings and privacy assurance. Particularly, monitoring and tracking the observed signals in continuous form are common tasks for a variety of near real-time processing IoT devices, such as in smart homes, body-area, and environmental sensing applications. However, these systems are likely low-cost resource-constrained embedded systems, equipped with compact memory space, whereby the ability to store the full information state of continuous signals is limited. Hence, in this article,* we develop solutions of efficient timely processing embedded systems for online classification and tracking of continuous signals with compact memory space. Particularly, we focus on the application of smart plugs that are capable of timely classification of appliance types and tracking of appliance behavior in a standalone manner. We implemented a smart plug prototype using low-cost Arduino platform with small amount of memory space to demonstrate the following timely processing operations: (1) learning and classifying the patterns associated with the continuous power consumption signals and (2) tracking the occurrences of signal patterns using small local memory space. Furthermore, our system designs are also sufficiently generic for timely monitoring and tracking applications in other resource-constrained IoT devices.},
journal = {ACM Trans. Internet Things},
month = jul,
articleno = {20},
numpages = {29},
keywords = {Internet-of-things, Smart power plugs, online information processing, resource-constrained systems}
}

@inproceedings{10.5555/2486788.2486898,
author = {Haiduc, Sonia and Bavota, Gabriele and Marcus, Andrian and Oliveto, Rocco and De Lucia, Andrea and Menzies, Tim},
title = {Automatic query reformulations for text retrieval in software engineering},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {There are more than twenty distinct software engineering tasks addressed with text retrieval (TR) techniques, such as, traceability link recovery, feature location, refactoring, reuse, etc. A common issue with all TR applications is that the results of the retrieval depend largely on the quality of the query. When a query performs poorly, it has to be reformulated and this is a difficult task for someone who had trouble writing a good query in the first place.  We propose a recommender (called Refoqus) based on machine learning, which is trained with a sample of queries and relevant results. Then, for a given query, it automatically recommends a reformulation strategy that should improve its performance, based on the properties of the query. We evaluated Refoqus empirically against four baseline approaches that are used in natural language document retrieval. The data used for the evaluation corresponds to changes from five open source systems in Java and C++ and it is used in the context of TR-based concept location in source code. Refoqus outperformed the baselines and its recommendations lead to query performance improvement or preservation in 84% of the cases (in average).},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {842–851},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1145/2580950,
author = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
title = {A Classification and Survey of Analysis Strategies for Software Product Lines},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2580950},
doi = {10.1145/2580950},
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {6},
numpages = {45},
keywords = {Product-line analysis, model checking, program family, software analysis, software product line, static analysis, theorem proving, type checking}
}

@inproceedings{10.1145/1993498.1993509,
author = {Jung, Changhee and Rus, Silvius and Railing, Brian P. and Clark, Nathan and Pande, Santosh},
title = {Brainy: effective selection of data structures},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993509},
doi = {10.1145/1993498.1993509},
abstract = {Data structure selection is one of the most critical aspects of developing effective applications. By analyzing data structures' behavior and their interaction with the rest of the application on the underlying architecture, tools can make suggestions for alternative data structures better suited for the program input on which the application runs. Consequently, developers can optimize their data structure usage to make the application conscious of an underlying architecture and a particular program input.This paper presents the design and evaluation of Brainy, a new program analysis tool that automatically selects the best data structure for a given program and its input on a specific microarchitecture. The data structure's interface functions are instrumented to dynamically monitor how the data structure interacts with the application for a given input. The instrumentation records traces of various runtime characteristics including underlying architecture-specific events. These generated traces are analyzed and fed into an offline model, constructed using machine learning, to select the best data structure. That is, Brainy exploits runtime feedback of data structures to model the situation an application runs on, and selects the best data structure for a given application/input/architecture combination based on the constructed model. The empirical evaluation shows that this technique is highly accurate across several real-world applications with various program input sets on two different state-of-the-art microarchitectures. Consequently, Brainy achieved an average performance improvement of 27% and 33% on both microarchitectures, respectively.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {86–97},
numpages = {12},
keywords = {application generator, data structure selection, performance counters, training framework},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@inbook{10.5555/2184075.2184076,
author = {Harman, Mark and McMinn, Phil and de Souza, Jerffeson Teixeira and Yoo, Shin},
title = {Search based software engineering: techniques, taxonomy, tutorial},
year = {2012},
isbn = {9783642252303},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The aim of Search Based Software Engineering (SBSE) research is to move software engineering problems from human-based search to machine-based search, using a variety of techniques from the metaheuristic search, operations research and evolutionary computation paradigms. The idea is to exploit humans' creativity and machines' tenacity and reliability, rather than requiring humans to perform the more tedious, error prone and thereby costly aspects of the engineering process. SBSE can also provide insights and decision support. This tutorial will present the reader with a step-by-step guide to the application of SBSE techniques to Software Engineering. It assumes neither previous knowledge nor experience with Search Based Optimisation. The intention is that the tutorial will cover sufficient material to allow the reader to become productive in successfully applying search based optimisation to a chosen Software Engineering problem of interest.},
booktitle = {Empirical Software Engineering and Verification: International Summmer Schools},
pages = {1–59},
numpages = {59}
}

@inproceedings{10.1145/2855321.2855361,
author = {Alebrahim, Azadeh and Fassbender, Stephan and Filipczyk, Martin and Goedicke, Michael and Heisel, Maritta},
title = {Towards a reliable mapping between performance and security tactics, and architectural patterns},
year = {2015},
isbn = {9781450338479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2855321.2855361},
doi = {10.1145/2855321.2855361},
abstract = {The software architecture of a system-to-be affects the fulfillment of the desired quality requirements for this system. For building upon common knowledge and best practices, the use of architectural patterns in the software architecture has shown to be valuable. Besides their functional properties, each architectural pattern has benefits and liabilities regarding the fulfillment of quality requirements. Whereas architectural patterns contribute to the fulfillment of several quality requirements positively or negatively in a larger scale, tactics aim at improving one specific quality requirement in a more local manner. In order to tailor a software architecture for satisfying one or more specific quality requirements, tactics have to be integrated into the structure of the architectural patterns. In this paper, we investigate the relationship between several architectural patterns and performance as well as security tactics. We study how easily one tactic can be implemented in an architectural pattern. Based on our investigation, we provide mappings between the architectural patterns and tactics. Additionally, we provide a reasoning for the relations we found. To discover the relevant factors for creating reliable relations, we conduct an experiment at the EuroPLoP 2015 as a Focus Group. We describe the definition and planning of this experiment. The results will be used to define a method for creating a reliable mapping between tactics and architectural patterns. The expected target audience for our paper is software architects as well as researchers with an interest in software architectures and controlled experiments.},
booktitle = {Proceedings of the 20th European Conference on Pattern Languages of Programs},
articleno = {39},
numpages = {43},
keywords = {patterns, performance, security, tactics},
location = {Kaufbeuren, Germany},
series = {EuroPLoP '15}
}

@article{10.5555/992846.992849,
author = {Gomes, Paulo and Pereira, Francisco C. and Paiva, Paulo and Seco, Nuno and Carreiro, Paulo and Ferreira, Jos\'{e} L. and Bento, Carlos},
title = {Using WordNet for case-based retrieval of UML models},
year = {2004},
issue_date = {January 2004},
publisher = {IOS Press},
address = {NLD},
volume = {17},
number = {1},
issn = {0921-7126},
abstract = {Software complexity has increased substantially in the last decade. This has made software development teams work faster and under tight budgets. Reusing software can be a way of solving this problem. Companies should reuse previous working solutions in new projects, decreasing the development time and increasing software quality. But there are at least two requirements to implement this solution. One is that the company must have a central knowledge repository with software specifications, designs and code from previous system developments. The second one is to have at the disposal tools capable of using this repository in an intelligent way. We have developed a system capable of providing these requirements. It has a central knowledge base that can be used through Case-Based Reasoning. The knowledge base integrates a common ontology called WordNet. providing classification for software objects. This paper focuses on the retrieval of design models using the combination of WordNet and Case-Based Reasoning. We also present a retrieval example, and experimental work showing the performance of the retrieval and ranking mechanisms.},
journal = {AI Commun.},
month = jan,
pages = {13–23},
numpages = {11},
keywords = {WordNet, case-based reasoning, information retrieval}
}

@inproceedings{10.1109/ASE.2019.00065,
author = {M\"{u}hlbauer, Stefan and Apel, Sven and Siegmund, Norbert},
title = {Accurate modeling of performance histories for evolving software systems},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00065},
doi = {10.1109/ASE.2019.00065},
abstract = {Learning from the history of a software system's performance behavior does not only help discovering and locating performance bugs, but also identifying evolutionary performance patterns and general trends, such as when technical debt accumulates. Exhaustive regression testing is usually impractical, because rigorous performance benchmarking requires executing a realistic workload per revision, which results in large execution times. In this paper, we propose a novel active revision sampling approach, which aims at tracking and understanding a system's performance history by approximating the performance behavior of a software system across all of its revisions. In a nutshell, we iteratively sample and measure the performance of specific revisions that help us building an exact performance-evolution model, and we use Gaussian Process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement. We have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real-world software systems. Our evaluation demonstrates that Gaussian Process models are able to accurately estimate the performance-evolution history of real-world software systems with only few measurements and to reveal interesting behaviors and trends.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {640–652},
numpages = {13},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1007/s10664-021-10015-3,
author = {Yang, Nan and Cuijpers, Pieter and Schiffelers, Ramon and Lukkien, Johan and Serebrenik, Alexander},
title = {Single-state state machines in model-driven software engineering: an exploratory study},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10015-3},
doi = {10.1007/s10664-021-10015-3},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {46},
keywords = {Model-driven engineering, Single-state state machines, modeling practice}
}

@inproceedings{10.1145/3238147.3238175,
author = {Bao, Liang and Liu, Xin and Xu, Ziheng and Fang, Baoyin},
title = {AutoConfig: automatic configuration tuning for distributed message systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238175},
doi = {10.1145/3238147.3238175},
abstract = {Distributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {29–40},
numpages = {12},
keywords = {automatic configuration tuning, comparison-based model, distributed message system, weighted Latin hypercube sampling},
location = {Montpellier, France},
series = {ASE '18}
}

@article{10.1016/j.elerap.2008.11.005,
author = {Kiekintveld, Christopher and Miller, Jason and Jordan, Patrick R. and Callender, Lee F. and Wellman, Michael P.},
title = {Forecasting market prices in a supply chain game},
year = {2009},
issue_date = {March, 2009},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {8},
number = {2},
issn = {1567-4223},
url = {https://doi.org/10.1016/j.elerap.2008.11.005},
doi = {10.1016/j.elerap.2008.11.005},
abstract = {Predicting the uncertain and dynamic future of market conditions on the supply chain, as reflected in prices, is an essential component of effective operational decision-making. We present and evaluate methods used by our agent, Deep Maize, to forecast market prices in the trading agent competition supply chain management game (TAC/SCM). We employ a variety of machine learning and representational techniques to exploit as many types of information as possible, integrating well-known methods in novel ways. We evaluate these techniques through controlled experiments as well as performance in both the main TAC/SCM tournament and supplementary Prediction Challenge. Our prediction methods demonstrate strong performance in controlled experiments and achieved the best overall score in the Prediction Challenge.},
journal = {Electron. Commer. Rec. Appl.},
month = mar,
pages = {63–77},
numpages = {15},
keywords = {Forecasting, Machine learning, Markets, Price prediction, Supply chain management, Trading agent competition}
}

@article{10.1007/s10664-018-9595-8,
author = {Li, Heng and Chen, Tse-Hsun (Peter) and Shang, Weiyi and Hassan, Ahmed E.},
title = {Studying software logging using topic models},
year = {2018},
issue_date = {October   2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-018-9595-8},
doi = {10.1007/s10664-018-9595-8},
abstract = {Software developers insert logging statements in their source code to record important runtime information; such logged information is valuable for understanding system usage in production and debugging system failures. However, providing proper logging statements remains a manual and challenging task. Missing an important logging statement may increase the difficulty of debugging a system failure, while too much logging can increase system overhead and mask the truly important information. Intuitively, the actual functionality of a software component is one of the major drivers behind logging decisions. For instance, a method maintaining network communications is more likely to be logged than getters and setters. In this paper, we used automatically-computed topics of a code snippet to approximate the functionality of a code snippet. We studied the relationship between the topics of a code snippet and the likelihood of a code snippet being logged (i.e., to contain a logging statement). Our driving intuition is that certain topics in the source code are more likely to be logged than others. To validate our intuition, we conducted a case study on six open source systems, and we found that i) there exists a small number of "log-intensive" topics that are more likely to be logged than other topics; ii) each pair of the studied systems share 12% to 62% common topics, and the likelihood of logging such common topics has a statistically significant correlation of 0.35 to 0.62 among all the studied systems; and iii) our topic-based metrics help explain the likelihood of a code snippet being logged, providing an improvement of 3% to 13% on AUC and 6% to 16% on balanced accuracy over a set of baseline metrics that capture the structural information of a code snippet. Our findings highlight that topics contain valuable information that can help guide and drive developers' logging decisions.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {2655–2694},
numpages = {40},
keywords = {Mining software repositories, Software logging, Topic model}
}

@article{10.1016/j.jss.2018.07.054,
author = {Ochoa, Lina and Gonz\'{a}lez-Rojas, Oscar and Juliana, Alves Pereira and Castro, Harold and Saake, Gunter},
title = {A systematic literature review on the semi-automatic configuration of extended product lines},
year = {2018},
issue_date = {Oct 2018},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {144},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2018.07.054},
doi = {10.1016/j.jss.2018.07.054},
journal = {J. Syst. Softw.},
month = oct,
pages = {511–532},
numpages = {22},
keywords = {Extended product line, Product configuration, Systematic literature review}
}

@inproceedings{10.1145/1287624.1287714,
author = {Stefaniak, Marcin},
title = {Visualising exemplary program values},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287714},
doi = {10.1145/1287624.1287714},
abstract = {We describe an idea of a tool to aid software developers, similar to tracing and software visualization. The tool monitors a running program and log some values of its variables. The exemplary values, chosen by the tool, are later displayed onto the source code. Each variable occurrence in the source code is visualized with a few examples of its runtime values. We are unaware of such a tool implemented already, and the question which values should be selected seems interesting.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {575–578},
numpages = {4},
keywords = {branches, tracing, variables, visualization},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inproceedings{10.1145/2499393.2499399,
author = {Heckman, Sarah and Williams, Laurie},
title = {A comparative evaluation of static analysis actionable alert identification techniques},
year = {2013},
isbn = {9781450320160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2499393.2499399},
doi = {10.1145/2499393.2499399},
abstract = {Automated static analysis (ASA) tools can identify potential source code anomalies that could lead to field failures. Developer inspection is required to determine if an ASA alert is important enough to fix, or an actionable alert. Supplementing current ASA tools with automated identification of actionable alerts could reduce developer inspection overhead, leading to an increase in industry adoption of ASA tools. The goal of this research is to inform the selection of an actionable alert identification technique for ranking the output of automated static analysis through a comparative evaluation of actionable alert identification techniques. We investigated six actionable alert identification techniques on three subject projects. Among these six techniques, the systematic actionable alert identification (SAAI) technique reported an average accuracy of 82.5% across the three subject projects when considering both ASA tools evaluated. Check 'n' Crash reported an average accuracy of 85.8% for the single ASA tool evaluated. The other actionable alert identification techniques had average accuracies ranging from 42.2%-78.2%.},
booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering},
articleno = {4},
numpages = {10},
keywords = {actionable static analysis alert identification, automated static analysis, comparative evaluation},
location = {Baltimore, Maryland, USA},
series = {PROMISE '13}
}

@inproceedings{10.1145/1295014.1295046,
author = {Stefaniak, Marcin},
title = {Visualising exemplary program values},
year = {2007},
isbn = {9781595938121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1295014.1295046},
doi = {10.1145/1295014.1295046},
abstract = {We describe an idea of a tool to aid software developers, similar to tracing and software visualization. The tool monitors a running program and log some values of its variables. The exemplary values, chosen by the tool, are later displayed onto the source code. Each variable occurrence in the source code is visualized with a few examples of its runtime values. We are unaware of such a tool implemented already, and the question which values should be selected seems interesting.},
booktitle = {The 6th Joint Meeting on European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering: Companion Papers},
pages = {575–578},
numpages = {4},
keywords = {branches, tracing, variables, visualization},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE companion '07}
}

@inproceedings{10.5555/2535461.2535498,
author = {Kwon, Yongin and Lee, Sangmin and Yi, Hayoon and Kwon, Donghyun and Yang, Seungjun and Chun, Byung-Gon and Huang, Ling and Maniatis, Petros and Naik, Mayur and Paek, Yunheung},
title = {Mantis: automatic performance prediction for smartphone applications},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {We present Mantis, a framework for predicting the performance of Android applications on given inputs automatically, accurately, and efficiently. A key insight underlying Mantis is that program execution runs often contain features that correlate with performance and are automatically computable efficiently. Mantis synergistically combines techniques from program analysis and machine learning. It constructs concise performance models by choosing from many program execution features only a handful that are most correlated with the program's execution time yet can be evaluated efficiently from the program's input. We apply program slicing to accurately estimate the evaluation cost of a feature and automatically generate executable code snippets for efficiently evaluating features. Our evaluation shows that Mantis predicts the execution time of six Android apps with estimation error in the range of 2.2-11.9% by executing predictor code costing at most 1.3% of their execution time on Galaxy Nexus.},
booktitle = {Proceedings of the 2013 USENIX Conference on Annual Technical Conference},
pages = {297–308},
numpages = {12},
location = {San Jose, CA},
series = {USENIX ATC'13}
}

@article{10.1016/j.future.2017.09.003,
author = {R, Reginaldo and Meloca, Rmulo Manciola and Roma, Douglas Nassif and Ismael, Marcelo Alexandre da Cruz and Silva, Gabriel Costa},
title = {An empirical study for evaluating the performance of multi-cloud APIs},
year = {2018},
issue_date = {February 2018},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {79},
number = {P2},
issn = {0167-739X},
url = {https://doi.org/10.1016/j.future.2017.09.003},
doi = {10.1016/j.future.2017.09.003},
abstract = {The massive use of cloud APIs for workload orchestration and the increased adoption of multiple cloud platforms prompted the rise of multi-cloud APIs. Multi-cloud APIs abstract cloud differences and provide a single interface regardless of the target cloud platform. Identifying whether the performance of multi-cloud APIs differs significantly from platform-specific APIs is central for driving technological decisions on cloud applications that require maximum performance when using multiple clouds. This study aims to evaluate the performance of multi-cloud APIs when compared to platform-specific APIs. We carried out three rigorous quasi-experiments to measure the performance (dependent variable) of cloud APIs (independent variable) regarding CPU time, memory consumption and response time. jclouds and Libcloud were the two multi-cloud APIs used (experimental treatment). Their performance were compared to platform-specific APIs (control treatment) provided by Amazon Web Services and Microsoft Azure. These APIs were used for uploading and downloading (tasks) 39722 files in five different sizes to/from storage services during five days (trials). Whereas jclouds performed significantly worse than platform-specific APIs for all performance indicators on both cloud platforms and operations for all five file sizes, Libcloud outperformed platform-specific APIs in most tests (p-value not exceeding 0.00125, A-statistic greater than 0.64). Once confirmed by independent replications, our results suggest that jclouds developers should review the API design to ensure minimal overhead whereas jclouds users should evaluate the extent to which this trade-off affect the performance of their applications. Multi-cloud users should carefully evaluate what quality attribute is more important when selecting a cloud API. The performance of multi-cloud differs significantly from platform-specific APIs.jclouds performed significantly worse than platform-specific APIs in all tests.Libcloud outperformed platform-specific APIs in most tests.Multi-cloud users should evaluate what quality attribute is more important.},
journal = {Future Gener. Comput. Syst.},
month = feb,
pages = {726–738},
numpages = {13},
keywords = {Evaluation, Experiment, Libcloud, Multi-cloud, Performance, jclouds}
}

@article{10.1016/j.jss.2015.11.040,
author = {Gonz\'{a}lez-Ladr\'{o}n-de-Guevara, Fernando and Fern\'{a}ndez-Diego, Marta and Lokan, Chris},
title = {The usage of ISBSG data fields in software effort estimation},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.11.040},
doi = {10.1016/j.jss.2015.11.040},
abstract = {We performed a systematic mapping study over 107 papers that use ISBSG data for effort estimation.Usage of dependent, filtering, and independent variables in effort models is described.Factors that guide the selection of relevant independent variables are described.Influence of estimation methods in the selection of variables has been outlined.References that have worked with a specific independent variable are listed. The International Software Benchmarking Standards Group (ISBSG) maintains a repository of data about completed software projects. A common use of the ISBSG dataset is to investigate models to estimate a software project's size, effort, duration, and cost. The aim of this paper is to determine which and to what extent variables in the ISBSG dataset have been used in software engineering to build effort estimation models. For that purpose a systematic mapping study was applied to 107 research papers, obtained after a filtering process, that were published from 2000 until the end of 2013, and which listed the independent variables used in the effort estimation models. The usage of ISBSG variables for filtering, as dependent variables, and as independent variables is described. The 20 variables (out of 71) mostly used as independent variables for effort estimation are identified and analysed in detail, with reference to the papers and types of estimation methods that used them. We propose guidelines that can help researchers make informed decisions about which ISBSG variables to select for their effort estimation models.},
journal = {J. Syst. Softw.},
month = mar,
pages = {188–215},
numpages = {28},
keywords = {ISBSG data field, Software effort estimation, Systematic mapping study}
}

@article{10.1007/s11036-021-01832-3,
author = {Zhaoxue, Jiang and Tong, Li and Zhenguo, Zhang and Jingguo, Ge and Junling, You and Liangxiong, Li},
title = {A Survey On Log Research Of AIOps: Methods and Trends},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {26},
number = {6},
issn = {1383-469X},
url = {https://doi.org/10.1007/s11036-021-01832-3},
doi = {10.1007/s11036-021-01832-3},
abstract = {With the development of Artificial Intelligence (AI), Internet of Things (IoT), cloud computing, new-generation mobile communication, etc., digital transformation is changing the technical architecture of IT systems. It brings more requirements for performance and reliability. The traditional human-dependent development and maintenance methods are overwhelmed, and need to transform to Artificial Intelligence for IT Operations (AIOps). As one of the most useful data resources in IT system, the log plays an important role in AIOps. There are many research on enhancing log quality, analyzing log structure, understanding system behavior, helping users to mine the effective information in logs. Based on the characteristics of logs and different strategies, this paper reviews and categorizes the existing works around the three key processes in the log processing framework of log enhancement, log parsing, and log analysis in academia, and establishes evaluation indicators for comparison and summary. Finally, we discussed the potential directions and future development trends.},
journal = {Mob. Netw. Appl.},
month = dec,
pages = {2353–2364},
numpages = {12},
keywords = {AIOps, Log enhancement, Log parsing, Log analysis}
}

@article{10.1016/S1877-0509(15)00434-2,
title = {Contents},
year = {2015},
issue_date = {2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {46},
number = {C},
issn = {1877-0509},
url = {https://doi.org/10.1016/S1877-0509(15)00434-2},
doi = {10.1016/S1877-0509(15)00434-2},
journal = {Procedia Comput. Sci.},
month = jan,
pages = {iii–x},
numpages = {8}
}

@inproceedings{10.1109/ICSE.2017.45,
author = {Xiong, Yingfei and Wang, Jie and Yan, Runfa and Zhang, Jiachen and Han, Shi and Huang, Gang and Zhang, Lu},
title = {Precise condition synthesis for program repair},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.45},
doi = {10.1109/ICSE.2017.45},
abstract = {Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting.In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an "if" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects.Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {416–426},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1023/A:1009815306795,
author = {Mayrhauser, A. Von and Wohlin, C. and Ohlsson, M. C.},
title = {Assessing and Understanding Efficiency and Success of SoftwareProduction},
year = {2000},
issue_date = {June 2000},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {5},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1023/A:1009815306795},
doi = {10.1023/A:1009815306795},
abstract = {One of the goals of collecting

project data during software development and evolution is to

assess how well the project did and what should be done to improve

in the future. With the wide range of data often collected and

the many complicated relationships between them, this is not

always easy. This paper suggests to use production models (Data

Envelope Analysis) to analyze objective variables and their impact

on efficiency. To understand the effect of subjective variables,

it is suggested to apply principal component analysis (PCA).

Further, we propose to combine the results from the production

models and the analysis of the subjective variables. We show

capabilities of production models and illustrate how production

models can be combined with other approaches to allow for assessing

and hence understanding software project data. The approach is

illustrated on a data set consisting of 46 software projects

from the NASA-SEL database (NASA-SEL, 1992). The data analyzed

is of the type that is commonly found in project databases.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {125–154},
numpages = {30},
keywords = {Software project database, analysis of subjective factors, principal components analysis, production models, project assessment, quantitative}
}

@article{10.1016/j.sysarc.2013.08.003,
author = {Powell, Adam and Savvas-Bouganis, Christos and Cheung, Peter Y. K.},
title = {High-level power and performance estimation of FPGA-based soft processors and its application to design space exploration},
year = {2013},
issue_date = {November, 2013},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {59},
number = {10},
issn = {1383-7621},
url = {https://doi.org/10.1016/j.sysarc.2013.08.003},
doi = {10.1016/j.sysarc.2013.08.003},
abstract = {This paper presents a design space exploration framework for an FPGA-based soft processor that is built on the estimation of power and performance metrics using algorithm and architecture parameters. The proposed framework is based on regression trees, a popular machine learning technique, that can capture the relationship of low-level soft-processor parameters and high-level algorithm parameters of a specific application domain, such as image compression. In doing this, power and execution time of an algorithm can be predicted before implementation and on unseen configurations of soft processors. For system designers this can result in fast design space exploration at an early stage in design.},
journal = {J. Syst. Archit.},
month = nov,
pages = {1144–1156},
numpages = {13},
keywords = {Design space exploration, Image compression, Modeling, Performance estimation, Power estimation, Regression tree, Soft processors}
}

@article{10.1016/S1568-4946(11)00348-6,
title = {Subject Index},
year = {2011},
issue_date = {December, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {11},
number = {8},
issn = {1568-4946},
url = {https://doi.org/10.1016/S1568-4946(11)00348-6},
doi = {10.1016/S1568-4946(11)00348-6},
journal = {Appl. Soft Comput.},
month = dec,
pages = {5829–5873},
numpages = {45}
}

@article{10.1007/s00521-021-05720-5,
author = {Alabool, Hamzeh Mohammad and Alarabiat, Deemah and Abualigah, Laith and Heidari, Ali Asghar},
title = {Harris hawks optimization: a comprehensive review of recent variants and applications},
year = {2021},
issue_date = {Aug 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {15},
issn = {0941-0643},
url = {https://doi.org/10.1007/s00521-021-05720-5},
doi = {10.1007/s00521-021-05720-5},
abstract = {Harris hawks optimizer (HHO) has received widespread attention among researchers in terms of the performance, quality of results, and its acceptable convergence in dealing with different applications in real-world problems. This increased interest led to the emergence of many versions of HHO applied to various optimization problems in different fields. Therefore, this study aims to identify, retrieve, summarize, and analyze the critical studies related to the development of HHO. For this aim, we applied a review methodology. The applied methodology led to identified and selection of 69 related studies from different electronic sources. The review result revealed that although HHO algorithm is still in the infant stage, its superiority over several well-established metaheuristic algorithms in terms of speed and accuracy for addressing various benchmark problems and tackling several real-world optimization problems has been clearly observed. The HHO algorithm was evaluated, and its strengths and weaknesses were discussed. This review not only suggested possible future directions in this domain but also serves as a comprehensive source of information about HHO and HHO variants for future researchers due to the inclusion of charts and tabular comparison across a wide variety of attributes. A public website supports open access to this research and also source codes of the HHO in a different language and its supplementary materials at .},
journal = {Neural Comput. Appl.},
month = aug,
pages = {8939–8980},
numpages = {42},
keywords = {Harris hawks, Optimization, Swarm intelligence, Metaheuristic, Nature-inspired algorithm}
}

@article{10.1016/j.jss.2021.111059,
author = {Opdebeeck, Ruben and Zerouali, Ahmed and Vel\'{a}zquez-Rodr\'{\i}guez, Camilo and De Roover, Coen},
title = {On the practice of semantic versioning for Ansible galaxy roles: An empirical study and a change classification model},
year = {2021},
issue_date = {Dec 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {182},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111059},
doi = {10.1016/j.jss.2021.111059},
journal = {J. Syst. Softw.},
month = dec,
numpages = {21},
keywords = {Ansible, Infrastructure as code, Semantic versioning, Empirical study, Mining software repositories}
}

@article{10.1007/s10664-017-9529-x,
author = {Soh, Z\'{e}phyrin and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Antoniol, Giuliano},
title = {Noise in Mylyn interaction traces and its impact on developers and recommendation systems},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9529-x},
doi = {10.1007/s10664-017-9529-x},
abstract = {Interaction traces (ITs) are developers' logs collected while developers maintain or evolve software systems. Researchers use ITs to study developers' editing styles and recommend relevant program entities when developers perform changes on source code. However, when using ITs, they make assumptions that may not necessarily be true. This article assesses the extent to which researchers' assumptions are true and examines noise in ITs. It also investigates the impact of noise on previous studies. This article describes a quasi-experiment collecting both Mylyn ITs and video-screen captures while 15 participants performed four realistic software maintenance tasks. It assesses the noise in ITs by comparing Mylyn ITs and the ITs obtained from the video captures. It proposes an approach to correct noise and uses this approach to revisit previous studies. The collected data show that Mylyn ITs can miss, on average, about 6% of the time spent by participants performing tasks and can contain, on average, about 85% of false edit events, which are not real changes to the source code. The approach to correct noise reveals about 45% of misclassification of ITs. It can improve the precision and recall of recommendation systems from the literature by up to 56% and 62%, respectively. Mylyn ITs include noise that biases subsequent studies and, thus, can prevent researchers from assisting developers effectively. They must be cleaned before use in studies and recommendation systems. The results on Mylyn ITs open new perspectives for the investigation of noise in ITs generated by other monitoring tools such as DFlow, FeedBag, and Mimec, and for future studies based on ITs.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {645–692},
numpages = {48},
keywords = {Editing behaviour, Mylyn Interaction traces, Noise, Recommendation systems, Software maintenance}
}

@inproceedings{10.1145/2993259.2993266,
author = {Nayebi, Maleknaz and Farrahi, Homayoon and Lee, Ada and Cho, Henry and Ruhe, Guenther},
title = {More insight from being more focused: analysis of clustered market apps},
year = {2016},
isbn = {9781450343985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993259.2993266},
doi = {10.1145/2993259.2993266},
abstract = {The increasing attraction of mobile apps has inspired researchers to analyze apps from different perspectives. As any software product, apps have different attributes such as size, content maturity, rating, category or number of downloads. Current research studies mostly consider sampling across all apps. This often results in comparisons of apps being quite different in nature and category (games compared with weather and calendar apps), also being different in size and complexity. Similar to proprietary software and web-based services, more specific results can be expected from looking at more homogeneous samples as they can be received as a result of applying clustering.  In this paper, we target homogeneous samples of apps to increase to degree of insight gained from analytics. As a proof-of-concept, we applied clustering technique DBSCAN and subsequent correlation analysis between app attributes for a set of 940 open source mobile apps from F-Droid. We showed that (i) clusters of apps with similar characteristics provided more insight compared to applying the same to the whole data and (ii) defining similarity of apps based on similarity of topics as created from topic modeling technique Latent Dirichlet Allocation does not significantly improve clustering results.},
booktitle = {Proceedings of the International Workshop on App Market Analytics},
pages = {30–36},
numpages = {7},
keywords = {App store analysis, Clustering, Data analytics, Mobile apps},
location = {Seattle, WA, USA},
series = {WAMA 2016}
}

@article{10.1016/j.jss.2021.111005,
author = {Bi, Tingting and Liang, Peng and Tang, Antony and Xia, Xin},
title = {Mining Architecture Tactics and Quality Attributes knowledge in Stack Overflow},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2021.111005},
doi = {10.1016/j.jss.2021.111005},
journal = {J. Syst. Softw.},
month = oct,
numpages = {18},
keywords = {Architecture Tactic, Quality Attribute, Knowledge mining, Empirical analysis, Stack Overflow}
}

@inproceedings{10.1145/2442754.2442761,
author = {Hussain, Aftab and Rahman, Md. Saidur},
title = {A new hierarchical clustering technique for restructuring software at the function level},
year = {2013},
isbn = {9781450319874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2442754.2442761},
doi = {10.1145/2442754.2442761},
abstract = {Ill-structured code is difficult to understand and thereby is costly to maintain and reuse. Software restructuring techniques based on hierarchical agglomerative clustering (HAC) algorithms have been widely used to restructure large modules with low cohesion into smaller modules with high cohesion, without changing the overall behaviour of the software. These techniques generate clustering trees, of modules, that are sliced at different cut-points to obtain desired restructurings. Choosing appropriate cut-points has always been a difficult problem in clustering. Previous HAC techniques generate clustering trees that have large number of cut-points. Moreover, many of those cut-points return clusters of which only a few lead to a meaningful restructuring of the software. In this paper, we give a new hierarchical clustering technique, the (k, w)-Core Clustering ((k, w)-CC) technique, for restructuring software at the function level that generates clustering trees with lower number of cut-points, which yield a lower number of redundant clusters. (k, w)-CC gives good restructurings. To establish this, we provide an experimental comparison of (k, w)-CC with four previous HAC techniques: single linkage algorithm (SLINK), complete linkage algorithm (CLINK), weighted pair group method of arithmetic averages (WPGMA), and adaptive k-nearest neighbour algorithm (A-KNN). In the experiments, the techniques were implemented on Java functions extracted from real-life industrial programs.},
booktitle = {Proceedings of the 6th India Software Engineering Conference},
pages = {45–54},
numpages = {10},
keywords = {cohesion, dendrogram, hierarchical clustering, software restructuring},
location = {New Delhi, India},
series = {ISEC '13}
}

@article{10.1145/2791577,
author = {Xu, Tianyin and Zhou, Yuanyuan},
title = {Systems Approaches to Tackling Configuration Errors: A Survey},
year = {2015},
issue_date = {July 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/2791577},
doi = {10.1145/2791577},
abstract = {In recent years, configuration errors (i.e., misconfigurations) have become one of the dominant causes of system failures, resulting in many severe service outages and downtime. Unfortunately, it is notoriously difficult for system users (e.g., administrators and operators) to prevent, detect, and troubleshoot configuration errors due to the complexity of the configurations as well as the systems under configuration. As a result, the cost of resolving configuration errors is often tremendous from the aspects of both compensating the service disruptions and diagnosing, recovering from the failures. The prevalence, severity, and cost have made configuration errors one of the most thorny system problems that desire to be addressed.This survey article provides a holistic and structured overview of the systems approaches that tackle configuration errors. To understand the problem fundamentally, we first discuss the characteristics of configuration errors and the challenges of tackling such errors. Then, we discuss the state-of-the-art systems approaches that address different types of configuration errors in different scenarios. Our primary goal is to equip the stakeholder with a better understanding of configuration errors and the potential solutions for resolving configuration errors in the spectrum of system development and management. To inspire follow-up research, we further discuss the open problems with regard to system configuration. To the best of our knowledge, this is the first survey on the topic of tackling configuration errors.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {70},
numpages = {41},
keywords = {Configuration, automation, configuration error, deployment, detection, diagnosis, failure, management, misconfiguration, testing, troubleshooting, validation, vulnerability}
}

@article{10.1016/j.infsof.2008.09.001,
author = {Karhu, Katja and Taipale, Ossi and Smolander, Kari},
title = {Investigating the relationship between schedules and knowledge transfer in software testing},
year = {2009},
issue_date = {March, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {3},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.09.001},
doi = {10.1016/j.infsof.2008.09.001},
abstract = {This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {663–677},
numpages = {15},
keywords = {Case study, Knowledge transfer, Schedule over-runs, Software testing, Survey}
}

@inproceedings{10.1145/1557019.1557083,
author = {Lo, David and Cheng, Hong and Han, Jiawei and Khoo, Siau-Cheng and Sun, Chengnian},
title = {Classification of software behaviors for failure detection: a discriminative pattern mining approach},
year = {2009},
isbn = {9781605584959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1557019.1557083},
doi = {10.1145/1557019.1557083},
abstract = {Software is a ubiquitous component of our daily life. We often depend on the correct working of software systems. Due to the difficulty and complexity of software systems, bugs and anomalies are prevalent. Bugs have caused billions of dollars loss, in addition to privacy and security threats. In this work, we address software reliability issues by proposing a novel method to classify software behaviors based on past history or runs. With the technique, it is possible to generalize past known errors and mistakes to capture failures and anomalies. Our technique first mines a set of discriminative features capturing repetitive series of events from program execution traces. It then performs feature selection to select the best features for classification. These features are then used to train a classifier to detect failures. Experiments and case studies on traces of several benchmark software systems and a real-life concurrency bug from MySQL server show the utility of the technique in capturing failures and anomalies. On average, our pattern-based classification technique outperforms the baseline approach by 24.68% in accuracy.},
booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {557–566},
numpages = {10},
keywords = {closed unique patterns, failure detection, iterative patterns, pattern-based classification, sequential database, software behaviors},
location = {Paris, France},
series = {KDD '09}
}

@inproceedings{10.1145/1463788.1463819,
author = {Antoniol, Giuliano and Ayari, Kamel and Di Penta, Massimiliano and Khomh, Foutse and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {Is it a bug or an enhancement? a text-based approach to classify change requests},
year = {2008},
isbn = {9781450378826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1463788.1463819},
doi = {10.1145/1463788.1463819},
abstract = {Bug tracking systems are valuable assets for managing maintenance activities. They are widely used in open-source projects as well as in the software industry. They collect many different kinds of issues: requests for defect fixing, enhancements, refactoring/restructuring activities and organizational issues. These different kinds of issues are simply labeled as "bug" for lack of a better classification support or of knowledge about the possible kinds.This paper investigates whether the text of the issues posted in bug tracking systems is enough to classify them into corrective maintenance and other kinds of activities.We show that alternating decision trees, naive Bayes classifiers, and logistic regression can be used to accurately distinguish bugs from other kinds of issues. Results from empirical studies performed on issues for Mozilla, Eclipse, and JBoss indicate that issues can be classified with between 77% and 82% of correct decisions.},
booktitle = {Proceedings of the 2008 Conference of the Center for Advanced Studies on Collaborative Research: Meeting of Minds},
articleno = {23},
numpages = {15},
location = {Ontario, Canada},
series = {CASCON '08}
}

@article{10.1007/s10664-019-09695-9,
author = {Ghaleb, Taher Ahmed and Da Costa, Daniel Alencar and Zou, Ying},
title = {An empirical study of the long duration of continuous integration builds},
year = {2019},
issue_date = {August    2019},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-019-09695-9},
doi = {10.1007/s10664-019-09695-9},
abstract = {Continuous Integration (CI) is a set of software development practices that allow software development teams to generate software builds more quickly and periodically (e.g., daily or even hourly). CI brings many advantages, such as the early identification of errors when integrating code. When builds are generated frequently, a long build duration may hold developers from performing other important tasks. Recent research has shown that a considerable amount of development time is invested on optimizing the generation of builds. However, the reasons behind long build durations are still vague and need an in-depth study. Our initial investigation shows that many projects have build durations that far exceed the acceptable build duration (i.e., 10 minutes) as reported by recent studies. In this paper, we study several characteristics of CI builds that may be associated with the long duration of CI builds. We perform an empirical study on 104,442 CI builds from 67 GitHub projects. We use mixed-effects logistic models to model long build durations across projects. Our results reveal that, in addition to common wisdom factors (e.g., project size, team size, build configuration size, and test density), there are other highly important factors to explain long build durations. We observe that rerunning failed commands multiple times is most likely to be associated with long build durations. We also find that builds may run faster if they are configured (a) to cache content that does not change often or (b) to finish as soon as all the required jobs finish. However, we observe that about 40% of the studied projects do not use or misuse such configurations in their builds. In addition, we observe that triggering builds on weekdays or at daytime is most likely to have a direct relationship with long build durations. Our results suggest that developers should use proper CI build configurations to maintain successful builds and to avoid long build durations. Tool builders should supply development teams with tools to identify cacheable spots of the project in order to accelerate the generation of CI builds.},
journal = {Empirical Softw. Engg.},
month = aug,
pages = {2102–2139},
numpages = {38},
keywords = {CI build duration, Continuous integration, Empirical software engineering, Mining software repositories}
}

@article{10.1016/j.jss.2017.05.097,
author = {Zhu, Mengmeng and Pham, Hoang},
title = {Environmental factors analysis and comparison affecting software reliability in development of multi-release software},
year = {2017},
issue_date = {October 2017},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {132},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2017.05.097},
doi = {10.1016/j.jss.2017.05.097},
abstract = {Revisit the environmental factors based on their impact on software reliability for multi-release software by conducting a survey of environmental factors affecting software reliability for the next release's development.Investigate the significant environmental factors in each development phase to deliver valuable information for software management team.Compare the significant environmental factors between the development of multi-release software and the development of single release software. As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors in each development phase and significance level of each development phase between the development of single release software and multi-release software are also discussed.},
journal = {J. Syst. Softw.},
month = oct,
pages = {72–84},
numpages = {13},
keywords = {Environmental factors, Lasso regression, Multi-release software, Multiple linear regression, Principle component analysis, Stepwise backward elimination}
}

@inproceedings{10.1145/3372224.3380897,
author = {Li, Mingliang and Lin, Hao and Liu, Cai and Li, Zhenhua and Qian, Feng and Liu, Yunhao and Sun, Nian and Xu, Tianyin},
title = {Experience: aging or glitching? why does android stop responding and what can we do about it?},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3380897},
doi = {10.1145/3372224.3380897},
abstract = {Almost every Android user has unsatisfying experiences regarding responsiveness, in particular Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. Unfortunately, the community have limited understanding of the prevalence, characteristics, and root causes of unresponsiveness. In this paper, we make an in-depth study of ANR and SNR at scale based on fine-grained system-level traces crowdsourced from 30,000 Android systems. We find that ANR and SNR occur prevalently on all the studied 15 hardware models, and better hardware does not seem to relieve the problem. Moreover, as Android evolves from version 7.0 to 9.0, there are fewer ANR events but more SNR events. Most importantly, we uncover multifold root causes of ANR and SNR and pinpoint the largest inefficiency which roots in Android's flawed implementation of Write Amplification Mitigation (WAM). We design a practical approach to eliminating this largest root cause; after large-scale deployment, it reduces almost all (&gt;99%) ANR and SNR caused by WAM while only decreasing 3% of the data write speed. In addition, we document important lessons we have learned from this study, and have also released our measurement code/data to the research community.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {20},
numpages = {11},
keywords = {Android, application not responding (ANR), responsiveness, system not responding (SNR), write amplification mitigation (WAM)},
location = {London, United Kingdom},
series = {MobiCom '20}
}

@inproceedings{10.5555/1885930.1885939,
author = {Eichinger, Frank and Pankratius, Victor and Gro\ss{}e, Philipp W. L. and B\"{o}hm, Klemens},
title = {Localizing defects in multithreaded programs by mining dynamic call graphs},
year = {2010},
isbn = {3642155847},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {Writing multithreaded software for multicore computers confronts many developers with the difficulty of finding parallel programming errors. In the past, most parallel debugging techniques have concentrated on finding race conditions due to wrong usage of synchronization constructs. A widely unexplored issue, however, is that a wrong usage of non-parallel programming constructs may also cause wrong parallel application behavior. This paper presents a novel defect-localization technique for multithreaded shared-memory programs that is based on analyzing execution anomalies. Compared to race detectors that report just on wrong synchronization, this method can detect a wider range of defects affecting parallel execution. It works on a condensed representation of the call graphs of multithreaded applications and employs data-mining techniques to locate a method containing a defect. Our results from controlled application experiments show that we found race conditions, but also other programming errors leading to incorrect parallel program behavior. On average, our approach reduced in our benchmark the amount of code to be inspected to just 7.1% of all methods.},
booktitle = {Proceedings of the 5th International Academic and Industrial Conference on Testing - Practice and Research Techniques},
pages = {56–71},
numpages = {16},
location = {Windsor, UK},
series = {TAIC PART'10}
}

@article{10.1145/3130944,
author = {Lu, Xuan and Chen, Zhenpeng and Liu, Xuanzhe and Li, Huoran and Xie, Tao and Mei, Qiaozhu},
title = {PRADO: Predicting App Adoption by Learning the Correlation between Developer-Controllable Properties and User Behaviors},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130944},
doi = {10.1145/3130944},
abstract = {To survive and stand out from the fierce market competition nowadays, it is critical for app developers to know (desirably ahead of time) whether, how well, and why their apps would be adopted by users. Ideally, the adoption of an app could be predicted by factors that can be controlled by app developers in the development process, and factors that app developers are able to take actions on and improve according to the predictions. To this end, this paper proposes PRADO, an approach to measuring various aspects of user adoption, including app download and installation, uninstallation, and user ratings. PRADO employs advanced machine learning algorithms to predict user adoption based on how these metrics correlate to a comprehensive taxonomy of 108 developer-controllable features of the app. To evaluate PRADO, we use 9,824 free apps along with their behavioral data from 12.57 million Android users, demonstrating that user adoption of a new app can be accurately predicted. We also derive insights on which factors are statistically significant to user adoption, and suggest what kinds of actions can be possibly performed by developers in practice.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {79},
numpages = {30},
keywords = {Mobile apps, usage data, user adoption}
}

@article{10.1007/s10664-011-9159-7,
author = {Revelle, Meghan and Gethers, Malcom and Poshyvanyk, Denys},
title = {Using structural and textual information to capture feature coupling in object-oriented software},
year = {2011},
issue_date = {December  2011},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {16},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-011-9159-7},
doi = {10.1007/s10664-011-9159-7},
abstract = {Previous studies have demonstrated the relationship between coupling and external software quality attributes, such as fault-proneness, and the application of coupling to software maintenance tasks, such as impact analysis. These previous studies concentrate on class coupling. However, there is a growing focus on the study of features in software, and features are often implemented across multiple classes, meaning class-level coupling measures are not applicable. We ask the pertinent question, "Is measuring coupling at the feature-level also useful?" We define new feature coupling metrics based on structural and textual source code information and extend the unified framework for coupling measurement to include these new metrics. We also conduct three extensive case studies to evaluate these new metrics and answer this research question. The first study examines the relationship between feature coupling and fault-proneness, the second assesses feature coupling in the context of impact analysis, and the third study surveys developers to determine if the metrics align with what they consider to be coupled features. All three studies provide evidence that feature coupling metrics are indeed useful new measures that capture coupling at a higher level of abstraction than classes and can be useful for finding bugs, guiding testing effort, and assessing change impact.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {773–811},
numpages = {39},
keywords = {Fault-proneness, Feature coupling, Information retrieval, Latent Semantic Indexing, Open source software, Program comprehension}
}

@inproceedings{10.5555/978-3-030-87013-3_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-87012-6},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part IX},
pages = {i–xxxix},
location = {Cagliari, Italy}
}

@article{10.1007/s10664-016-9496-7,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Predicting the delay of issues with due dates in software projects},
year = {2017},
issue_date = {June      2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {3},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9496-7},
doi = {10.1007/s10664-016-9496-7},
abstract = {Issue-tracking systems (e.g. JIRA) have increasingly been used in many software projects. An issue could represent a software bug, a new requirement or a user story, or even a project task. A deadline can be imposed on an issue by either explicitly assigning a due date to it, or implicitly assigning it to a release and having it inherit the release's deadline. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether an issue is at risk of being delayed against its deadline. A set of features (hereafter called risk factors) characterizing delayed issues were extracted from eight open source projects: Apache, Duraspace, Java.net, JBoss, JIRA, Moodle, Mulesoft, and WSO2. Risk factors with good discriminative power were selected to build predictive models to predict if the resolution of an issue will be at risk of being delayed. Our predictive models are able to predict both the the extend of the delay and the likelihood of the delay occurrence. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 79 % precision, 61 % recall, 68 % F-measure, and 83 % Area Under the ROC Curve. Our predictive models also have low error rates: on average 0.66 for Macro-averaged Mean Cost-Error and 0.72 Macro-averaged Mean Absolute Error.},
journal = {Empirical Softw. Engg.},
month = jun,
pages = {1223–1263},
numpages = {41},
keywords = {Empirical software engineering, Mining software engineering repositories, Project management}
}

@book{10.5555/2692450,
author = {Mistrik, Ivan and Bahsoon, Rami and Eeles, Peter and Roshandel, Roshanak and Stal, Michael},
title = {Relating System Quality and Software Architecture},
year = {2014},
isbn = {0124170099},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st},
abstract = {System Quality and Software Architecture collects state-of-the-art knowledge on how to intertwine software quality requirements with software architecture and how quality attributes are exhibited by the architecture of the system. Contributions from leading researchers and industry evangelists detail the techniques required to achieve quality management in software architecting, and the best way to apply these techniques effectively in various application domains (especially in cloud, mobile and ultra-large-scale/internet-scale architecture) Taken together, these approaches show how to assess the value of total quality management in a software development process, with an emphasis on architecture. The book explains how to improve system quality with focus on attributes such as usability, maintainability, flexibility, reliability, reusability, agility, interoperability, performance, and more. It discusses the importance of clear requirements, describes patterns and tradeoffs that can influence quality, and metrics for quality assessment and overall system analysis. The last section of the book leverages practical experience and evidence to look ahead at the challenges faced by organizations in capturing and realizing quality requirements, and explores the basis of future work in this area.Explains how design decisions and method selection influence overall system quality, and lessons learned from theories and frameworks on architectural qualityShows how to align enterprise, system, and software architecture for total qualityIncludes case studies, experiments, empirical validation, and systematic comparisons with other approaches already in practice.}
}

@inproceedings{10.1145/1273463.1273493,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Automating algorithms for the identification of fault-prone files},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273493},
doi = {10.1145/1273463.1273493},
abstract = {This research investigates ways of predicting which files would be most likely to contain large numbers of faults in the next release of a large industrial software system. Previous work involved making predictions using several different models ranging from a simple, fully-automatable model (the LOC model) to several different variants of a negative binomial regression model that were customized for the particular software system under study. Not surprisingly, the custom models invariably predicted faults more accurately than the simple model. However, development of customized models requires substantial time and analytic effort, as well as statistical expertise. We now introduce new, more sophisticated models that yield more accurate predictions than the earlier LOC model, but which nonetheless can be fully automated. We also extend our earlier research by presenting another large-scale empirical study of the value of these prediction models, using a new industrial software system over a nine year period.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {219–227},
numpages = {9},
keywords = {empirical study, fault-prone, prediction, regression model, software faults, software testing},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@article{10.1007/s00766-018-0293-2,
author = {Kurtanovi\'{c}, Zijad and Maalej, Walid},
title = {On user rationale in software engineering},
year = {2018},
issue_date = {Sep 2018},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {23},
number = {3},
issn = {0947-3602},
url = {https://doi.org/10.1007/s00766-018-0293-2},
doi = {10.1007/s00766-018-0293-2},
abstract = {Rationale refers to the reasoning and justification behind human decisions, opinions, and beliefs. In software engineering, rationale management focuses on capturing design and requirements decisions and on organizing and reusing project knowledge. This paper takes a different view on rationale written by users in online reviews. We studied 32,414 reviews for 52 software applications in the Amazon Store. Through a grounded theory approach and peer content analysis, we investigated how users argue and justify their decisions, e.g., about upgrading, installing, or switching software applications. We also studied the occurrence frequency of rationale concepts such as issues encountered or alternatives considered in the reviews and found that assessment criteria like performance, compatibility, and usability represent the most pervasive concept. We identified a moderate positive correlation between issues and criteria and furthermore assessed the distribution of rationale concepts with respect to rating and verbosity. We found that issues tend to appear more in lower star rated reviews, while criteria, alternatives, and justifications seem to appear more in three star rated reviews. Also, reviews reporting alternatives seem to be more verbose than reviews reporting criteria. A follow-up qualitative study of sub-concepts revealed, that users also report other alternatives (e.g., alternative software provider), criteria (e.g., cost), and decisions (e.g., on rating software). We then used the truth set of manually labeled review sentences to explore how accurately we can mine rationale concepts from the reviews. We evaluated the classification algorithms Naive Bayes, Support Vector Machine, Logistic Regression, Decision Tree, Gaussian Process, Random Forest, and Multilayer Perceptron Classifier using a baseline and random configuration. Support Vector Classifier, Naive Bayes, and Logistic Regression, trained on the review metadata, syntax tree of the review text, and influential terms, achieved a precision around 80% for predicting sentences with alternatives and decisions, with top recall values of 98%. On the review level, precision was up to 13% higher with recall values reaching 99%. Using only word features, we achieved in most cases the highest precision and highest recall respectively using the Random Forest and Naive Bayes algorithm. We discuss the findings and the rationale importance for supporting deliberation in user communities and synthesizing the reviews for developers.},
journal = {Requir. Eng.},
month = sep,
pages = {357–379},
numpages = {23},
keywords = {App analytics, Rationale, Review mining}
}

@article{10.1016/j.jss.2006.09.009,
author = {Chang, Ching-Pao and Chu, Chih-Ping},
title = {Defect prevention in software processes},
year = {2007},
issue_date = {April, 2007},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {80},
number = {4},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2006.09.009},
doi = {10.1016/j.jss.2006.09.009},
abstract = {In addition to degrading the quality of software products, software defects also require additional efforts in rewriting software and jeopardize the success of software projects. Software defects should be prevented to reduce the variance of projects and increase the stability of the software process. Factors causing defects vary according to the different attributes of a project, including the experience of the developers, the product complexity, the development tools and the schedule. The most significant challenge for a project manager is to identify actions that may incur defects before the action is performed. Actions performed in different projects may yield different results, which are hard to predict in advance. To alleviate this problem, this study proposes an Action-Based Defect Prevention (ABDP) approach, which applies the classification and Feature Subset Selection (FSS) technologies to project data during execution.Accurately predicting actions that cause many defects by mining records of performed actions is a challenging task due to the rarity of such actions. To address this problem, the under-sampling is applied to the data set to increase the precision of predictions for subsequence actions. To demonstrate the efficiency of this approach, it is applied to a business project, revealing that under-sampling with FSS successfully predicts the problematic actions during project execution. The main advantage utilizing ABDP is that the actions likely to produce defects can be predicted prior to their execution. The detected actions not only provide the information to avoid possible defects, but also facilitate the software process improvement.},
journal = {J. Syst. Softw.},
month = apr,
pages = {559–570},
numpages = {12},
keywords = {Classification, Mining rarity, Software process improvement, Software repositories}
}

@article{10.1016/S0167-4048(97)00005-9,
author = {Krsul, Ivan and Spafford, Eugene H.},
title = {Refereed paper: Authorship analysis: identifying the author of a program},
year = {1997},
issue_date = {January, 1997},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {16},
number = {3},
issn = {0167-4048},
url = {https://doi.org/10.1016/S0167-4048(97)00005-9},
doi = {10.1016/S0167-4048(97)00005-9},
abstract = {Authorship analysis on computer software is a difficult problem. In this paper we explore the classification of programmers' style, and try to find a set of characteristics that remain constant for a significant portion of the programs that this programmer might produce. Our goal is to show that it is possible to identify the author of a program by examining programming style characteristics. Within a closed environment, the results of this paper support the conclusion that, for a specific set of programmers, it is possible to identify the author of any individual program. Also, based on previous work and our observations during the experiments described herein, we believe that the probability of finding two programmers who share exactly those same characteristics should be very small.},
journal = {Comput. Secur.},
month = jan,
pages = {233–257},
numpages = {25}
}

@inproceedings{10.5555/978-3-319-49586-6_fm,
title = {Front Matter},
year = {2016},
isbn = {978-3-319-49585-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Advanced Data Mining and Applications: 12th International Conference, ADMA 2016, Gold Coast, QLD, Australia, December 12-15, 2016, Proceedings},
pages = {I–XVI},
location = {Gold Coast, Australia}
}

@book{10.5555/2930830,
author = {Menzies, Tim and Kocaguneli, Ekrem and Turhan, Burak and Minku, Leandro and Peters, Fayola},
title = {Sharing Data and Models in Software Engineering},
year = {2014},
isbn = {9780124173071},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Data Science for Software Engineering: Sharing Data and Models presents guidance and procedures for reusing data and models between projects to produce results that are useful and relevant. Starting with a background section of practical lessons and warnings for beginner data scientists for software engineering, this edited volume proceeds to identify critical questions of contemporary software engineering related to data and models. Learn how to adapt data from other organizations to local problems, mine privatized data, prune spurious information, simplify complex results, how to update models for new platforms, and more. Chapters share largely applicable experimental results discussed with the blend of practitioner focused domain expertise, with commentary that highlights the methods that are most useful, and applicable to the widest range of projects. Each chapter is written by a prominent expert and offers a state-of-the-art solution to an identified problem facing data scientists in software engineering. Throughout, the editors share best practices collected from their experience training software engineering students and practitioners to master data science, and highlight the methods that are most useful, and applicable to the widest range of projects. Shares the specific experience of leading researchers and techniques developed to handle data problems in the realm of software engineering Explains how to start a project of data science for software engineering as well as how to identify and avoid likely pitfalls Provides a wide range of useful qualitative and quantitative principles ranging from very simple to cutting edge research Addresses current challenges with software engineering data such as lack of local data, access issues due to data privacy, increasing data quality via cleaning of spurious chunks in data Table of Contents Introduction Data Science 101 Cross company data: Friend or Foe Pruning: Relevancy is the Removal of Irrelevancy Easy Path: Smarter Design Instance Weighting: How not to elaborate on analogies Privacy: Data in Disguise Stability: How to find a silver-bullet model Complexity: How to ensemble multiple models}
}

@article{10.1145/1921532.1921547,
author = {Sharma, Ashish and Kushwaha, Dharmender Singh},
title = {Natural language based component extraction from requirement engineering document and its complexity analysis},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1921532.1921547},
doi = {10.1145/1921532.1921547},
abstract = {Requirement engineering document (IEEE-830: 1998) plays a very significant role in software development. The size and complexity of software systems are continuously increasing. As scale changes to more complex and larger systems, new problems occur that did not exist in smaller systems This leads to redefining priorities of the activities that go into developing software. As systems ges complex, it becomes evident that the goals of the entire system can't be easily comprehended. Hence the need of more rigorous requirement analysis arises. The requirement analyst has to identify the requirements by using various methods like interviews, brainstorming, FAST (facilitated application specification techniques), quality function deployment, use-case etc. To overcome these issues, this paper proposes object based semi-automated system that categorize the requirements and further identifies the component based on requirement engineering document in a component library and further analyses the complexity of components and its usage.},
journal = {SIGSOFT Softw. Eng. Notes},
month = jan,
pages = {1–14},
numpages = {14},
keywords = {input output complexity, interface complexity, personal complexity attributes, product complexity, requirement based complexity, requirement complexity, user location complexity}
}

@inproceedings{10.1145/2491956.2462173,
author = {Chen, Yang and Groce, Alex and Zhang, Chaoqiang and Wong, Weng-Keen and Fern, Xiaoli and Eide, Eric and Regehr, John},
title = {Taming compiler fuzzers},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462173},
doi = {10.1145/2491956.2462173},
abstract = {Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {197–208},
numpages = {12},
keywords = {automated testing, bug reporting, compiler defect, compiler testing, fuzz testing, random testing, test-case reduction},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@inproceedings{10.1145/1996130.1996143,
author = {Zhou, Bowen and Kulkarni, Milind and Bagchi, Saurabh},
title = {Vrisha: using scaling properties of parallel programs for bug detection and localization},
year = {2011},
isbn = {9781450305525},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1996130.1996143},
doi = {10.1145/1996130.1996143},
abstract = {Detecting and isolating bugs that arise in parallel programs is a tedious and a challenging task. An especially subtle class of bugs are those that are scale-dependent: while small-scale test cases may not exhibit the bug, the bug arises in large-scale production runs, and can change the result or performance of an application. A popular approach to finding bugs is statistical bug detection, where abnormal behavior is detected through comparison with bug-free behavior. Unfortunately, for scale-dependent bugs, there may not be bug-free runs at large scales and therefore traditional statistical techniques are not viable. In this paper, we propose Vrisha, a statistical approach to detecting and localizing scale-dependent bugs. Vrisha detects bugs in large-scale programs by building models of behavior based on bug-free behavior at small scales. These models are constructed using kernel canonical correlation analysis (KCCA) and exploit scale-determined properties, whose values are predictably dependent on application scale. We use Vrisha to detect and diagnose two bugs caused by errors in popular MPI libraries and show that our techniques can be implemented with low overhead and low false-positive rates.},
booktitle = {Proceedings of the 20th International Symposium on High Performance Distributed Computing},
pages = {85–96},
numpages = {12},
keywords = {KCCA, bug detection, large-scale bugs},
location = {San Jose, California, USA},
series = {HPDC '11}
}

@inproceedings{10.1145/2664243.2664250,
author = {Jing, Yiming and Zhao, Ziming and Ahn, Gail-Joon and Hu, Hongxin},
title = {Morpheus: automatically generating heuristics to detect Android emulators},
year = {2014},
isbn = {9781450330053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664243.2664250},
doi = {10.1145/2664243.2664250},
abstract = {Emulator-based dynamic analysis has been widely deployed in Android application stores. While it has been proven effective in vetting applications on a large scale, it can be detected and evaded by recent Android malware strains that carry detection heuristics. Using such heuristics, an application can check the presence or contents of certain artifacts and infer the presence of emulators. However, there exists little work that systematically discovers those heuristics that would be eventually helpful to prevent malicious applications from bypassing emulator-based analysis. To cope with this challenge, we propose a framework called Morpheus that automatically generates such heuristics. Morpheus leverages our insight that an effective detection heuristic must exploit discrepancies observable by an application. To this end, Morpheus analyzes the application sandbox and retrieves observable artifacts from both Android emulators and real devices. Afterwards, Morpheus further analyzes the retrieved artifacts to extract and rank detection heuristics. The evaluation of our proof-of-concept implementation of Morpheus reveals more than 10,000 novel detection heuristics that can be utilized to detect existing emulator-based malware analysis tools. We also discuss the discrepancies in Android emulators and potential countermeasures.},
booktitle = {Proceedings of the 30th Annual Computer Security Applications Conference},
pages = {216–225},
numpages = {10},
keywords = {Android, emulator, malware},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '14}
}

@inproceedings{10.1145/2020390.2020394,
author = {Zhang, Wen and Yang, Ye and Wang, Qing},
title = {Handling missing data in software effort prediction with naive Bayes and EM algorithm},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020394},
doi = {10.1145/2020390.2020394},
abstract = {Background: Missing data, which usually appears in software effort datasets, is becoming an important problem in software effort prediction.Aims: In this paper, we adapt na\"{\i}ve Bayes and EM (Expectation Maximization) for software effort prediction, and develop two embedded strategies: missing data toleration and missing data imputation, to handle the missing data in software effort datasets.Method: The missing data toleration strategy ignores missing values in software effort datasets while missing data imputation strategy uses observed values to impute missing values.Results: Experiments on ISBSG and CSBSG datasets demonstrate that: 1)both proposed strategies outperform BPNN with classic imputation techniques as MI and MINI. Meanwhile, the imputation strategy outperforms toleration strategy in most cases and has produced the highest accuracy as 75.15%; 2) the unlabeled projects used in training prediction model has significantly improved the performances of effort prediction of na\"{\i}ve Bayes and EM with both strategies, especially when the size of training data to the size of unlabeled data is at a relatively optimal level; 3) each class of software effort data exactly corresponds to a Gaussian component for both ISBSG and CSBSG datasets.Conclusion: Although initial experiments on ISBSG data set demonstrate some promising aspects of the proposed strategies, we cannot draw that they can be generalized to be applied in all the other software effort datasets.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {4},
numpages = {10},
keywords = {EM algorithm, missing data, naive Bayes},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1007/s10664-016-9452-6,
author = {Thongtanunam, Patanamon and Mcintosh, Shane and Hassan, Ahmed E. and Iida, Hajimu},
title = {Review participation in modern code review},
year = {2017},
issue_date = {April     2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {22},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-016-9452-6},
doi = {10.1007/s10664-016-9452-6},
abstract = {Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. Our prior work shows that review participation plays an important role in MCR practices, since the amount of review participation shares a relationship with software quality. However, little is known about which factors influence review participation in the MCR process. Hence, in this study, we set out to investigate the characteristics of patches that: (1) do not attract reviewers, (2) are not discussed, and (3) receive slow initial feedback. Through a case study of 196,712 reviews spread across the Android, Qt, and OpenStack open source projects, we find that the amount of review participation in the past is a significant indicator of patches that will suffer from poor review participation. Moreover, we find that the description length of a patch shares a relationship with the likelihood of receiving poor reviewer participation or discussion, while the purpose of introducing new features can increase the likelihood of receiving slow initial feedback. Our findings suggest that the patches with these characteristics should be given more attention in order to increase review participation, which will likely lead to a more responsive review process.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {768–817},
numpages = {50},
keywords = {Code review, Developer involvement, Review participation}
}

@article{10.1017/S0269888910000299,
title = {From the journals???},
year = {2011},
issue_date = {February 2011},
publisher = {Cambridge University Press},
address = {USA},
volume = {26},
number = {1},
issn = {0269-8889},
url = {https://doi.org/10.1017/S0269888910000299},
doi = {10.1017/S0269888910000299},
journal = {Knowl. Eng. Rev.},
month = feb,
pages = {73–97},
numpages = {25}
}

@article{10.1016/j.jss.2019.110422,
author = {Edded, Sabrine and Sassi, Sihem Ben and Mazo, Ra\'{u}l and Salinesi, Camille and Ghezala, Henda Ben},
title = {Collaborative configuration approaches in software product lines engineering: A systematic mapping study},
year = {2019},
issue_date = {Dec 2019},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {158},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2019.110422},
doi = {10.1016/j.jss.2019.110422},
journal = {J. Syst. Softw.},
month = dec,
numpages = {17},
keywords = {Product lines, Collaborative configuration, Systematic mapping study, Framework}
}

@article{10.1016/j.infsof.2017.06.002,
title = {Research patterns and trends in software effort estimation},
year = {2017},
issue_date = {November 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {91},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2017.06.002},
doi = {10.1016/j.infsof.2017.06.002},
abstract = {This study identified research trends prevailing in software effort estimation literature.Latent Dirichlet Allocation (LDA) was applied to the corpus of 1178 articles.This study established the semantic mapping between research patterns and trends. ContextSoftware effort estimation(SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. ObjectiveTo identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. MethodA generative statistical method, called Latent Dirichlet Allocation(LDA), applied on a literature dataset of 1178 articles published on SEE. ResultsAs many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. ConclusionsThis study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas.},
journal = {Inf. Softw. Technol.},
month = nov,
pages = {1–21},
numpages = {21}
}

@proceedings{10.1145/2970276,
title = {ASE '16: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/2541940.2541973,
author = {Arulraj, Joy and Jin, Guoliang and Lu, Shan},
title = {Leveraging the short-term memory of hardware to diagnose production-run software failures},
year = {2014},
isbn = {9781450323055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541940.2541973},
doi = {10.1145/2541940.2541973},
abstract = {Failures caused by software bugs are widespread in production runs, causing severe losses for end users. Unfortunately, diagnosing production-run failures is challenging. Existing work cannot satisfy privacy, run-time overhead, diagnosis capability, and diagnosis latency requirements all at once.This paper designs a low overhead, low latency, privacy preserving production-run failure diagnosis system based on two observations. First, short-term memory of program execution is often sufficient for failure diagnosis, as many bugs have short propagation distances. Second, maintaining a short-term memory of execution is much cheaper than maintaining a record of the whole execution. Following these observations, we first identify an existing hardware unit, Last Branch Record (LBR), that records the last few taken branches to help diagnose sequential bugs. We then propose a simple hardware extension, Last Cache-coherence Record (LCR), to record the last few cache accesses with specified coherence states and hence help diagnose concurrency bugs. Finally, we design LBRA and LCRA to automatically locate failure root causes using LBR and LCR.Our evaluation uses 31 real-world sequential and concurrency bug failures from 18 representative open-source software. The results show that with just 16 record entries, LBR and LCR enable our system to automatically locate the root causes for 27 out of 31 failures, with less than 3% run-time overhead. As our system does not rely on sampling,},
booktitle = {Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {207–222},
numpages = {16},
keywords = {concurrency bugs, failure diagnosis, hardware performance monitoring unit, production runs},
location = {Salt Lake City, Utah, USA},
series = {ASPLOS '14}
}

@article{10.1016/j.cose.2021.102328,
author = {Azeez, Nureni Ayofe and Misra, Sanjay and Margaret, Ihotu Agbo and Fernandez-Sanz, Luis and Abdulhamid, Shafi'i Muhammad},
title = {Adopting automated whitelist approach for detecting phishing attacks},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {108},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2021.102328},
doi = {10.1016/j.cose.2021.102328},
journal = {Comput. Secur.},
month = sep,
numpages = {18},
keywords = {Phishing, Blacklist, Whitelist, Cybersecurity, False positive, False negative}
}

@article{10.1007/s10664-021-10021-5,
author = {Uddin, Gias and Sabir, Fatima and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Alam, Omar and Khomh, Foutse},
title = {An empirical study of IoT topics in IoT developer discussions on Stack Overflow},
year = {2021},
issue_date = {Nov 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {26},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-021-10021-5},
doi = {10.1007/s10664-021-10021-5},
abstract = {Internet of Things (IoT) is defined as the connection between places and physical objects (i.e., things) over the Internet via smart computing devices. It is a rapidly emerging paradigm that encompasses almost every aspect of our modern life, such as smart home, cars, and so on. With interest in IoT growing, we observe that the IoT discussions are becoming prevalent in online developer forums, such as Stack Overflow (SO). An understanding of such discussions can offer insights into the prevalence, popularity, and difficulty of various IoT topics. For this paper, we download a large number of SO posts that contain discussions about various IoT technologies. We apply topic modeling on the textual contents of the posts. We label the topics and categorize the topics into hierarchies. We analyze the popularity and difficulty of the topics. Our study offers several findings. First, IoT developers discuss a range of topics in SO related to Hardware, Software, Network, and Tutorials. Second, secure messaging using IoT devices from the Network category is the most prevalent topic, followed by scheduling of IoT script in the Software category. Third, all the topic categories are evolving rapidly in SO, i.e., new questions are being added more and more in SO about IoT tools and techniques. Fourth, the “How” type of questions are asked more across the three topic categories (Software, Network, and Hardware), although a large number of questions are also of the “What” type: IoT developers are using SO not only to discuss how to address a problem related to IoT, but also to learn what the different IoT techniques and tools offer. Fifth, topics related to data parsing and micro-controller configuration are the most popular. Sixth, topics related to multimedia streaming and Bluetooth are the most difficult. Our study findings have implications for all four different IoT stakeholders: tool builders, developers, educators, and researchers. For example, IoT developers and newcomers can use our findings on topic popularity to learn about popular IoT techniques. Educators and researchers can make more tutorials or develop new techniques to make difficult IoT topics easier. IoT tool builders can look at our identified topics and categories to learn about IoT developers’ preferences, which then can help them develop new tools or enhance their current offerings.},
journal = {Empirical Softw. Engg.},
month = nov,
numpages = {45},
keywords = {Stack overflow, IoT, Topic modeling}
}

@article{10.1007/s10664-017-9511-7,
author = {Munir, Hussan and Lin\r{a}ker, Johan and Wnuk, Krzysztof and Runeson, Per and Regnell, Bj\"{o}rn},
title = {Open innovation using open source tools: a case study at Sony Mobile},
year = {2018},
issue_date = {February  2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9511-7},
doi = {10.1007/s10664-017-9511-7},
abstract = {Despite growing interest of Open Innovation (OI) in Software Engineering (SE), little is known about what triggers software organizations to adopt it and how this affects SE practices. OI can be realized in numerous of ways, including Open Source Software (OSS) involvement. Outcomes from OI are not restricted to product innovation but also include process innovation, e.g. improved SE practices and methods. This study explores the involvement of a software organization (Sony Mobile) in OSS communities from an OI perspective and what SE practices (requirements engineering and testing) have been adapted in relation to OI. It also highlights the innovative outcomes resulting from OI. An exploratory embedded case study investigates how Sony Mobile use and contribute to Jenkins and Gerrit; the two central OSS tools in their continuous integration tool chain. Quantitative analysis was performed on change log data from source code repositories in order to identify the top contributors and triangulated with the results from five semi-structured interviews to explore the nature of the commits. The findings of the case study include five major themes: i) The process of opening up towards the tool communities correlates in time with a general adoption of OSS in the organization. ii) Assets not seen as competitive advantage nor a source of revenue are made open to OSS communities, and gradually, the organization turns more open. iii) The requirements engineering process towards the community is informal and based on engagement. iv) The need for systematic and automated testing is still in its infancy, but the needs are identified. v) The innovation outcomes included free features and maintenance, and were believed to increase speed and quality in development. Adopting OI was a result of a paradigm shift of moving from Windows to Linux. This shift enabled Sony Mobile to utilize the Jenkins and Gerrit communities to make their internal development process better for its software developers and testers.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {186–223},
numpages = {38},
keywords = {Case study, Gerrit, Jenkins, OSS communities, Open innovation, Open source}
}

@inproceedings{10.1007/978-3-030-29983-5_14,
author = {Busch, Axel and Fuch\ss{}, Dominik and Eckert, Maximilian and Koziolek, Anne},
title = {Assessing the Quality Impact of Features in Component-Based Software Architectures},
year = {2019},
isbn = {978-3-030-29982-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-29983-5_14},
doi = {10.1007/978-3-030-29983-5_14},
abstract = {In modern software development processes, existing software components are increasingly used to implement functionality instead of developing it from scratch. Reuse of individual components or even more complex subsystems leads to more cost-efficient development and higher quality of software. Subsystems often offer a variety of features whose use is associated with unclear effects on the quality attributes of the software architecture, such as performance. It is unclear, whether the quality requirements for the system can be met by using a certain feature of a particular subsystem. After initial selection, features must be incorporated in the target architecture. Due to a multitude of possibilities of placing the subsystem in the target system to be used, many architectural candidates may result which have to be evaluated in existing decision support solutions. The approach presented here enables software architects to automatically evaluate with the help of software architecture models the effects on quality of using individual features in an existing software architecture. The result helps to automatically evaluate design decisions regarding features and to decide whether their use is compatible with the quality requirements. We show the benefits of our approach using different decision scenarios driven by features and their placement alternatives. All scenarios are automatically evaluated, demonstrating how decisions can be made to best meet the requirements.},
booktitle = {Software Architecture: 13th European Conference, ECSA 2019, Paris, France, September 9–13, 2019, Proceedings},
pages = {211–219},
numpages = {9},
keywords = {Automated design decision optimization, Quality impact of features, CBSE},
location = {Paris, France}
}

@article{10.1504/IJISTA.2017.088056,
title = {A fuzzy-based approach for bug report categorisation},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {16},
number = {4},
issn = {1740-8865},
url = {https://doi.org/10.1504/IJISTA.2017.088056},
doi = {10.1504/IJISTA.2017.088056},
abstract = {Various studies conducted on bug repositories utilise issue reports labelled as 'bug'. Research conducted on a number of bug repositories have shown that not all issue reports labelled as 'bug' are actually bugs but can also be a request for additional feature, improvement or documentation. This not only threatens the validity of studies that have used mislabelled data but may also give wrong prediction results in future. This has necessitated need for correct labelling of issue reports. The proposed work using fuzzy logic classifier suggests improvement and also reduces the complexity. Validation of this work is done using five open source projects. Experimental results have shown that our approach gives better F-measure scores. The study also elaborated on use of issue reports from other similar projects for training a model; the impact of frequent terms from the training data and applicability of our approach to fine grained categorisation of issues.},
journal = {Int. J. Intell. Syst. Technol. Appl.},
month = jan,
pages = {319–341},
numpages = {23}
}

@article{10.1016/j.knosys.2021.107190,
author = {Wang, Xilu and Jin, Yaochu and Schmitt, Sebastian and Olhofer, Markus and Allmendinger, Richard},
title = {Transfer learning based surrogate assisted evolutionary bi-objective optimization for objectives with different evaluation times},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {227},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2021.107190},
doi = {10.1016/j.knosys.2021.107190},
journal = {Know.-Based Syst.},
month = sep,
numpages = {13},
keywords = {Bi-objective optimization, Different evaluation times, Transfer learning, Domain adaptation, Co-training, Surrogate-assisted evolutionary algorithm, Bayesian optimizations}
}

@inproceedings{10.1145/1453101.1453106,
author = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
title = {Predicting failures with developer networks and social network analysis},
year = {2008},
isbn = {9781595939951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1453101.1453106},
doi = {10.1145/1453101.1453106},
abstract = {Software fails and fixing it is expensive. Research in failure prediction has been highly successful at modeling software failures. Few models, however, consider the key cause of failures in software: people. Understanding the structure of developer collaboration could explain a lot about the reliability of the final product. We examine this collaboration structure with the developer network derived from code churn information that can predict failures at the file level. We conducted a case study involving a mature Nortel networking product of over three million lines of code. Failure prediction models were developed using test and post-release failure data from two releases, then validated against a subsequent release. One model's prioritization revealed 58% of the failures in 20% of the files compared with the optimal prioritization that would have found 61% in 20% of the files, indicating that a significant correlation exists between file-based developer network metrics and failures.},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {13–23},
numpages = {11},
keywords = {developer network, failure prediction, logistic regression, negative binomial regression, social network analysis},
location = {Atlanta, Georgia},
series = {SIGSOFT '08/FSE-16}
}

@article{10.1016/j.jss.2015.12.019,
author = {Yan, Meng and Fu, Ying and Zhang, Xiaohong and Yang, Dan and Xu, Ling and Kymer, Jeffrey D.},
title = {Automatically classifying software changes via discriminative topic model},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {113},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.12.019},
doi = {10.1016/j.jss.2015.12.019},
abstract = {The discovered topics have a one-to-one correspondence with category labels.The method performs both single-category and multi-category change classification.The method overcomes the ambiguity coming from manually assigning weights.The method is applicable to cross-project analysis without the need of re-learning. Accurate classification of software changes as corrective, adaptive and perfective can enhance software decision making activities. However, a major challenge which remains is how to automatically classify multi-category changes. This paper presents a discriminative Probability Latent Semantic Analysis (DPLSA) model with a novel initialization method which initializes the word distributions for different topics using labeled samples. This method creates a one-to-one correspondence between the discovered topics and the change categories. As a result, the discriminative semantic representation of the software change messages whose largest topic entry directly corresponds to the category label of the change message which is directly used to perform single-category and multi-category change classification. In the evaluation on five open source projects, the experimental results show that the proposed approach achieves a more accurate performance than the four baseline methods. Especially with the multi-category classification task which improves the recall rate. Moreover, the different projects share the same vocabulary and the estimated model so that DPLSA is well applicable to cross-project software change message analysis.},
journal = {J. Syst. Softw.},
month = mar,
pages = {296–308},
numpages = {13},
keywords = {Discriminative topic model, Multi-category change, Software change classification}
}

@article{10.1007/s10270-013-0387-8,
author = {Kahraman, G\"{o}khan and Bilgen, Semih},
title = {A framework for qualitative assessment of domain-specific languages},
year = {2015},
issue_date = {October   2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {4},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-013-0387-8},
doi = {10.1007/s10270-013-0387-8},
abstract = {Domain-specific languages (DSLs) are used for improving many facets of software development, but whether and to what extent this aim is achieved is an important issue that must be addressed. This paper presents a proposal for a Framework for Qualitative Assessment of DSLs (FQAD). FQAD is used for determining the perspective of the evaluator, understanding the goal of the assessment and selecting fundamental DSL quality characteristics to guide the evaluator in the process. This framework adapts and integrates the ISO/IEC 25010:2011 standard, CMMI maturity level evaluation approach and the scaling approach used in DESMET into a perspective-based assessment. A detailed list of domain-specific language quality characteristics is elaborated, and a novel assessment method is proposed. Two case studies through which FQAD is matured and evaluated are reported. The case studies have shown that stakeholders find the FQAD process beneficial.},
journal = {Softw. Syst. Model.},
month = oct,
pages = {1505–1526},
numpages = {22},
keywords = {CMMI, Domain-specific languages, ISO/IEC 25010, Qualitative assessment, Quality measures}
}

@article{10.1007/s10664-007-9038-4,
author = {Genero, Marcela and Manso, Esperanza and Visaggio, Aaron and Canfora, Gerardo and Piattini, Mario},
title = {Building measure-based prediction models for UML class diagram maintainability},
year = {2007},
issue_date = {October   2007},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {12},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-007-9038-4},
doi = {10.1007/s10664-007-9038-4},
abstract = {The usefulness of measures for the analysis and design of object oriented (OO) software is increasingly being recognized in the field of software engineering research. In particular, recognition of the need for early indicators of external quality attributes is increasing. We investigate through experimentation whether a collection of UML class diagram measures could be good predictors of two main subcharacteristics of the maintainability of class diagrams: understandability and modifiability. Results obtained from a controlled experiment and a replica support the idea that useful prediction models for class diagrams understandability and modifiability can be built on the basis of early measures, in particular, measures that capture structural complexity through associations and generalizations. Moreover, these measures seem to be correlated with the subjective perception of the subjects about the complexity of the diagrams. This fact shows, to some extent, that the objective measures capture the same aspects as the subjective ones. However, despite our encouraging findings, further empirical studies, especially using data taken from real projects performed in industrial settings, are needed. Such further study will yield a comprehensive body of knowledge and experience about building prediction models for understandability and modifiability.},
journal = {Empirical Softw. Engg.},
month = oct,
pages = {517–549},
numpages = {33},
keywords = {Class diagrams, Controlled experiments, Empirical validation, Maintainability, Measures, Modifiability, Prediction model, Size, Structural complexity, UML, Understandability}
}

@article{10.1145/3355048,
author = {Tian, Cong and Chen, Chu and Duan, Zhenhua and Zhao, Liang},
title = {Differential Testing of Certificate Validation in SSL/TLS Implementations: An RFC-guided Approach},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3355048},
doi = {10.1145/3355048},
abstract = {Certificate validation in Secure Sockets Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS implementations is correctly implemented. With this motivation, we propose a novel differential testing approach that is based on the standard Request for Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e., certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted, since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations. In addition, by providing seed certificates for mutation approaches with RFCcert, the ability of mutation approaches in finding distinct discrepancies is significantly enhanced.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = oct,
articleno = {24},
numpages = {37},
keywords = {Differential testing, SSL/TLS, certificate validation, dynamic symbolic execution, request for comments}
}

@inproceedings{10.1109/ASE.2009.93,
author = {II, Phillip Green and Menzies, Tim and Williams, Steven and El-Rawas, Oussama},
title = {Understanding the Value of Software Engineering Technologies},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.93},
doi = {10.1109/ASE.2009.93},
abstract = {When AI search methods are applied to software process models, then appropriate technologies can be discovered for a software project. We show that those recommendations are greatly affected by the business context of its use. For example, the automatic defect reduction tools explored by the ASE community are only relevant to a subset of software projects, and only according to certain value criteria. Therefore, when arguing for the value of a particular technology, that argument should include a description of the value function of the target user community.},
booktitle = {Proceedings of the 24th IEEE/ACM International Conference on Automated Software Engineering},
pages = {52–61},
numpages = {10},
keywords = {artificial intelligence, software economics},
series = {ASE '09}
}

@inproceedings{10.5555/3340730.3340743,
author = {Hong, David Ke and Nikravesh, Ashkan and Mao, Z. Morley and Ketkar, Mahesh and Kishinevsky, Michael},
title = {PerfProbe: a systematic, cross-layer performance diagnosis framework for mobile platforms},
year = {2019},
publisher = {IEEE Press},
abstract = {User-perceived performance slowdown in mobile apps can occur in unpredictable and sophisticated ways, with root cause spanning at different layers (app or system layer). There is a lack of effective approaches to provide cross-layer, holistic insights to diagnose unpredictable performance slowdown on mobile platforms, motivating us to develop PerfProbe as a performance diagnosis framework for mobile platforms. PerfProbe monitors app performance and records app and system-layer runtime information in a lightweight manner on mobile devices, and performs systematic, novel statistical analysis on collected runtime traces at different layers to localize code-level performance variance in the form of critical functions and zoom into them to pinpoint system-level root causes in the form of relevant resource factors to explain the performance slowdown. PerfProbe effectively diagnoses performance slowdown due to various root causes in 22 popular Android apps from real-world usage monitoring and in-lab testing, by providing holistic, cross-layer insights to help the root cause diagnosis. Diagnosis findings from PerfProbe provide actionable insights for root cause finding and guiding real-world app developers' code fixing or adjustment of platform-level policies to reduce user-perceived latency of 6 real Android apps by 32--86%. PerfProbe incurs small system overhead and impact to app performance at runtime and is suitable for real-world deployment.},
booktitle = {Proceedings of the 6th International Conference on Mobile Software Engineering and Systems},
pages = {50–61},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MOBILESoft '19}
}

@article{10.1145/3392031,
author = {Xue, Yinxing and Li, Yan-Fu},
title = {Multi-objective Integer Programming Approaches for Solving the Multi-criteria Test-suite Minimization Problem: Towards Sound and Complete Solutions of a Particular Search-based Software-engineering Problem},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3392031},
doi = {10.1145/3392031},
abstract = {Test-suite minimization is one key technique for optimizing the software testing process. Due to the need to balance multiple factors, multi-criteria test-suite minimization (MCTSM) becomes a popular research topic in the recent decade. The MCTSM problem is typically modeled as integer linear programming (ILP) problem and solved with weighted-sum single objective approach. However, there is no existing approach that can generate sound (i.e., being Pareto-optimal) and complete (i.e., covering the entire Pareto front) Pareto-optimal solution set, to the knowledge of the authors. In this work, we first prove that the ILP formulation can accurately model the MCTSM problem and then propose the multi-objective integer programming (MOIP) approaches to solve it. We apply our MOIP approaches on three specific MCTSM problems and compare the results with those of the cutting-edge methods, namely, NonlinearFormulation_LinearSolver (NF_LS) and two Multi-Objective Evolutionary Algorithms (MOEAs). The results show that our MOIP approaches can always find sound and complete solutions on five subject programs, using similar or significantly less time than NF_LS and two MOEAs do. The current experimental results are quite promising, and our approaches have the potential to be applied for other similar search-based software engineering problems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
articleno = {20},
numpages = {50},
keywords = {ε-constraint method, CWMOIP, Regression testing, big-M method, multi-objective integer programming, search-based software engineering, test-suite minimization}
}

@article{10.1007/s10799-009-0062-5,
author = {Raja, Uzma and Tretter, Marietta J.},
title = {Antecedents of open source software defects: A data mining approach to model formulation, validation and testing},
year = {2009},
issue_date = {December  2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {10},
number = {4},
issn = {1385-951X},
url = {https://doi.org/10.1007/s10799-009-0062-5},
doi = {10.1007/s10799-009-0062-5},
abstract = {This paper develops tests and validates a model for the antecedents of open source software (OSS) defects, using Data and Text Mining. The public archives of OSS projects are used to access historical data on over 5,000 active and mature OSS projects. Using domain knowledge and exploratory analysis, a wide range of variables is identified from the process, product, resource, and end-user characteristics of a project to ensure that the model is robust and considers all aspects of the system. Multiple Data Mining techniques are used to refine the model and data is enriched by the use of Text Mining for knowledge discovery from qualitative information. The study demonstrates the suitability of Data Mining and Text Mining for model building. Results indicate that project type, end-user activity, process quality, team size and project popularity have a significant impact on the defect density of operational OSS projects. Since many organizations, both for profit and not for profit, are beginning to use Open Source Software as an economic alternative to commercial software, these results can be used in the process of deciding what software can be reasonably maintained by an organization.},
journal = {Inf. Technol. and Management},
month = dec,
pages = {235–251},
numpages = {17},
keywords = {Data mining, Model building, Open source software, Project performance, Text mining}
}

@inproceedings{10.1109/ASE.2008.14,
author = {De Lucia, A. and Oliveto, R. and Tortora, G.},
title = {IR-Based Traceability Recovery Processes: An Empirical Comparison of "One-Shot" and Incremental Processes},
year = {2008},
isbn = {9781424421879},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2008.14},
doi = {10.1109/ASE.2008.14},
abstract = {We present the results of a controlled experiment aiming at analysing the role played by the approach adopted during an IR-based traceability recovery process. In particular, we compare the tracing performances achieved by subjects using the "one-shot" process, where the full ranked list of candidate links is proposed, and the incremental process, where a similarity threshold is used to cut the ranked list and the links are classified step-by-step. The analysis of the achieved results shows that, in general, the incremental process improves the tracing accuracy and reduces the effort to analyse the proposed links.},
booktitle = {Proceedings of the 23rd IEEE/ACM International Conference on Automated Software Engineering},
pages = {39–48},
numpages = {10},
keywords = {incremental process, information retrieval, one-shot process, traceability recovery},
series = {ASE '08}
}

@article{10.1016/j.comcom.2012.12.002,
author = {Kanda, Yoshiki and Fontugne, Romain and Fukuda, Kensuke and Sugawara, Toshiharu},
title = {ADMIRE: Anomaly detection method using entropy-based PCA with three-step sketches},
year = {2013},
issue_date = {March, 2013},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {36},
number = {5},
issn = {0140-3664},
url = {https://doi.org/10.1016/j.comcom.2012.12.002},
doi = {10.1016/j.comcom.2012.12.002},
abstract = {Network anomaly detection using dimensionality reduction has recently been well studied in order to overcome the weakness of signature-based detection. Previous works have proposed a method for detecting particular anomalous IP-flows by using random projection (sketch) and a Principal Component Analysis (PCA). It yields promising high detection capability results without needing a pre-defined anomaly database. However, the detection method cannot be applied to the traffic flows at a single measurement point, and the appropriate parameter settings (e.g., the relationship between the sketch size and the number of IP addresses) have not yet been sufficiently studied. We propose in this paper a PCA-based anomaly detection algorithm called ADMIRE to supplement and expand the previous works. The key idea of ADMIRE is the use of three-step sketches and an adaptive parameter setting to improve the detection performance and ease its use in practice. We evaluate the effectiveness of ADMIRE using the longitudinal traffic traces captured from a transpacific link. The main findings of this paper are as follows: (1) We reveal the correlation between the number of IP addresses in the measured traffic and the appropriate sketch size. We take advantage of this relation to set the sketch size parameter. (2) ADMIRE outperforms traditional PCA-based detector and other detectors based on different theoretical backgrounds. (3) The types of anomalies reported by ADMIRE depend on the traffic features that are selected as input. Moreover, we found that a simple aggregation of several traffic features degrades the detection performance.},
journal = {Comput. Commun.},
month = mar,
pages = {575–588},
numpages = {14},
keywords = {Anomaly detection, Entropy, Hash, PCA, Sketch}
}

@inproceedings{10.1145/2581122.2544144,
author = {de Oliveira Castro, Pablo and Kashnikov, Yuriy and Akel, Chadi and Popov, Mihail and Jalby, William},
title = {Fine-grained Benchmark Subsetting for System Selection},
year = {2014},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2581122.2544144},
doi = {10.1145/2581122.2544144},
abstract = {System selection aims at finding the best architecture for a set of programs and workloads. It traditionally requires long running benchmarks. We propose a method to reduce the cost of system selection. We break down benchmarks into elementary fragments of source code, called codelets. Then, we identify two causes of redundancy: first, similar codelets; second, codelets called repeatedly. The key idea is to minimize redundancy inside the benchmark suite to speed it up. For each group of similar codelets, only one representative is kept. For codelets called repeatedly and for which the performance does not vary across calls, the number of invocations is reduced. Given an initial benchmark suite, our method produces a set of reduced benchmarks that can be used in place of the original one for system selection.We evaluate our method on the NAS SER benchmarks, producing a reduced benchmark suite 30 times faster in average than the original suite, with a maximum of 44 times. The reduced suite predicts the execution time on three target architectures with a median error between 3.9% and 8%.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {132–142},
numpages = {11},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@article{10.1016/j.cmpb.2021.106334,
author = {P\'{e}rez-S\'{a}nchez, Juanjo and Carrillo de Gea, Juan M. and Rodr\'{\i}guez Barcel\'{o}, Sandra and Toval, \'{A}ngel and Fern\'{a}ndez-Alem\'{a}n, Jos\'{e} L. and Garc\'{\i}a-Bern\'{a}, Jos\'{e} A. and Popovi\'{c}, Miroljub and Toval, Ambrosio},
title = {Intracranial pressure analysis software: A mapping study and proposal},
year = {2021},
issue_date = {Sep 2021},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {209},
number = {C},
issn = {0169-2607},
url = {https://doi.org/10.1016/j.cmpb.2021.106334},
doi = {10.1016/j.cmpb.2021.106334},
journal = {Comput. Methods Prog. Biomed.},
month = sep,
numpages = {14},
keywords = {Intracranial pressure, ICP, ICP Monitoring, ICP Automated analyisis, Mapping study,}
}

@inproceedings{10.1145/2544137.2544144,
author = {de Oliveira Castro, Pablo and Kashnikov, Yuriy and Akel, Chadi and Popov, Mihail and Jalby, William},
title = {Fine-grained Benchmark Subsetting for System Selection},
year = {2018},
isbn = {9781450326704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2544137.2544144},
doi = {10.1145/2544137.2544144},
abstract = {System selection aims at finding the best architecture for a set of programs and workloads. It traditionally requires long running benchmarks. We propose a method to reduce the cost of system selection. We break down benchmarks into elementary fragments of source code, called codelets. Then, we identify two causes of redundancy: first, similar codelets; second, codelets called repeatedly. The key idea is to minimize redundancy inside the benchmark suite to speed it up. For each group of similar codelets, only one representative is kept. For codelets called repeatedly and for which the performance does not vary across calls, the number of invocations is reduced. Given an initial benchmark suite, our method produces a set of reduced benchmarks that can be used in place of the original one for system selection.We evaluate our method on the NAS SER benchmarks, producing a reduced benchmark suite 30 times faster in average than the original suite, with a maximum of 44 times. The reduced suite predicts the execution time on three target architectures with a median error between 3.9% and 8%.},
booktitle = {Proceedings of Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {132–142},
numpages = {11},
location = {Orlando, FL, USA},
series = {CGO '14}
}

@article{10.1016/j.fss.2009.11.007,
title = {Recent Literature},
year = {2010},
issue_date = {April, 2010},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {161},
number = {8},
issn = {0165-0114},
url = {https://doi.org/10.1016/j.fss.2009.11.007},
doi = {10.1016/j.fss.2009.11.007},
journal = {Fuzzy Sets Syst.},
month = apr,
pages = {1152–1160},
numpages = {9}
}

@inproceedings{10.5555/1768904.1768920,
author = {Laukaitis, Algirdas and Vasilecas, Olegas},
title = {Integrating all stages of information systems development by means of natural language processing},
year = {2007},
isbn = {9783540730309},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {In this paper, we present the methodology and architecture of the natural language processing integration into all stages of the information systems development. We show that if the IS textual documentation is preprocessed and integrated into the business knowledge base development then the whole information systems modeling process can be speeded and improved. Self-organizing map received from information systems documentation and the formal concept analysis are suggested to test the IS documentation comprehensibility and reusability. IBM's Information Framework (IFW) Financial Services Data Model (FSDM) has been used for the present research. By using FSDM we demonstrate that the IS model can be partially recreated from IS textual documents by combining techniques based on self-organizing map and formal concept analysis. Finally the numerical experiment is provided to show that IS documents supplemented with the suggested techniques can be reused in natural language interfaces and save the resources and time needed to develop such interfaces.},
booktitle = {Proceedings of the 13th International Working Conference on Requirements Engineering: Foundation for Software Quality},
pages = {218–231},
numpages = {14},
keywords = {IS documents self-organization, formal concept analysis, information systems engineering, natural language processing},
location = {Trondheim, Norway},
series = {REFSQ'07}
}

@inproceedings{10.1145/1831708.1831716,
author = {Wei, Yi and Pei, Yu and Furia, Carlo A. and Silva, Lucas S. and Buchholz, Stefan and Meyer, Bertrand and Zeller, Andreas},
title = {Automated fixing of programs with contracts},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831716},
doi = {10.1145/1831708.1831716},
abstract = {In program debugging, finding a failing run is only the first step; what about correcting the fault? Can we automate the second task as well as the first? The AutoFix-E tool automatically generates and validates fixes for software faults. The key insights behind AutoFix-E are to rely on contracts present in the software to ensure that the proposed fixes are semantically sound, and on state diagrams using an abstract notion of state based on the boolean queries of a class. Out of 42 faults found by an automatic testing tool in two widely used Eiffel libraries, AutoFix-E proposes successful fixes for 16 faults. Submitting some of these faults to experts shows that several of the proposed fixes are identical or close to fixes proposed by humans.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {61–72},
numpages = {12},
keywords = {automatic debugging, automatic fixing, dynamic invariants, program synthesis},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/2351676.2351702,
author = {Seo, Hyunmin and Kim, Sunghun},
title = {Predicting recurring crash stacks},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351702},
doi = {10.1145/2351676.2351702},
abstract = {Software crash is one of the most severe bug manifestations and developers want to fix crash bugs quickly and efficiently. The Crash Reporting System (CRS) is widely deployed for this purpose. Even with the help of CRS, fixes are largely by manual effort, which is error-prone and results in recurring crashes even after the fixes. Our empirical study reveals that 48% of fixed crashes in Firefox CRS are recurring mostly due to incomplete or missing fixes. It is desirable to automatically check if a crash fix misses some reported crash traces at the time of the first fix. This paper proposes an automatic technique to predict recurring crash traces. We first extract stack traces and then compare them with bug fix locations to predict recurring crash traces. Evaluation using the real Firefox crash data shows that the approach yields reasonable accuracy in prediction of recurring crashes. Had our technique been deployed earlier, more than 2,225 crashes in Firefox 3.6 could have been avoided.},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {180–189},
numpages = {10},
keywords = {Crash, bug, crash reporting system},
location = {Essen, Germany},
series = {ASE '12}
}

@article{10.1145/2768829,
author = {Ceccato, Mariano and Marchetto, Alessandro and Mariani, Leonardo and Nguyen, Cu D. and Tonella, Paolo},
title = {Do Automatically Generated Test Cases Make Debugging Easier? An Experimental Assessment of Debugging Effectiveness and Efficiency},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2768829},
doi = {10.1145/2768829},
abstract = {Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests.We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers), since a major difference between manual and automatically generated test cases is that the latter contain meaningless (obfuscated) identifiers.We show that automatically generated test cases are as useful for debugging as manual test cases. Furthermore, we find that, for less experienced developers, automatic tests are more useful on average due to their lower static and dynamic complexity.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
articleno = {5},
numpages = {38},
keywords = {Empirical software engineering, automatic test case generation, debugging}
}

@article{10.1023/A:1021830128811,
author = {Lane, Terran and Brodley, Carla E.},
title = {An Empirical Study of Two Approaches to Sequence Learning for Anomaly Detection},
year = {2003},
issue_date = {April 2003},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {51},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1021830128811},
doi = {10.1023/A:1021830128811},
abstract = {This paper introduces the computer security domain of anomaly detection and formulates it as a machine learning task on temporal sequence data. In this domain, the goal is to develop a model or profile of the normal working state of a system user and to detect anomalous conditions as long-term deviations from the expected behavior patterns. We introduce two approaches to this problem: one employing instance-based learning (IBL) and the other using hidden Markov models (HMMs). Though not suitable for a comprehensive security solution, both approaches achieve anomaly identification performance sufficient for a low-level “focus of attention” detector in a multitier security system. Further, we evaluate model scaling techniques for the two approaches: two clustering techniques for the IBL approach and variation of the number of hidden states for the HMM approach. We find that over both model classes and a wide range of model scales, there is no significant difference in performance at recognizing the profiled user. We take this invariance as evidence that, in this security domain, limited memory models (e.g., fixed-length instances or low-order Markov models) can learn only part of the user identity information in which we're interested and that substantially different models will be necessary if dramatic improvements in user-based anomaly detection are to be achieved.},
journal = {Mach. Learn.},
month = apr,
pages = {73–107},
numpages = {35},
keywords = {anomaly detection, application, computer security, hidden Markov models, instance-based learning}
}

@proceedings{10.1145/2950290,
title = {FSE 2016: Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Seattle, WA, USA}
}

@inproceedings{10.1145/1985441.1985451,
author = {Rao, Shivani and Kak, Avinash},
title = {Retrieval from software libraries for bug localization: a comparative study of generic and composite text models},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985451},
doi = {10.1145/1985441.1985451},
abstract = {From the standpoint of retrieval from large software libraries for the purpose of bug localization, we compare five generic text models and certain composite variations thereof. The generic models are: the Unigram Model (UM), the Vector Space Model (VSM), the Latent Semantic Analysis Model (LSA), the Latent Dirichlet Allocation Model (LDA), and the Cluster Based Document Model (CBDM). The task is to locate the files that are relevant to a bug reported in the form of a textual description by a software developer. We use for our study iBUGS, a benchmarked bug localization dataset with 75 KLOC and a large number of bugs (291). A major conclusion of our comparative study is that simple text models such as UM and VSM are more effective at correctly retrieving the relevant files from a library as compared to the more sophisticated models such as LDA. The retrieval effectiveness for the various models was measured using the following two metrics: (1) Mean Average Precision; and (2) Rank-based metrics. Using the SCORE metric, we also compare the retrieval effectiveness of the models in our study with some other bug localization tools.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {43–52},
numpages = {10},
keywords = {bug localization, information retrieval, latent dirichlet allocation, latent semantic analysis, software engineering},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@article{10.1007/s10664-015-9387-3,
author = {Hindle, Abram and Alipour, Anahita and Stroulia, Eleni},
title = {A contextual approach towards more accurate duplicate bug report detection and ranking},
year = {2016},
issue_date = {April     2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9387-3},
doi = {10.1007/s10664-015-9387-3},
abstract = {The issue-tracking systems used by software projects contain issues, bugs, or tickets written by a wide variety of bug reporters, with different levels of training and knowledge about the system under development. Typically, reporters lack the skills and/or time to search the issue-tracking system for similar issues already reported. As a result, many reports end up referring to the same issue, which effectively makes the bug-report triaging process time consuming and error prone. Many researchers have approached the bug-deduplication problem using off-the-shelf information-retrieval (IR) tools. In this work, we extend the state of the art by investigating how contextual information about software-quality attributes, software-architecture terms, and system-development topics can be exploited to improve bug deduplication. We demonstrate the effectiveness of our contextual bug-deduplication method at ranking duplicates on the bug repositories of the Android, Eclipse, Mozilla, and OpenOffice software systems. Based on this experience, we conclude that taking into account domain-specific context can improve IR methods for bug deduplication.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {368–410},
numpages = {43},
keywords = {Bug deduplication, Bug-tracing systems, Duplicate bug reports, Information retrieval, Issue-tracking systems, Software context, Triaging}
}

@article{10.1007/s10664-015-9410-8,
author = {Unterkalmsteiner, Michael and Gorschek, Tony and Feldt, Robert and Lavesson, Niklas},
title = {Large-scale information retrieval in software engineering - an experience report from industrial application},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {21},
number = {6},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-015-9410-8},
doi = {10.1007/s10664-015-9410-8},
abstract = {Software Engineering activities are information intensive. Research proposes Information Retrieval (IR) techniques to support engineers in their daily tasks, such as establishing and maintaining traceability links, fault identification, and software maintenance. We describe an engineering task, test case selection, and illustrate our problem analysis and solution discovery process. The objective of the study is to gain an understanding of to what extent IR techniques (one potential solution) can be applied to test case selection and provide decision support in a large-scale, industrial setting. We analyze, in the context of the studied company, how test case selection is performed and design a series of experiments evaluating the performance of different IR techniques. Each experiment provides lessons learned from implementation, execution, and results, feeding to its successor. The three experiments led to the following observations: 1) there is a lack of research on scalable parameter optimization of IR techniques for software engineering problems; 2) scaling IR techniques to industry data is challenging, in particular for latent semantic analysis; 3) the IR context poses constraints on the empirical evaluation of IR techniques, requiring more research on developing valid statistical approaches. We believe that our experiences in conducting a series of IR experiments with industry grade data are valuable for peer researchers so that they can avoid the pitfalls that we have encountered. Furthermore, we identified challenges that need to be addressed in order to bridge the gap between laboratory IR experiments and real applications of IR in the industry.},
journal = {Empirical Softw. Engg.},
month = dec,
pages = {2324–2365},
numpages = {42},
keywords = {Data mining, Experiment, Information retrieval, Test case selection}
}

@book{10.5555/1972541,
author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
title = {Data Mining: Concepts and Techniques},
year = {2011},
isbn = {0123814790},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {3rd},
abstract = {The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data}
}

@inproceedings{10.1109/ASE.2013.6693100,
author = {Izs\'{o}, Benedek and Szatm\'{a}ri, Zolt\'{a}n and Bergmann, G\'{a}bor and Horv\'{a}th, Aacute;kos and R\'{a}th, Istv\'{a}n},
title = {Towards precise metrics for predicting graph query performance},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693100},
doi = {10.1109/ASE.2013.6693100},
abstract = {Queries are the foundations of data intensive applications. In model-driven software engineering (MDSE), model queries are core technologies of tools and transformations. As software models are rapidly increasing in size and complexity, most MDSE tools frequently exhibit scalability issues that decrease developer productivity and increase costs. As a result, choosing the right model representation and query evaluation approach is a significant challenge for tool engineers. In the current paper, we aim to provide a benchmarking framework for the systematic investigation of query evaluation performance. More specifically, we experimentally evaluate (existing and novel) query and instance model metrics to highlight which provide sufficient performance estimates for different MDSE scenarios in various model query tools. For that purpose, we also present a comparative benchmark, which is designed to differentiate model representation and graph query evaluation approaches according to their performance when using large models and complex queries.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {421–431},
numpages = {11},
keywords = {model metrics, model queries, performance benchmark, query metrics},
location = {Silicon Valley, CA, USA},
series = {ASE '13}
}

@article{10.1016/j.ins.2010.01.026,
author = {Cruz-Lemus, Jos\'{e} A. and Maes, Ann and Genero, Marcela and Poels, Geert and Piattini, Mario},
title = {The impact of structural complexity on the understandability of UML statechart diagrams},
year = {2010},
issue_date = {June, 2010},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {180},
number = {11},
issn = {0020-0255},
url = {https://doi.org/10.1016/j.ins.2010.01.026},
doi = {10.1016/j.ins.2010.01.026},
abstract = {The effectiveness of current software development strategies, such as Model-Driven Development (MDD), depends largely on the quality of their primary artefacts, i.e. software models. As the standard modelling language for software systems is the Unified Modelling Language (UML), quality assurance of UML models is a major research field in Computer Science. Understandability, i.e. a model's ability to be easily understood, is one model quality property that is currently heavily under investigation. In particular, researchers are searching for the factors that determine an UML model's understandability and are looking for ways to manipulate these factors. This paper presents an empirical study investigating the effect that structural complexity has on the understandability of one particular type of UML model, i.e. the statechart diagram. Based on data collected in a family of three experiments, we have identified three dimensions of structural complexity that affect understandability: (i) the size and control flow complexity of the statechart in terms of features such as the number of states, events, guards and state transitions; (ii) the actions that are performed when entering or leaving a state; (iii) the sequence of actions that is performed while staying within a state. Based on these structural complexity dimensions we have built an understandability prediction model using a regression technique that is specifically recommended for data obtained through a repeated measures design. Our test results show that each of the underlying structural complexity dimensions has a significant impact on the understandability of a statechart diagram.},
journal = {Inf. Sci.},
month = jun,
pages = {2209–2220},
numpages = {12},
keywords = {Empirical validation, Experiment, Metrics, Model quality, Prediction, Statechart diagram, Structural complexity, UML, Understandability}
}

@inproceedings{10.1145/3460120.3485364,
author = {Jiang, Zhiyuan and Jiang, Xiyue and Hazimeh, Ahmad and Tang, Chaojing and Zhang, Chao and Payer, Mathias},
title = {Igor: Crash Deduplication Through Root-Cause Clustering},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485364},
doi = {10.1145/3460120.3485364},
abstract = {Fuzzing has emerged as the most effective bug-finding technique. The output of a fuzzer is a set of proof-of-concept (PoC) test cases for all observed "unique'' crashes. It costs developers substantial efforts to analyze each crashing test case. This, mostly manual, process has lead to the number of reported crashes out-pacing the number of bug fixes. Automatic crash deduplication techniques, which mostly rely on coverage profiles and stack hashes, are supposed to alleviate these pressures. However, these techniques both inflate actual bug counts and falsely conflate unrelated bugs. This hinders, rather than helps, developers, and calls for more accurate techniques.The highly-stochastic nature of fuzzing means that PoCs commonly exercise many program behaviors that are orthogonal to the crash's underlying root cause. This diversity in program behaviors manifests as a diversity in crashes, contributing to bug-count inflation and conflation. Based on this insight, we develop Igor, an automated dual-phase crash deduplication technique. By minimizing each PoC's execution trace, we obtain pruned test cases that exercise the critical behavior necessary for triggering a bug. Then, we use a graph similarity comparison to cluster crashes based on the control-flow graph of the minimized execution traces, with each cluster mapping back to a single, unique root cause.We evaluate Igor against 39 bugs resulting from 254,000 PoCs, distributed over 10 programs. Our results show that Igor accurately groups these crashes into 48 uniquely identifiable clusters, while other state-of-the-art methods yield bug counts at least one order of magnitude larger.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {3318–3336},
numpages = {19},
keywords = {crash grouping, fuzzing},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.5555/2337223.2337277,
author = {Ceccato, Mariano and Marchetto, Alessandro and Mariani, Leonardo and Nguyen, Cu D. and Tonella, Paolo},
title = {An empirical study about the effectiveness of debugging when random test cases are used},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Automatically generated test cases are usually evaluated in terms of their fault revealing or coverage capability. Beside these two aspects, test cases are also the major source of information for fault localization and fixing. The impact of automatically generated test cases on the debugging activity, compared to the use of manually written test cases, has never been studied before.  In this paper we report the results obtained from two controlled experiments with human subjects performing debugging tasks using automatically generated or manually written test cases. We investigate whether the features of the former type of test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on accuracy and efficiency of debugging. The empirical study is aimed at investigating whether, despite the lack of readability in automatically generated test cases, subjects can still take advantage of them during debugging.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {452–462},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2024436.2024443,
author = {Garvin, Brady J. and Cohen, Myra B. and Dwyer, Matthew B.},
title = {Using feature locality: can we leverage history to avoid failures during reconfiguration?},
year = {2011},
isbn = {9781450308533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2024436.2024443},
doi = {10.1145/2024436.2024443},
abstract = {Despite the best efforts of software engineers, faults still escape into deployed software. Developers need time to prepare and distribute fixes, and in the interim deployments must either tolerate or avoid failures. Self-adaptive systems, systems that adapt to meet changing requirements in a dynamic environment, have a daunting task if their reconfiguration involves adding or removing functional features, because configurable software is known to suffer from failures that appear only under certain feature combinations.Although configuration-dependent failures may be difficult to provoke, and thus hard to detect in testing, we posit that they also constitute opportunities for reconfiguration to increase system reliability. We further conjecture that the failures that are sensitive to a system configuration depend on similar feature combinations, a phenomenon we call feature-locality, and that this locality can be combined with historical data to predict failure-prone configurations. In a case study on 128 failures reported against released versions of an open source configurable system, we find evidence to support our hypothesis. We show that only a small number of features affect the visibility of these failures, and that over time we can learn these features to avoid future failures.},
booktitle = {Proceedings of the 8th Workshop on Assurances for Self-Adaptive Systems},
pages = {24–33},
numpages = {10},
keywords = {highly-configurable systems, self-adaptive software},
location = {Szeged, Hungary},
series = {ASAS '11}
}

@article{10.1016/j.cl.2016.11.002,
author = {Malhotra, Ruchika},
title = {Special issue on search-based techniques and their hybridizations in software engineering},
year = {2017},
issue_date = {January 2017},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {47},
number = {P2},
issn = {1477-8424},
url = {https://doi.org/10.1016/j.cl.2016.11.002},
doi = {10.1016/j.cl.2016.11.002},
journal = {Comput. Lang. Syst. Struct.},
month = jan,
pages = {151–152},
numpages = {2}
}

@inproceedings{10.5555/2337223.2337270,
author = {Lucia and Lo, David and Jiang, Lingxiao and Budi, Aditya},
title = {Active refinement of clone anomaly reports},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = {Software clones have been widely studied in the recent literature and shown useful for finding bugs because inconsistent changes among clones in a clone group may indicate potential bugs. However, many inconsistent clone groups are not real bugs. The excessive number of false positives could easily impede broad adoption of clone-based bug detection approaches.  In this work, we aim to improve the usability of clonebased bug detection tools by increasing the rate of true positives found when a developer analyzes anomaly reports. Our idea is to control the number of anomaly reports a user can see at a time and actively incorporate incremental user feedback to continually refine the anomaly reports. Our system first presents top few anomaly reports from the list of reports generated by a tool in its default ordering. Users then either accept or reject each of the reports. Based on the feedback, our system automatically and iteratively refines a classification model for anomalies and re-sorts the rest of the reports. Our goal is to present the true positives to the users earlier than the default ordering. The rationale of the idea is based on our observation that false positives among the inconsistent clone groups could share common features (in terms of code structure, programming patterns, etc.), and these features can be learned from the incremental user feedback.  We evaluate our refinement process on three sets of clonebased anomaly reports from three large real programs: the Linux Kernel (C), Eclipse, and ArgoUML (Java), extracted by a clone-based anomaly detection tool. The results show that compared to the original ordering of bug reports, we can improve the rate of true positives found (i.e., true positives are found faster) by 11%, 87%, and 86% for Linux kernel, Eclipse, and ArgoUML, respectively.},
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {397–407},
numpages = {11},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical pairwise testing for software product lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {feature modelling, software product lines, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@book{10.5555/1207001,
author = {Hornick, Mark F. and Marcad\'{e}, Erik and Venkayala, Sunil},
title = {Java Data Mining: Strategy, Standard, and Practice: A Practical Guide for architecture, design, and implementation},
year = {2006},
isbn = {0123704529},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Whether you are a software developer, systems architect, data analyst, or business analyst, if you want to take advantage of data mining in the development of advanced analytic applications, Java Data Mining, JDM, the new standard now implemented in core DBMS and data mining/analysis software, is a key solution component. This book is the essential guide to the usage of the JDM standard interface, written by contributors to the JDM standard. The book discusses and illustrates how to solve real problems using the JDM API. The authors provide you with: * Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems; * JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; * JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. * Free, downloadable KJDM source code referenced in the book available here * Data mining introduction--an overview of data mining and the problems it can address across industries; JDM's place in strategic solutions to data mining-related problems;* JDM essentials--concepts, design approach and design issues, with detailed code examples in Java; a Web Services interface to enable JDM functionality in an SOA environment; and illustration of JDM XML Schema for JDM objects; * JDM in practice--the use of JDM from vendor implementations and approaches to customer applications, integration, and usage; impact of data mining on IT infrastructure; a how-to guide for building applications that use the JDM API. * Free, downloadable KJDM source code referenced in the book available here}
}

@article{10.5555/3192134.3192138,
title = {Analysing the effect of multi-versioning for software updates on reliability: a utility following pheromone trail of social insects},
year = {2017},
issue_date = {January 2017},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {9},
number = {5–6},
issn = {1755-0386},
abstract = {Software evolves, with new versions and patches being released frequently have always been a challenge. The conventional users refuse to upgrade their regular software application, relying instead on outdated versions flawed with vulnerabilities or missing useful features and bug fixes. Software engineering community improvises such requirement for version migrations and also is looking for analytical support system to scrutinise post convergence of version control. Hence, this paper presents a novel bio-inspired utility to analyse post convergence of version control of software applications and automatically tests each submitted patch, looking for potential bugs it introduces. The bio-inspired phenomena signifies a pheromone deposition and evaporation property of social insects and it demonstrates that how it largely effects the optimisation of local minima problem across the path of social insects. Similarly, here, the version control effect has been modelled with its parameters, which can yield useful post-version control paradigm, and may assist the software community with this developed application plug-in.},
journal = {Int. J. Adv. Intell. Paradigms},
month = jan,
pages = {505–518},
numpages = {14}
}

@article{10.1145/3234940,
author = {Sarro, Federica and Petrozziello, Alessio},
title = {Linear Programming as a Baseline for Software Effort Estimation},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3234940},
doi = {10.1145/3234940},
abstract = {Software effort estimation studies still suffer from discordant empirical results (i.e., conclusion instability) mainly due to the lack of rigorous benchmarking methods. So far only one baseline model, namely, Automatically Transformed Linear Model (ATLM), has been proposed yet it has not been extensively assessed. In this article, we propose a novel method based on Linear Programming (dubbed as Linear Programming for Effort Estimation, LP4EE) and carry out a thorough empirical study to evaluate the effectiveness of both LP4EE and ATLM for benchmarking widely used effort estimation techniques. The results of our study confirm the need to benchmark every other proposal against accurate and robust baselines. They also reveal that LP4EE is more accurate than ATLM for 17% of the experiments and more robust than ATLM against different data splits and cross-validation methods for 44% of the cases. These results suggest that using LP4EE as a baseline can help reduce conclusion instability. We make publicly available an open-source implementation of LP4EE in order to facilitate its adoption in future studies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {12},
numpages = {28},
keywords = {Software effort estimation, benchmarking, linear programming}
}

@article{10.1023/A:1018971212809,
author = {Ebert, Christof and Liedtke, Thomas and Baisch, Ekkehard},
title = {Improving reliability of large software systems},
year = {1999},
issue_date = {1999},
publisher = {J. C. Baltzer AG, Science Publishers},
address = {USA},
volume = {8},
number = {1–4},
issn = {1022-7091},
url = {https://doi.org/10.1023/A:1018971212809},
doi = {10.1023/A:1018971212809},
abstract = {Improving field performance of telecommunication systems is the key objective of both telecom suppliers and operators, as an increasing amount of business critical systems worldwide are relying on dependable telecommunication. Early defect detection improves field performance in terms of reduced field failure rates and reduced intrinsic downtime. This paper describes an integrated approach to improve early defect detection and thus field reliability of telecommunication switching systems. The assumptions at the start of the projects discussed in this paper are: Wide application of code inspections and thorough module testing must lead to a lower fault detection density in subsequent phases. At the same time criteria for selecting the most critical components for code reviews, code inspections and module test are provided in order to optimize efficiency. The primary goal is to identify critical components and to make failure predictions as early as possible during the life cycle and hence reduce managerial risk combined with too early or too late release of such a system to the field. During test release time prediction and field performance prediction are both based on tailored and superposed ENHPP reliability models. Experiences from projects of Alcatel’s Switching and Routing Division are included to show practical impacts.},
journal = {Ann. Softw. Eng.},
month = aug,
pages = {3–51},
numpages = {49}
}

@inproceedings{10.1109/MSR.2017.58,
author = {Bao, Lingfeng and Xing, Zhenchang and Xia, Xin and Lo, David and Li, Shanping},
title = {Who will leave the company? a large-scale industry study of developer turnover by mining monthly work report},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.58},
doi = {10.1109/MSR.2017.58},
abstract = {Software developer turnover has become a big challenge for information technology (IT) companies. The departure of key software developers might cause big loss to an IT company since they also depart with important business knowledge and critical technical skills. Understanding developer turnover is very important for IT companies to retain talented developers and reduce the loss due to developers' departure. Previous studies mainly perform qualitative observations or simple statistical analysis of developers' activity data to understand developer turnover. In this paper, we investigate whether we can predict the turnover of software developers in non-open source companies by automatically analyzing monthly self-reports. The monthly work reports in our study are from two IT companies. Monthly reports in these two companies are used to report a developer's activities and working hours in a month. We would like to investigate whether a developer will leave the company after he/she enters company for one year based on his/her first six monthly reports.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {170–181},
numpages = {12},
keywords = {developer turnover, mining software repositories, prediction model},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.5555/978-3-030-86973-1_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-86972-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part IV},
pages = {i–xxxix},
location = {Cagliari, Italy}
}

@article{10.1016/j.infsof.2016.11.009,
author = {Mariani, Thain\'{a} and Vergilio, Silvia Regina},
title = {A systematic review on search-based refactoring},
year = {2017},
issue_date = {March 2017},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {83},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2016.11.009},
doi = {10.1016/j.infsof.2016.11.009},
abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest.Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities.Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation.Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics.Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.},
journal = {Inf. Softw. Technol.},
month = mar,
pages = {14–34},
numpages = {21},
keywords = {Evolutionary algorithms, Refactoring, Search-based software engineering}
}

@inproceedings{10.1007/978-3-642-33678-2_30,
author = {Braga, Rosana T. Vaccare and Trindade Junior, Onofre and Castelo Branco, Kalinka Regina and Neris, Luciano De Oliveira and Lee, Jaejoon},
title = {Adapting a software product line engineering process for certifying safety critical embedded systems},
year = {2012},
isbn = {9783642336775},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-33678-2_30},
doi = {10.1007/978-3-642-33678-2_30},
abstract = {Software Product Line Engineering (SPLE) is a software development paradigm that aims at reducing the development effort and shorting time-to-market through systematic software reuse. While this paradigm has been successfully applied for the development of embedded systems in various domains, new challenges have emerged from the development of safety critical systems that require certification against a specific standard. Existing SPLE approaches do not explicitly consider the various certification standards or levels that products should satisfy. In this paper, we focus on several practical issues involved in the SPLE process, establishing an infrastructure of a product line engineering for certified products. A metamodel is proposed to capture the entities involved in SPL certification and the relationships among them. ProLiCES, which is a model-driven process for the development of SPLs, was modified to serve as an example of our approach, in the context of the UAV (Unmanned Aerial Vehicle) domain.},
booktitle = {Proceedings of the 31st International Conference on Computer Safety, Reliability, and Security},
pages = {352–363},
numpages = {12},
keywords = {development process, safety-critical embedded systems, software certification},
location = {Magdeburg, Germany},
series = {SAFECOMP'12}
}

@article{10.1007/s10664-008-9090-8,
author = {De Lucia, Andrea and Oliveto, Rocco and Tortora, Genoveffa},
title = {Assessing IR-based traceability recovery tools through controlled experiments},
year = {2009},
issue_date = {February  2009},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {1},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-008-9090-8},
doi = {10.1007/s10664-008-9090-8},
abstract = {We report the results of a controlled experiment and a replication performed with different subjects, in which we assessed the usefulness of an Information Retrieval-based traceability recovery tool during the traceability link identification process. The main result achieved in the two experiments is that the use of a traceability recovery tool significantly reduces the time spent by the software engineer with respect to manual tracing. Replication with different subjects allowed us to investigate if subjects' experience and ability play any role in the traceability link identification process. In particular, we made some observations concerning the retrieval accuracy achieved by the software engineers with and without the tool support and with different levels of experience and ability.},
journal = {Empirical Softw. Engg.},
month = feb,
pages = {57–92},
numpages = {36},
keywords = {Impact analysis, Information retrieval, Latent semantic indexing, Program comprehension, Singular value decomposition, Traceability recovery}
}

@article{10.1007/s10664-020-09859-y,
author = {Feyzi, Farid},
title = {CGT-FL: using cooperative game theory to effective fault localization in presence of coincidental correctness},
year = {2020},
issue_date = {Sep 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {25},
number = {5},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-020-09859-y},
doi = {10.1007/s10664-020-09859-y},
abstract = {In this article we emphasize that most of the faults, appearing in real-world programs, are complicated and there exists a high interaction between faulty and other correlated statements, that is likely to cause coincidental correctness in many cases. To effectively diminish the negative impact of coincidentally correct tests on localization effectiveness, we suggest analyzing the combinatorial effect of program statements on the failure. To this end, we develop a new framework, CGT-FL, for evaluation and ranking program statements in a manner that statements which have strong discriminatory power as a group but are weak as individuals could be identified. The framework firstly evaluates the interactivity degree of each statement according to its influence on the intricate interrelation among statements by a Shapley value-based cooperative game-theoretic method. Then, statements are selected in a forward way by considering both interactivity and relevance measures. To verify the effectiveness of CGT-FL, we provide the results of our extensive experiments with different subject programs, containing seeded and real faults. The experimental results are then compared with those provided by different fault localization techniques for both single-fault and multiple-fault programs. The results prove the outperformance of CGT-FL compared to state-of-the-art techniques.},
journal = {Empirical Softw. Engg.},
month = sep,
pages = {3873–3927},
numpages = {55},
keywords = {Coincidental correctness, Cooperative game theory, Shapley value, Fault localization, Debugging}
}

@article{10.5555/ios.IFS31AI,
author = {Sun, Shilei and Ionita, Silviu and Voln\'{a}, Eva and Gavrilov, Andrey and Liu, Feng},
title = {Author Index Volume 31 (2016)},
year = {2016},
issue_date = {2016},
publisher = {IOS Press},
address = {NLD},
volume = {31},
number = {6},
issn = {1064-1246},
journal = {J. Intell. Fuzzy Syst.},
month = jan,
pages = {3211–3227},
numpages = {17}
}

@article{10.1016/j.asoc.2016.08.024,
author = {dos Santos Neto, Pedro de Alcntara and Britto, Ricardo and Rablo, Ricardo de Andrade Lira and Cruz, Jonathas Jivago de Almeida and Lira, Werney Ayala Luz},
title = {A hybrid approach to suggest software product line portfolios},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {49},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2016.08.024},
doi = {10.1016/j.asoc.2016.08.024},
abstract = {Graphical abstractDisplay Omitted HighlightsThe work proposes a hybrid approach to deal with the Product Portfolio Scope Problem.The approach is composed by a solution to deploy the feature relevance indicated by the customers into code assets of a SPL, based on a systematic method (SQFD).The approach includes a method to estimate the cost of an asset based on common and relevant measures related to source code, together with a fuzzy system to deal with the imprecision to set reference values.The work presents an application of an NSGA-II to search for products minimizing the cost and maximizing the relevance of the candidate products.The approach was evaluated using different scenarios, exploring the mains aspects related to method in the practice: size, granularity of features and products search space.The previous version of our hybrid approach was dependent on the employed technologies and algorithms. Herein we reformulate our approach, detaching it from any particular technique/algorithm.The data collection process associated with our approach was improved to facilitate the hybrid approach's usage and mitigate associated construct validity threats.A more comprehensive evaluation, which focused on show the real word usefulness and scalability of our hybrid approach. To validate the usefulness of our approach, it was used the SPL associated with a tool broadly employed in both industrial and academic contexts (ArgoUML-SPL). The scalability of our approach was evaluated using a synthetic SPL.All the experiments were based on the guidelines defined by Arcuri and Briand in order to evaluate the statistical significance of this kind of work. Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
journal = {Appl. Soft Comput.},
month = dec,
pages = {1243–1255},
numpages = {13},
keywords = {Feature model selection problem, Fuzzy inference systems, NSGA-II, Product portfolio scoping, Search based feature model selection, Search based software engineering, Software product lines}
}

@article{10.1145/3410468,
author = {Bessghaier, Narjes and Soui, Makram and Kolski, Christophe and Chouchane, Mabrouka},
title = {On the Detection of Structural Aesthetic Defects of Android Mobile User Interfaces with a Metrics-based Tool},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3410468},
doi = {10.1145/3410468},
abstract = {Smartphone users are striving for easy-to-learn and use mobile apps user interfaces. Accomplishing these qualities demands an iterative evaluation of the Mobile User Interface (MUI). Several studies stress the value of providing a MUI with a pleasing look and feel to engaging end-users. The MUI, therefore, needs to be free from all kinds of structural aesthetic defects. Such defects are indicators of poor design decisions interfering with the consistency of a MUI and making it more difficult to use. To this end, we are proposing a tool (Aesthetic Defects DEtection Tool (ADDET)) to determine the structural aesthetic dimension of MUIs. Automating this process is useful to designers in evaluating the quality of their designs. Our approach is composed of two modules. (1) Metrics assessment is based on the static analysis of a tree-structured layout of the MUI. We used 15 geometric metrics (also known as structural or aesthetic metrics) to check various structural properties before a defect is triggered. (2) Defects detection: The manual combination of metrics and defects are time-consuming and user-dependent when determining a detection rule. Thus, we perceive the process of identification of defects as an optimization problem. We aim to automatically combine the metrics related to a particular defect and optimize the accuracy of the rules created by assigning a weight, representing the metric importance in detecting a defect. We conducted a quantitative and qualitative analysis to evaluate the accuracy of the proposed tool in computing metrics and detecting defects. The findings affirm the tool’s reliability when assessing a MUI’s structural design problems with 71% accuracy.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = mar,
articleno = {3},
numpages = {27},
keywords = {Structural aesthetic defects, automated evaluation, Android MUI, optimization algorithm, NSGA-II}
}

@article{10.1007/s11192-015-1612-8,
author = {Singh, Vivek Kumar and Uddin, Ashraf and Pinto, David},
title = {Computer science research: the top 100 institutions in India and in the world},
year = {2015},
issue_date = {August    2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {104},
number = {2},
issn = {0138-9130},
url = {https://doi.org/10.1007/s11192-015-1612-8},
doi = {10.1007/s11192-015-1612-8},
abstract = {This paper aims to perform a detailed scientometric and text-based analysis of Computer Science (CS) research output of the 100 most productive institutions in India and in the world. The analytical characterization is based on research output data indexed in Scopus during the last 25 years period (1989---2013). Our computational analysis involves a two-dimensional approach involving the standard scientometric methodology and text-based analysis. The scientometric characterization aims to assess CS domain research output in leading Indian institutions vis-\`{a}-vis the leading world institutions and to bring out the similarities and differences among them. It involves analysis along traditional scientometric indicators such as total output, citation-based impact assessment, co-authorship patterns, international collaboration levels etc. The text-based characterization aims to identify the key research themes and their temporal trends for the two sets. The key contribution of the experimental work is that it's an analytical characterization of its kind, which identifies characteristic similarities and differences in CS research landscape of Indian institutions vis-\`{a}-vis world institutions.},
journal = {Scientometrics},
month = aug,
pages = {529–553},
numpages = {25},
keywords = {Computer Science research, I23, India, Information Technology, Informetrics, Scientometrics}
}

@article{10.1007/s10664-017-9557-6,
author = {Dintzner, Nicolas and Deursen, Arie and Pinzger, Martin},
title = {FEVER: An approach to analyze feature-oriented changes and artefact co-evolution in highly configurable systems},
year = {2018},
issue_date = {April     2018},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {23},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-017-9557-6},
doi = {10.1007/s10664-017-9557-6},
abstract = {The evolution of highly configurable systems is known to be a challenging task. Thorough understanding of configuration options their relationships, and their implementation in various types of artefacts (variability model, mapping, and implementation) is required to avoid compilation errors, invalid products, or dead code. Recent studies focusing on co-evolution of artefacts detailed feature-oriented change scenarios, describing how related artefacts might change over time. However, relying on manual analysis of commits, such work do not provide the means to obtain quantitative information on the frequency of described scenarios nor information on the exhaustiveness of the presented scenarios for the evolution of a large scale system. In this work, we propose FEVER and its instantiation for the Linux kernel. FEVER extracts detailed information on changes in variability models (KConfig files), assets (preprocessor based C code), and mappings (Makefiles). We apply this methodology to the Linux kernel and build a dataset comprised of 15 releases of the kernel history. We performed an evaluation of the FEVER approach by manually inspecting the data and compared it with commits in the system's history. The evaluation shows that FEVER accurately captures feature related changes for more than 85% of the 810 manually inspected commits. We use the collected data to reflect on occurrences of co-evolution in practice. Our analysis shows that complex co-evolution scenarios occur in every studied release but are not among the most frequent change scenarios, as they only occur for 8 to 13% of the evolving features. Moreover, only a minority of developers working on a given release will make changes to all artefacts related to a feature (between 10% and 13% of authors). While our conclusions are derived from observations on the evolution of the Linux kernel, we believe that they may have implications for tool developers as well as guide further research in the field of co-evolution of artefacts.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {905–952},
numpages = {48},
keywords = {Co-evolution, Feature, Highly variable systems, Variability}
}

@article{10.1007/s10922-017-9402-7,
author = {Pinno, Otto J. and Correa, Sand L. and Santos, Aldri L. and Cardoso, Kleber V.},
title = {Decreasing the Management Burden in Multi-tier Systems Through Partial Correlation-Based Monitoring},
year = {2017},
issue_date = {July      2017},
publisher = {Plenum Press},
address = {USA},
volume = {25},
number = {3},
issn = {1064-7570},
url = {https://doi.org/10.1007/s10922-017-9402-7},
doi = {10.1007/s10922-017-9402-7},
abstract = {Modern web applications often consist of hundreds of services distributed in different servers or tiers. On one hand, this architecture may provide easy abstraction and modularity for software development and reuse. On the other hand, such architecture makes difficult to predict the behavior of the systems, as each tier has its own functionality, configuration, and demands for computing resources. Thus, anomaly detection becomes an important aspect for the management and operation of multi-tier web systems. In order to track their operation and aid on their behavior analysis, web systems expose numerous metrics in all the tiers. However, collecting and analyzing all available metrics reduces the system performance due to a non-negligible overhead on communication, storage, and processing. Another concern is the nature of the workload of these systems, which may fluctuate widely over time. One of the approaches to support anomaly detection in web systems is to use stable correlations among monitoring metrics. This approach, called correlation-based monitoring, does not require any deep understanding about the system internals or metric semantic, and also does not demand the existence of data about the faults. In addition, as only the metrics involved in stable correlations are periodically collected, the monitoring overhead is reduced. Stable correlations also have the desired property of holding for long period of time before becoming invalid due to workload fluctuations. The challenge, however, is to identify the stable correlations. In this work, we address this challenge by proposing three novel strategies based on partial correlation, a statistical tool commonly employed to summarize the relevant information of complex systems. We evaluate our strategies using traces obtained from an e-commerce, web transaction benchmark deployed in our testbed. Results show that our best strategy allows the construction of a monitoring network with less metrics than a state-of-the-art solution while achieving larger fault coverage. They also show that the correlations are reasonably stable, and the models can be applied for sufficiently long periods of time (at least 50 times the training time) before they become invalid.},
journal = {J. Netw. Syst. Manage.},
month = jul,
pages = {612–642},
numpages = {31},
keywords = {Anomaly detection, Fault coverage, Selection of correlations and metrics, Statistical-based fault filtering}
}

@article{10.1016/j.pmcj.2015.10.001,
author = {Kwon, Yongin and Yi, Hayoon and Kwon, Donghyun and Yang, Seungjun and Cho, Yeongpil and Paek, Yunheung},
title = {Precise execution offloading for applications with dynamic behavior in mobile cloud computing},
year = {2016},
issue_date = {April 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {27},
number = {C},
issn = {1574-1192},
url = {https://doi.org/10.1016/j.pmcj.2015.10.001},
doi = {10.1016/j.pmcj.2015.10.001},
abstract = {In order to accommodate the high demand for performance in smartphones, mobile cloud computing techniques, which aim to enhance a smartphone's performance through utilizing powerful cloud servers, were suggested. Among such techniques, execution offloading, which migrates a thread between a mobile device and a server, is often employed. In such execution offloading techniques, it is typical to dynamically decide what code part is to be offloaded through decision making algorithms. In order to achieve optimal offloading performance, however, the gain and cost of offloading must be predicted accurately for such algorithms. Previous works did not try hard to do this because it is usually expensive to make an accurate prediction. Thus in this paper, we introduce novel techniques to automatically generate accurate and efficient method-wise performance predictors for mobile applications and empirically show they enhance the performance of offloading.},
journal = {Pervasive Mob. Comput.},
month = apr,
pages = {58–74},
numpages = {17},
keywords = {Execution offloading, Mobile cloud computing, Performance prediction}
}

@inproceedings{10.1145/3377813.3381369,
author = {Qian, Rebecca and Yu, Yang and Park, Wonhee and Murali, Vijayaraghavan and Fink, Stephen and Chandra, Satish},
title = {Debugging crashes using continuous contrast set mining},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381369},
doi = {10.1145/3377813.3381369},
abstract = {Facebook operates a family of services used by over two billion people daily on a huge variety of mobile devices. Many devices are configured to upload crash reports should the app crash for any reason. Engineers monitor and triage millions of crash reports logged each day to check for bugs, regressions, and any other quality problems. Debugging groups of crashes is a manually intensive process that requires deep domain expertise and close inspection of traces and code, often under time constraints.We use contrast set mining, a form of discriminative pattern mining, to learn what distinguishes one group of crashes from another. Prior works focus on discretization to apply contrast mining to continuous data. We propose the first direct application of contrast learning to continuous data, without the need for discretization. We also define a weighted anomaly score that unifies continuous and categorical contrast sets while mitigating bias, as well as uncertainty measures that communicate confidence to developers. We demonstrate the value of our novel statistical improvements by applying it on a challenging dataset from Facebook production logs, where we achieve 40x speedup over baseline approaches using discretization.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {61–70},
numpages = {10},
keywords = {contrast set mining, crash analysis, descriptive rules, emerging patterns, multiple hypothesis testing, rule learning, subgroup discovery},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@article{10.1155/2017/9150965,
author = {Iturbe, Mikel and Garitano, I\~{n}aki and Zurutuza, Urko and Uribeetxeberria, Roberto and Lopez, Javier},
title = {Towards Large-Scale, Heterogeneous Anomaly Detection Systems in Industrial Networks: A Survey of Current Trends},
year = {2017},
issue_date = {2017},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
volume = {2017},
issn = {1939-0114},
url = {https://doi.org/10.1155/2017/9150965},
doi = {10.1155/2017/9150965},
abstract = {Industrial Networks (INs) are widespread environments where heterogeneous devices collaborate to control and monitor physical processes. Some of the controlled processes belong to Critical Infrastructures (CIs), and, as such, IN protection is an active research field. Among different types of security solutions, IN Anomaly Detection Systems (ADSs) have received wide attention from the scientific community. While INs have grown in size and in complexity, requiring the development of novel, Big Data solutions for data processing, IN ADSs have not evolved at the same pace. In parallel, the development of Big Data frameworks such as Hadoop or Spark has led the way for applying Big Data Analytics to the field of cyber-security, mainly focusing on the Information Technology (IT) domain. However, due to the particularities of INs, it is not feasible to directly apply IT security mechanisms in INs, as IN ADSs face unique characteristics. In this work we introduce three main contributions. First, we survey the area of Big Data ADSs that could be applicable to INs and compare the surveyed works. Second, we develop a novel taxonomy to classify existing IN-based ADSs. And, finally, we present a discussion of open problems in the field of Big Data ADSs for INs that can lead to further development.},
journal = {Sec. and Commun. Netw.},
month = jan,
numpages = {17}
}

@article{10.1016/j.knosys.2015.01.013,
title = {Revisiting Evolutionary Fuzzy Systems},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {80},
number = {C},
issn = {0950-7051},
url = {https://doi.org/10.1016/j.knosys.2015.01.013},
doi = {10.1016/j.knosys.2015.01.013},
abstract = {Evolutionary Fuzzy Systems are a successful hybridization between fuzzy systems and Evolutionary Algorithms. They integrate both the management of imprecision/uncertainty and inherent interpretability of Fuzzy Rule Based Systems, with the learning and adaptation capabilities of evolutionary optimization. Over the years, many different approaches in Evolutionary Fuzzy Systems have been developed for improving the behavior of fuzzy systems, either acting on the Fuzzy Rule Base Systems' elements, or by defining new approaches for the evolutionary components.All these efforts have enabled Evolutionary Fuzzy Systems to be successfully applied in several areas of Data Mining and engineering. In accordance with the former, a wide number of applications have been also taken advantage of these types of systems. However, with the new advances in computation, novel problems and challenges are raised every day. All these issues motivate researchers to make an effort in releasing new ways of addressing them with Evolutionary Fuzzy Systems.In this paper, we will review the progression of Evolutionary Fuzzy Systems by analyzing their taxonomy and components. We will also stress those problems and applications already tackled by this type of approach. We will present a discussion on the most recent and difficult Data Mining tasks to be addressed, and which are the latest trends in the development of Evolutionary Fuzzy Systems.},
journal = {Know.-Based Syst.},
month = may,
pages = {109–121},
numpages = {13}
}

@article{10.5555/1552038.1552043,
title = {From the journals…},
year = {2009},
issue_date = {June 2009},
publisher = {Cambridge University Press},
address = {USA},
volume = {24},
number = {2},
issn = {0269-8889},
journal = {Knowl. Eng. Rev.},
month = jun,
pages = {191–199},
numpages = {9}
}

@article{10.1016/j.asoc.2010.08.024,
author = {Dasgupta, Dipankar and Yu, Senhua and Nino, Fernando},
title = {Review Article: Recent Advances in Artificial Immune Systems: Models and Applications},
year = {2011},
issue_date = {March, 2011},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {11},
number = {2},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2010.08.024},
doi = {10.1016/j.asoc.2010.08.024},
abstract = {The immune system is a remarkable information processing and self learning system that offers inspiration to build artificial immune system (AIS). The field of AIS has obtained a significant degree of success as a branch of Computational Intelligence since it emerged in the 1990s. This paper surveys the major works in the AIS field, in particular, it explores up-to-date advances in applied AIS during the last few years. This survey has revealed that recent research is centered on four major AIS algorithms: (1) negative selection algorithms; (2) artificial immune networks; (3) clonal selection algorithms; (4) Danger Theory and dendritic cell algorithms. However, other aspects of the biological immune system are motivating computer scientists and engineers to develop new models and problem solving methods. Though an extensive amount of AIS applications has been developed, the success of these applications is still limited by the lack of any exemplars that really stand out as killer AIS applications.},
journal = {Appl. Soft Comput.},
month = mar,
pages = {1574–1587},
numpages = {14},
keywords = {AIS, Applications, Models, Review}
}

@inproceedings{10.1145/1985441.1985454,
author = {Posnett, Daryl and Hindle, Abram and Devanbu, Premkumar},
title = {A simpler model of software readability},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985454},
doi = {10.1145/1985441.1985454},
abstract = {Software readability is a property that influences how easily a given piece of code can be read and understood. Since readability can affect maintainability, quality, etc., programmers are very concerned about the readability of code. If automatic readability checkers could be built, they could be integrated into development tool-chains, and thus continually inform developers about the readability level of the code. Unfortunately, readability is a subjective code property, and not amenable to direct automated measurement. In a recently published study, Buse et al. asked 100 participants to rate code snippets by readability, yielding arguably reliable mean readability scores of each snippet; they then built a fairly complex predictive model for these mean scores using a large, diverse set of directly measurable source code properties. We build on this work: we present a simple, intuitive theory of readability, based on size and code entropy, and show how this theory leads to a much sparser, yet statistically significant, model of the mean readability scores produced in Buse's studies. Our model uses well-known size metrics and Halstead metrics, which are easily extracted using a variety of tools. We argue that this approach provides a more theoretically well-founded, practically usable, approach to readability measurement.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {73–82},
numpages = {10},
keywords = {entropy, halstead, readability, replication},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@article{10.1007/s10619-015-7189-7,
author = {Oβner, Christopher and Buchmann, Erik and B\"{o}hm, Klemens},
title = {Identifying defective nodes in wireless sensor networks},
year = {2016},
issue_date = {December  2016},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {34},
number = {4},
issn = {0926-8782},
url = {https://doi.org/10.1007/s10619-015-7189-7},
doi = {10.1007/s10619-015-7189-7},
abstract = {Wireless sensor networks (WSNs) have become ubiquitous, e.g., in logistics, smart manufacturing, smart city infrastructures or vehicular ad-hoc networks. WSNs tend to rely on ad-hoc infrastructures that are prone to a wide range of different defects, e.g., communication failures, faulty sensors or nodes that have been tampered with. Additionally, dealing with defects is challenging, as defects might occur only occasionally. In this paper, we introduce SEDEL, our approach for Sensor nEtwork DEfect Localization. SEDEL helps the WSN operator to pinpoint defective nodes in the routing topology of a WSN. In particular, we let the operator store graph representations of the routing topology, together with information if the WSN has produced errors. Based on this information, SEDEL assigns each WSN node a suspiciousness score that is correlated with the defect probability. Thus, our approach can be used with any kind of defect, and the kind does not have to be known, as long as the operator can decide if a certain processing is correct or not. We have evaluated SEDEL with a real sensor-node deployment. Our evaluation shows that the defective node is assigned a high probability in the vast majority of the experiments.},
journal = {Distrib. Parallel Databases},
month = dec,
pages = {591–610},
numpages = {20},
keywords = {Defect localization, Graph mining, Wireless sensor networks}
}

@inproceedings{10.5555/2486788.2486852,
author = {Apel, Sven and Rhein, Alexander von and Wendler, Philipp and Gr\"{o}\ss{}linger, Armin and Beyer, Dirk},
title = {Strategies for product-line verification: case studies and experiments},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = {Product-line technology is increasingly used in mission-critical and safety-critical applications. Hence, researchers are developing verification approaches that follow different strategies to cope with the specific properties of product lines. While the research community is discussing the mutual strengths and weaknesses of the different strategies—mostly at a conceptual level—there is a lack of evidence in terms of case studies, tool implementations, and experiments. We have collected and prepared six product lines as subject systems for experimentation. Furthermore, we have developed a model-checking tool chain for C-based and Java-based product lines, called SPLVERIFIER, which we use to compare sample-based and family-based strategies with regard to verification performance and the ability to find defects. Based on the experimental results and an analytical model, we revisit the discussion of the strengths and weaknesses of product-line–verification strategies.},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {482–491},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/978-3-030-86653-2_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-86652-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part I},
pages = {i–xxxix},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/1075405.1075415,
author = {Fox, Armando and Kiciman, Emre and Patterson, David},
title = {Combining statistical monitoring and predictable recovery for self-management},
year = {2004},
isbn = {1581139896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1075405.1075415},
doi = {10.1145/1075405.1075415},
abstract = {Complex distributed Internet services form the basis not only of e-commerce but increasingly of mission-critical network-based applications. What is new is that the workload and internal architecture of three-tier enterprise applications presents the opportunity for a new approach to keeping them running in the face of many common recoverable failures. The core of the approach is anomaly detection and localization based on statistical machine learning techniques. Unlike previous approaches, we propose anomaly detection and pattern mining not only for operational statistics such as mean response time, but also for structural behaviors of the system---what parts of the system, in what combinations, are being exercised in response to different kinds of external stimuli. In addition, rather than building baseline models a priori, we extract them by observing the behavior of the system over a short period of time during normal operation. We explain the necessary underlying assumptions and why they can be realized by systems research, report on some early successes using the approach, describe benefits of the approach that make it competitive as a path toward self-managing systems, and outline some research challenges. Our hope is that this approach will enable "new science" in the design of self-managing systems by allowing the rapid and widespread application of statistical learning theory techniques (SLT) to problems of system dependability.},
booktitle = {Proceedings of the 1st ACM SIGSOFT Workshop on Self-Managed Systems},
pages = {49–53},
numpages = {5},
location = {Newport Beach, California},
series = {WOSS '04}
}

@inproceedings{10.5555/978-3-030-86976-2_fm,
title = {Front Matter},
year = {2021},
isbn = {978-3-030-86975-5},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Computational Science and Its Applications – ICCSA 2021: 21st International Conference, Cagliari, Italy, September 13–16, 2021, Proceedings, Part V},
pages = {i–xl},
location = {Cagliari, Italy}
}

@article{10.1007/s10515-017-0218-1,
author = {Lou, Jian-Guang and Lin, Qingwei and Ding, Rui and Fu, Qiang and Zhang, Dongmei and Xie, Tao},
title = {Experience report on applying software analytics in incident management of online service},
year = {2017},
issue_date = {December  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {24},
number = {4},
issn = {0928-8910},
url = {https://doi.org/10.1007/s10515-017-0218-1},
doi = {10.1007/s10515-017-0218-1},
abstract = {As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out 2-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our experience about using software analytics to solve engineers pain points in incident management, the developed data-analysis techniques, and the lessons learned from the process of research development and technology transfer.},
journal = {Automated Software Engg.},
month = dec,
pages = {905–941},
numpages = {37},
keywords = {Incident management, Online service, Service incident diagnosis, Software analytics}
}

@inproceedings{10.5555/2964958.2964971,
author = {Lopes, Lucelene and Scalabrin, Edson Emilio and Fernandes, Paulo},
title = {An Empirical Study of Combined Classifiers for Knowledge Discovery on Medical Data Bases},
year = {2008},
isbn = {9783540893752},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {This paper compares the accuracy of combined classifiers in medical data bases to the same knowledge discovery techniques applied to generic data bases. Specifically, we apply Bagging and Boosting methods for 16 medical and 16 generic data bases and compare the accuracy results with a more traditional approach C4.5 algorithm. Bagging and Boosting methods are applied using different numbers of classifiers and the accuracy is computed using a cross-validation technique. This paper main contribution resides in recommend the most accurate method and possible parameterization for medical data bases and an initial identification of some characteristics that make medical data bases different from generic ones.},
booktitle = {Revised Selected Papers of the APWeb 2008 International Workshops on Advanced Web and Network Technologies, and Applications - Volume 4977},
pages = {110–121},
numpages = {12},
location = {and DeWeb Shenyang, China},
series = {IWHDM}
}

@inproceedings{10.1145/3239235.3239240,
author = {Aljarallah, Sulaiman and Lock, Russell},
title = {An exploratory study of software sustainability dimensions and characteristics: end user perspectives in the kingdom of Saudi Arabia (KSA)},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239240},
doi = {10.1145/3239235.3239240},
abstract = {Background: Sustainability has become an important topic globally and the focus on ICT sustainability is increasing. However, issues exist, including vagueness and complexity of the concept itself, in addition to immaturity of the Software Engineering (SE) field. Aims: The study surveys respondents on software sustainability dimensions and characteristics from their perspectives, and seeks to derive rankings for their priority. Method: An exploratory study was conducted to quantitatively investigate Saudi Arabian (KSA) software user's perceptions with regard to the concept itself, the dimensions and characteristics of the software sustainability. Survey data was gathered from 906 respondents. Results: The results highlight key dimensions for sustainability and their priorities to users. The results also indicate that the characteristics perceived to be the most significant, were security, usability, reliability, maintainability, extensibility and portability, whereas respondents were relatively less concerned with computer ethics (e.g. privacy and trust), functionality, efficiency and reusability. A key finding was that females considered the environmental dimension to be more important than males. Conclusions: The dimensions and characteristics identified here can be used as a means of providing valuable feedback for the planning and implementation of future development of sustainable software.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {14},
numpages = {10},
keywords = {empirical study, software sustainability, sustainability dimensions},
location = {Oulu, Finland},
series = {ESEM '18}
}

@article{10.4018/IJWSR.2016070102,
author = {Zhang, Pengcheng and Leung, Hareton and Xiao, Yan and Zhu, Yuelong and Feng, Jun and Wan, Dingsheng and Li, Wenrui},
title = {A New Symbolization and Distance Measure Based Anomaly Mining Approach for Hydrological Time Series},
year = {2016},
issue_date = {July 2016},
publisher = {IGI Global},
address = {USA},
volume = {13},
number = {3},
issn = {1545-7362},
url = {https://doi.org/10.4018/IJWSR.2016070102},
doi = {10.4018/IJWSR.2016070102},
abstract = {Most of the time series data mining tasks attempt to discover data patterns that appear frequently. Abnormal data is often ignored as noise. There are some data mining techniques based on time series to extract anomaly. However, most of these techniques cannot suit big unstable data existing in various fields. Their key problems are high fitting error after dimension reduction and low accuracy of mining results. This paper studies an approach of mining time series abnormal patterns in the hydrological field. The authors propose a new idea to solve the problem of hydrological anomaly mining based on time series. They propose Feature Points Symbolic Aggregate Approximation FP_SAX to improve the selection of feature points, and then measures the distance of strings by Symbol Distance based Dynamic Time Warping SD_DTW. Finally, the distances generated are sorted. A set of dedicated experiments are performed to validate the authors' approach. The results show that their approach has lower fitting error and higher accuracy compared to other approaches.},
journal = {Int. J. Web Serv. Res.},
month = jul,
pages = {26–45},
numpages = {20},
keywords = {Data Mining, Distance Measure, Hydrological Time Series, Pattern Representation}
}

@article{10.1007/s10270-017-0610-0,
author = {Guo, Jianmei and Liang, Jia Hui and Shi, Kai and Yang, Dingyu and Zhang, Jingsong and Czarnecki, Krzysztof and Ganesh, Vijay and Yu, Huiqun},
title = {SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines},
year = {2019},
issue_date = {Apr 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {18},
number = {2},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-017-0610-0},
doi = {10.1007/s10270-017-0610-0},
abstract = {A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.},
journal = {Softw. Syst. Model.},
month = apr,
pages = {1447–1466},
numpages = {20},
keywords = {Constraint solving, Feature models, Multi-objective evolutionary algorithms, Search-based software engineering, Software product lines}
}

@proceedings{10.1145/2993236,
title = {GPCE 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
year = {2016},
isbn = {9781450344463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@proceedings{10.1145/3411764,
title = {CHI '21: Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@proceedings{10.1145/3411763,
title = {CHI EA '21: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Yokohama, Japan}
}

@article{10.5555/975771.975783,
author = {Anonymous},
title = {From the Journals...},
year = {1999},
issue_date = {August 1999},
publisher = {Cambridge University Press},
address = {USA},
volume = {14},
number = {2},
issn = {0269-8889},
journal = {Knowl. Eng. Rev.},
month = aug,
pages = {187–197},
numpages = {11}
}

@inproceedings{10.1145/2396761.2396869,
author = {Zhou, Jian and Zhang, Hongyu},
title = {Learning to rank duplicate bug reports},
year = {2012},
isbn = {9781450311564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396761.2396869},
doi = {10.1145/2396761.2396869},
abstract = {For a large and complex software system, the project team could receive a large number of bug reports. Some bug reports could be duplicates as they essentially report the same problem. It is often tedious and costly to manually check if a newly reported bug is a duplicate of an already reported bug. In this paper, we propose BugSim, a method that can automatically retrieve duplicate bug reports given a new bug report. BugSim is based on learning to rank concepts. We identify textual and statistical features of bug reports and propose a similarity function for bug reports based on the features. We then construct a training set by assembling pairs of duplicate and non-duplicate bug reports. We train the weights of features by applying the stochastic gradient descent algorithm over the training set. For a new bug report, we retrieve candidate duplicate reports using the trained model. We evaluate BugSim using more than 45,100 real bug reports of twelve Eclipse projects. The evaluation results show that the proposed method is effective. On average, the recall rate for the top 10 retrieved reports is 76.11%. Furthermore, BugSim outperforms the previous state-of-art methods that are implemented using SVM and BM25Fext.},
booktitle = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management},
pages = {852–861},
numpages = {10},
keywords = {bug reports, duplicate bug retrieval, duplicate documents, learning to rank, software maintenance},
location = {Maui, Hawaii, USA},
series = {CIKM '12}
}

@article{10.25300/MISQ/2016/40.3.01,
author = {Larsen, Kai R. and Bong, Chih How},
title = {A tool for addressing construct identity in literature reviews and meta-analyses},
year = {2016},
issue_date = {September 2016},
publisher = {Society for Information Management and The Management Information Systems Research Center},
address = {USA},
volume = {40},
number = {3},
issn = {0276-7783},
url = {https://doi.org/10.25300/MISQ/2016/40.3.01},
doi = {10.25300/MISQ/2016/40.3.01},
abstract = {The problem of detecting whether two behavioral constructs reference the same real-world phenomenon has existed for over 100 years. Discordant naming of constructs is here termed the construct identity fallacy (CIF). We designed and evaluated the construct identity detector (CID), the first tool with large-scale construct identity detection properties and the first tool that does not require respondent data. Through the adaptation and combination of different natural language processing (NLP) algorithms, six designs were created and evaluated against human expert decisions. All six designs were found capable of detecting construct identity, and a design combining two existing algorithms significantly outperformed the other approaches. A set of follow-up studies suggests the tool is valuable as a supplement to expert efforts in literature review and meta-analysis. Beyond design science contributions, this article has important implications related to the taxonomic structure of social and behavioral science constructs, for the jingle and jangle fallacy, the core of the Information Systems nomological network, and the inaccessibility of social and behavioral science knowledge. In sum, CID represents an important, albeit tentative, step toward discipline-wide identification of construct identities.},
journal = {MIS Q.},
month = sep,
pages = {529–551},
numpages = {23},
keywords = {construct identity detector (CID), construct identity fallacy (CIF), construct validity, design science, inter-nomological network, jingle and jangle fallacy, latent semantic analysis (LSA), natural language processing (NLP), nomological networks, ontologies, synonymy and polysemy}
}

@article{10.1007/s11042-016-3637-2,
author = {Liapis, Alexandros and Katsanos, Christos and Sotiropoulos, Dimitris G. and Karousos, Nikos and Xenos, Michalis},
title = {Stress in interactive applications: analysis of the valence-arousal space based on physiological signals and self-reported data},
year = {2017},
issue_date = {February  2017},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {76},
number = {4},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-016-3637-2},
doi = {10.1007/s11042-016-3637-2},
abstract = {Measuring users' emotional reaction to interactive multimedia and hypermedia is important. One particularly popular self-reported method for emotion assessment is the Valence-Arousal (VA) Scale: a 9\'{z} \'{z}9 affective grid. This paper aims to identify specific stress region(s) in the VA space by combining self-reported ratings (pairs of VA) and physiological signals (skin conductance). To this end, 31 healthy volunteers participated in an experiment by performing five stressful interaction tasks while their skin conductance was monitored. The selected interaction tasks were most frequently listed as stressful by a separate group of 15 interviewees. After each task, participants expressed their perceived emotional experience using the VA rating space. Our findings show which regions in the VA rating space may reliably indicate self-reported stress that is in alignment with one's measured skin conductance while using interactive applications. One additional important contribution of this work is the proposed approach for the empirical identification of affect regions in the VA space based on physiological signals.},
journal = {Multimedia Tools Appl.},
month = feb,
pages = {5051–5071},
numpages = {21},
keywords = {Valence, Interactive multimedia environments, Human computer interaction, Galvanic skin response, Emotional experience evaluation, Arousal, Affect grid}
}

@book{10.5555/1557461,
author = {van Harmelen, Frank and van Harmelen, Frank and Lifschitz, Vladimir and Porter, Bruce},
title = {Handbook of Knowledge Representation},
year = {2007},
isbn = {0444522115},
publisher = {Elsevier Science},
address = {San Diego, CA, USA},
abstract = {Knowledge Representation, which lies at the core of Artificial Intelligence, is concerned with encoding knowledge on computers to enable systems to reason automatically. The Handbook of Knowledge Representation is an up-to-date review of twenty-five key topics in knowledge representation, written by the leaders of each field.This book is an essential resource for students, researchers and practitioners in all areas of Artificial Intelligence. * Make your computer smarter* Handle qualitative and uncertain information* Improve computational tractability to solve your problems easily}
}

@inproceedings{10.1145/2020390.2020401,
author = {Marks, Lionel and Zou, Ying and Hassan, Ahmed E.},
title = {Studying the fix-time for bugs in large open source projects},
year = {2011},
isbn = {9781450307093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2020390.2020401},
doi = {10.1145/2020390.2020401},
abstract = {Background: Bug fixing lies at the core of most software maintenance efforts. Most prior studies examine the effort needed to fix a bug (fix-effort). However, the effort needed to fix a bug may not correlate with the calendar time needed to fix it (fix-time). For example, the fix-time for bugs with low fix-effort may be long if they are considered to be of low priority.Aims: We study the fix-time for bugs in large open source projects.Method: We study the fix-time along three dimensions: (1) the location of the bug (e.g., which component), (2) the reporter of the bug, and (3) the description of the bug. Using these three dimensions and their associated attributes, we examine the fix-time for bugs in two large open source projects: Eclipse and Mozilla, using a random forest classifier.Results: We show that we can correctly classify ~65% of the time the fix-time for bugs in these projects. We perform a sensitivity analysis to identify the most important attributes in each dimension. We find that the time of the filing of a bug and its location are the most important attributes in the Mozilla project for determining the fix-time of a bug. On the other hand, the fix-time in the Eclipse project is highly dependant on the severity of the bug. Surprisingly, the priority of the bug is not an important attribute when determining the fix-time for a bug in both projects.Conclusion: Attributes affecting the fix-time vary between projects and vary over time within the same project.},
booktitle = {Proceedings of the 7th International Conference on Predictive Models in Software Engineering},
articleno = {11},
numpages = {8},
keywords = {bug fix-time, empirical software engineering, mining software repositories},
location = {Banff, Alberta, Canada},
series = {Promise '11}
}

@article{10.1016/j.jss.2015.01.028,
author = {Azzeh, Mohammad and Nassif, Ali Bou and Minku, Leandro L.},
title = {An empirical evaluation of ensemble adjustment methods for analogy-based effort estimation},
year = {2015},
issue_date = {May 2015},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {103},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2015.01.028},
doi = {10.1016/j.jss.2015.01.028},
abstract = {Ensembles of adjustment methods are not always superior to single methods.Ensembles of linear methods are more accurate than ensembles of nonlinear methods.Adjustment methods based on GA and NN got the worst accuracy.Changing the value of k makes the prediction models behave diversely.RTM variants is the top ranked type based on Scott-Knott and two-way ANOVA. ContextEffort adjustment is an essential part of analogy-based effort estimation, used to tune and adapt nearest analogies in order to produce more accurate estimations. Currently, there are plenty of adjustment methods proposed in literature, but there is no consensus on which method produces more accurate estimates and under which settings. ObjectiveThis paper investigates the potential of ensemble learning for variants of adjustment methods used in analogy-based effort estimation. The number k of analogies to be used is also investigated. MethodWe perform a large scale comparison study where many ensembles constructed from n out of 40 possible valid variants of adjustment methods are applied to eight datasets. The performance of each method was evaluated based on standardized accuracy and effect size. ResultsThe results have been subjected to statistical significance testing, and show reasonable significant improvements on the predictive performance where ensemble methods are applied. ConclusionOur conclusions suggest that ensembles of adjustment methods can work well and achieve good performance, even though they are not always superior to single methods. We also recommend constructing ensembles from only linear adjustment methods, as they have shown better performance and were frequently ranked higher.},
journal = {J. Syst. Softw.},
month = may,
pages = {36–52},
numpages = {17},
keywords = {Ensemble learning, Analogy based estimation, Adjustment methods}
}

@proceedings{10.1145/2776880,
title = {SIGGRAPH '15: ACM SIGGRAPH 2015 Courses},
year = {2015},
isbn = {9781450336345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {In SIGGRAPH 2015 Courses, attendees learn from the experts in the field and gain inside knowledge that is critical to career advancement. Courses help attendees learn "how to do something" or "how to do something faster, better, smarter, more easily, etc." They are presented in one of two formats (1.5 hours or 3.25 hours) and may include elements of interactive demonstration, performance, or other imaginative approaches to teaching.The spectrum of Courses ranges from an introduction to the foundations of computer graphics and interactive techniques for those new to the field to advanced instruction on the most current techniques and topics. Topics covered may be subjects found in any SIGGRAPH program and may include methods of reapplying skills and processes from one area of computer graphics to another. Courses include core curricula taught by invited instructors as well as Courses selected from juried proposals.},
location = {Los Angeles, California}
}

@article{10.1145/2827872,
author = {Harper, F. Maxwell and Konstan, Joseph A.},
title = {The MovieLens Datasets: History and Context},
year = {2015},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/2827872},
doi = {10.1145/2827872},
abstract = {The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = dec,
articleno = {19},
numpages = {19},
keywords = {recommendations, ratings, MovieLens, Datasets}
}

@article{10.1016/j.infsof.2005.03.002,
author = {van Koten, C. and Gray, A. R.},
title = {An application of Bayesian network for predicting object-oriented software maintainability},
year = {2006},
issue_date = {January, 2006},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {48},
number = {1},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2005.03.002},
doi = {10.1016/j.infsof.2005.03.002},
abstract = {As the number of object-oriented software systems increases, it becomes more important for organizations to maintain those systems effectively. However, currently only a small number of maintainability prediction models are available for object-oriented systems. This paper presents a Bayesian network maintainability prediction model for an object-oriented software system. The model is constructed using object-oriented metric data in Li and Henry's datasets, which were collected from two different object-oriented systems. Prediction accuracy of the model is evaluated and compared with commonly used regression-based models. The results suggest that the Bayesian network model can predict maintainability more accurately than the regression-based models for one system, and almost as accurately as the best regression-based model for the other system.},
journal = {Inf. Softw. Technol.},
month = jan,
pages = {59–67},
numpages = {9},
keywords = {Regression tree, Regression, Object-oriented systems, Maintainability, Bayesian network}
}

@article{10.1016/j.asoc.2015.11.041,
author = {Jin, Cong and Jin, Shu-Wei},
title = {Parameter optimization of software reliability growth model with S-shaped testing-effort function using improved swarm intelligent optimization},
year = {2016},
issue_date = {March 2016},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {40},
number = {C},
issn = {1568-4946},
url = {https://doi.org/10.1016/j.asoc.2015.11.041},
doi = {10.1016/j.asoc.2015.11.041},
abstract = {The proposed approach does not require any assumption for software failure data.Implementation of the proposed approach is very easy.The proposed SRGMTEF has a reasonable predictive capability. Software reliability growth model (SRGM) with testing-effort function (TEF) is very helpful for software developers and has been widely accepted and applied. However, each SRGM with TEF (SRGMTEF) contains some undetermined parameters. Optimization of these parameters is a necessary task. Generally, these parameters are estimated by the Least Square Estimation (LSE) or the Maximum Likelihood Estimation (MLE). We found that the MLE can be used only when the software failure data to satisfy some assumptions such as to satisfy a certain distribution. However, the software failure data may not satisfy such a distribution. In this paper, we investigate the improvement and application of a swarm intelligent optimization algorithm, namely quantum particle swarm optimization (QPSO) algorithm, to optimize these parameters of SRGMTEF. The performance of the proposed SRGMTEF model with optimized parameters is also compared with other existing models. The experiment results show that the proposed parameter optimization approach using QPSO is very effective and flexible, and the better software reliability growth performance can be obtained based on SRGMTEF on the different software failure datasets.},
journal = {Appl. Soft Comput.},
month = mar,
pages = {283–291},
numpages = {9},
keywords = {Software testing, Software reliability growth model, S-shaped testing-effort function, Quantum particle swarm optimization, Parameter optimization}
}

@article{10.1109/TNET.2012.2230448,
author = {Tariq, Mukarram Bin and Bhandankar, Kaushik and Valancius, Vytautas and Zeitoun, Amgad and Feamster, Nick and Ammar, Mostafa},
title = {Answering: techniques and deployment experience},
year = {2013},
issue_date = {February 2013},
publisher = {IEEE Press},
volume = {21},
number = {1},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2012.2230448},
doi = {10.1109/TNET.2012.2230448},
abstract = {Designers of content distribution networks (CDNs) often need to determine how changes to infrastructure deployment and configuration affect service response times when they deploy a new data center, change ISP peering, or change the mapping of clients to servers. Today, the designers use coarse, back-of-the-envelope calculations or costly field deployments; they need better ways to evaluate the effects of such hypothetical "what-if" questions before the actual deployments. This paper presents What-If Scenario Evaluator (WISE), a tool that predicts the effects of possible configuration and deployment changes in content distribution networks. WISE makes three contributions: 1) an algorithm that uses traces from existing deployments to learn causality among factors that affect service responsetime distributions; 2) an algorithm that uses the learned causal structure to estimate a dataset that is representative of the hypothetical scenario that a designer may wish to evaluate, and uses these datasets to predict hypothetical response-time distributions; 3) a scenario specification language that allows a network designer to easily express hypothetical deployment scenarios without being cognizant of the dependencies between variables that affect service response times. Our evaluation, both in a controlled setting and in a real-world field deployment on a large, global CDN, shows that WISE can quickly and accurately predict service response-time distributions for many practical what-if scenarios.},
journal = {IEEE/ACM Trans. Netw.},
month = feb,
pages = {1–13},
numpages = {13},
keywords = {what-if scenario evaluator (WISE), web performance, content distribution network (CDN), causality}
}

@article{10.5555/976205.976222,
author = {Anonymous},
title = {From the journals....},
year = {1998},
issue_date = {July 1998},
publisher = {Cambridge University Press},
address = {USA},
volume = {13},
number = {2},
issn = {0269-8889},
journal = {Knowl. Eng. Rev.},
month = jul,
pages = {209–224},
numpages = {16}
}

@inproceedings{10.1145/2934466.2934481,
author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter and de Jong, Gjalt},
title = {Systematic quality trade-off support in the software product-line configuration process},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2934481},
doi = {10.1145/2934466.2934481},
abstract = {Software product line engineering is a compelling methodology that accomplishes systematic reuse in families of systems by relying on two key principles: (i) the decomposition of complex systems into composable and reusable building blocks (often logical units called features), and (ii) on-demand construction of products and product variants by composing these building blocks.However, unless the stakeholder responsible for product configuration has detailed knowledge of the technical ins and outs of the software product line (e.g., the architectural impact of a specific feature, or potential feature interactions), he is in many cases flying in the dark. Although many initial approaches and techniques have been proposed that take into account quality considerations and involve trade-off decisions during product configuration, no systematic support exists.In this paper, we present a reference architecture for product configuration tooling, providing support for (i) up-front generation of variants, and (ii) quality analysis of these variants. This allows pro-actively assessing and predicting architectural quality properties for each product variant and in turn, product configuration tools can take into account architectural considerations. In addition, we provide an in-depth discussion of techniques and tactics for dealing with the problem of variant explosion, and as such to maintain practical feasibility of such approaches.We validated and implemented our reference architecture in the context of a real-world industrial application, a product-line for the firmware of an automotive sensor. Our prototype, based on FeatureIDE, is open for extension and readily available.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {164–173},
numpages = {10},
location = {Beijing, China},
series = {SPLC '16}
}

@article{10.1016/j.jss.2004.11.007,
author = {Canfora, G. and Garc\'{\i}a, F. and Piattini, M. and Ruiz, F. and Visaggio, C. A.},
title = {A family of experiments to validate metrics for software process models},
year = {2005},
issue_date = {August 2005},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {77},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2004.11.007},
doi = {10.1016/j.jss.2004.11.007},
abstract = {Process modelling is a key activity of software process management and it is the starting point for enacting, evaluating and improving software processes. The current competitive marketplace calls for the continuous improvement of processes and therefore, it is fundamental to have software process models with a high maintainability. In this paper we introduce a set of metrics for software process models and discuss how these can be used as maintainability indicators. In particular, we report the results of a family of experiments that assess relationships between the structural properties, as measured by the defined metrics, of the process models and their maintainability.},
journal = {J. Syst. Softw.},
month = aug,
pages = {113–129},
numpages = {17},
keywords = {Software process models maintainability, Metrics, Experimental software engineering}
}

@book{10.5555/1718010,
author = {Zeller, Andreas},
title = {Why Programs Fail, Second Edition: A Guide to Systematic Debugging},
year = {2009},
isbn = {0123745152},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {2nd},
abstract = {This book is proof that debugging has graduated from a black art to a systematic discipline. It demystifies one of the toughest aspects of software programming, showing clearly how to discover what caused software failures, and fix them with minimal muss and fuss. The fully updated second edition includes 100+ pages of new material, including new chapters on Verifying Code, Predicting Errors, and Preventing Errors. Cutting-edge tools such as FindBUGS and AGITAR are explained, techniques from integrated environments like Jazz.net are highlighted, and all-new demos with ESC/Java and Spec#, Eclipse and Mozilla are included.This complete and pragmatic overview of debugging is authored by Andreas Zeller, the talented researcher who developed the GNU Data Display Debugger(DDD), a tool that over 250,000 professionals use to visualize the data structures of programs while they are running. Unlike other books on debugging, Zeller's text is product agnostic, appropriate for all programming languages and skill levels.The book explains best practices ranging from systematically tracking error reports, to observing symptoms, reproducing errors, and correcting defects. It covers a wide range of tools and techniques from hands-on observation to fully automated diagnoses, and also explores the author's innovative techniques for isolating minimal input to reproduce an error and for tracking cause and effect through a program. It even includes instructions on how to create automated debugging tools. The text includes exercises and extensive references for further study, and a companion website with source code for all examples and additional debugging resources is available. The new edition of this award-winning productivity-booster is for any developer who has ever been frustrated by elusive bugs.Brand new chapters demonstrate cutting-edge debugging techniques and tools, enabling readers to put the latest time-saving developments to work for them.Learn by doing. New exercises and detailed examples focus on emerging tools, languages and environments, including AGITAR, FindBUGS, Python and Eclipse.}
}

@article{10.1007/s10270-012-0312-6,
author = {Bavota, Gabriele and Gravino, Carmine and Oliveto, Rocco and De Lucia, Andrea and Tortora, Genoveffa and Genero, Marcela and Cruz-Lemus, Jos\'{e} A.},
title = {A fine-grained analysis of the support provided by UML class diagrams and ER diagrams during data model maintenance},
year = {2015},
issue_date = {February  2015},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {14},
number = {1},
issn = {1619-1366},
url = {https://doi.org/10.1007/s10270-012-0312-6},
doi = {10.1007/s10270-012-0312-6},
abstract = {This paper presents the results of an empirical study aiming at comparing the support provided by ER and UML class diagrams during maintenance of data models. We performed one controlled experiment and two replications that focused on comprehension activities (the first activity in the maintenance process) and another controlled experiment on modification activities related to the implementation of given change requests. The results achieved were analyzed at a fine-grained level aiming at comparing the support given by each single building block of the two notations. Such an analysis is used to identify weaknesses (i.e., building blocks not easy to comprehend) in a notation and/or can justify the need of preferring ER or UML for data modeling. The analysis revealed that the UML class diagrams generally provided a better support for both comprehension and modification activities performed on data models as compared to ER diagrams. Nevertheless, the former has some weaknesses related to three building blocks, i.e., multi-value attribute, composite attribute, and weak entity. These findings suggest that an extension of UML class diagrams should be considered to overcome these weaknesses and improve the support provided by UML class diagrams during maintenance of data models.},
journal = {Softw. Syst. Model.},
month = feb,
pages = {287–306},
numpages = {20}
}

@article{10.1016/j.cose.2020.102155,
author = {Rosado, David G. and Moreno, Julio and S\'{a}nchez, Luis E. and Santos-Olmo, Antonio and Serrano, Manuel A. and Fern\'{a}ndez-Medina, Eduardo},
title = {MARISMA-BiDa pattern: Integrated risk analysis for big data},
year = {2021},
issue_date = {Mar 2021},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {102},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2020.102155},
doi = {10.1016/j.cose.2020.102155},
journal = {Comput. Secur.},
month = mar,
numpages = {25},
keywords = {Security standards, Information security, Risk analysis, Risk assessment, Big data}
}

@article{10.1145/1656250.1656253,
author = {Xu, Chang and Cheung, S. C. and Chan, W. K. and Ye, Chunyang},
title = {Partial constraint checking for context consistency in pervasive computing},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/1656250.1656253},
doi = {10.1145/1656250.1656253},
abstract = {Pervasive computing environments typically change frequently in terms of available resources and their properties. Applications in pervasive computing use contexts to capture these changes and adapt their behaviors accordingly. However, contexts available to these applications may be abnormal or imprecise due to environmental noises. This may result in context inconsistencies, which imply that contexts conflict with each other. The inconsistencies may set such an application into a wrong state or lead the application to misadjust its behavior. It is thus desirable to detect and resolve the context inconsistencies in a timely way. One popular approach is to detect context inconsistencies when contexts breach certain consistency constraints. Existing constraint checking techniques recheck the entire expression of each affected consistency constraint upon context changes. When a changed context affects only a constraint's subexpression, rechecking the entire expression can adversely delay the detection of other context inconsistencies. This article proposes a rigorous approach to identifying the parts of previous checking results that are reusable without entire rechecking. We evaluated our work on the Cabot middleware through both simulation experiments and a case study. The experimental results reported that our approach achieved over a fifteenfold performance improvement on context inconsistency detection than conventional approaches.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {9},
numpages = {61},
keywords = {validation, pervasive computing, performance, Constraints}
}

@article{10.1016/j.infsof.2015.01.008,
author = {Lopez-Herrejon, Roberto E. and Linsbauer, Lukas and Egyed, Alexander},
title = {A systematic mapping study of search-based software engineering for software product lines},
year = {2015},
issue_date = {May 2015},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {61},
number = {C},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2015.01.008},
doi = {10.1016/j.infsof.2015.01.008},
abstract = {ContextSearch-Based Software Engineering (SBSE) is an emerging discipline that focuses on the application of search-based optimization techniques to software engineering problems. Software Product Lines (SPLs) are families of related software systems whose members are distinguished by the set of features each one provides. SPL development practices have proven benefits such as improved software reuse, better customization, and faster time to market. A typical SPL usually involves a large number of systems and features, a fact that makes them attractive for the application of SBSE techniques which are able to tackle problems that involve large search spaces. ObjectiveThe main objective of our work is to identify the quantity and the type of research on the application of SBSE techniques to SPL problems. More concretely, the SBSE techniques that have been used and at what stage of the SPL life cycle, the type of case studies employed and their empirical analysis, and the fora where the research has been published. MethodA systematic mapping study was conducted with five research questions and assessed 77 publications from 2001, when the term SBSE was coined, until 2014. ResultsThe most common application of SBSE techniques found was testing followed by product configuration, with genetic algorithms and multi-objective evolutionary algorithms being the two most commonly used techniques. Our study identified the need to improve the robustness of the empirical evaluation of existing research, a lack of extensive and robust tool support, and multiple avenues worthy of further investigation. ConclusionsOur study attested the great synergy existing between both fields, corroborated the increasing and ongoing interest in research on the subject, and revealed challenging open research questions.},
journal = {Inf. Softw. Technol.},
month = may,
pages = {33–51},
numpages = {19},
keywords = {Systematic mapping study, Software product line, Search based software engineering, Metaheuristics, Evolutionary algorithm}
}

@inproceedings{10.5555/2814058.3252432,
author = {Siqueira, Sean W. M. and Carvalho, Sergio T.},
title = {Session details: Main Track - Evidence-based Studies},
year = {2015},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
location = {Goiania, Goias, Brazil},
series = {SBSI '15}
}

@article{10.1016/j.jss.2009.03.102,
author = {Kim, Suntae and Kim, Dae-Kyoo and Lu, Lunjin and Park, Sooyong},
title = {Quality-driven architecture development using architectural tactics},
year = {2009},
issue_date = {August, 2009},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {82},
number = {8},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2009.03.102},
doi = {10.1016/j.jss.2009.03.102},
abstract = {This paper presents a quality-driven approach to embodying non-functional requirements (NFRs) into software architecture using architectural tactics. Architectural tactics are reusable architectural building blocks, providing general architectural solutions for common issues pertaining to quality attributes. In this approach, architectural tactics are represented as feature models, and their semantics is defined using the Role-Based Metamodeling Language (RBML) which is a UML-based pattern specification notation. Given a set of NFRs, architectural tactics are selected and composed, and the composed tactic is used to instantiate an initial architecture for the application. The proposed approach addresses both the structural and behavioral aspects of architecture. We describe the approach using tactics for performance, availability and security to develop an architecture for a stock trading system. We demonstrate tool support for instantiating a composed tactic to generate an initial architecture of the stock trading system.},
journal = {J. Syst. Softw.},
month = aug,
pages = {1211–1231},
numpages = {21},
keywords = {UML, Software architecture, Role-based metamodeling language, Quality-driven, Feature modeling, Feature composition, Architectural tactics}
}

@article{10.1504/IJAHUC.2016.075378,
author = {Onderwater, Martijn},
title = {An overview of centralised middleware components for sensor networks},
year = {2016},
issue_date = {March 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {3},
issn = {1743-8225},
url = {https://doi.org/10.1504/IJAHUC.2016.075378},
doi = {10.1504/IJAHUC.2016.075378},
abstract = {Sensors are increasingly becoming part of our daily lives: lighting control, environmental monitoring, and keeping track of energy consumption all rely on sensors. Access to these sensors and their networks is, unfortunately, often provided via proprietary protocols and data formats. To overcome such issues, middleware components have been employed to provide a universal interface to the sensor networks, hiding vendor-specific details. Recently, much attention in literature is aimed at what we define as 'centralised' middleware components, which consider sensor networks that have no capacity to run middleware components on the sensor nodes. In this paper we introduce the term 'centralised' for these middleware components, and illustrate their relevance using a literature review of existing middleware components. After an overview of their general architectural setup, we describe and discuss four important centralised middleware components. Finally, we identify directions of further research that will impact centralised middleware components in the near future.},
journal = {Int. J. Ad Hoc Ubiquitous Comput.},
month = mar,
pages = {180–193},
numpages = {14}
}

@article{10.1016/j.infsof.2008.01.007,
author = {Ricca, Filippo and Torchiano, Marco and Di Penta, Massimiliano and Ceccato, Mariano and Tonella, Paolo},
title = {Using acceptance tests as a support for clarifying requirements: A series of experiments},
year = {2009},
issue_date = {February, 2009},
publisher = {Butterworth-Heinemann},
address = {USA},
volume = {51},
number = {2},
issn = {0950-5849},
url = {https://doi.org/10.1016/j.infsof.2008.01.007},
doi = {10.1016/j.infsof.2008.01.007},
abstract = {One of the main reasons for the failure of many software projects is the late discovery of a mismatch between the customers' expectations and the pieces of functionality implemented in the delivered system. At the root of such a mismatch is often a set of poorly defined, incomplete, under-specified, and inconsistent requirements. Test driven development has recently been proposed as a way to clarify requirements during the initial elicitation phase, by means of acceptance tests that specify the desired behavior of the system. The goal of the work reported in this paper is to empirically characterize the contribution of acceptance tests to the clarification of the requirements coming from the customer. We focused on Fit tables, a way to express acceptance tests, which can be automatically translated into executable test cases. We ran two experiments with students from University of Trento and Politecnico of Torino, to assess the impact of Fit tables on the clarity of requirements. We considered whether Fit tables actually improve requirement understanding and whether this requires any additional comprehension effort. Experimental results show that Fit helps in the understanding of requirements without requiring a significant additional effort.},
journal = {Inf. Softw. Technol.},
month = feb,
pages = {270–283},
numpages = {14},
keywords = {Requirements, Fit tables, Empirical studies, Acceptance testing}
}

@inproceedings{10.1145/588646.588652,
author = {Chou, Elaine},
title = {Redesigning a large and complex website: how to begin, and a method for success},
year = {2002},
isbn = {1581135645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/588646.588652},
doi = {10.1145/588646.588652},
abstract = {In a technology-oriented, information-intense world, one of the largest challenges facing higher education is the organization, communication, and presentation of information in a coherent and usable fashion via the world wide web. The Learning Team, a part of Information Technology at the College of William and Mary, offers vision and a solution for developing a robust web-based information architecture to centrally administer customized learning modules and services as well as promote web-based training initiatives.This paper focuses on the redesign of the large and complex technology-related Learning Team website, contrasting the design and structure of the old 3,000+ page Learning Team web architecture with the newly-developed site-- from conception in November 2001 to completion in June 2002. Directing the web redesign process, we took a team-oriented approach consisting of several staff and student employees. In this session, I will guide attendees through our product workflow processes from website conception, content presentation, storyboarding, navigational and schematic design, as well as delivery of content to well-conceived webpage layout and usability techniques. I will provide hands-on exercises of our content inventory process, task analysis, "card sorting" techniques as well as "paper prototype" testing.Web designers, developers, trainers, and those interested in creating an effective web presence may find this discussion particularly useful. Participants delivering services over the web, evaluating student employment, assessing the changing needs for services, or interested in providing preventive technical solutions and support to faculty and staff may also find this paper helpful.At its completion, readers will gain: (1) practical knowledge of how to redesign a large and complex website, (2) an understanding of usability practices and some core usability techniques, and (3) exposure to user-centered design. I will share tips and lessons learned as well as practical issues and solutions for developing a solid information architecture and for implementing web standards.},
booktitle = {Proceedings of the 30th Annual ACM SIGUCCS Conference on User Services},
pages = {22–28},
numpages = {7},
keywords = {web site, web services, usability, training, task analysis, support, needs assessment, learning, just-in-time, internet, information architecture, efficiency, consistency, communications, automation, accessibility},
location = {Providence, Rhode Island, USA},
series = {SIGUCCS '02}
}

@article{10.5555/3044222.3051232,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Requirement-driven evolution in software product lines},
year = {2016},
issue_date = {December 2016},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0164-1212},
abstract = {We conducted a systematic mapping study on SPL evolution.We identified 107 relevant contributions on the topic up to mid 2015.We elaborated on the traditional change mini-cycle to classify the contributions.We identified well-established topics, trends and open research issues. CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring.OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps.RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products).CONCLUSION. Analyses of the results indicate that "Solution proposals" are the most common type of contribution (31%). Regarding the evolution activity, "Implement change" (43%) and "Analyze and plan change" (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
journal = {J. Syst. Softw.},
month = dec,
pages = {110–143},
numpages = {34},
keywords = {Systematic mapping study, Software product lines, Evolution}
}

@article{10.1007/s10664-009-9118-8,
author = {Benestad, Hans Christian and Anda, Bente and Arisholm, Erik},
title = {Understanding cost drivers of software evolution: a quantitative and qualitative investigation of change effort in two evolving software systems},
year = {2010},
issue_date = {April     2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {2},
issn = {1382-3256},
url = {https://doi.org/10.1007/s10664-009-9118-8},
doi = {10.1007/s10664-009-9118-8},
abstract = {Making changes to software systems can prove costly and it remains a challenge to understand the factors that affect the costs of software evolution. This study sought to identify such factors by investigating the effort expended by developers to perform 336 change tasks in two different software organizations. We quantitatively analyzed data from version control systems and change trackers to identify factors that correlated with change effort. In-depth interviews with the developers about a subset of the change tasks further refined the analysis. Two central quantitative results found that dispersion of changed code and volatility of the requirements for the change task correlated with change effort. The analysis of the qualitative interviews pointed to two important, underlying cost drivers: Difficulties in comprehending dispersed code and difficulties in anticipating side effects of changes. This study demonstrates a novel method for combining qualitative and quantitative analysis to assess cost drivers of software evolution. Given our findings, we propose improvements to practices and development tools to manage and reduce the costs.},
journal = {Empirical Softw. Engg.},
month = apr,
pages = {166–203},
numpages = {38},
keywords = {Software evolution cost, Quantitative and qualitative methods, Change effort}
}

@inproceedings{10.1007/11752707_18,
author = {Lee, Kwang Chun and Choi, Ho-Jin and Lee, Dan Hyung and Kang, Sungwon},
title = {Quantitative measurement of quality attribute preferences using conjoint analysis},
year = {2005},
isbn = {3540341455},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11752707_18},
doi = {10.1007/11752707_18},
abstract = {Conjoint analysis has received considerable attention as a technique for measuring customer preferences through utility tradeoffs among products and services. This paper shows how the method can be applied to the area of software architecture to analyze architectural tradeoffs among quality attributes. By eliciting customer utilities through conjoint analysis, software engineers can identify and focus on the useful quality attributes, which will increase the chance of delivering satisfactory software products to the customers. This paper proposes a quantitative method of measuring quality attribute preferences using conjoint analysis and demonstrates its efficacy by applying it to the Project Management Center (PMCenter) project. The proposed method is complementary to the Architecture Trade-off Analysis Method (ATAM) in that ATAM relies on customer's feedback to elicit important quality attributes, whereas this method can be used to actually measure the utilities of quality attributes in a quantitative manner. Furthermore, our method provides a new framework for choosing architecture styles and design patterns based on customer's preferences of quality attributes.},
booktitle = {Proceedings of the 12th International Conference on Interactive Systems: Design, Specification, and Verification},
pages = {213–224},
numpages = {12},
location = {Newcastle upon Tyne, UK},
series = {DSVIS'05}
}

@article{10.1016/j.comnet.2019.106984,
author = {Barakabitze, Alcardo Alex and Ahmad, Arslan and Mijumbi, Rashid and Hines, Andrew},
title = {5G network slicing using SDN and NFV: A survey of taxonomy, architectures and future challenges},
year = {2020},
issue_date = {Feb 2020},
publisher = {Elsevier North-Holland, Inc.},
address = {USA},
volume = {167},
number = {C},
issn = {1389-1286},
url = {https://doi.org/10.1016/j.comnet.2019.106984},
doi = {10.1016/j.comnet.2019.106984},
journal = {Comput. Netw.},
month = feb,
numpages = {40},
keywords = {Network softwarization, Cloud/edge computing, Network slicing, NFV, SDN, 5G}
}

@article{10.1016/j.jss.2011.04.066,
author = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir\'{a} and Lucena, Carlos},
title = {Automating the product derivation process of multi-agent systems product lines},
year = {2012},
issue_date = {February, 2012},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {85},
number = {2},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2011.04.066},
doi = {10.1016/j.jss.2011.04.066},
abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification.},
journal = {J. Syst. Softw.},
month = feb,
pages = {258–276},
numpages = {19},
keywords = {Software product lines, Product derivation tool, Multi-agent systems, Model-driven development, Application engineering}
}

@article{10.1504/IJBIS.2016.074262,
author = {Thakurta, Rahul},
title = {A specification of principles governing the design of requirement prioritisation approaches},
year = {2016},
issue_date = {January 2016},
publisher = {Inderscience Publishers},
address = {Geneva 15, CHE},
volume = {21},
number = {2},
issn = {1746-0972},
url = {https://doi.org/10.1504/IJBIS.2016.074262},
doi = {10.1504/IJBIS.2016.074262},
abstract = {The importance of prioritising requirements stems from the fact that not all requirements can usually be met with available time and resource constraints. Although several papers have been published in this domain, they mainly focus on descriptive research endeavours to suggest different requirement prioritisation approaches. Prescriptive research dealing with design science for a systematic and holistic understanding of the prioritisation process is still scarce. The gap motivates our research, which aims at arriving at a set of design principles that explains the form and function of a requirement prioritisation approach. We resort to a non-experimental approach using content analysis to identify and analyse articles on requirement prioritisation in order to arrive at the set of six design principles addressed in the paper. This subsequently is evaluated based on expert feedbacks to validate our design principles. We finally give a brief outlook on implications and issues for further research.},
journal = {Int. J. Bus. Inf. Syst.},
month = jan,
pages = {206–220},
numpages = {15}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@article{10.1023/A:1008350923935,
author = {Simazu, Hideo and Shibata, Akihiro and Nihei, Katsumi},
title = {ExpertGuide: A Conversational Case-Based Reasoning Tool for Developing Mentors in Knowledge Spaces},
year = {2001},
issue_date = {January-February 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {14},
number = {1},
issn = {0924-669X},
url = {https://doi.org/10.1023/A:1008350923935},
doi = {10.1023/A:1008350923935},
abstract = {Case-based reasoning (CBR) has been used to improve knowledge management in corporate activities. It was initially used for problem solving and then for facilitating the distribution of knowledge and experiences. Since conversational CBR appeared, it has also been used to develop mentor systems in complex knowledge spaces. ExpertGuide was designed as a tool for developing WWW-based mentor systems. It was designed during the development of CBR systems used at NEC. Multilink retrieval provides a method for searching a case library from several viewpoints. Question selection by entropy finds most effective questions in discriminating cases. It does this by calculating the information gain of candidate questions. Indexing with scripts is a case-indexing method using Schank's scripts and is effective when situations and problems are hard to express and assess. ExpertGuide tightly integrates these techniques and provides a user interface for users who want advice from mentors. It is currently being used when building WWW-based mentor systems with scales ranging from division-wide to corporate-wide and nationwide.},
journal = {Applied Intelligence},
month = feb,
pages = {33–48},
numpages = {16},
keywords = {knowledge management, information retrieval, case-based reasoning}
}

@proceedings{10.1145/2983990,
title = {OOPSLA 2016: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Amsterdam, Netherlands}
}

@book{10.5555/1523319,
author = {Buford, John and Yu, Heather and Lua, Eng Keong},
title = {P2P Networking and Applications},
year = {2008},
isbn = {9780080921198},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Peer-to-Peer (P2P) networks enable users to directly share digital content (such as audio, video, and text files) as well as real-time data (such as telephony traffic) with other users without depending on a central server. Although originally popularized by unlicensed online music services such as Napster, P2P networking has recently emerged as a viable multimillion dollar business model for the distribution of academic and clinical information, telecommunications, and social networking. Written at an accessible level for any reader familiar with fundamental Internet protocols, Peer-to-Peer Networking and Applications explains the conceptual operations and architecture underlying basic P2P systems using well-known commercial systems as models. The book also delineates the latest research directions, thereby providing not only a sophisticated understanding of current systems, but also the means to improve upon these systems with innovations that will better performance, security, and flexibility. Peer-to-Peer Networking and Applications is thus both a valuable starting point and an important reference to those practioners employed by any of the 200 companies with approximately $400 million invested in this new and lucrative technology. Uses well-known commercial P2P systems as models, thus demonstrating real-world applicability. Discusses how current research trends in wireless networking, high-def content, DRM, etc. will intersect with P2P, allowing readers to account for future developments in their designs. Provides online access to the Overlay Weaver P2P emulator, an open-source tool that supports a number of peer-to-peer applications with which readers can practice.}
}

@book{10.5555/1208706,
author = {Reinhard, Erik and Ward, Greg and Pattanaik, Sumanta and Debevec, Paul},
title = {High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics)},
year = {2005},
isbn = {0125852630},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {This landmark book is the first to describe HDRI technology in its entirety and covers a wide-range of topics, from capture devices to tone reproduction and image-based lighting. The techniques described enable you to produce images that have a dynamic range much closer to that found in the real world, leading to an unparalleled visual experience. As both an introduction to the field and an authoritative technical reference, it is essential to anyone working with images, whether in computer graphics, film, video, photography, or lighting design. New material includes chapters on High Dynamic Range Video Encoding, High Dynamic Range Image Encoding, and High Dynammic Range Display Devices Written by the inventors and initial implementors of High Dynamic Range Imaging Covers the basic concepts (including just enough about human vision to explain why HDR images are necessary), image capture, image encoding, file formats, display techniques, tone mapping for lower dynamic range display, and the use of HDR images and calculations in 3D rendering Range and depth of coverage is good for the knowledgeable researcher as well as those who are just starting to learn about High Dynamic Range imaging Table of Contents Introduction; Light and Color; HDR Image Encodings; HDR Video Encodings; HDR Image and Video Capture; Display Devices; The Human Visual System and HDR Tone Mapping; Spatial Tone Reproduction; Frequency Domain and Gradient Domain Tone Reproduction; Inverse Tone Reproduction; Visible Difference Predictors; Image-Based Lighting.}
}

